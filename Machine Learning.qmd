---
webr: 
  show-startup-message: true
  packages: [
            'dplyr',
            'ggplot2',
            'tidyr',
            ]
---

# Machine Learning

Das Gradientenabstiegsverfahren (Gradient Descent) ist ein iteratives Optimierungsverfahren zur Minimierung einer differenzierbaren Zielfunktion $f(x)$. Es wird häufig eingesetzt, um die Verlustfunktionen in maschinellen Lernmodellen zu minimieren. Der Algorithmus aktualisiert die Variablen schrittweise in die entgegengesetzte Richtung des Gradienten der Funktion an der aktuellen Position. Der Gradient gibt dabei die Richtung des *steilsten Anstiegs* an, wodurch die entgegengesetzte Richtung zum schnellsten *Abstieg* (Descent) führt.

Der folgende Pseudocode zeigt die grundlegende Vorgehensweise des Gradientenabstiegsverfahrens unter Einbeziehung eines Momentum-Terms, der dazu dient, das Konvergenzverhalten zu verbessern und lokale Minima effektiver zu überwinden.

\begin{align*}
& \textbf{Algorithmus: Gradientenabstiegsverfahren mit Momentum} \\
& \textup{Initialisiere: }\\ 
& \quad x_0 \text{ (Startpunkt) }\\
& \quad \eta \text{ (Lernrate) }\\
& \quad \alpha \text{ (Momentum-Faktor) }\\ 
& \quad v_0 = 0 \text{ (Anfangsmomentum) } \\[1em]
& \text{Für } t = 0, 1, 2, \dots \text{ bis Konvergenz} \\
& \quad \text{1. Berechne den Gradienten: } \nabla f(x_t) \\
& \quad \text{2. Aktualisiere den Momentum-Term: } v_{t+1} = \alpha v_t - \eta \nabla f(x_t) \\
& \quad \text{3. Aktualisiere die Position: } x_{t+1} = x_t + v_{t+1} \\
& \quad \text{4. Überprüfe das Abbruchkriterium (z.B. } \| \nabla f(x_t) \| < \epsilon\text{)} \\
\end{align*}

<iframe class="obs-soft-box-shadow" width="100%" height="1172" frameborder="0"
  src="https://observablehq.com/embed/@mca91/gradient-descent-in-3d-three-js?cells=renderer%2Cviewof+restart%2Cviewof+gridinit%2Cviewof+themin%2Cviewof+themin2%2Cviewof+themin3%2Cviewof+themin4%2Cviewof+themin5%2Cviewof+themin6%2Cscene%2Ccamera"></iframe>
