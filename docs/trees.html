<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="de" xml:lang="de"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>15&nbsp; Baum-basierte Methoden – Kausalanalyse mit R</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Machine Learning.html" rel="next">
<link href="./svm.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/live-runtime/live-runtime.js" type="module"></script>
<link href="site_libs/quarto-contrib/live-runtime/live-runtime.css" rel="stylesheet"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Keine Treffer",
    "search-matching-documents-text": "Treffer",
    "search-copy-link-title": "Link in die Suche kopieren",
    "search-hide-matches-text": "Zusätzliche Treffer verbergen",
    "search-more-match-text": "weitere Treffer in diesem Dokument",
    "search-more-matches-text": "weitere Treffer in diesem Dokument",
    "search-clear-button-title": "Zurücksetzen",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Abbrechen",
    "search-submit-button-title": "Abschicken",
    "search-label": "Suchen"
  }
}</script><script type="module" src="site_libs/quarto-ojs/quarto-ojs-runtime.js"></script><link href="site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">
<meta name="robots" content="noindex">
<script>
  MathJax = {
    tex: {
      tags: 'ams'  // should be 'ams', 'none', or 'all'
    }
  };
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.js" integrity="sha512-aoZChv+8imY/U1O7KIHXvO87EOzCuKO0GhFtpD6G2Cyjo/xPeTgdf3/bchB10iB+AojMTDkMHDPLKNxPJVqDcw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css">
<style>
  .panel-tabset .tab-content {
    border: 0;
    padding: 1em 0 0 0;
  }
  
  .panel-tabset .nav-item a {
    border-radius: 5px 5px 0 0;
  }
  
  .scientific_borders {
    border: 0;
    border-top: 2px solid black !important; 
    border-bottom: 2px solid black !important;
  }
  .table:not(.gt_table) > :not(caption)>*>* {
    border-bottom-width: 0;
  }
  .table:not(.gt_table) > thead {
    border-bottom: 1px solid black;
  }
  .soft-box-shadow {
    border: 1px solid rgba(233,236,239,.9) !important;
    border-radius: .5rem !important;
    background-color: rgba(250,250,250,.9) !important;
    box-shadow: 0px 1px 2px rgba(0,0,0,.1),
                0px 3px 7px rgba(0,0,0,.1),
                0px 12px 30px rgba(0,0,0,.08);
    margin-top: 2rem !important;
    margin-bottom: 2.5rem !important;
    padding: .25rem !important;
  }
  .obs-soft-box-shadow {
    border: 1px solid rgba(233,236,239,.9) !important;
    border-radius: .5rem !important;
    background-color: white !important;
    box-shadow: 0px 1px 2px rgba(0,0,0,.1),
                0px 3px 7px rgba(0,0,0,.1),
                0px 12px 30px rgba(0,0,0,.08);
    margin-top: 2rem !important;
    margin-bottom: 2.5rem !important;
    padding: .25rem !important;
  }
  
</style>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    
    var gt_tables = document.querySelectorAll(".gt_table");
    gt_tables.forEach(function(table) {
      table.classList.remove("table-striped");
    });
    
    var tables = document.querySelectorAll("table.table:not(.gt_table)");
    tables.forEach(function(table) {
      table.classList.remove("table-striped");
      table.classList.add("scientific_borders");
    });
    
    document.querySelectorAll("div.sourceCode").forEach(function(block) {
      block.classList.add("soft-box-shadow");
    });
    
    document.querySelectorAll("div.bg-white").forEach(function(block) {
      block.classList.remove("bg-white");
    });
    
const elements = document.querySelectorAll('[id^="qwebr-interactive-area"]');

    elements.forEach(element => {
        element.classList.add('box-shadow');
    });
    
    document.querySelectorAll('[id^="webr"]').forEach(function(block) {
      block.classList.add("box-shadow");
    });
    
        document.querySelectorAll('.card-header').forEach(function(block) {
      block.classList.add("box-shadow");
    });
    
  });
</script><style>
.qwebr-code-output-stdout {background-color: powderblue;}

.qwebr-button-run {
 width = 100%; 
}

.centered-caption {
   text-align: center;
}
</style>
<script src="https://cdn.jsdelivr.net/npm/quizdown@latest/public/build/quizdown.js"></script><script>quizdown.init();</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script><link rel="stylesheet" href="custom_styles.css">
</head>
<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Seitenleiste umschalten" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./RegReg.html">Machine Learning</a></li><li class="breadcrumb-item"><a href="./trees.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Baum-basierte Methoden</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Seitenleiste umschalten" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Suchen" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./"></a><a href="./index.html">Kausalanalyse mit R</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Lesemodus umschalten">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Suchen"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Einführung</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Grundlagen</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Abschnitt umschalten">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R_Einfuehrung.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistische Programmierung mit R</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./RR.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Reproduzierbarkeit</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Reg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Simulation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Simulation</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Kausale Inferenz</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Abschnitt umschalten">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Matching.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./FixedEffects.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Panel-Daten</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./IV.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">IV-Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./DiD.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Difference-in-Differences</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./EventStudies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Event Studies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./RDD.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Regression Discontiniuty Designs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./SyntheticControl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Synthetic Control</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Abschnitt umschalten">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./RegReg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Regularisierte Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./svm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./trees.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Baum-basierte Methoden</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Machine Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Neuronale Netzwerke</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Übungsaufgaben</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Abschnitt umschalten">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lineare Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ex_simulation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Simulation mit R</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Literatur.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Literatur</span></a>
  </div>
</li>
    </ul>
</div>
    <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title"><a href="./index.html">Übersicht</a></h2>
   
  <ul>
<li><a href="#sec-simpletrees" id="toc-sec-simpletrees" class="nav-link active" data-scroll-target="#sec-simpletrees"><span class="header-section-number">15.1</span> Entscheidungsbäume</a></li>
  <li><a href="#training-von-b%C3%A4umen" id="toc-training-von-bäumen" class="nav-link" data-scroll-target="#training-von-b%C3%A4umen"><span class="header-section-number">15.2</span> Training von Bäumen</a></li>
  <li><a href="#bagging" id="toc-bagging" class="nav-link" data-scroll-target="#bagging"><span class="header-section-number">15.3</span> Bagging</a></li>
  <li><a href="#sec-brf" id="toc-sec-brf" class="nav-link" data-scroll-target="#sec-brf"><span class="header-section-number">15.4</span> Random Forests</a></li>
  <li><a href="#sec-boosting" id="toc-sec-boosting" class="nav-link" data-scroll-target="#sec-boosting"><span class="header-section-number">15.5</span> Boosting</a></li>
  <li><a href="#zusammenfassung" id="toc-zusammenfassung" class="nav-link" data-scroll-target="#zusammenfassung"><span class="header-section-number">15.6</span> Zusammenfassung</a></li>
  </ul></nav>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./RegReg.html">Machine Learning</a></li><li class="breadcrumb-item"><a href="./trees.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Baum-basierte Methoden</span></a></li></ol></nav><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-trees" class="quarto-section-identifier"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Baum-basierte Methoden</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><p>Baum-basierte Methoden bieten eine vielseitige und leistungsstarke Herangehensweise für Vorhersage und Klassifikation in komplexen Datensätzen mit nicht-linearen Zusammenhängen. Ein Vorteil baum-basierter Methoden ist ihre inhärente Fähigkeit, die Bedeutung einzelner Variablen für die Vorhersage zu quantifizieren – eine Eigenschaft, die viele Machine-Learning-Modelle nicht ohne weiteres bieten und insbesondere in hoch-dimensionalen Anwendungen (mit vielen potentiellen Regressoren) nicht trivial ist. Dies ermöglicht es, tiefere Einblicke in den Einfluss einzelner Merkmale auf die Vorhersagen des Modells zu erhalten, was besonders in empirischen Anwendungen für die Entscheidungsstützung mit Machine Learning hilfreich sein kann.</p>
<p><em>Entscheidungsbäume</em> stellen die Grundlage dieser Methoden dar. Sie ermöglichen die Aufteilung der Daten in immer kleinere, homogenere Gruppen, basierend auf <em>binären</em> Entscheidungsregeln, die aus den Prädiktoren abgleitet werden. Die trainierten Regeln eines solchen Modells lassen sich anhand eines Binärbaums visualisieren, was eine intuitive Interpretierbarkeit der Ergebnisse erlaubt.</p>
<p><em>Random Forests</em> ist ein Ensemble-Ansatz, bei dem viele Entscheidungsbäume kombiniert werden. Jeder Baum wird auf einer zufälligen Teilmenge der Daten trainiert (<em>Bagging</em>), und bei jedem Knoten wird zusätzlich eine zufällige Teilmenge der Merkmale berücksichtigt. Die finale Vorhersage des Random Forests basiert auf der Aggregation der Vorhersagen aller Bäume (Mehrheitsvotum für Klassifikation, Durchschnitt für Regression). Dieses Verfahren reduziert das Risiko einer Überanpassung und erhöht oft die Vorhersagegenauigkeit im Vergleich zu einzelnen Entscheidungsbäumen.</p>
<p><em>Boosting</em> ist eine weitere Ensemble-Methode zur Anpassung von Modellen mit hoher Vorhersagegüte durch Kombination einfacher Modelle (<em>Base learner</em>), wobei Regressions- oder Klassifikationsbäume eingesetzt werden können. Alternativ zu Random Forests trainieren Boosting-Algorithmen sukzessiv einfache (Klassifikations- oder Regressions-)Bäume, wobei jeder nachfolgende Baum das Ziel hat, die Vorhersagefehler der vorherigen Bäume zu korrigieren.</p>
<p>In diesem Kapitel erläutern wir die Anwendung baum-basierter Methoden in R anhand von Beispieldatensätzen. Wir zeigen, wie Regressionsbäume, Random Forests und Boosting-Modelle im <code>parsnip</code>-Framework trainiert werden und wie die Vorhersageleistung durch die Wahl geeigneter Hyperparameter mit Cross-Validation und Out-of-Sample-Evaluierungsmethoden optimiert werden kann.</p>
<section id="sec-simpletrees" class="level2 page-columns page-full" data-number="15.1"><h2 data-number="15.1" class="anchored" data-anchor-id="sec-simpletrees">
<span class="header-section-number">15.1</span> Entscheidungsbäume</h2>
<p>Ein Entscheidungsbaum ist ein Modell, das auf der Basis von hierarchischen Bedingungen bzgl. der Regressoren Vorhersagen für die Outcome-Variable trifft. Jeder Baum beginnt mit einem Wurzelknoten (<em>root node</em>) und verzweigt sich binär. Jede Verzweigung (<em>split</em>) stellt eine Bedingung dar, die auf einem bestimmten Regressor basiert. Der Baum trifft Entscheidungen, indem er diese Bedingungen sukzessive überprüft, bis er zu einem Blattknoten (<em>leaf node</em> / <em>terminal node</em>) gelangt, der die finale Vorhersage liefert. Hierbei handelt es sich eine Mehrheitsentscheidung für Klassifikation und einen Mittelwert, jeweils gebildet anhand Beobachten des Trainingsdatensatzes im leaf node.</p>
<p><a href="#fig-exdectree" class="quarto-xref">Abbildung&nbsp;<span>15.1</span></a> zeigt ein einfaches Beispiel eines Entscheidungsbaums zur Klassifikation der Kreditwürdigkeit einer Person. Die Klassfikation erfolgt, in dem die Beobachtung basierend auf den Merkmalen Alter, Einkommen und Eigentum durch den Baum geleitet wird. Zunächst wird geprüft, die Person 30 Jahre oder jünger ist. Fall ja, entscheidet der Baum anhand des Einkommens: Bei einem Jahreseinkommen von 40.000 oder weniger wird die Person als wenig kreditwürdig klassifiziert, bei höherem Einkommen als mäßig kreditwürdig. Für Personen älter als 30 Jahre überprüft das Modell lediglich, ob die Person eine Immobilie besitzt, um zwischen mäßiger Kreditwürdigkeit und guter Bonität zu unterscheiden.</p>
<div class="cell page-columns page-full" data-fig-width="6" data-fig-height="5" data-layout-align="default">
<div class="cell-output-display page-columns page-full">
<div id="fig-exdectree" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-exdectree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<svg width="576" height="480" viewbox="10.80 10.80 571.96 304.40" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none"><g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(14.8 300.4)"><title>exdectree</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-289.6 557.16,-289.6 557.16,4 -4,4"></polygon><!-- 1 --><g id="node1" class="node"><title>1</title>
<polygon fill="none" stroke="black" points="314.96,-285.6 227.42,-285.6 227.42,-249.6 314.96,-249.6 314.96,-285.6"></polygon><text text-anchor="middle" x="271.19" y="-263.4" font-family="Times,serif" font-size="14.00">Alter &lt;= 30?</text></g><!-- 2 --><g id="node2" class="node"><title>2</title>
<polygon fill="none" stroke="black" points="229.13,-160.8 73.25,-160.8 73.25,-124.8 229.13,-124.8 229.13,-160.8"></polygon><text text-anchor="middle" x="151.19" y="-138.6" font-family="Times,serif" font-size="14.00">Einkommen &lt;= 40 Tsd.?</text></g><!-- 1&#45;&gt;2 --><g id="edge1" class="edge"><title>1-&gt;2</title>
<path fill="none" stroke="black" d="M254.51,-249.53C233.93,-228.47 198.82,-192.54 175.31,-168.48"></path><polygon fill="black" stroke="black" points="177.61,-165.83 168.12,-161.13 172.61,-170.72 177.61,-165.83"></polygon><text text-anchor="middle" x="206.02" y="-201" font-family="Times,serif" font-size="14.00">Ja</text></g><!-- 3 --><g id="node3" class="node"><title>3</title>
<polygon fill="none" stroke="black" points="431.34,-160.8 355.04,-160.8 355.04,-124.8 431.34,-124.8 431.34,-160.8"></polygon><text text-anchor="middle" x="393.19" y="-138.6" font-family="Times,serif" font-size="14.00">Eigentum?</text></g><!-- 1&#45;&gt;3 --><g id="edge2" class="edge"><title>1-&gt;3</title>
<path fill="none" stroke="black" d="M288.15,-249.53C309.07,-228.47 344.77,-192.54 368.67,-168.48"></path><polygon fill="black" stroke="black" points="371.41,-170.69 375.98,-161.13 366.45,-165.75 371.41,-170.69"></polygon><text text-anchor="middle" x="353.8" y="-201" font-family="Times,serif" font-size="14.00">Nein</text></g><!-- 4 --><g id="node4" class="node"><title>4</title>
<polygon fill="none" stroke="black" points="100.57,-36 -0.19,-36 -0.19,0 100.57,0 100.57,-36"></polygon><text text-anchor="middle" x="50.19" y="-13.8" font-family="Times,serif" font-size="14.00">Status: Niedrig</text></g><!-- 2&#45;&gt;4 --><g id="edge3" class="edge"><title>2-&gt;4</title>
<path fill="none" stroke="black" d="M137.15,-124.73C119.98,-103.86 90.79,-68.36 71.01,-44.31"></path><polygon fill="black" stroke="black" points="73.5,-41.83 64.44,-36.33 68.09,-46.27 73.5,-41.83"></polygon><text text-anchor="middle" x="55.02" y="-76.2" font-family="Times,serif" font-size="14.00">Ja</text></g><!-- 5 --><g id="node5" class="node"><title>5</title>
<polygon fill="none" stroke="black" points="329.03,-36 237.35,-36 237.35,0 329.03,0 329.03,-36"></polygon><text text-anchor="middle" x="283.19" y="-13.8" font-family="Times,serif" font-size="14.00">Status: Mittel</text></g><!-- 2&#45;&gt;5 --><g id="edge4" class="edge"><title>2-&gt;5</title>
<path fill="none" stroke="black" d="M169.55,-124.73C192.27,-103.58 231.14,-67.43 256.99,-43.37"></path><polygon fill="black" stroke="black" points="259.63,-45.7 264.57,-36.33 254.86,-40.57 259.63,-45.7"></polygon><text text-anchor="middle" x="238.8" y="-76.2" font-family="Times,serif" font-size="14.00">Nein</text></g><!-- 3&#45;&gt;5 --><g id="edge6" class="edge"><title>3-&gt;5</title>
<path fill="none" stroke="black" d="M377.9,-124.73C359.12,-103.77 327.13,-68.05 305.58,-44"></path><polygon fill="black" stroke="black" points="307.99,-41.44 298.71,-36.33 302.78,-46.11 307.99,-41.44"></polygon><text text-anchor="middle" x="347.8" y="-76.2" font-family="Times,serif" font-size="14.00">Nein</text></g><!-- 6 --><g id="node6" class="node"><title>6</title>
<polygon fill="none" stroke="black" points="553.13,-36 465.25,-36 465.25,0 553.13,0 553.13,-36"></polygon><text text-anchor="middle" x="509.19" y="-13.8" font-family="Times,serif" font-size="14.00">Status: Hoch</text></g><!-- 3&#45;&gt;6 --><g id="edge5" class="edge"><title>3-&gt;6</title>
<path fill="none" stroke="black" d="M409.32,-124.73C429.21,-103.67 463.15,-67.74 485.87,-43.68"></path><polygon fill="black" stroke="black" points="488.5,-46 492.83,-36.33 483.41,-41.19 488.5,-46"></polygon><text text-anchor="middle" x="464.02" y="-76.2" font-family="Times,serif" font-size="14.00">Ja</text></g></g></svg>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-exdectree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Abbildung&nbsp;15.1: Entscheidungsbaum: Klassifikation von Kreditwürdigkeit
</figcaption></figure>
</div>
</div>
</div>
</section><section id="training-von-bäumen" class="level2 page-columns page-full" data-number="15.2"><h2 data-number="15.2" class="anchored" data-anchor-id="training-von-bäumen">
<span class="header-section-number">15.2</span> Training von Bäumen</h2>
<p>Zur Konstruktion von Binär-Bäumen werden etablierte Algorithmen wie <em>Classification and Regression Trees</em> (<a href="https://de.wikipedia.org/wiki/CART_(Algorithmus)">CART</a> von <span class="citation" data-cites="Breimanetal1984">Breiman u.&nbsp;a. (<a href="Literatur.html#ref-Breimanetal1984" role="doc-biblioref">1984</a>)</span> verwendet. Die wesentliche Vorgehensweise für das Training eines Baums <span class="math inline">\(T\)</span> ist wie folgt:</p>
<ol type="1">
<li>
<p><strong>Splitting</strong>: Beginnend am root node sucht der Algorithmus nach der “besten” Regel, die Daten anhand eines Merkmals in zwei Gruppen zu teilen. Die Qualität des Splits wird in Abhängigkeit der Definition der Outcome-Variable beurteilt:</p>
<ul>
<li><p><strong>Bei Klassifikation</strong>: Die Reinheit (<em>purtity</em>) der Klassen in den unmittelbar nachfolgen nodes wird maximiert. Ein gängiges Kriterium hierfür ist der <a href="https://de.wikipedia.org/wiki/Gini-Koeffizient"><em>Gini-Koeffizient</em></a>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p></li>
<li><p><strong>Bei Regression</strong>: Die Fehlerquadratsumme bei Vorhersage des Outcomes durch Mittelwertbildung für Beobachtungen in den unmittelbar nachfolgenden nodes wird minimiert.</p></li>
</ul>
</li>
<li>
<p><strong>Rekursion</strong>: Der Prozess wird rekursiv fortgesetzt, bis Abbruchkriterien greifen eine weitere Verzewigung verhindern:</p>
<ul>
<li>Die maximale Baumtiefe (<em>tree depth</em>) ist erreicht</li>
<li>Die leaf nodes sind hinreichend “rein”: Alle Beobachtungen in einem leaf node gehören zur gleichen Klasse oder die Verbesserung des Loss durch weitere Splits fällt unter einen festgelegten Schwellenwert</li>
<li>Weitere Splits führen zu leaf nodes, die eine Mindestanzahl an Beobachtungen (<em>minimum split</em>) unterschreiten würden</li>
</ul>
</li>
<li>
<p><strong>Pruning</strong>: Um Überanpassung an die Trainingsdaten zu vermeiden, kann der Baum beschnitten werden (<em>pruning</em>). Der Grundgedanke ist, dass tief verzweigte Bäume die Trainingsdaten zwar gut modellieren können, aber schlecht auf neue, unbekannte Daten generalisieren.</p>
<p>Bei <em>cost complexity (CP) pruning</em> werden, beginnend auf Ebene der leaf nodes sukuzessive Äste entfernt, und eine Balance zwischen Komplexität des Baums und dem Anpassungsfehler zu finden. Ähnlich wie bei regularisierter KQ-Schätzung (<a href="RegReg.html" class="quarto-xref"><span>Kapitel 13</span></a>), wird die Verlustfunktion <span class="math inline">\(L\)</span> um einen Strafterm für die Komplexität erweitert. Der Effekt der Strafe wird durch den CP-Parameter <span class="math inline">\(\alpha\in[0,1]\)</span> geregelt,</p>
<p><span class="math display">\[\begin{align*}
   L_{\alpha}(T) = L(T) + \alpha \lvert T\rvert,
\end{align*}\]</span></p>
</li>
</ol>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Der Gini-Koeffizient <span class="math inline">\(0\leq G\leq1\)</span> misst die Homogenität der Outcome-Variable für die Beobachtungen eines Knotens. <span class="math inline">\(G=0\)</span> ergibt sich bei vollständiger “Reinheit” (alle Beobachtungen im Knoten gehören zur gleichen Klasse). <span class="math inline">\(G &gt; 0\)</span> zeigt Heterogenität der Klassen an, die mit <span class="math inline">\(G\)</span> zunimmt</p></div></div><p>für einen Baum <span class="math inline">\(T\)</span> mit Komplexitätsmaß <span class="math inline">\(\lvert T\rvert\)</span> (Anzahl der leaf nodes) <span class="citation" data-cites="Hastieetal2013">(<a href="Literatur.html#ref-Hastieetal2013" role="doc-biblioref">Hastie, Tibshirani, und Friedman 2013</a>)</span>.</p>
<p>Zur Demonstation der Schätzung von Regressionsbäumen mit R betrachten wir nachfolgend den Datensatz <code>MASS::Bosten</code>. Ziel hierbei ist es, mittlere Hauswerte <code>medv</code> in Stadteilen von Boston, MA vorherzusagen. Wir verwenden hierzu Funktionen aus dem Paket <code>parsnip</code>.</p>
<p>Zunächst transformieren wir den Datensatz in ein <code>tibble</code>-Objekt und definieren Trainings- und Test-Daten.</p>
<div class="cell">
<div>
<div id="webr-1">

</div>
<script type="webr-1-contents">
eyJjb2RlIjoibGlicmFyeShwYXJzbmlwKVxubGlicmFyeShjb3dwbG90KVxuXG4jIFNlZWQgc2V0emVuXG5zZXQuc2VlZCgxMjM0KVxuXG4jIERhdGVuc2F0eiBhbHMgdGliYmxlXG5Cb3N0b24gPC0gYXNfdGliYmxlKE1BU1M6OkJvc3RvbilcblxuIyBTcGxpdHRpbmcgaW4gVHJhaW5pbmctIHVuZCBUZXN0LURhdGVuXG5Cb3N0b25fc3BsaXQgPC0gaW5pdGlhbF9zcGxpdChcbiAgZGF0YSA9IEJvc3RvbiwgXG4gIHByb3AgPSAwLjgsIFxuICBzdHJhdGEgPSBtZWR2XG4gIClcblxuQm9zdG9uX3RyYWluIDwtIHRyYWluaW5nKEJvc3Rvbl9zcGxpdClcbkJvc3Rvbl90ZXN0IDwtIHRlc3RpbmcoQm9zdG9uX3NwbGl0KVxuXG5zbGljZV9oZWFkKEJvc3Rvbl90cmFpbiwgbiA9IDEwKSIsImF0dHIiOnsiZXZhbCI6dHJ1ZSwiZWRpdCI6dHJ1ZSwiZmlnLXdpZHRoIjoiOCJ9fQ==
</script>
</div>
</div>
<p><code>parsnip</code> bietet eine vereinheitlichetes Framework für das Training von Modellen mit R und eine flexible API für Machine Learning. Wir definieren zunächst mit <code><a href="https://parsnip.tidymodels.org/reference/decision_tree.html">parsnip::decision_tree()</a></code> eine Spezifikation zum Training von Entschieundgsmodellen und übergeben beispielhaft einen CP-Parameter <span class="math inline">\(\alpha=.1\)</span>. Mit <code><a href="https://parsnip.tidymodels.org/reference/set_engine.html">parsnip::set_engine</a></code> wählen wir das Paket <code>raprt</code>. Der hier implementierte Agorithmus ist CART. Zuletzt legen wir mit <code><a href="https://parsnip.tidymodels.org/reference/set_args.html">parsnip::set_mode()</a></code> fest, dass der Algorithmus für Regression durchgeführt werden soll.</p>
<div class="cell">
<div>
<div id="webr-2">

</div>
<script type="webr-2-contents">
eyJjb2RlIjoiIyBTcGV6aWZpa2F0aW9uIGZlc3RsZWdlblxudHJlZV9zcGVjIDwtIGRlY2lzaW9uX3RyZWUoXG4gIGNvc3RfY29tcGxleGl0eSA9IDAuMVxuICApICU+JVxuICBzZXRfZW5naW5lKFwicnBhcnRcIikgJT4lXG4gIHNldF9tb2RlKFwicmVncmVzc2lvblwiKVxuXG4jIE1vZGVsbCB0cmFpbmllcmVuXG50cmVlX2ZpdCA8LSB0cmVlX3NwZWMgJT4lXG4gIGZpdChcbiAgICBmb3JtdWxhID0gbWVkdiB+IC4sIFxuICAgIGRhdGEgPSBCb3N0b25fdHJhaW4sIFxuICAgIG1vZGVsID0gVFJVRVxuICApXG5cbiMgVHJhaW5pZXJ0ZW4gQmF1bSBpbiBLb25zb2xlIGF1c2dlYmVuXG50cmVlX2ZpdCRmaXQiLCJhdHRyIjp7ImV2YWwiOnRydWUsImVkaXQiOnRydWUsImZpZy13aWR0aCI6IjgifX0=
</script>
</div>
</div>
<p>Der Output in <code>tree_fit$fit</code> zeigt, dass CP-Pruning zu einem kleinen Baum mit 3 Hierarchie-Ebenen geführt hat. Die Struktur zeigt, dass <code>lstat</code> und <code>rm</code> für Splitting-Regeln (<code>split</code>) verwendet werden, wie viele Beobachtungen den nodes zugeordnet sind (<code>n</code>), den Wert der Verlustfunktion (<code>deviance</code>) sowie den Durchschnitt von <code>medv</code> für jede node (<code>yval</code>). Für die drei leaf nodes (gekennzeichnet mit <code>*</code>) ist <code>yval</code> die Vorhersage der Outcome-Varibale für entsprechend gruppierte Beobachtungen.</p>
<p>Eine besser interpretierbare Darstellung des angepassten Baums in <code>tree_fit$fit</code> erhalten wir mit <code><a href="https://rdrr.io/pkg/rattle/man/fancyRpartPlot.html">rattle::fancyRpartPlot()</a></code>.</p>
<div class="cell">
<div>
<div id="webr-3">

</div>
<script type="webr-3-contents">
eyJjb2RlIjoibGlicmFyeShyYXR0bGUpXG5cbiMgUGxvdCB0aGUgZGVjaXNpb24gdHJlZVxuZmFuY3lScGFydFBsb3QoXG4gIHRyZWVfZml0JGZpdCxcbiAgc3BsaXQuY29sID0gXCJibGFja1wiLCBcbiAgbm4uY29sID0gXCJibGFja1wiLCBcbiAgY2FwdGlvbiA9IFwiVHJhaW5pZXJ0ZXIgRW50c2NoZWlkdW5nc2JhdW0gZsO8ciBjcCA9IDAuMVwiLFxuICBwYWxldHRlID0gXCJTZXQxXCIsXG4gIGJyYW5jaC5jb2wgPSBcImJsYWNrXCJcbikiLCJhdHRyIjp7ImV2YWwiOnRydWUsImVkaXQiOnRydWUsImZpZy1oZWlnaHQiOjgsImZpZy13aWR0aCI6OH19
</script>
</div>
</div>
<p>Für eine datengetriebene Wahl des CP-Parameters <span class="math inline">\(\alpha\)</span> kann Cross Validation (CV) verwendet werden. Hierzu erstellen wir zunächst eine <code>parsnip</code>-Spezifikation mit <code>cost_complexity = tune::tune()</code> in <code>decision_tree()</code> und erstellen einen <em>workflow</em> mit <code>parsnip::workflow()</code></p>
<div class="cell">
<div>
<div id="webr-4">

</div>
<script type="webr-4-contents">
eyJjb2RlIjoiIyBTcGV6aWZpa2F0aW9uIGbDvHIgQ1Ygdm9uIGNvc3RfY29tcGxleGl0eVxudHJlZV9zcGVjX2N2IDwtIGRlY2lzaW9uX3RyZWUoXG4gIGNvc3RfY29tcGxleGl0eSA9IHR1bmUoKVxuICApICU+JVxuICBzZXRfZW5naW5lKFwicnBhcnRcIikgJT4lXG4gIHNldF9tb2RlKFwicmVncmVzc2lvblwiKVxuXG4jIFdvcmtmbG93IGRlZmluaWVyZW5cbnRyZWVfd2ZfY3YgPC0gd29ya2Zsb3coKSAlPiVcbiAgYWRkX21vZGVsKHRyZWVfc3BlY19jdikgJT4lXG4gIGFkZF9mb3JtdWxhKG1lZHYgfiAuKSIsImF0dHIiOnsiZXZhbCI6dHJ1ZSwiZWRpdCI6dHJ1ZSwiZmlnLXdpZHRoIjoiOCJ9fQ==
</script>
</div>
</div>
<p>Mit <code><a href="https://rsample.tidymodels.org/reference/vfold_cv.html">rsample::vfold_cv()</a></code> definieren wir den CV-Prozess: 10-fold CV mit 2 Wiederholungen. <code><a href="https://tune.tidymodels.org/reference/tune_grid.html">tune::tune_grid()</a></code> führt CV anhand des in <code>tree_wf_cv</code> definierten workflows durch. Hierbei werden in <code>cp_grid</code> festgelegte Werte von <code>cost_complexity</code> berücksichtigt. Die mit <code>yardstick::metric_set(rmse)</code> festgelegte Verlustfunktion ist der mittlere quadratische Fehler (RMSE).<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Die hier verwedete Funktion ist <code><a href="https://yardstick.tidymodels.org/reference/rmse.html">yardstick::rmse()</a></code>.</p></div></div><div class="cell">
<div>
<div id="webr-5">

</div>
<script type="webr-5-contents">
eyJjb2RlIjoiIyBDVi1Qcm96ZXNzIGRlZmluaWVyZW5cbmN2X2ZvbGRzIDwtIHZmb2xkX2N2KFxuICBkYXRhID0gQm9zdG9uX3RyYWluLCBcbiAgdiA9IDEwLCBcbiAgcmVwZWF0cyA9IDJcbilcblxuIyBDViBkdXJjaGbDvGhyZW46XG5zZXQuc2VlZCgxMjM0KVxuXG4jIEdyaWQgZGVmaW5pZXJlblxuY3BfZ3JpZCA8LSB0aWJibGUoXG4gIGNvc3RfY29tcGxleGl0eSA9IGMoXG4gICAgMC4xLCAuMDc1LCAwLjA1LCAwLjAxLCAwLjAwMSwgMC4wMDAxXG4gICAgKVxuICApXG5cbiMgVHVuaW5nIG1pdCBDVlxudHJlZV9maXRfY3YgPC0gdHJlZV93Zl9jdiAlPiVcbiAgICB0dW5lX2dyaWQoXG4gICAgICAgIHJlc2FtcGxlcyA9IGN2X2ZvbGRzLCBcbiAgICAgICAgZ3JpZCA9IGNwX2dyaWQsXG4gICAgICAgIG1ldHJpY3MgPSBtZXRyaWNfc2V0KHJtc2UpXG4gICAgKVxuXG4jIENWLUVyZ2Vibmlzc2VcbnRyZWVfZml0X2N2IiwiYXR0ciI6eyJldmFsIjp0cnVlLCJlZGl0Ijp0cnVlLCJmaWctd2lkdGgiOiI4In19
</script>
</div>
</div>
<p>Mit <code><a href="https://ggplot2.tidyverse.org/reference/autoplot.html">workflowsets::autoplot()</a></code> kann der CV-RMSE für als Funktion des CP-Parameter leicht grafisch betrachtet dargestellt werden.</p>
<div class="cell">
<div>
<div id="webr-6">

</div>
<script type="webr-6-contents">
eyJjb2RlIjoiIyBDVi1FcmdlYm5pc3NlIHZpc3VhbGlzaWVyZW5cbmF1dG9wbG90KHRyZWVfZml0X2N2KSArXG4gIGxhYnMoXG4gICAgdGl0bGUgPSBcIkNWIGbDvHIgQ1AtUGFyYW1ldGVyOiBSTVNFIHZzLiBLb21wbGV4aXTDpHRcIlxuICApICtcbiAgdGhlbWVfY293cGxvdCgpIiwiYXR0ciI6eyJldmFsIjp0cnVlLCJlZGl0Ijp0cnVlLCJmaWctd2lkdGgiOiI4In19
</script>
</div>
</div>
<p>Für eine tabellierte Übersicht der besten Modelle kann <code><a href="https://tune.tidymodels.org/reference/show_best.html">tune::show_best()</a></code> verwendet werden. <code><a href="https://tune.tidymodels.org/reference/show_best.html">tune::select_best()</a></code> liest die beste Parameter-Kombination aus.</p>
<div class="cell">
<div>
<div id="webr-7">

</div>
<script type="webr-7-contents">
eyJjb2RlIjoiIyBUYWJlbGxhcmlzY2hlIMOcYmVyc2ljaHRcbnNob3dfYmVzdChcbiAgeCA9IHRyZWVfZml0X2N2LCBcbiAgbWV0cmljID0gXCJybXNlXCJcbilcblxuIyBHZXR1bnRlciBQYXJlbWF0ZXJcbmJlc3RfdHJlZV9maXQgPC0gc2VsZWN0X2Jlc3QoXG4gIHggPSB0cmVlX2ZpdF9jdiwgXG4gIG1ldHJpYyA9IFwicm1zZVwiXG4pXG5cbmJlc3RfdHJlZV9maXQiLCJhdHRyIjp7ImV2YWwiOnRydWUsImVkaXQiOnRydWUsImZpZy13aWR0aCI6IjgifX0=
</script>
</div>
</div>
<p>Anhand <code>tree_fit_cv</code> trainieren wir die finale Spezifikation.</p>
<div class="cell">
<div>
<div id="webr-8">

</div>
<script type="webr-8-contents">
eyJjb2RlIjoiIyBGaW5hbGVzIE1vZGVsbCBzY2jDpHR6ZW5cbmZpbmFsX3RyZWVfc3BlYyA8LSBkZWNpc2lvbl90cmVlKFxuICBjb3N0X2NvbXBsZXhpdHkgPSBiZXN0X3RyZWVfZml0JGNvc3RfY29tcGxleGl0eVxuICApICU+JVxuICBzZXRfZW5naW5lKFwicnBhcnRcIikgJT4lXG4gIHNldF9tb2RlKFwicmVncmVzc2lvblwiKVxuXG5maW5hbF90cmVlX2ZpdCA8LSBmaW5hbF90cmVlX3NwZWMgJT4lXG4gIGZpdChcbiAgICBmb3JtdWxhID0gbWVkdiB+IC4sIFxuICAgIGRhdGEgPSBCb3N0b25fdHJhaW5cbiAgKVxuXG4jIGZpbmFsX3RyZWVfZml0IiwiYXR0ciI6eyJldmFsIjp0cnVlLCJlZGl0Ijp0cnVlLCJmaWctd2lkdGgiOiI4In19
</script>
</div>
</div>
<p>Der geringe CP-Parameter führt zu einem großen Entscheidungsbaum.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Die Dimension der Grafik wurde hier zwecks Darstellung des gesamten Baums gewählt. <code>print(final_tree_fit$fit)</code> druckt die Entscheidungsregeln in die R-Konsole (hierzu die letzte Zeile ausführen).</p></div></div><div class="cell">
<div>
<div id="webr-9">

</div>
<script type="webr-9-contents">
eyJjb2RlIjoiXG4jIENWLUZpdCBwbG90dGVuXG5mYW5jeVJwYXJ0UGxvdChcbiAgZmluYWxfdHJlZV9maXQkZml0LFxuICBzcGxpdC5jb2wgPSBcImJsYWNrXCIsIFxuICBubi5jb2wgPSBcImJsYWNrXCIsIFxuICBjYXB0aW9uID0gXCJNaXQgQ1YgZXJtaXR0ZWx0ZXIgRW50c2NoZWlkdW5nc2JhdW1cIixcbiAgcGFsZXR0ZSA9IFwiU2V0MVwiLFxuICBicmFuY2guY29sID0gXCJibGFja1wiXG4pIiwiYXR0ciI6eyJldmFsIjp0cnVlLCJlZGl0Ijp0cnVlLCJmaWctaGVpZ2h0Ijo4LCJmaWctd2lkdGgiOjh9fQ==
</script>
</div>
</div>
<p>Zur Beurteilung der Relevanz von Variablen für die Reduktion des Anpassungsfehlers (<em>variable importance</em>) kann der Eintrag <code>variable.importance</code> des <code>rpart</code>-Objekts herangezogen werden. Variable importance misst hier die Gesamtreduktion der Fehlerquadratsumem über alle Knoten, an denen die jeweilige Variable für Splits verwendet wird.</p>
<div class="cell">
<div>
<div id="webr-10">

</div>
<script type="webr-10-contents">
eyJjb2RlIjoiIyBWYXJpYWJsZS1JbXBvcnRhbmNlIGF1c2xlc2VuXG5maW5hbF90cmVlX2ZpdCRmaXQkdmFyaWFibGUuaW1wb3J0YW5jZSIsImF0dHIiOnsiZXZhbCI6dHJ1ZSwiZWRpdCI6dHJ1ZSwiZmlnLXdpZHRoIjoiOCJ9fQ==
</script>
</div>
</div>
<p>Die Werte von Variable Importance zeigen, dass der mit CV ermittelte Baum <em>alle</em> Regressoren in <code>boston_train</code> für Splits nutzt, wobei <code>lstat</code> und <code>rm</code> die relevantesten Variablen sind.</p>
<p>Anhand von Vorhersagen für <code>medv</code> mit dem Test-Datensatz <code>boston_test</code> können wir das naive Baum-Modell <code>tree_fit</code> mit dem durch CV ermittelten Modell <code>tree_fit_cv</code> hinsichtlich des Vorhersagefehlers für ungesehene Beobachtungen vergleich. <code>yardstick::metric()</code> berechnet hierzu automatisch gängige Statistiken für Regressionsprobleme.</p>
<div class="cell">
<div>
<div id="webr-11">

</div>
<script type="webr-11-contents">
eyJjb2RlIjoiIyBWb3JoZXJzYWdlZ8O8dGUgbmFpdmVzIE1vZGVsbFxudHJlZV9wcmVkIDwtIHByZWRpY3QoXG4gIG9iamVjdCA9IHRyZWVfZml0LCBcbiAgbmV3X2RhdGEgPSBCb3N0b25fdGVzdFxuKSAlPiVcbiAgYmluZF9jb2xzKEJvc3Rvbl90ZXN0KSAlPiVcbiAgbWV0cmljcyh0cnV0aCA9IG1lZHYsIGVzdGltYXRlID0gLnByZWQpXG5cbiMgVm9yaGVyc2FnZWfDvHRlIGJlaSBDVlxudHJlZV9wcmVkX2N2IDwtIHByZWRpY3QoXG4gIG9iamVjdCA9IGZpbmFsX3RyZWVfZml0LCBcbiAgbmV3X2RhdGEgPSBCb3N0b25fdGVzdFxuKSAlPiVcbiAgYmluZF9jb2xzKEJvc3Rvbl90ZXN0KSAlPiVcbiAgbWV0cmljcyh0cnV0aCA9IG1lZHYsIGVzdGltYXRlID0gLnByZWQpXG5cbnRyZWVfcHJlZFxudHJlZV9wcmVkX2N2IiwiYXR0ciI6eyJldmFsIjp0cnVlLCJlZGl0Ijp0cnVlLCJmaWctd2lkdGgiOiI4In19
</script>
</div>
</div>
<p>Der Vergleich zeigt eine bessere Vorsageleistung des großen Baums in <code>tree_fit_cv</code>. In diesem Fall scheint CP-Pruning wenig hilfreich zu sein. Tatsächlich liefert ein Baum mit <span class="math inline">\(\alpha=0\)</span> bessere Vorhersagen als <code>tree_fit_cv</code> (überprüfe dies!).</p>
</section><section id="bagging" class="level2" data-number="15.3"><h2 data-number="15.3" class="anchored" data-anchor-id="bagging">
<span class="header-section-number">15.3</span> Bagging</h2>
<p><em>Bagging</em> ist eine Ensemble-Modelle, die durch aus einer Kombination von vielen Entscheidungsbäumen bestehen. Bagging steht für <em>Bootstrap Aggregating</em> und nutzt einen Algorithmus, bei dem Bäume auf <em>zufälligen</em> Stichproben aus dem Trainingsdatensatz angepasst werden: Jeder Baum wird auf einer <em>Bootstrap-Stichprobe</em> (siehe <a href="Simulation.html" class="quarto-xref"><span>Kapitel 5</span></a>) trainiert, die durch zufällige Züge (mit Zurücklegen) erstellt wird. Nach dem Training aggregiert Bagging die Vorhersagen aller Bäume des Ensembles.</p>
<p>Der Vorteil von Bagging gegenüber einem einzelnen Entscheidungsbaum ist, dass die Varianz der Vorhersage deutlich reduziert werden kann: Einzelne Entscheidungsbäume neigen dazu, Muster in den Trainingsdaten zu lernen, die sich zufällig aus der Zusammensetzung der Stichprobe ergeben und nicht repräsentativ für Zusammenhänge zwischen den Regressoren und der Outcome-Variable sind. Diese Überanpassung führt zu hoher Varianz auf von Vorhersagen für ungesehene Daten. Durch das Training vieler Bäume auf unterschiedlichen <em>zufälligen</em> Stichproben aus den Trainingsdaten und das anschließende Aggregieren kann der negative Effekt der Überanpassung auf die Unsicherheit der Vorhersage einzelner Bäume reduziert werden.</p>
<p>Eine Bagging-Spezifikation kann mit <code><a href="https://parsnip.tidymodels.org/reference/bag_tree.html">parsnip::bag_tree()</a></code> festgelegt werden. Mit <code>times = 500</code> wird definiert, dass der Bagging-Algorithmus ein Ensemble mit 500 Bäumen (mit CART) anpassen soll. Das Training und die Vorhersage auf den Testdaten erfolgt analog zur Vorgehensweise in <a href="#sec-simpletrees" class="quarto-xref"><span>Kapitel 15.1</span></a>.</p>
<div class="cell">
<div>
<div id="webr-12">

</div>
<script type="webr-12-contents">
eyJjb2RlIjoiIyBTcGV6aWZpa2F0aW9uIGbDvHIgQmFnZ2luZ1xuYmFnZ2luZ19zcGVjIDwtIGJhZ190cmVlKCkgJT4lXG4gIHNldF9lbmdpbmUoXG4gICAgZW5naW5lID0gXCJycGFydFwiLFxuICAgIHRpbWVzID0gNTAwXG4gICkgJT4lXG4gIHNldF9tb2RlKFwicmVncmVzc2lvblwiKVxuXG5cbiMgVHJhaW5pbmcgZHVyY2hmw7xocmVuXG5zZXQuc2VlZCgxMjM0KVxuXG5iYWdnaW5nX2ZpdCA8LSBiYWdnaW5nX3NwZWMgJT4lXG4gIGZpdChcbiAgICBmb3JtdWxhID0gbWVkdiB+IC4sIFxuICAgIGRhdGEgPSBCb3N0b25fdHJhaW5cbiAgKVxuXG4jIEF1c3dlcnR1bmdcbmJhZ2dpbmdfcHJlZCA8LSBwcmVkaWN0KFxuICBvYmplY3QgPSBiYWdnaW5nX2ZpdCwgXG4gIG5ld19kYXRhID0gQm9zdG9uX3Rlc3RcbiAgKSAlPiVcbiAgYmluZF9jb2xzKEJvc3Rvbl90ZXN0KSAlPiVcbiAgbWV0cmljcyhcbiAgICB0cnV0aCA9IG1lZHYsXG4gICAgZXN0aW1hdGUgPSAucHJlZFxuICApXG5cbmJhZ2dpbmdfcHJlZCIsImF0dHIiOnsiZXZhbCI6dHJ1ZSwiZWRpdCI6dHJ1ZSwiZmlnLXdpZHRoIjoiOCJ9fQ==
</script>
</div>
</div>
<p>Die Auswertung auf den Testdatensatz ergibt eine deutliche Verbesserung der Vorhersageleistung gegenüber einem einfachen Regressionsbaum.</p>
<p>Obwohl die Bäume beim Bagging auf unterschiedlichen Stichproben trainiert werden, kann innerhalb des Ensembles dennoch eine deutliche Korrelation vorliegen: Da jeder Baum auf alle Regressoren für Splits zugreift, können trotz Bootstrapping ähnliche (unverteilhafte) Muster aus dem Datensatz erlernt werden, was sich nachteilig auf die Generalisierungsfähigkeit auswirken kann. Diese Korrelation mindert die Effektivität von Bagging, da stark korrelierte Bäume dazu neigen, ähnliche Fehler zu machen.</p>
</section><section id="sec-brf" class="level2 page-columns page-full" data-number="15.4"><h2 data-number="15.4" class="anchored" data-anchor-id="sec-brf">
<span class="header-section-number">15.4</span> Random Forests</h2>
<p><em>Random Forests</em> erweitern Bagging, indem zusätzlich bei jedem Knoten innerhalb jedes Baumes eine <em>zufällige Teilmenge der Regressoren</em> als potentielle Variable für die Split-Regel ausgewählt wird. Dies führt zu einer Reduktion der Korrelation zwischen den Bäumen, was die Genauigkeit verbessert und das Risiko von Overfitting weiter verringert.</p>
<p>In R erstellen wir die Spezifikation mit <code><a href="https://parsnip.tidymodels.org/reference/rand_forest.html">parsnip::rand_forest()</a></code>. Der Parameter <code>mtry</code> legt fest, wie viele Regressoren <span class="math inline">\(m\)</span> zufällig für jeden Split zur Verfügung stehen. Wir nutzen den im <code>randomForest</code>-Paket implementierten Algorithmus und legen in <code>set_engine()</code> fest, dass die von <code><a href="https://rdrr.io/pkg/randomForest/man/randomForest.html">randomForest::randomForest()</a></code> berechnete Fehler-Metrik im Output-Objekt ausgegeben wird (<code>tree.err = TRUE</code>). Um die Spezifikation für verschiedene Werte von <code>mtry</code> anwenden zu können, implementieren wir die Spezifikation innerhalb einer Wrapper-Funktion <code>rf_spec_mtry()</code>. Mit <code><a href="https://purrr.tidyverse.org/reference/map.html">purrr::map()</a></code> iterieren wir <code>rf_spec_mtry()</code> über drei verschiedene Werte für den Tuning-Parameter <code>mtry</code> (4, 6 und 10 Variablen).<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;Eine Faustregel für die Wahl von <span class="math inline">\(m\)</span> bei <span class="math inline">\(k\)</span> verfügbaren Regressoren ist <span class="math inline">\(m\approx\sqrt{k}\)</span>.</p></div></div><div class="cell">
<div>
<div id="webr-13">

</div>
<script type="webr-13-contents">
eyJjb2RlIjoic2V0LnNlZWQoMTIzNClcblxuIyBXZXJ0ZSBmw7xyIG10cnlcbm10cnlfdmFsdWVzIDwtIGMoNCwgNiwgMTApXG5cbiMgRnVua3Rpb246IFJhbmRvbSBGb3Jlc3QgZsO8ciBtdHJ5ID0gbVxucmZfc3BlY19tdHJ5IDwtIGZ1bmN0aW9uKG0pIHtcbiAgcmFuZF9mb3Jlc3QobXRyeSA9IG0sIHRyZWVzID0gNTAwKSAlPiVcbiAgICBzZXRfZW5naW5lKFxuICAgICAgZW5naW5lID0gXCJyYW5kb21Gb3Jlc3RcIiwgXG4gICAgICB0cmVlLmVyciA9IFRSVUVcbiAgICApICU+JVxuICAgIHNldF9tb2RlKFwicmVncmVzc2lvblwiKVxufVxuXG4jIE1vZGVsbGUgZsO8ciB2ZXJzY2hpZWRlbmUgbXRyeS1XZXJ0ZSB0cmFpbmllcmVuXG5yZl9maXRzIDwtIG1hcChcbiAgLnggPSBtdHJ5X3ZhbHVlcywgXG4gIC5mID0gfiByZl9zcGVjX210cnkoLngpICU+JVxuICAgIGZpdChcbiAgICAgIGZvcm11bGEgPSBtZWR2IH4gLiwgXG4gICAgICBkYXRhID0gQm9zdG9uX3RyYWluXG4gICAgKVxuKVxuXG5yZl9maXRzIDwtIHNldF9uYW1lcyhcbiAgeCA9IHJmX2ZpdHMsXG4gIG5tID0gIHBhc3RlMChcInJmX210cnlcIiwgbXRyeV92YWx1ZXMsIFwiX2ZpdFwiKVxuKVxuXG4jIEF1c2dhYmUgZGVyIEVyZ2Vibmlzc2VcbnJmX2ZpdHMiLCJhdHRyIjp7ImV2YWwiOnRydWUsImVkaXQiOnRydWUsImZpZy13aWR0aCI6IjgifX0=
</script>
</div>
</div>
<p>Für eine Beurteilung des Vorhersageleistung dieser drei Modelle können wir den <em>Out-of-Bag</em>-Fehler (OOB) verwenden:</p>
<p>Der OOB-Fehler ist eine Schätzung des Generalisierungsfehlers ohne einen separaten Testdatensatzes. Bei Random Forests (und Bagging) ist dies aufgrund der Berechnung des Ensembles für Bootstrap-Stichproben möglich: Grob ein Drittel der Beobachtungen des Datensatzes sind nicht Teil der Stichprobe, die für das Training jedes Baums im Ensemble genereiert werden.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> Diese nicht gezogenen Datenpunkte sind OOB-Beobachtungen. Der OOB-Fehler des Ensembles ist der durchschnittliche Fehler für die aggregierten Vorhersagen der Bäume des Forests.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;Beachte, dass beim Bootstrap <span class="math inline">\(n\)</span> aus <span class="math inline">\(n\)</span> Beobachtungen mit Zurücklegen gezogen werden. Die Wahrscheinlicht, dass eine Beobachtung <em>nicht</em> gezogen wird (“Out-of-Bag”), ist <span class="math inline">\((1-1/n)^n\approx37\%\)</span>.</p></div></div><p>Der OOB-Fehler kann auch verwendet werden, um die erforderliche Größe des Random Forests zu beurteilen: Eine größere Anzahl von Bäumen reduziert tendenziell die Varianz der Vorhersagen und verbessert die Generalisierungsfähigkeit. Allerdings nimmt dieser Effekt ab, und ab einer bestimmten Baumanzahl sind weitere Verbesserungen marginal. Obwohl das Risiko von Überanpassung durch viele Bäume aufgrund des Bagging minimal ist, kann es bei großen Datensätzen sinnvoll sein, kleinere Wälder zu trainieren, um den Rechenaufwand zu verringern. Wir plotten hierfür den OOB-Fehler für das Modell mit <code>mtry = 10</code> gegen die Anzahl der Bäume.</p>
<div class="cell">
<div>
<div id="webr-14">

</div>
<script type="webr-14-contents">
eyJjb2RlIjoibGlicmFyeShnZ1JhbmRvbUZvcmVzdHMpXG5cbiMgT09CLUZlaGxlciBhbHMgRnVua3Rpb24gZGVyIEJhdW1hbnphaGxcbnJmX2ZpdHMkcmZfbXRyeTEwX2ZpdCRmaXQgJT4lIFxuICBnZ19lcnJvcigpICU+JSBcbiAgXG4gIHBsb3QoKSArIFxuICBsYWJzKFxuICAgIHRpdGxlID0gXCJSYW5kb20gRm9yZXN0OiBFbnNlbWJsZWdyw7bDn2UgdnMuIE9PQi1GZWhsZXIgKG10cnkgPSAxMClcIlxuICApICsgXG4gIHRoZW1lX2Nvd3Bsb3QoKSIsImF0dHIiOnsiZXZhbCI6dHJ1ZSwiZWRpdCI6dHJ1ZSwiZmlnLXdpZHRoIjoiOCJ9fQ==
</script>
</div>
</div>
<p>Die Grafik zeigt, dass die Verbesserung des OOB-Fehlers jenseits von 250 Beobachtungen deutlich nachlässt, sodass ein Training von 500 Bäumen ausreichend scheint.</p>
<p>Zur Beurteiliung der Vorhersagegüte mit dem Testdatensatz gehen wir analog zum Training vor und iterieren mit <code>map()</code> über <code>rf_fits</code>, die Liste der angepassten Modelle.</p>
<div class="cell">
<div>
<div id="webr-15">

</div>
<script type="webr-15-contents">
eyJjb2RlIjoiIyBWb3JoZXJzYWdlIHVuZCBCZXJlY2hudW5nIHYuIE1ldHJpa2VuIGbDvHIgamVkZW4gUkZcbnJmX3ByZWRpY3Rpb25zIDwtIG1hcChcbiAgLnggPSByZl9maXRzLCBcbiAgLmYgPSAgfiBwcmVkaWN0KC54LCBCb3N0b25fdGVzdCkgJT4lXG4gICAgYmluZF9jb2xzKEJvc3Rvbl90ZXN0KSAlPiVcbiAgICBtZXRyaWNzKFxuICAgICAgdHJ1dGggPSBtZWR2LCBcbiAgICAgIGVzdGltYXRlID0gLnByZWRcbiAgICApXG4pXG5cbiMgRWludHLDpGdlIGJlbmVubmVuXG5yZl9wcmVkaWN0aW9ucyA8LSBzZXRfbmFtZXMoXG4gIHggPSByZl9wcmVkaWN0aW9ucyxcbiAgbm0gPSAgcGFzdGUwKFwicmZfbXRyeVwiLCBtdHJ5X3ZhbHVlcywgXCJfcHJlZFwiKVxuKVxuXG5yZl9wcmVkaWN0aW9ucyIsImF0dHIiOnsiZXZhbCI6dHJ1ZSwiZWRpdCI6dHJ1ZSwiZmlnLXdpZHRoIjoiOCJ9fQ==
</script>
</div>
</div>
<p>Ähnlich wie für einen einzelnen Baum kann die Relevanz von Variablen anhand der Reduktion der Loss-Funktion durch das Ensemble beurteilt werden. Für einen einfachen Vergleich der Variable Importance für den Random Forests mit <code>mtry = 10</code> in <code>rf_fits$rf_mtry10_fit</code> nutzen wir <code><a href="https://rdrr.io/pkg/ggRandomForests/man/gg_vimp.html">ggRandomForests::gg_vimp()</a></code>.</p>
<div class="cell">
<div>
<div id="webr-16">

</div>
<script type="webr-16-contents">
eyJjb2RlIjoiIyBWYXJpYWJsZSBpbXBvcnRhbmNlIGbDvHIgbXRyeSA9IDEwXG5yZl9maXRzJHJmX210cnkxMF9maXQkZml0ICU+JVxuICBnZ192aW1wKCkgICU+JVxuICBwbG90KCkgK1xuICAgIGxhYnMoXG4gICAgICB0aXRsZSA9IFwiVmFyaWFibGUgSW1wb3J0YW5jZSBmw7xyIFJhbmRvbSBGb3Jlc3QgKG10cnkgPSAxMClcIlxuICAgICkgK1xuICAgIHRoZW1lX2Nvd3Bsb3QoKSIsImF0dHIiOnsiZXZhbCI6dHJ1ZSwiZWRpdCI6dHJ1ZSwiZmlnLXdpZHRoIjoiOCJ9fQ==
</script>
</div>
</div>
<p>Die Grafik bestärkt unsere Schlussfolgerung aus der Analyse des (mit CART trainierten) einzelnen Entscheidungsbaums in <a href="#sec-simpletrees" class="quarto-xref"><span>Kapitel 15.1</span></a>, dass <code>rm</code> und <code>lstat</code> die wichtigsten Regressoren für die Vorhersage von <code>medv</code> sind.</p>
</section><section id="sec-boosting" class="level2 page-columns page-full" data-number="15.5"><h2 data-number="15.5" class="anchored" data-anchor-id="sec-boosting">
<span class="header-section-number">15.5</span> Boosting</h2>
<p>Boosting ist eine leistungsstarke Ensemble-Methode für Vorhersagen, die kleine Modelle (oft Entscheidungsbäume geringer Tiefe) sukzessiv trainiert und zu einem starken Modell kombiniert. Anders als bei Random Forests, bei denen viele Bäume unabhängig voneinander auf zufälligen Stichproben der Daten trainiert werden, geht ein Boosting-Algorithmuss sequentiell vor: Jeder nachfolgende Baum wird darauf optimiert, die Fehler des vorherigen Modells zu reduzieren. Die Idee hierbei ist es, iterativ “schwache” Modelle zu erzeugen, die eine gute Anpassung für Datenpunkte liefern, die in den vorherigen Durchläufen schlecht vorhergesagt wurden.</p>
<p>Für einen Trainingsdatensatz <span class="math inline">\(\{(x_i, y_i)\}_{i=1}^n\)</span>, wobei <span class="math inline">\(x_i\)</span> die Input-Features und <span class="math inline">\(y_i\)</span> Beobachtungen des Outcomes sind, kann Boosting wiefolgt durchgeführt werden.</p>
<ol type="1">
<li><p><strong>Initialisierung</strong>: Initialisiere das Boosting-Modell als <span class="math inline">\(\widehat{F}_0(x)\)</span>. Setze die Residuen <span class="math inline">\(r^0_i=y_i\)</span> für alle <span class="math inline">\(i\)</span></p></li>
<li>
<p><strong>Iteration</strong>: Wiederhole die folgenden Schritte für <span class="math inline">\(b = 1,2,\dots,B\)</span> mit <span class="math inline">\(B\)</span> hinreichend groß:</p>
<p>2.1 <strong>Base Learner</strong>: Trainiere Baum <span class="math inline">\(T_b\)</span> mit <span class="math inline">\(\{(\boldsymbol{x}_i, r^{b-1}_i)\}_{i=1}^n\)</span> für die Vorhersage des <em>Fehlers</em> der vorherigen Iteration <span class="math inline">\(r^{b-1}\)</span>.</p>
<p>2.2 <strong>Aktualisierung</strong>: Aktualisiere das Boosting-Modell,</p>
<p><span class="math display">\[\begin{align*}
   \widehat{F}_{b}(\boldsymbol{x}) = \widehat{F}_{b-1}(\boldsymbol{x}) + \eta \cdot T_{b}(\boldsymbol{x}),
   \end{align*}\]</span></p>
<p>wobei <span class="math inline">\(\eta\)</span> die (oft klein gewählte) <em>Lernrate</em> ist.</p>
<p>2.3 <strong>Fehlerberechnung</strong>: Berechne die Residuen <span class="math inline">\(r^b_i\)</span> als Differenzen zwischen dem tatsächlichen Werten <span class="math inline">\(y_i\)</span> und den Vorhersage des aktuellen Modells <span class="math inline">\(\widehat{F}_m(\boldsymbol{x}_i)\)</span>,</p>
<p><span class="math display">\[\begin{align*}
   r^b_i = y_i - \widehat{F}_b(\boldsymbol{x}_i).
   \end{align*}\]</span></p>
</li>
<li>
<p><strong>Output</strong>: Gib das finale Modell aus:</p>
<p><span class="math display">\[\begin{align*}
   \widehat{F}(\boldsymbol{x}) := \sum_{b=1}^B \eta\cdot \widehat{F}^b(\boldsymbol{x})
\end{align*}\]</span></p>
</li>
</ol>
<p>Der Parameter <span class="math inline">\(0\leq\eta\leq0\)</span> steuert, wie stark der Einfluss jedes neuen Baumes auf das Modell ist. Eine kleine Lernrate führt dazu, dass viele Bäume benötigt werden, was Vorhersagen (ähnlich wie bei Bagging) stabiler macht. Beachte die sequentielle Natur des Trainings: Die <span class="math inline">\(r^b_i\)</span> in Schritt 2.3 sind die zu vorhersagenden Outcome-Variable für den nächsten Baum. <span class="math inline">\(T_{b+1}\)</span> wird trainiert wird, um den <em>Fehler des bisherigen Modells</em> <span class="math inline">\(\widehat{F}_b\)</span> zu erklären.</p>
<p>Für die Anwendung auf <code><a href="https://rdrr.io/pkg/MASS/man/Boston.html">MASS::Boston</a></code> in R nutzen wir den im Paket <code>gbm</code> implementierten <em>Gradient-Boosting</em>-Algorithmus. Bei Gradient Boosting wird jeder Baum so trainiert, dass er den negativen Gradienten einer Verlustfunktion approximiert, also die Richtung des größten Fehlers. Das Modell wird schrittweise verbessert, indem es entlang des Gradienten aktualisiert wird, um die Vorhersagegüe zu optimieren; siehe <span class="citation" data-cites="Hastieetal2013">Hastie, Tibshirani, und Friedman (<a href="Literatur.html#ref-Hastieetal2013" role="doc-biblioref">2013</a>)</span> für eine detaillierte Erläuterung.</p>
<p>Mit dem nachfolgenden Code-Chunk trainieren wir ein Boosting-Modell für Regression mit 5000 einfachen Bäumen (<code>n.trees = 5000</code>) mit einer maximalen Tiefe von 2 (<code>interaction.depth = 2</code>), d.h. es folgen maximal 2 Entscheidungs-Regeln nacheinander. Um das Risiko von Overfitting gering zu halten, erlauben wir nur Splits, die zu mindestens zwei Beobachtungen in resultierenden nodes führen (<code>n.minobsinnode = 2</code>). Die Lernrate (Beitrag der Base Learner zum Ensemble) wird typischerweise klein (und in Abhängigkeit von <code>n.trees</code>) gewählt (<code>shrinkage = 0.001</code>).<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;Je kleiner die Lernrate, desto größer sollte <code>n.trees</code> gewählt werden.</p></div></div><div class="cell">
<div>
<div id="webr-17">

</div>
<script type="webr-17-contents">
eyJjb2RlIjoic2V0LnNlZWQoMTIzNClcblxuIyBHcmFkaWVudCBCb29zdGluZyBkdXJjaGbDvGhyZW5cbmdibV9tb2RlbCA8LSBnYm0oXG4gIGZvcm11bGEgPSBtZWR2IH4gLiwgXG4gIGRhdGEgPSBCb3N0b25fdHJhaW4sIFxuICBkaXN0cmlidXRpb24gPSBcImdhdXNzaWFuXCIsICMgZsO8ciBSZWdyZXNzaW9uXG4gIG4udHJlZXMgPSA1MDAwLCAgICAgICAgICAgIyBBbnouIELDpHVtZVxuICBpbnRlcmFjdGlvbi5kZXB0aCA9IDIsICAgICAjIE1heGltYWxlIFRpZWZlIGRlciBiYXNlIGxlYXJuZXJcbiAgc2hyaW5rYWdlID0gMC4wMSwgICAgICAgICAjIExlcm5yYXRlXG4gIG4ubWlub2JzaW5ub2RlID0gMiAgICAgICAgICMgTWluLiBCZW9iYWNodHVuZ2VuIGluIG5vZGVzXG4pXG5cbmdibV9tb2RlbCAiLCJhdHRyIjp7ImV2YWwiOnRydWUsImVkaXQiOnRydWUsImZpZy13aWR0aCI6IjgifX0=
</script>
</div>
</div>
<p>Für die Vorhersagen auf dem Test-Datensatz legen wir mit <code>n.trees = gbm_model$n.trees</code> fest, dass das gesamte Ensemble genutzt werden soll.</p>
<div class="cell">
<div>
<div id="webr-18">

</div>
<script type="webr-18-contents">
eyJjb2RlIjoiIyBWb3JoZXJzYWdlbiBUZXN0LURhdGVuc2F0elxuZ2JtX3ByZWRpY3Rpb25zIDwtIHByZWRpY3QoXG4gIG9iamVjdCA9IGdibV9tb2RlbCwgXG4gIG5ld2RhdGEgPSBCb3N0b25fdGVzdCwgXG4gIG4udHJlZXMgPSBnYm1fbW9kZWwkbi50cmVlcyAjIGdlc2FtdGVzIEVuc2VtYmxlIG51dHplblxuKVxuXG4jIEF1c3dlcnR1bmcgVGVzdC1EYXRlbnNhdHpcbnJlc3VsdHMgPC0gQm9zdG9uX3Rlc3QgJT4lXG4gIG11dGF0ZShwcmVkaWN0aW9ucyA9IGdibV9wcmVkaWN0aW9ucykgJT4lXG4gIG1ldHJpY3MoXG4gICAgdHJ1dGggPSBtZWR2LCBcbiAgICBlc3RpbWF0ZSA9IHByZWRpY3Rpb25zXG4gIClcblxucmVzdWx0cyIsImF0dHIiOnsiZXZhbCI6dHJ1ZSwiZWRpdCI6dHJ1ZSwiZmlnLXdpZHRoIjoiOCJ9fQ==
</script>
</div>
</div>
<p>Die Ergebnisse zeigen, dass Gradient Boosting bereits für die naive Parameterwahl im Aufruf von <code><a href="https://rdrr.io/pkg/gbm/man/gbm.html">gbm::gbm()</a></code> zu einer Verbesserung der Vorhersageleistung gegenüber den Random-Forest-Modellen führt.</p>
<p>Anstatt <code>n.trees = 5000</code> können wir <code>n.trees</code> in <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> einen Vektor mit verschiedenen Ensemble-Größen übergeben. Für <code>n.trees = 5000</code> erhalten wir Vorhersagen für jeden Status, den das Boosting-Modell im Training nach seiner Initialisierung bis zu der in <code><a href="https://rdrr.io/pkg/gbm/man/gbm.html">gbm::gbm()</a></code> festgelgten Größe durchläuft. Anhand dieser Vorhersagen können wir die Generalisierungsfähigkeit des Modells in Abhängigkeit der gewählten Lernrate und der Größe beurteilen, in dem wir den RMSE für den gesamten Trainingsprozess berechnen. Für eine leichtere Interpretation erzeugen wir eine Grafik ählich wie bei der OOB-Analyse des Random-Forest-Modells.</p>
<div class="cell">
<div>
<div id="webr-19">

</div>
<script type="webr-19-contents">
eyJjb2RlIjoiIyBWb3JoZXJzYWdlbiBzdWt6ZXNzaXYgdHJlZmZlblxucHJlZGljdChcbiAgICBvYmplY3QgPSBnYm1fbW9kZWwsIFxuICAgIG5ld2RhdGEgPSBCb3N0b25fdGVzdCwgXG4gICAgbi50cmVlcyA9IDE6NTAwMFxuKSAlPiVcbiAgICBcbiAgICMgVGVzdHNldC1STVNFIGJlcmVjaG5lblxuICAgIGFzX3RpYmJsZSgpICU+JVxuICAgIG1hcF9kYmwoXG4gICAgICAuZiA9IH4gc3FydChtZWFuKCgueCAtIEJvc3Rvbl90ZXN0JG1lZHYpXjIpKVxuICAgICkgJT4lXG4gICAgYmluZF9jb2xzKHJtc2UgPSAuLCB0cmVlcyA9IDE6NTAwMCkgJT4lXG4gIFxuICAjIFBsb3R0ZW5cbiAgZ2dwbG90KG1hcHBpbmcgPSBhZXMoeCA9IHRyZWVzLCB5ID0gcm1zZSkpICtcbiAgICBnZW9tX2xpbmUoKSArXG4gICAgbGFicyhcbiAgICAgIHRpdGxlID0gXCJCb29zdGluZzogVGVzdHNldC1STVNFIGFscyBGdW5rdGlvbiB2b24gbi50cmVlc1wiXG4gICAgKSArXG4gICAgdGhlbWVfY293cGxvdCgpIiwiYXR0ciI6eyJldmFsIjp0cnVlLCJlZGl0Ijp0cnVlLCJmaWctd2lkdGgiOiI4In19
</script>
</div>
</div>
<p>Die Grafik zeigt eine schnelle Verbesserung des Out-of-sample-Fehlers mit der Größe des Ensembles. Für die gewählte Lernrate scheinen 5000 Bäume adäquat zu sein.</p>
<p>Analog zu Bagging und Random Forests können wir die Relevanz der Regressoren in <code>Boston</code> für die Vorhersage von <code>medv</code> anhand der mit <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> berechneten (relativen) Variable Importance für die Anpassung auf den Trainingsdatensatz einschätzen.</p>
<div class="cell">
<div>
<div id="webr-20">

</div>
<script type="webr-20-contents">
eyJjb2RlIjoiIyBWYXJpYWJsZSBJbXBvcnRhbmNlIGJlcmVjaG5lblxudmFyX2ltcG9ydGFuY2UgPC0gc3VtbWFyeShcbiAgb2JqZWN0ID0gZ2JtX21vZGVsLCBcbiAgcGxvdGl0ID0gRkFMU0UgIyBrLiBncmFwaGlzY2hlIEF1c2dhYmVcbilcblxuIyAuLi4gdW5kIHBsb3R0ZW5cbnZhcl9pbXBvcnRhbmNlIDwtIHZhcl9pbXBvcnRhbmNlICU+JVxuICBhc190aWJibGUoKSAlPiVcbiAgYXJyYW5nZShcbiAgICBkZXNjKHJlbC5pbmYpXG4gIClcblxuZ2dwbG90KFxuICBkYXRhID0gdmFyX2ltcG9ydGFuY2UsXG4gIG1hcHBpbmcgPSBhZXMoXG4gICAgeCA9IHJlb3JkZXIodmFyLCByZWwuaW5mKSwgXG4gICAgeSA9IHJlbC5pbmZcbiAgKVxuKSArXG4gIGdlb21fYmFyKHN0YXQgPSBcImlkZW50aXR5XCIpICtcbiAgY29vcmRfZmxpcCgpICtcbiAgbGFicyhcbiAgICB0aXRsZSA9IFwiVmFyaWFibGUgSW1wb3J0YW5jZSBmw7xyIEdyYWRpZW50IEJvb3N0aW5nXCIsXG4gICAgeCA9IFwiVmFyaWFibGVcIixcbiAgICB5ID0gXCJSZWxhdGl2ZXIgRWluZmx1c3MgKCUpXCJcbiAgKSArXG4gIHRoZW1lX2Nvd3Bsb3QoKSIsImF0dHIiOnsiZXZhbCI6dHJ1ZSwiZWRpdCI6dHJ1ZSwiZmlnLXdpZHRoIjoiOCJ9fQ==
</script>
</div>
</div>
<p>Obwohl erneut <code>lstat</code> und <code>rm</code> als die wichtigsten Prädiktoren gelistet sind, identifiziert Gradient Boosting im Gegensatz zu Bagging und Random Forests <code>lstat</code> als die Variable mit der größten Vorhersagekraft für <code>medv</code>.</p>
</section><section id="zusammenfassung" class="level2" data-number="15.6"><h2 data-number="15.6" class="anchored" data-anchor-id="zusammenfassung">
<span class="header-section-number">15.6</span> Zusammenfassung</h2>
<p>In diesem Kapitel haben wir die Anwendung baum-basierter Methoden in R diskutiert. Darunter Entscheidungsbäume, Bagging, Random Forests und Boosting. Entscheidungsbäume sind Modelle, die die Daten anhand binärer Entscheidungsregeln sukzessiv in kleinere, homogene Gruppen aufgeteilt werden. Baum-Modelle bieten intuitive Interpretierbarkeit, neigen jedoch zur Überanpassung, was durch Beschneiden (Pruning) vermieden werden kann. Die Vorhersage einzelner Bäume ist tendentiell mit hoher Varianz verbunden. Random Forests kombinieren mit Bagging viele Entscheidungsbäume, die auf zufälligen Teilmengen der Daten und Merkmale trainiert werden. Durch die Aggregation der Vorhersagen vieler Bäume reduziert der Random Forest die Varianz und verbessert so die Vorhersagegenauigkeit. Boosting-Methoden mit Entscheidungsbäumen trainieren kleine Bäume sukzessive, wobei jeder weitere Baum zur Korrektur der gegenwärtigen Fehler des Ensembles trainiert wird. Gradient Boosting nutzt den Gradienten der Verlustfunktion, um die Vorhersagequalität des Ensembles optimieren. Für alle Methoden wurden Implementierungen im <code>parsnip</code>-Framework in R vorgestellt. Zudem wurde gezeigt, wie die Vorhersagegüte durch Testdatensätze beurteilt und die Bedeutung einzelner Variablen mit Variable-Importance-Metriken analysiert werden kann.</p>


<!-- -->

<script type="webr-data">
eyJyZW5kZXJfZGYiOiJkZWZhdWx0IiwicGFja2FnZXMiOnsicmVwb3MiOltdLCJwa2dzIjpbImV2YWx1YXRlIiwia25pdHIiLCJodG1sdG9vbHMiLCJiYWd1ZXR0ZSIsImNvd3Bsb3QiLCJnYm0iLCJkcGx5ciIsImdncGxvdDIiLCJnZ1JhbmRvbUZvcmVzdHMiLCJNQVNTIiwicHVycnIiLCJyYW5kb21Gb3Jlc3QiLCJyYXR0bGUiLCJ0aWR5bW9kZWxzIiwidGlkeXIiXX0sIm9wdGlvbnMiOnsiYmFzZVVybCI6Imh0dHBzOi8vd2Vici5yLXdhc20ub3JnL3YwLjQuMS8ifX0=
</script><script type="ojs-module-contents">
{"contents":[{"inline":false,"cellName":"webr-widget-20","methodName":"interpretQuiet","source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_20;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"inline":false,"cellName":"webr-20","methodName":"interpret","source":"viewof _webr_editor_20 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-20-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-20-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_20 = webROjs.process(_webr_editor_20, {});\n"},{"inline":false,"cellName":"webr-widget-19","methodName":"interpretQuiet","source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_19;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"inline":false,"cellName":"webr-19","methodName":"interpret","source":"viewof _webr_editor_19 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-19-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-19-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_19 = webROjs.process(_webr_editor_19, {});\n"},{"inline":false,"cellName":"webr-widget-18","methodName":"interpretQuiet","source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_18;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"inline":false,"cellName":"webr-18","methodName":"interpret","source":"viewof _webr_editor_18 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-18-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-18-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_18 = webROjs.process(_webr_editor_18, {});\n"},{"inline":false,"cellName":"webr-widget-17","methodName":"interpretQuiet","source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_17;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"inline":false,"cellName":"webr-17","methodName":"interpret","source":"viewof _webr_editor_17 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-17-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-17-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_17 = webROjs.process(_webr_editor_17, {});\n"},{"inline":false,"cellName":"webr-widget-16","methodName":"interpretQuiet","source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_16;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"inline":false,"cellName":"webr-16","methodName":"interpret","source":"viewof _webr_editor_16 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-16-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-16-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_16 = webROjs.process(_webr_editor_16, {});\n"},{"inline":false,"cellName":"webr-widget-15","methodName":"interpretQuiet","source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_15;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"inline":false,"cellName":"webr-15","methodName":"interpret","source":"viewof _webr_editor_15 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-15-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-15-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_15 = webROjs.process(_webr_editor_15, {});\n"},{"inline":false,"cellName":"webr-widget-14","methodName":"interpretQuiet","source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_14;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"inline":false,"cellName":"webr-14","methodName":"interpret","source":"viewof _webr_editor_14 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-14-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-14-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_14 = webROjs.process(_webr_editor_14, {});\n"},{"inline":false,"cellName":"webr-widget-13","methodName":"interpretQuiet","source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_13;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"inline":false,"cellName":"webr-13","methodName":"interpret","source":"viewof _webr_editor_13 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-13-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-13-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_13 = webROjs.process(_webr_editor_13, {});\n"},{"inline":false,"cellName":"webr-widget-12","methodName":"interpretQuiet","source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_12;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"inline":false,"cellName":"webr-12","methodName":"interpret","source":"viewof _webr_editor_12 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-12-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-12-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_12 = webROjs.process(_webr_editor_12, {});\n"},{"inline":false,"cellName":"webr-widget-11","methodName":"interpretQuiet","source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_11;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"inline":false,"cellName":"webr-11","methodName":"interpret","source":"viewof _webr_editor_11 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-11-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-11-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_11 = webROjs.process(_webr_editor_11, {});\n"},{"inline":false,"cellName":"webr-widget-10","methodName":"interpretQuiet","source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_10;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"inline":false,"cellName":"webr-10","methodName":"interpret","source":"viewof _webr_editor_10 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-10-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-10-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_10 = webROjs.process(_webr_editor_10, {});\n"},{"inline":false,"cellName":"webr-widget-9","methodName":"interpretQuiet","source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_9;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"inline":false,"cellName":"webr-9","methodName":"interpret","source":"viewof _webr_editor_9 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-9-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-9-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_9 = webROjs.process(_webr_editor_9, {});\n"},{"inline":false,"cellName":"webr-widget-8","methodName":"interpretQuiet","source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_8;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"inline":false,"cellName":"webr-8","methodName":"interpret","source":"viewof _webr_editor_8 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-8-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-8-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_8 = webROjs.process(_webr_editor_8, {});\n"},{"inline":false,"cellName":"webr-widget-7","methodName":"interpretQuiet","source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_7;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"inline":false,"cellName":"webr-7","methodName":"interpret","source":"viewof _webr_editor_7 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-7-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-7-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_7 = webROjs.process(_webr_editor_7, {});\n"},{"inline":false,"cellName":"webr-widget-6","methodName":"interpretQuiet","source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_6;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"inline":false,"cellName":"webr-6","methodName":"interpret","source":"viewof _webr_editor_6 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-6-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-6-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_6 = webROjs.process(_webr_editor_6, {});\n"},{"inline":false,"cellName":"webr-widget-5","methodName":"interpretQuiet","source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_5;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"inline":false,"cellName":"webr-5","methodName":"interpret","source":"viewof _webr_editor_5 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-5-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-5-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_5 = webROjs.process(_webr_editor_5, {});\n"},{"inline":false,"cellName":"webr-widget-4","methodName":"interpretQuiet","source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_4;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"inline":false,"cellName":"webr-4","methodName":"interpret","source":"viewof _webr_editor_4 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-4-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-4-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_4 = webROjs.process(_webr_editor_4, {});\n"},{"inline":false,"cellName":"webr-widget-3","methodName":"interpretQuiet","source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_3;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"inline":false,"cellName":"webr-3","methodName":"interpret","source":"viewof _webr_editor_3 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-3-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-3-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_3 = webROjs.process(_webr_editor_3, {});\n"},{"inline":false,"cellName":"webr-widget-2","methodName":"interpretQuiet","source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_2;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"inline":false,"cellName":"webr-2","methodName":"interpret","source":"viewof _webr_editor_2 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-2-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-2-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_2 = webROjs.process(_webr_editor_2, {});\n"},{"inline":false,"cellName":"webr-widget-1","methodName":"interpretQuiet","source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_1;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"inline":false,"cellName":"webr-1","methodName":"interpret","source":"viewof _webr_editor_1 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-1-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-1-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_1 = webROjs.process(_webr_editor_1, {});\n"},{"inline":false,"cellName":"webr-prelude","methodName":"interpretQuiet","source":"webROjs = {\n  const { WebR } = window._exercise_ojs_runtime.WebR;\n  const {\n    WebREvaluator,\n    WebREnvironmentManager,\n    setupR,\n    b64Decode,\n    collapsePath\n  } = window._exercise_ojs_runtime;\n\n  const statusContainer = document.getElementById(\"exercise-loading-status\");\n  const indicatorContainer = document.getElementById(\"exercise-loading-indicator\");\n  indicatorContainer.classList.remove(\"d-none\");\n\n  let statusText = document.createElement(\"div\")\n  statusText.classList = \"exercise-loading-details\";\n  statusText = statusContainer.appendChild(statusText);\n  statusText.textContent = `Initialise`;\n\n  // Hoist indicator out from final slide when running under reveal\n  const revealStatus = document.querySelector(\".reveal .exercise-loading-indicator\");\n  if (revealStatus) {\n    revealStatus.remove();\n    document.querySelector(\".reveal > .slides\").appendChild(revealStatus);\n  }\n\n  // webR supplemental data and options\n  const dataContent = document.querySelector(`script[type=\\\"webr-data\\\"]`).textContent;\n  const data = JSON.parse(b64Decode(dataContent));\n\n  // Grab list of resources to be downloaded\n  const filesContent = document.querySelector(`script[type=\\\"vfs-file\\\"]`).textContent;\n  const files = JSON.parse(b64Decode(filesContent));\n\n  // Initialise webR and setup for R code evaluation\n  let webRPromise = (async (webR) => {\n    statusText.textContent = `Downloading webR`;\n    await webR.init();\n\n    // Install provided list of packages\n    // Ensure webR default repo is included\n    data.packages.repos.push(\"https://repo.r-wasm.org\")\n    await data.packages.pkgs.map((pkg) => () => {\n      statusText.textContent = `Downloading package: ${pkg}`;\n      return webR.evalRVoid(`\n        webr::install(pkg, repos = repos)\n        library(pkg, character.only = TRUE)\n      `, { env: {\n        pkg: pkg,\n        repos: data.packages.repos,\n      }});\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n\n    // Download and install resources\n    await files.map((file) => async () => {\n      const name = file.substring(file.lastIndexOf('/') + 1);\n      statusText.textContent = `Downloading resource: ${name}`;\n      const response = await fetch(file);\n      if (!response.ok) {\n        throw new Error(`Can't download \\`${file}\\`. Error ${response.status}: \"${response.statusText}\".`);\n      }\n      const data = await response.arrayBuffer();\n\n      // Store URLs in the cwd without any subdirectory structure\n      if (file.includes(\"://\")) {\n        file = name;\n      }\n\n      // Collapse higher directory structure\n      file = collapsePath(file);\n\n      // Create directory tree, ignoring \"directory exists\" VFS errors\n      const parts = file.split('/').slice(0, -1);\n      let path = '';\n      while (parts.length > 0) {\n        path += parts.shift() + '/';\n        try {\n          await webR.FS.mkdir(path);\n        } catch (e) {\n          if (!e.message.includes(\"FS error\")) {\n            throw e;\n          }\n        }\n      }\n\n      // Write this file to the VFS\n      return await webR.FS.writeFile(file, new Uint8Array(data));\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n\n    statusText.textContent = `Installing webR shims`;\n    await webR.evalRVoid(`webr::shim_install()`);\n\n    statusText.textContent = `WebR environment setup`;\n    await setupR(webR, data);\n\n    statusText.remove();\n    if (statusContainer.children.length == 0) {\n      statusContainer.parentNode.remove();\n    }\n    return webR;\n  })(new WebR(data.options));\n\n  // Keep track of initial OJS block render\n  const renderedOjs = {};\n\n  const process = async (context, inputs) => {\n    const webR = await webRPromise;\n    const evaluator = new WebREvaluator(webR, context)\n    await evaluator.process(inputs);\n    return evaluator.container;\n  }\n\n  return {\n    process,\n    webRPromise,\n    renderedOjs,\n  };\n}\n"}]}
</script><div id="exercise-loading-indicator" class="exercise-loading-indicator d-none d-flex align-items-center gap-2">
<div id="exercise-loading-status" class="d-flex gap-2">

</div>
<div class="spinner-grow spinner-grow-sm">

</div>
</div>
<script type="vfs-file">
W10=
</script><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Breimanetal1984" class="csl-entry" role="listitem">
Breiman, L., J. Friedman, C. J. Stone, und R. A. Olshen. 1984. <em>Classification and Regression Trees</em>. Taylor &amp; Francis.
</div>
<div id="ref-Hastieetal2013" class="csl-entry" role="listitem">
Hastie, T., R. Tibshirani, und J. Friedman. 2013. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Springer Series in Statistics. Springer New York.
</div>
</div>
</section></main><!-- /main --><script type="ojs-module-contents">
eyJjb250ZW50cyI6W119
</script><script type="module">
if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
window._ojs.paths.runtimeToDoc = "../..";
window._ojs.paths.runtimeToRoot = "../..";
window._ojs.paths.docToRoot = "";
window._ojs.selfContained = false;
window._ojs.runtime.interpretFromScriptTags();
</script><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Kopiert");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Kopiert");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./svm.html" class="pagination-link" aria-label="Support Vector Machines">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Machine Learning.html" class="pagination-link" aria-label="Neuronale Netzwerke">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Neuronale Netzwerke</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Quellcode</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span><span class="co"> </span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">  live-html:</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">    webr: </span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">      packages:</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'baguette'</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'cowplot'</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'gbm'</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'dplyr'</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'ggplot2'</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'ggRandomForests'</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'MASS'</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'purrr'</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'randomForest'</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'rattle'</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'tidymodels'</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'tidyr'</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co">      cell-options:</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co">        fig-width: 8</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="an">engine:</span><span class="co"> knitr</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>{{&lt; include ./_extensions/r-wasm/live/_knitr.qmd &gt;}}</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="fu"># Baum-basierte Methoden {#sec-trees}</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>Baum-basierte Methoden bieten eine vielseitige und leistungsstarke Herangehensweise für Vorhersage und Klassifikation in komplexen Datensätzen mit nicht-linearen Zusammenhängen. Ein Vorteil baum-basierter Methoden ist ihre inhärente Fähigkeit, die Bedeutung einzelner Variablen für die Vorhersage zu quantifizieren – eine Eigenschaft, die viele Machine-Learning-Modelle nicht ohne weiteres bieten und insbesondere in hoch-dimensionalen Anwendungen (mit vielen potentiellen Regressoren) nicht trivial ist. Dies ermöglicht es, tiefere Einblicke in den Einfluss einzelner Merkmale auf die Vorhersagen des Modells zu erhalten, was besonders in empirischen Anwendungen für die Entscheidungsstützung mit Machine Learning hilfreich sein kann.</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>*Entscheidungsbäume* stellen die Grundlage dieser Methoden dar. Sie ermöglichen die Aufteilung der Daten in immer kleinere, homogenere Gruppen, basierend auf *binären* Entscheidungsregeln, die aus den Prädiktoren abgleitet werden. Die trainierten Regeln eines solchen Modells lassen sich anhand eines Binärbaums visualisieren, was eine intuitive Interpretierbarkeit der Ergebnisse erlaubt. </span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>*Random Forests* ist ein Ensemble-Ansatz, bei dem viele Entscheidungsbäume kombiniert werden. Jeder Baum wird auf einer zufälligen Teilmenge der Daten trainiert (*Bagging*), und bei jedem Knoten wird zusätzlich eine zufällige Teilmenge der Merkmale berücksichtigt. Die finale Vorhersage des Random Forests basiert auf der Aggregation der Vorhersagen aller Bäume (Mehrheitsvotum für Klassifikation, Durchschnitt für Regression). Dieses Verfahren reduziert das Risiko einer Überanpassung und erhöht oft die Vorhersagegenauigkeit im Vergleich zu einzelnen Entscheidungsbäumen.</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>*Boosting* ist eine weitere Ensemble-Methode zur Anpassung von Modellen mit hoher Vorhersagegüte durch Kombination einfacher Modelle (*Base learner*), wobei Regressions- oder Klassifikationsbäume eingesetzt werden können. Alternativ zu Random Forests trainieren Boosting-Algorithmen sukzessiv einfache (Klassifikations- oder Regressions-)Bäume, wobei jeder nachfolgende Baum das Ziel hat, die Vorhersagefehler der vorherigen Bäume zu korrigieren. </span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>In diesem Kapitel erläutern wir die Anwendung baum-basierter Methoden in R anhand von Beispieldatensätzen. Wir zeigen, wie Regressionsbäume, Random Forests und Boosting-Modelle im <span class="in">`parsnip`</span>-Framework trainiert werden und wie die Vorhersageleistung durch die Wahl geeigneter Hyperparameter mit Cross-Validation und Out-of-Sample-Evaluierungsmethoden optimiert werden kann.</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="fu">## Entscheidungsbäume {#sec-simpletrees}</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>Ein Entscheidungsbaum ist ein Modell, das auf der Basis von hierarchischen Bedingungen bzgl. der Regressoren Vorhersagen für die Outcome-Variable trifft. Jeder Baum beginnt mit einem Wurzelknoten (*root node*) und verzweigt sich binär. Jede Verzweigung (*split*) stellt eine Bedingung dar, die auf einem bestimmten Regressor basiert. Der Baum trifft Entscheidungen, indem er diese Bedingungen sukzessive überprüft, bis er zu einem Blattknoten (*leaf node* / *terminal node*) gelangt, der die finale Vorhersage liefert. Hierbei handelt es sich eine Mehrheitsentscheidung für Klassifikation und einen Mittelwert, jeweils gebildet anhand Beobachten des Trainingsdatensatzes im leaf node.</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>@fig-exdectree zeigt ein einfaches Beispiel eines Entscheidungsbaums zur Klassifikation der Kreditwürdigkeit einer Person. Die Klassfikation erfolgt, in dem die Beobachtung basierend auf den Merkmalen Alter, Einkommen und Eigentum durch den Baum geleitet wird. Zunächst wird geprüft, die Person 30 Jahre oder jünger ist. Fall ja, entscheidet der Baum anhand des Einkommens: Bei einem Jahreseinkommen von 40.000 oder weniger wird die Person als wenig kreditwürdig klassifiziert, bei höherem Einkommen als mäßig kreditwürdig. Für Personen älter als 30 Jahre überprüft das Modell lediglich, ob die Person eine Immobilie besitzt, um zwischen mäßiger Kreditwürdigkeit und guter Bonität zu unterscheiden.</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="in">```{dot}</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="in">//| fig-width: 6</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="in">//| fig-height: 5</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="in">//| fig-cap: "Entscheidungsbaum: Klassifikation von Kreditwürdigkeit"</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a><span class="in">//| label: fig-exdectree</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="in">digraph exdectree {</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a><span class="in">    node [shape=box];</span></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="in">    splines=false;</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="in">    ranksep = 1;</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="in">    nodesep = 1.75;</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="in">    margin = 0.15;</span></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a><span class="in">    1 [label="Alter &lt;= 30?"];</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a><span class="in">    2 [label="Einkommen &lt;= 40 Tsd.?"];</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a><span class="in">    3 [label="Eigentum?"];</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="in">    4 [label="Status: Niedrig"];</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a><span class="in">    5 [label="Status: Mittel"];</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a><span class="in">    6 [label="Status: Hoch"];</span></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="in">    1 -&gt; 2 [label="Ja"];</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="in">    1 -&gt; 3 [label="Nein"];</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="in">    2 -&gt; 4 [label="Ja"];</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a><span class="in">    2 -&gt; 5 [label="Nein"];</span></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a><span class="in">    3 -&gt; 6 [label="Ja"];</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a><span class="in">    3 -&gt; 5 [label="Nein"];</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a><span class="fu">## Training von Bäumen</span></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>Zur Konstruktion von Binär-Bäumen werden etablierte Algorithmen wie *Classification and Regression Trees* (<span class="co">[</span><span class="ot">CART</span><span class="co">]</span>(https://de.wikipedia.org/wiki/CART_(Algorithmus)) von @Breimanetal1984 verwendet. Die wesentliche Vorgehensweise für das Training eines Baums $T$ ist wie folgt:</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Splitting**: Beginnend am root node sucht der Algorithmus nach der "besten" Regel, die Daten anhand eines Merkmals in zwei Gruppen zu teilen. Die Qualität des Splits wird in Abhängigkeit der Definition der Outcome-Variable beurteilt:</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>**Bei Klassifikation**: Die Reinheit (*purtity*) der Klassen in den unmittelbar nachfolgen nodes wird maximiert. Ein gängiges Kriterium hierfür ist der [*Gini-Koeffizient*](https://de.wikipedia.org/wiki/Gini-Koeffizient).^<span class="co">[</span><span class="ot">Der Gini-Koeffizient $0\leq G\leq1$ misst die Homogenität der Outcome-Variable für die Beobachtungen eines Knotens. $G=0$ ergibt sich bei vollständiger "Reinheit" (alle Beobachtungen im Knoten gehören zur gleichen Klasse). $G &gt; 0$ zeigt Heterogenität der Klassen an, die mit $G$ zunimmt</span><span class="co">]</span></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>**Bei Regression**: Die Fehlerquadratsumme bei Vorhersage des Outcomes durch Mittelwertbildung für Beobachtungen in den unmittelbar nachfolgenden nodes wird minimiert.</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Rekursion**: Der Prozess wird rekursiv fortgesetzt, bis Abbruchkriterien greifen eine weitere Verzewigung verhindern:</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Die maximale Baumtiefe (*tree depth*) ist erreicht </span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Die leaf nodes sind hinreichend "rein": Alle Beobachtungen in einem leaf node gehören zur gleichen Klasse oder die Verbesserung des Loss durch weitere Splits fällt unter einen festgelegten Schwellenwert</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Weitere Splits führen zu leaf nodes, die eine Mindestanzahl an Beobachtungen (*minimum split*) unterschreiten würden</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Pruning**: Um Überanpassung an die Trainingsdaten zu vermeiden, kann der Baum beschnitten werden (*pruning*). Der Grundgedanke ist, dass tief verzweigte Bäume die Trainingsdaten zwar gut modellieren können, aber schlecht auf neue, unbekannte Daten generalisieren. </span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>    Bei *cost complexity (CP) pruning* werden, beginnend auf Ebene der leaf nodes sukuzessive Äste entfernt, und eine Balance zwischen Komplexität des Baums und dem Anpassungsfehler zu finden. Ähnlich wie bei regularisierter KQ-Schätzung (@sec-regreg), wird die Verlustfunktion $L$ um einen Strafterm für die Komplexität erweitert. Der Effekt der Strafe wird durch den CP-Parameter $\alpha\in<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ geregelt,</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>    \begin{align*}</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>      L_{\alpha}(T) = L(T) + \alpha \lvert T\rvert,</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>    \end{align*}</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>  für einen Baum $T$ mit Komplexitätsmaß $\lvert T\rvert$ (Anzahl der leaf nodes) <span class="co">[</span><span class="ot">@Hastieetal2013</span><span class="co">]</span>.</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>Zur Demonstation der Schätzung von Regressionsbäumen mit R betrachten wir nachfolgend den Datensatz <span class="in">`MASS::Bosten`</span>. Ziel hierbei ist es, mittlere Hauswerte <span class="in">`medv`</span> in Stadteilen von Boston, MA vorherzusagen. Wir verwenden hierzu Funktionen aus dem Paket <span class="in">`parsnip`</span>. </span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>Zunächst transformieren wir den Datensatz in ein <span class="in">`tibble`</span>-Objekt und definieren Trainings- und Test-Daten.</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a><span class="in">library(parsnip)</span></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a><span class="in">library(cowplot)</span></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a><span class="in"># Seed setzen</span></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(1234)</span></span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a><span class="in"># Datensatz als tibble</span></span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a><span class="in">Boston &lt;- as_tibble(MASS::Boston)</span></span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a><span class="in"># Splitting in Training- und Test-Daten</span></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a><span class="in">Boston_split &lt;- initial_split(</span></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a><span class="in">  data = Boston, </span></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a><span class="in">  prop = 0.8, </span></span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a><span class="in">  strata = medv</span></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a><span class="in">Boston_train &lt;- training(Boston_split)</span></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a><span class="in">Boston_test &lt;- testing(Boston_split)</span></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a><span class="in">slice_head(Boston_train, n = 10)</span></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a><span class="in">`parsnip`</span> bietet eine vereinheitlichetes Framework für das Training von Modellen mit R und eine flexible API für Machine Learning. Wir definieren zunächst mit <span class="in">`parsnip::decision_tree()`</span> eine Spezifikation zum Training von Entschieundgsmodellen und übergeben beispielhaft einen CP-Parameter $\alpha=.1$. Mit <span class="in">`parsnip::set_engine`</span> wählen wir das Paket <span class="in">`raprt`</span>. Der hier implementierte Agorithmus ist CART. Zuletzt legen wir mit <span class="in">` parsnip::set_mode()`</span> fest, dass der Algorithmus für Regression durchgeführt werden soll.</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a><span class="in"># Spezifikation festlegen</span></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a><span class="in">tree_spec &lt;- decision_tree(</span></span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a><span class="in">  cost_complexity = 0.1</span></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a><span class="in">  set_engine("rpart") %&gt;%</span></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a><span class="in">  set_mode("regression")</span></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a><span class="in"># Modell trainieren</span></span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a><span class="in">tree_fit &lt;- tree_spec %&gt;%</span></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a><span class="in">  fit(</span></span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a><span class="in">    formula = medv ~ ., </span></span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a><span class="in">    data = Boston_train, </span></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a><span class="in">    model = TRUE</span></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a><span class="in"># Trainierten Baum in Konsole ausgeben</span></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a><span class="in">tree_fit$fit</span></span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>Der Output in <span class="in">`tree_fit$fit`</span> zeigt, dass CP-Pruning zu einem kleinen Baum mit 3 Hierarchie-Ebenen geführt hat. Die Struktur zeigt, dass <span class="in">`lstat`</span> und <span class="in">`rm`</span> für Splitting-Regeln (<span class="in">`split`</span>) verwendet werden, wie viele Beobachtungen den  nodes zugeordnet sind (<span class="in">`n`</span>), den Wert der Verlustfunktion (<span class="in">`deviance`</span>) sowie den Durchschnitt von <span class="in">`medv`</span> für jede node (<span class="in">`yval`</span>). Für die drei leaf nodes (gekennzeichnet mit <span class="in">`*`</span>) ist <span class="in">`yval`</span> die Vorhersage der Outcome-Varibale für entsprechend gruppierte Beobachtungen.</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>Eine besser interpretierbare Darstellung des angepassten Baums in <span class="in">`tree_fit$fit`</span>  erhalten wir mit <span class="in">`rattle::fancyRpartPlot()`</span>.</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-width: 8</span></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-height: 8</span></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a><span class="in">library(rattle)</span></span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a><span class="in"># Plot the decision tree</span></span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a><span class="in">fancyRpartPlot(</span></span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a><span class="in">  tree_fit$fit,</span></span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a><span class="in">  split.col = "black", </span></span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a><span class="in">  nn.col = "black", </span></span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a><span class="in">  caption = "Trainierter Entscheidungsbaum für cp = 0.1",</span></span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a><span class="in">  palette = "Set1",</span></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a><span class="in">  branch.col = "black"</span></span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>Für eine datengetriebene Wahl des CP-Parameters $\alpha$ kann Cross Validation (CV) verwendet werden. Hierzu erstellen wir zunächst eine <span class="in">`parsnip`</span>-Spezifikation mit <span class="in">`cost_complexity = tune::tune()`</span> in <span class="in">`decision_tree()`</span> und erstellen einen *workflow* mit <span class="in">`parsnip::workflow()`</span></span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a><span class="in"># Spezifikation für CV von cost_complexity</span></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a><span class="in">tree_spec_cv &lt;- decision_tree(</span></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a><span class="in">  cost_complexity = tune()</span></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a><span class="in">  set_engine("rpart") %&gt;%</span></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a><span class="in">  set_mode("regression")</span></span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a><span class="in"># Workflow definieren</span></span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a><span class="in">tree_wf_cv &lt;- workflow() %&gt;%</span></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a><span class="in">  add_model(tree_spec_cv) %&gt;%</span></span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a><span class="in">  add_formula(medv ~ .)</span></span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>Mit <span class="in">`rsample::vfold_cv()`</span> definieren wir den CV-Prozess: 10-fold CV mit 2 Wiederholungen. <span class="in">`tune::tune_grid()`</span> führt CV anhand des in <span class="in">`tree_wf_cv`</span> definierten workflows durch. Hierbei werden in <span class="in">`cp_grid`</span> festgelegte Werte von <span class="in">`cost_complexity`</span> berücksichtigt. Die mit <span class="in">`yardstick::metric_set(rmse)`</span> festgelegte Verlustfunktion ist der mittlere quadratische Fehler (RMSE).^<span class="co">[</span><span class="ot">Die hier verwedete Funktion ist `yardstick::rmse()`.</span><span class="co">]</span></span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a><span class="in"># CV-Prozess definieren</span></span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a><span class="in">cv_folds &lt;- vfold_cv(</span></span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a><span class="in">  data = Boston_train, </span></span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a><span class="in">  v = 10, </span></span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a><span class="in">  repeats = 2</span></span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a><span class="in"># CV durchführen:</span></span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(1234)</span></span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a><span class="in"># Grid definieren</span></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a><span class="in">cp_grid &lt;- tibble(</span></span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a><span class="in">  cost_complexity = c(</span></span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a><span class="in">    0.1, .075, 0.05, 0.01, 0.001, 0.0001</span></span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a><span class="in">    )</span></span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a><span class="in"># Tuning mit CV</span></span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a><span class="in">tree_fit_cv &lt;- tree_wf_cv %&gt;%</span></span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a><span class="in">    tune_grid(</span></span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a><span class="in">        resamples = cv_folds, </span></span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a><span class="in">        grid = cp_grid,</span></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a><span class="in">        metrics = metric_set(rmse)</span></span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a><span class="in">    )</span></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a><span class="in"># CV-Ergebnisse</span></span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a><span class="in">tree_fit_cv</span></span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a>Mit <span class="in">`workflowsets::autoplot()`</span> kann der CV-RMSE für als Funktion des CP-Parameter leicht grafisch betrachtet dargestellt werden.</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a><span class="in"># CV-Ergebnisse visualisieren</span></span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a><span class="in">autoplot(tree_fit_cv) +</span></span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a><span class="in">  labs(</span></span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a><span class="in">    title = "CV für CP-Parameter: RMSE vs. Komplexität"</span></span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a><span class="in">  ) +</span></span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a><span class="in">  theme_cowplot()</span></span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>Für eine tabellierte Übersicht der besten Modelle kann <span class="in">`tune::show_best()`</span> verwendet werden. <span class="in">`tune::select_best()`</span> liest die beste Parameter-Kombination aus. </span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a><span class="in"># Tabellarische Übersicht</span></span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a><span class="in">show_best(</span></span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a><span class="in">  x = tree_fit_cv, </span></span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a><span class="in">  metric = "rmse"</span></span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a><span class="in"># Getunter Paremater</span></span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a><span class="in">best_tree_fit &lt;- select_best(</span></span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a><span class="in">  x = tree_fit_cv, </span></span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a><span class="in">  metric = "rmse"</span></span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a><span class="in">best_tree_fit</span></span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a>Anhand <span class="in">`tree_fit_cv`</span> trainieren wir die finale Spezifikation.</span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a><span class="in"># Finales Modell schätzen</span></span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a><span class="in">final_tree_spec &lt;- decision_tree(</span></span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a><span class="in">  cost_complexity = best_tree_fit$cost_complexity</span></span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a><span class="in">  set_engine("rpart") %&gt;%</span></span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a><span class="in">  set_mode("regression")</span></span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a><span class="in">final_tree_fit &lt;- final_tree_spec %&gt;%</span></span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a><span class="in">  fit(</span></span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a><span class="in">    formula = medv ~ ., </span></span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a><span class="in">    data = Boston_train</span></span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a><span class="in"># final_tree_fit</span></span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a>Der geringe CP-Parameter führt zu einem großen Entscheidungsbaum.^<span class="co">[</span><span class="ot">Die Dimension der Grafik wurde hier zwecks Darstellung des gesamten Baums gewählt. `print(final_tree_fit$fit)` druckt die Entscheidungsregeln in die R-Konsole (hierzu die letzte Zeile ausführen).</span><span class="co">]</span> </span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-width: 8</span></span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-height: 8</span></span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a><span class="in"># CV-Fit plotten</span></span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a><span class="in">fancyRpartPlot(</span></span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a><span class="in">  final_tree_fit$fit,</span></span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a><span class="in">  split.col = "black", </span></span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a><span class="in">  nn.col = "black", </span></span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a><span class="in">  caption = "Mit CV ermittelter Entscheidungsbaum",</span></span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a><span class="in">  palette = "Set1",</span></span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a><span class="in">  branch.col = "black"</span></span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a>Zur Beurteilung der Relevanz von Variablen für die Reduktion des Anpassungsfehlers (*variable importance*) kann der Eintrag <span class="in">`variable.importance`</span> des <span class="in">`rpart`</span>-Objekts herangezogen werden. Variable importance misst hier die Gesamtreduktion der Fehlerquadratsumem über alle Knoten, an denen die jeweilige Variable für Splits verwendet wird. </span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a><span class="in"># Variable-Importance auslesen</span></span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a><span class="in">final_tree_fit$fit$variable.importance</span></span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a>Die Werte von Variable Importance zeigen, dass der mit CV ermittelte Baum *alle* Regressoren in <span class="in">`boston_train`</span> für Splits nutzt, wobei <span class="in">`lstat`</span> und <span class="in">`rm`</span> die relevantesten Variablen sind.</span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a>Anhand von Vorhersagen für <span class="in">`medv`</span> mit dem Test-Datensatz <span class="in">`boston_test`</span> können wir das naive Baum-Modell <span class="in">`tree_fit`</span> mit dem durch CV ermittelten Modell <span class="in">`tree_fit_cv`</span> hinsichtlich des Vorhersagefehlers für ungesehene Beobachtungen vergleich. <span class="in">`yardstick::metric()`</span> berechnet hierzu automatisch gängige Statistiken für Regressionsprobleme. </span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a><span class="in"># Vorhersagegüte naives Modell</span></span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a><span class="in">tree_pred &lt;- predict(</span></span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a><span class="in">  object = tree_fit, </span></span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a><span class="in">  new_data = Boston_test</span></span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a><span class="in">) %&gt;%</span></span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a><span class="in">  bind_cols(Boston_test) %&gt;%</span></span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a><span class="in">  metrics(truth = medv, estimate = .pred)</span></span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a><span class="in"># Vorhersagegüte bei CV</span></span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a><span class="in">tree_pred_cv &lt;- predict(</span></span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a><span class="in">  object = final_tree_fit, </span></span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a><span class="in">  new_data = Boston_test</span></span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a><span class="in">) %&gt;%</span></span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a><span class="in">  bind_cols(Boston_test) %&gt;%</span></span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a><span class="in">  metrics(truth = medv, estimate = .pred)</span></span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a><span class="in">tree_pred</span></span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a><span class="in">tree_pred_cv</span></span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a>Der Vergleich zeigt eine bessere Vorsageleistung des großen Baums in <span class="in">`tree_fit_cv`</span>. In diesem Fall scheint CP-Pruning wenig hilfreich zu sein. Tatsächlich liefert ein Baum mit $\alpha=0$ bessere Vorhersagen als <span class="in">`tree_fit_cv`</span> (überprüfe dies!). </span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bagging</span></span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a>*Bagging* ist eine Ensemble-Modelle, die durch aus einer Kombination von vielen Entscheidungsbäumen bestehen. Bagging steht für *Bootstrap Aggregating* und nutzt einen Algorithmus, bei dem Bäume auf *zufälligen* Stichproben aus dem Trainingsdatensatz angepasst werden: Jeder Baum wird auf einer *Bootstrap-Stichprobe* (siehe @sec-sim) trainiert, die durch zufällige Züge (mit Zurücklegen) erstellt wird. Nach dem Training aggregiert Bagging die Vorhersagen aller Bäume des Ensembles.</span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a>Der Vorteil von Bagging gegenüber einem einzelnen Entscheidungsbaum ist, dass die Varianz der Vorhersage deutlich reduziert werden kann: Einzelne Entscheidungsbäume neigen dazu, Muster in den Trainingsdaten zu lernen, die sich zufällig aus der Zusammensetzung der Stichprobe ergeben und nicht repräsentativ für Zusammenhänge zwischen den Regressoren und der Outcome-Variable sind. Diese Überanpassung führt zu hoher Varianz auf von Vorhersagen für ungesehene Daten. Durch das Training vieler Bäume auf unterschiedlichen *zufälligen* Stichproben aus den Trainingsdaten und das anschließende Aggregieren kann der negative Effekt der Überanpassung auf die Unsicherheit der Vorhersage einzelner Bäume reduziert werden.</span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a>Eine Bagging-Spezifikation kann mit <span class="in">`parsnip::bag_tree()`</span> festgelegt werden. Mit <span class="in">`times = 500`</span> wird definiert, dass der Bagging-Algorithmus ein Ensemble mit 500 Bäumen (mit CART) anpassen soll. Das Training und die Vorhersage auf den Testdaten erfolgt analog zur Vorgehensweise in @sec-simpletrees.</span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a><span class="in"># Spezifikation für Bagging</span></span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a><span class="in">bagging_spec &lt;- bag_tree() %&gt;%</span></span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a><span class="in">  set_engine(</span></span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a><span class="in">    engine = "rpart",</span></span>
<span id="cb1-354"><a href="#cb1-354" aria-hidden="true" tabindex="-1"></a><span class="in">    times = 500</span></span>
<span id="cb1-355"><a href="#cb1-355" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb1-356"><a href="#cb1-356" aria-hidden="true" tabindex="-1"></a><span class="in">  set_mode("regression")</span></span>
<span id="cb1-357"><a href="#cb1-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-358"><a href="#cb1-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-359"><a href="#cb1-359" aria-hidden="true" tabindex="-1"></a><span class="in"># Training durchführen</span></span>
<span id="cb1-360"><a href="#cb1-360" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(1234)</span></span>
<span id="cb1-361"><a href="#cb1-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-362"><a href="#cb1-362" aria-hidden="true" tabindex="-1"></a><span class="in">bagging_fit &lt;- bagging_spec %&gt;%</span></span>
<span id="cb1-363"><a href="#cb1-363" aria-hidden="true" tabindex="-1"></a><span class="in">  fit(</span></span>
<span id="cb1-364"><a href="#cb1-364" aria-hidden="true" tabindex="-1"></a><span class="in">    formula = medv ~ ., </span></span>
<span id="cb1-365"><a href="#cb1-365" aria-hidden="true" tabindex="-1"></a><span class="in">    data = Boston_train</span></span>
<span id="cb1-366"><a href="#cb1-366" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb1-367"><a href="#cb1-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-368"><a href="#cb1-368" aria-hidden="true" tabindex="-1"></a><span class="in"># Auswertung</span></span>
<span id="cb1-369"><a href="#cb1-369" aria-hidden="true" tabindex="-1"></a><span class="in">bagging_pred &lt;- predict(</span></span>
<span id="cb1-370"><a href="#cb1-370" aria-hidden="true" tabindex="-1"></a><span class="in">  object = bagging_fit, </span></span>
<span id="cb1-371"><a href="#cb1-371" aria-hidden="true" tabindex="-1"></a><span class="in">  new_data = Boston_test</span></span>
<span id="cb1-372"><a href="#cb1-372" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb1-373"><a href="#cb1-373" aria-hidden="true" tabindex="-1"></a><span class="in">  bind_cols(Boston_test) %&gt;%</span></span>
<span id="cb1-374"><a href="#cb1-374" aria-hidden="true" tabindex="-1"></a><span class="in">  metrics(</span></span>
<span id="cb1-375"><a href="#cb1-375" aria-hidden="true" tabindex="-1"></a><span class="in">    truth = medv,</span></span>
<span id="cb1-376"><a href="#cb1-376" aria-hidden="true" tabindex="-1"></a><span class="in">    estimate = .pred</span></span>
<span id="cb1-377"><a href="#cb1-377" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb1-378"><a href="#cb1-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-379"><a href="#cb1-379" aria-hidden="true" tabindex="-1"></a><span class="in">bagging_pred</span></span>
<span id="cb1-380"><a href="#cb1-380" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-381"><a href="#cb1-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-382"><a href="#cb1-382" aria-hidden="true" tabindex="-1"></a>Die Auswertung auf den Testdatensatz ergibt eine deutliche Verbesserung der Vorhersageleistung gegenüber einem einfachen Regressionsbaum.</span>
<span id="cb1-383"><a href="#cb1-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-384"><a href="#cb1-384" aria-hidden="true" tabindex="-1"></a>Obwohl die Bäume beim Bagging auf unterschiedlichen Stichproben trainiert werden, kann innerhalb des Ensembles dennoch eine deutliche Korrelation vorliegen: Da jeder Baum auf alle Regressoren für Splits zugreift, können trotz Bootstrapping ähnliche (unverteilhafte) Muster aus dem Datensatz erlernt werden, was sich nachteilig auf die Generalisierungsfähigkeit auswirken kann. Diese Korrelation mindert die Effektivität von Bagging, da stark korrelierte Bäume dazu neigen, ähnliche Fehler zu machen.</span>
<span id="cb1-385"><a href="#cb1-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-386"><a href="#cb1-386" aria-hidden="true" tabindex="-1"></a><span class="fu">## Random Forests {#sec-brf}</span></span>
<span id="cb1-387"><a href="#cb1-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-388"><a href="#cb1-388" aria-hidden="true" tabindex="-1"></a>*Random Forests* erweitern Bagging, indem zusätzlich bei jedem Knoten innerhalb jedes Baumes eine *zufällige Teilmenge der Regressoren* als potentielle Variable für die Split-Regel ausgewählt wird. Dies führt zu einer Reduktion der Korrelation zwischen den Bäumen, was die Genauigkeit verbessert und das Risiko von Overfitting weiter verringert.</span>
<span id="cb1-389"><a href="#cb1-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-390"><a href="#cb1-390" aria-hidden="true" tabindex="-1"></a>In R erstellen wir die Spezifikation mit <span class="in">`parsnip::rand_forest()`</span>. Der Parameter <span class="in">`mtry`</span> legt fest, wie viele Regressoren $m$ zufällig für jeden Split zur Verfügung stehen. Wir nutzen den im <span class="in">`randomForest`</span>-Paket implementierten Algorithmus und legen in <span class="in">`set_engine()`</span> fest, dass die von <span class="in">`randomForest::randomForest()`</span> berechnete Fehler-Metrik im Output-Objekt ausgegeben wird (<span class="in">`tree.err = TRUE`</span>). Um die Spezifikation für verschiedene Werte von <span class="in">`mtry`</span> anwenden zu können, implementieren wir die Spezifikation innerhalb einer Wrapper-Funktion <span class="in">`rf_spec_mtry()`</span>. Mit <span class="in">`purrr::map()`</span> iterieren wir <span class="in">`rf_spec_mtry()`</span> über drei verschiedene Werte für den Tuning-Parameter <span class="in">`mtry`</span> (4, 6 und 10 Variablen).^<span class="co">[</span><span class="ot">Eine Faustregel für die Wahl von $m$ bei $k$ verfügbaren Regressoren ist $m\approx\sqrt{k}$.</span><span class="co">]</span></span>
<span id="cb1-391"><a href="#cb1-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-394"><a href="#cb1-394" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb1-395"><a href="#cb1-395" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(1234)</span></span>
<span id="cb1-396"><a href="#cb1-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-397"><a href="#cb1-397" aria-hidden="true" tabindex="-1"></a><span class="in"># Werte für mtry</span></span>
<span id="cb1-398"><a href="#cb1-398" aria-hidden="true" tabindex="-1"></a><span class="in">mtry_values &lt;- c(4, 6, 10)</span></span>
<span id="cb1-399"><a href="#cb1-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-400"><a href="#cb1-400" aria-hidden="true" tabindex="-1"></a><span class="in"># Funktion: Random Forest für mtry = m</span></span>
<span id="cb1-401"><a href="#cb1-401" aria-hidden="true" tabindex="-1"></a><span class="in">rf_spec_mtry &lt;- function(m) {</span></span>
<span id="cb1-402"><a href="#cb1-402" aria-hidden="true" tabindex="-1"></a><span class="in">  rand_forest(mtry = m, trees = 500) %&gt;%</span></span>
<span id="cb1-403"><a href="#cb1-403" aria-hidden="true" tabindex="-1"></a><span class="in">    set_engine(</span></span>
<span id="cb1-404"><a href="#cb1-404" aria-hidden="true" tabindex="-1"></a><span class="in">      engine = "randomForest", </span></span>
<span id="cb1-405"><a href="#cb1-405" aria-hidden="true" tabindex="-1"></a><span class="in">      tree.err = TRUE</span></span>
<span id="cb1-406"><a href="#cb1-406" aria-hidden="true" tabindex="-1"></a><span class="in">    ) %&gt;%</span></span>
<span id="cb1-407"><a href="#cb1-407" aria-hidden="true" tabindex="-1"></a><span class="in">    set_mode("regression")</span></span>
<span id="cb1-408"><a href="#cb1-408" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb1-409"><a href="#cb1-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-410"><a href="#cb1-410" aria-hidden="true" tabindex="-1"></a><span class="in"># Modelle für verschiedene mtry-Werte trainieren</span></span>
<span id="cb1-411"><a href="#cb1-411" aria-hidden="true" tabindex="-1"></a><span class="in">rf_fits &lt;- map(</span></span>
<span id="cb1-412"><a href="#cb1-412" aria-hidden="true" tabindex="-1"></a><span class="in">  .x = mtry_values, </span></span>
<span id="cb1-413"><a href="#cb1-413" aria-hidden="true" tabindex="-1"></a><span class="in">  .f = ~ rf_spec_mtry(.x) %&gt;%</span></span>
<span id="cb1-414"><a href="#cb1-414" aria-hidden="true" tabindex="-1"></a><span class="in">    fit(</span></span>
<span id="cb1-415"><a href="#cb1-415" aria-hidden="true" tabindex="-1"></a><span class="in">      formula = medv ~ ., </span></span>
<span id="cb1-416"><a href="#cb1-416" aria-hidden="true" tabindex="-1"></a><span class="in">      data = Boston_train</span></span>
<span id="cb1-417"><a href="#cb1-417" aria-hidden="true" tabindex="-1"></a><span class="in">    )</span></span>
<span id="cb1-418"><a href="#cb1-418" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb1-419"><a href="#cb1-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-420"><a href="#cb1-420" aria-hidden="true" tabindex="-1"></a><span class="in">rf_fits &lt;- set_names(</span></span>
<span id="cb1-421"><a href="#cb1-421" aria-hidden="true" tabindex="-1"></a><span class="in">  x = rf_fits,</span></span>
<span id="cb1-422"><a href="#cb1-422" aria-hidden="true" tabindex="-1"></a><span class="in">  nm =  paste0("rf_mtry", mtry_values, "_fit")</span></span>
<span id="cb1-423"><a href="#cb1-423" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb1-424"><a href="#cb1-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-425"><a href="#cb1-425" aria-hidden="true" tabindex="-1"></a><span class="in"># Ausgabe der Ergebnisse</span></span>
<span id="cb1-426"><a href="#cb1-426" aria-hidden="true" tabindex="-1"></a><span class="in">rf_fits</span></span>
<span id="cb1-427"><a href="#cb1-427" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-428"><a href="#cb1-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-429"><a href="#cb1-429" aria-hidden="true" tabindex="-1"></a>Für eine Beurteilung des Vorhersageleistung dieser drei Modelle können wir den *Out-of-Bag*-Fehler (OOB) verwenden: </span>
<span id="cb1-430"><a href="#cb1-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-431"><a href="#cb1-431" aria-hidden="true" tabindex="-1"></a>Der OOB-Fehler ist eine Schätzung des Generalisierungsfehlers ohne einen separaten Testdatensatzes. Bei Random Forests (und Bagging) ist dies aufgrund der Berechnung des Ensembles für Bootstrap-Stichproben möglich: Grob ein Drittel der Beobachtungen des Datensatzes sind nicht Teil der Stichprobe, die für das Training jedes Baums im Ensemble genereiert werden.^<span class="co">[</span><span class="ot">Beachte, dass beim Bootstrap $n$ aus $n$  Beobachtungen mit Zurücklegen gezogen werden. Die Wahrscheinlicht, dass eine Beobachtung *nicht* gezogen wird ("Out-of-Bag"), ist $(1-1/n)^n\approx37\%$.</span><span class="co">]</span> Diese nicht gezogenen Datenpunkte sind OOB-Beobachtungen. Der OOB-Fehler des Ensembles ist der durchschnittliche Fehler für die aggregierten Vorhersagen der Bäume des Forests.</span>
<span id="cb1-432"><a href="#cb1-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-433"><a href="#cb1-433" aria-hidden="true" tabindex="-1"></a>Der OOB-Fehler kann auch verwendet werden, um die erforderliche Größe des Random Forests zu beurteilen: Eine größere Anzahl von Bäumen reduziert tendenziell die Varianz der Vorhersagen und verbessert die Generalisierungsfähigkeit. Allerdings nimmt dieser Effekt ab, und ab einer bestimmten Baumanzahl sind weitere Verbesserungen marginal. Obwohl das Risiko von Überanpassung durch viele Bäume aufgrund des Bagging minimal ist, kann es bei großen Datensätzen sinnvoll sein, kleinere Wälder zu trainieren, um den Rechenaufwand zu verringern. Wir plotten hierfür den OOB-Fehler für das Modell mit <span class="in">`mtry = 10`</span> gegen die Anzahl der Bäume.</span>
<span id="cb1-434"><a href="#cb1-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-437"><a href="#cb1-437" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb1-438"><a href="#cb1-438" aria-hidden="true" tabindex="-1"></a><span class="in">library(ggRandomForests)</span></span>
<span id="cb1-439"><a href="#cb1-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-440"><a href="#cb1-440" aria-hidden="true" tabindex="-1"></a><span class="in"># OOB-Fehler als Funktion der Baumanzahl</span></span>
<span id="cb1-441"><a href="#cb1-441" aria-hidden="true" tabindex="-1"></a><span class="in">rf_fits$rf_mtry10_fit$fit %&gt;% </span></span>
<span id="cb1-442"><a href="#cb1-442" aria-hidden="true" tabindex="-1"></a><span class="in">  gg_error() %&gt;% </span></span>
<span id="cb1-443"><a href="#cb1-443" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb1-444"><a href="#cb1-444" aria-hidden="true" tabindex="-1"></a><span class="in">  plot() + </span></span>
<span id="cb1-445"><a href="#cb1-445" aria-hidden="true" tabindex="-1"></a><span class="in">  labs(</span></span>
<span id="cb1-446"><a href="#cb1-446" aria-hidden="true" tabindex="-1"></a><span class="in">    title = "Random Forest: Ensemblegröße vs. OOB-Fehler (mtry = 10)"</span></span>
<span id="cb1-447"><a href="#cb1-447" aria-hidden="true" tabindex="-1"></a><span class="in">  ) + </span></span>
<span id="cb1-448"><a href="#cb1-448" aria-hidden="true" tabindex="-1"></a><span class="in">  theme_cowplot()</span></span>
<span id="cb1-449"><a href="#cb1-449" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-450"><a href="#cb1-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-451"><a href="#cb1-451" aria-hidden="true" tabindex="-1"></a>Die Grafik zeigt, dass die Verbesserung des OOB-Fehlers jenseits von 250 Beobachtungen deutlich nachlässt, sodass ein Training von 500 Bäumen ausreichend scheint.</span>
<span id="cb1-452"><a href="#cb1-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-453"><a href="#cb1-453" aria-hidden="true" tabindex="-1"></a>Zur Beurteiliung der Vorhersagegüte mit dem Testdatensatz gehen wir analog zum Training vor und iterieren mit <span class="in">`map()`</span> über <span class="in">`rf_fits`</span>, die Liste der angepassten Modelle.</span>
<span id="cb1-454"><a href="#cb1-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-457"><a href="#cb1-457" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb1-458"><a href="#cb1-458" aria-hidden="true" tabindex="-1"></a><span class="in"># Vorhersage und Berechnung v. Metriken für jeden RF</span></span>
<span id="cb1-459"><a href="#cb1-459" aria-hidden="true" tabindex="-1"></a><span class="in">rf_predictions &lt;- map(</span></span>
<span id="cb1-460"><a href="#cb1-460" aria-hidden="true" tabindex="-1"></a><span class="in">  .x = rf_fits, </span></span>
<span id="cb1-461"><a href="#cb1-461" aria-hidden="true" tabindex="-1"></a><span class="in">  .f =  ~ predict(.x, Boston_test) %&gt;%</span></span>
<span id="cb1-462"><a href="#cb1-462" aria-hidden="true" tabindex="-1"></a><span class="in">    bind_cols(Boston_test) %&gt;%</span></span>
<span id="cb1-463"><a href="#cb1-463" aria-hidden="true" tabindex="-1"></a><span class="in">    metrics(</span></span>
<span id="cb1-464"><a href="#cb1-464" aria-hidden="true" tabindex="-1"></a><span class="in">      truth = medv, </span></span>
<span id="cb1-465"><a href="#cb1-465" aria-hidden="true" tabindex="-1"></a><span class="in">      estimate = .pred</span></span>
<span id="cb1-466"><a href="#cb1-466" aria-hidden="true" tabindex="-1"></a><span class="in">    )</span></span>
<span id="cb1-467"><a href="#cb1-467" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb1-468"><a href="#cb1-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-469"><a href="#cb1-469" aria-hidden="true" tabindex="-1"></a><span class="in"># Einträge benennen</span></span>
<span id="cb1-470"><a href="#cb1-470" aria-hidden="true" tabindex="-1"></a><span class="in">rf_predictions &lt;- set_names(</span></span>
<span id="cb1-471"><a href="#cb1-471" aria-hidden="true" tabindex="-1"></a><span class="in">  x = rf_predictions,</span></span>
<span id="cb1-472"><a href="#cb1-472" aria-hidden="true" tabindex="-1"></a><span class="in">  nm =  paste0("rf_mtry", mtry_values, "_pred")</span></span>
<span id="cb1-473"><a href="#cb1-473" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb1-474"><a href="#cb1-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-475"><a href="#cb1-475" aria-hidden="true" tabindex="-1"></a><span class="in">rf_predictions</span></span>
<span id="cb1-476"><a href="#cb1-476" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-477"><a href="#cb1-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-478"><a href="#cb1-478" aria-hidden="true" tabindex="-1"></a>Ähnlich wie für einen einzelnen Baum kann die Relevanz von Variablen anhand der Reduktion der Loss-Funktion durch das Ensemble beurteilt werden. Für einen einfachen Vergleich der Variable Importance für den Random Forests mit <span class="in">`mtry = 10`</span> in <span class="in">`rf_fits$rf_mtry10_fit`</span> nutzen wir <span class="in">`ggRandomForests::gg_vimp()`</span>.</span>
<span id="cb1-479"><a href="#cb1-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-482"><a href="#cb1-482" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb1-483"><a href="#cb1-483" aria-hidden="true" tabindex="-1"></a><span class="in"># Variable importance für mtry = 10</span></span>
<span id="cb1-484"><a href="#cb1-484" aria-hidden="true" tabindex="-1"></a><span class="in">rf_fits$rf_mtry10_fit$fit %&gt;%</span></span>
<span id="cb1-485"><a href="#cb1-485" aria-hidden="true" tabindex="-1"></a><span class="in">  gg_vimp()  %&gt;%</span></span>
<span id="cb1-486"><a href="#cb1-486" aria-hidden="true" tabindex="-1"></a><span class="in">  plot() +</span></span>
<span id="cb1-487"><a href="#cb1-487" aria-hidden="true" tabindex="-1"></a><span class="in">    labs(</span></span>
<span id="cb1-488"><a href="#cb1-488" aria-hidden="true" tabindex="-1"></a><span class="in">      title = "Variable Importance für Random Forest (mtry = 10)"</span></span>
<span id="cb1-489"><a href="#cb1-489" aria-hidden="true" tabindex="-1"></a><span class="in">    ) +</span></span>
<span id="cb1-490"><a href="#cb1-490" aria-hidden="true" tabindex="-1"></a><span class="in">    theme_cowplot()</span></span>
<span id="cb1-491"><a href="#cb1-491" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-492"><a href="#cb1-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-493"><a href="#cb1-493" aria-hidden="true" tabindex="-1"></a>Die Grafik bestärkt unsere Schlussfolgerung aus der Analyse des (mit CART trainierten) einzelnen Entscheidungsbaums in @sec-simpletrees, dass <span class="in">`rm`</span> und <span class="in">`lstat`</span> die wichtigsten Regressoren für die Vorhersage von <span class="in">`medv`</span> sind.</span>
<span id="cb1-494"><a href="#cb1-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-495"><a href="#cb1-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-496"><a href="#cb1-496" aria-hidden="true" tabindex="-1"></a><span class="fu">## Boosting {#sec-boosting}</span></span>
<span id="cb1-497"><a href="#cb1-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-498"><a href="#cb1-498" aria-hidden="true" tabindex="-1"></a>Boosting ist eine leistungsstarke Ensemble-Methode für Vorhersagen, die kleine Modelle (oft Entscheidungsbäume geringer Tiefe) sukzessiv trainiert und zu einem starken Modell kombiniert. Anders als bei Random Forests, bei denen viele Bäume unabhängig voneinander auf zufälligen Stichproben der Daten trainiert werden, geht ein Boosting-Algorithmuss sequentiell vor: Jeder nachfolgende Baum wird darauf optimiert, die Fehler des vorherigen Modells zu reduzieren. Die Idee hierbei ist es, iterativ "schwache" Modelle zu erzeugen, die eine gute Anpassung für Datenpunkte liefern, die in den vorherigen Durchläufen schlecht vorhergesagt wurden.</span>
<span id="cb1-499"><a href="#cb1-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-500"><a href="#cb1-500" aria-hidden="true" tabindex="-1"></a>Für einen Trainingsdatensatz $<span class="sc">\{</span>(x_i, y_i)<span class="sc">\}</span>_{i=1}^n$, wobei $x_i$ die Input-Features und $y_i$ Beobachtungen des Outcomes sind, kann Boosting wiefolgt durchgeführt werden.</span>
<span id="cb1-501"><a href="#cb1-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-502"><a href="#cb1-502" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Initialisierung**: Initialisiere das Boosting-Modell als $\widehat{F}_0(x)$. Setze die Residuen $r^0_i=y_i$ für alle $i$</span>
<span id="cb1-503"><a href="#cb1-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-504"><a href="#cb1-504" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Iteration**: Wiederhole die folgenden Schritte für $b = 1,2,\dots,B$ mit $B$ hinreichend groß:</span>
<span id="cb1-505"><a href="#cb1-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-506"><a href="#cb1-506" aria-hidden="true" tabindex="-1"></a>    2.1 **Base Learner**: Trainiere Baum $T_b$ mit $\{(\boldsymbol{x}_i, r^{b-1}_i)\}_{i=1}^n$ für die Vorhersage des *Fehlers* der vorherigen Iteration $r^{b-1}$.</span>
<span id="cb1-507"><a href="#cb1-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-508"><a href="#cb1-508" aria-hidden="true" tabindex="-1"></a>    2.2 **Aktualisierung**: Aktualisiere das Boosting-Modell,</span>
<span id="cb1-509"><a href="#cb1-509" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-510"><a href="#cb1-510" aria-hidden="true" tabindex="-1"></a>      \begin{align*}</span>
<span id="cb1-511"><a href="#cb1-511" aria-hidden="true" tabindex="-1"></a>      \widehat{F}_{b}(\boldsymbol{x}) = \widehat{F}_{b-1}(\boldsymbol{x}) + \eta \cdot T_{b}(\boldsymbol{x}),</span>
<span id="cb1-512"><a href="#cb1-512" aria-hidden="true" tabindex="-1"></a>      \end{align*}</span>
<span id="cb1-513"><a href="#cb1-513" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-514"><a href="#cb1-514" aria-hidden="true" tabindex="-1"></a>      wobei $\eta$ die (oft klein gewählte) *Lernrate* ist.</span>
<span id="cb1-515"><a href="#cb1-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-516"><a href="#cb1-516" aria-hidden="true" tabindex="-1"></a>    2.3 **Fehlerberechnung**: Berechne die Residuen $r^b_i$ als Differenzen zwischen dem tatsächlichen Werten $y_i$ und den Vorhersage des aktuellen Modells $\widehat{F}_m(\boldsymbol{x}_i)$,</span>
<span id="cb1-517"><a href="#cb1-517" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-518"><a href="#cb1-518" aria-hidden="true" tabindex="-1"></a>      \begin{align*}</span>
<span id="cb1-519"><a href="#cb1-519" aria-hidden="true" tabindex="-1"></a>      r^b_i = y_i - \widehat{F}_b(\boldsymbol{x}_i).</span>
<span id="cb1-520"><a href="#cb1-520" aria-hidden="true" tabindex="-1"></a>      \end{align*}</span>
<span id="cb1-521"><a href="#cb1-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-522"><a href="#cb1-522" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Output**: Gib das finale Modell aus:</span>
<span id="cb1-523"><a href="#cb1-523" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-524"><a href="#cb1-524" aria-hidden="true" tabindex="-1"></a>    \begin{align*}</span>
<span id="cb1-525"><a href="#cb1-525" aria-hidden="true" tabindex="-1"></a>      \widehat{F}(\boldsymbol{x}) := \sum_{b=1}^B \eta\cdot \widehat{F}^b(\boldsymbol{x})</span>
<span id="cb1-526"><a href="#cb1-526" aria-hidden="true" tabindex="-1"></a>    \end{align*}</span>
<span id="cb1-527"><a href="#cb1-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-528"><a href="#cb1-528" aria-hidden="true" tabindex="-1"></a>Der Parameter $0\leq\eta\leq0$ steuert, wie stark der Einfluss jedes neuen Baumes auf das Modell ist. Eine kleine Lernrate führt dazu, dass viele Bäume benötigt werden, was Vorhersagen (ähnlich wie bei Bagging) stabiler macht. Beachte die sequentielle Natur des Trainings: Die $r^b_i$ in Schritt 2.3 sind die zu vorhersagenden Outcome-Variable für den nächsten Baum. $T_{b+1}$ wird trainiert wird, um den *Fehler des bisherigen Modells* $\widehat{F}_b$ zu erklären.</span>
<span id="cb1-529"><a href="#cb1-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-530"><a href="#cb1-530" aria-hidden="true" tabindex="-1"></a>Für die Anwendung auf <span class="in">`MASS::Boston`</span> in R nutzen wir den im Paket <span class="in">`gbm`</span> implementierten *Gradient-Boosting*-Algorithmus. Bei Gradient Boosting wird jeder Baum so trainiert, dass er den negativen Gradienten einer Verlustfunktion approximiert, also die Richtung des größten Fehlers. Das Modell wird schrittweise verbessert, indem es entlang des Gradienten aktualisiert wird, um die Vorhersagegüe zu optimieren; siehe @Hastieetal2013 für eine detaillierte Erläuterung.</span>
<span id="cb1-531"><a href="#cb1-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-532"><a href="#cb1-532" aria-hidden="true" tabindex="-1"></a>Mit dem nachfolgenden Code-Chunk trainieren wir ein Boosting-Modell für Regression mit 5000 einfachen Bäumen (<span class="in">`n.trees = 5000`</span>) mit einer maximalen Tiefe von 2 (<span class="in">`interaction.depth = 2`</span>), d.h. es folgen maximal 2 Entscheidungs-Regeln nacheinander. Um das Risiko von Overfitting gering zu halten, erlauben wir nur Splits, die zu mindestens zwei Beobachtungen in resultierenden nodes führen (<span class="in">`n.minobsinnode = 2`</span>). Die Lernrate (Beitrag der Base Learner zum Ensemble) wird typischerweise klein (und in Abhängigkeit von <span class="in">`n.trees`</span>) gewählt (<span class="in">`shrinkage = 0.001`</span>).^<span class="co">[</span><span class="ot">Je kleiner die Lernrate, desto größer sollte `n.trees` gewählt werden.</span><span class="co">]</span></span>
<span id="cb1-533"><a href="#cb1-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-536"><a href="#cb1-536" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb1-537"><a href="#cb1-537" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(1234)</span></span>
<span id="cb1-538"><a href="#cb1-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-539"><a href="#cb1-539" aria-hidden="true" tabindex="-1"></a><span class="in"># Gradient Boosting durchführen</span></span>
<span id="cb1-540"><a href="#cb1-540" aria-hidden="true" tabindex="-1"></a><span class="in">gbm_model &lt;- gbm(</span></span>
<span id="cb1-541"><a href="#cb1-541" aria-hidden="true" tabindex="-1"></a><span class="in">  formula = medv ~ ., </span></span>
<span id="cb1-542"><a href="#cb1-542" aria-hidden="true" tabindex="-1"></a><span class="in">  data = Boston_train, </span></span>
<span id="cb1-543"><a href="#cb1-543" aria-hidden="true" tabindex="-1"></a><span class="in">  distribution = "gaussian", # für Regression</span></span>
<span id="cb1-544"><a href="#cb1-544" aria-hidden="true" tabindex="-1"></a><span class="in">  n.trees = 5000,           # Anz. Bäume</span></span>
<span id="cb1-545"><a href="#cb1-545" aria-hidden="true" tabindex="-1"></a><span class="in">  interaction.depth = 2,     # Maximale Tiefe der base learner</span></span>
<span id="cb1-546"><a href="#cb1-546" aria-hidden="true" tabindex="-1"></a><span class="in">  shrinkage = 0.01,         # Lernrate</span></span>
<span id="cb1-547"><a href="#cb1-547" aria-hidden="true" tabindex="-1"></a><span class="in">  n.minobsinnode = 2         # Min. Beobachtungen in nodes</span></span>
<span id="cb1-548"><a href="#cb1-548" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb1-549"><a href="#cb1-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-550"><a href="#cb1-550" aria-hidden="true" tabindex="-1"></a><span class="in">gbm_model </span></span>
<span id="cb1-551"><a href="#cb1-551" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-552"><a href="#cb1-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-553"><a href="#cb1-553" aria-hidden="true" tabindex="-1"></a>Für die Vorhersagen auf dem Test-Datensatz legen wir mit <span class="in">`n.trees = gbm_model$n.trees`</span> fest, dass das gesamte Ensemble genutzt werden soll.</span>
<span id="cb1-554"><a href="#cb1-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-557"><a href="#cb1-557" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb1-558"><a href="#cb1-558" aria-hidden="true" tabindex="-1"></a><span class="in"># Vorhersagen Test-Datensatz</span></span>
<span id="cb1-559"><a href="#cb1-559" aria-hidden="true" tabindex="-1"></a><span class="in">gbm_predictions &lt;- predict(</span></span>
<span id="cb1-560"><a href="#cb1-560" aria-hidden="true" tabindex="-1"></a><span class="in">  object = gbm_model, </span></span>
<span id="cb1-561"><a href="#cb1-561" aria-hidden="true" tabindex="-1"></a><span class="in">  newdata = Boston_test, </span></span>
<span id="cb1-562"><a href="#cb1-562" aria-hidden="true" tabindex="-1"></a><span class="in">  n.trees = gbm_model$n.trees # gesamtes Ensemble nutzen</span></span>
<span id="cb1-563"><a href="#cb1-563" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb1-564"><a href="#cb1-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-565"><a href="#cb1-565" aria-hidden="true" tabindex="-1"></a><span class="in"># Auswertung Test-Datensatz</span></span>
<span id="cb1-566"><a href="#cb1-566" aria-hidden="true" tabindex="-1"></a><span class="in">results &lt;- Boston_test %&gt;%</span></span>
<span id="cb1-567"><a href="#cb1-567" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(predictions = gbm_predictions) %&gt;%</span></span>
<span id="cb1-568"><a href="#cb1-568" aria-hidden="true" tabindex="-1"></a><span class="in">  metrics(</span></span>
<span id="cb1-569"><a href="#cb1-569" aria-hidden="true" tabindex="-1"></a><span class="in">    truth = medv, </span></span>
<span id="cb1-570"><a href="#cb1-570" aria-hidden="true" tabindex="-1"></a><span class="in">    estimate = predictions</span></span>
<span id="cb1-571"><a href="#cb1-571" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb1-572"><a href="#cb1-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-573"><a href="#cb1-573" aria-hidden="true" tabindex="-1"></a><span class="in">results</span></span>
<span id="cb1-574"><a href="#cb1-574" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-575"><a href="#cb1-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-576"><a href="#cb1-576" aria-hidden="true" tabindex="-1"></a>Die Ergebnisse zeigen, dass Gradient Boosting bereits für die naive Parameterwahl im Aufruf von <span class="in">`gbm::gbm()`</span> zu einer Verbesserung der Vorhersageleistung gegenüber den Random-Forest-Modellen führt.</span>
<span id="cb1-577"><a href="#cb1-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-578"><a href="#cb1-578" aria-hidden="true" tabindex="-1"></a>Anstatt <span class="in">`n.trees = 5000`</span> können wir <span class="in">`n.trees`</span> in <span class="in">`predict()`</span> einen Vektor mit verschiedenen Ensemble-Größen übergeben. Für <span class="in">`n.trees = 5000`</span> erhalten wir Vorhersagen für jeden Status, den das Boosting-Modell im Training nach seiner Initialisierung bis zu der in <span class="in">`gbm::gbm()`</span> festgelgten Größe durchläuft. Anhand dieser Vorhersagen können wir die Generalisierungsfähigkeit des Modells in Abhängigkeit der gewählten Lernrate und der Größe beurteilen, in dem wir den RMSE für den gesamten Trainingsprozess berechnen. Für eine leichtere Interpretation erzeugen wir eine Grafik ählich wie bei der OOB-Analyse des Random-Forest-Modells.</span>
<span id="cb1-579"><a href="#cb1-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-582"><a href="#cb1-582" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb1-583"><a href="#cb1-583" aria-hidden="true" tabindex="-1"></a><span class="in"># Vorhersagen sukzessiv treffen</span></span>
<span id="cb1-584"><a href="#cb1-584" aria-hidden="true" tabindex="-1"></a><span class="in">predict(</span></span>
<span id="cb1-585"><a href="#cb1-585" aria-hidden="true" tabindex="-1"></a><span class="in">    object = gbm_model, </span></span>
<span id="cb1-586"><a href="#cb1-586" aria-hidden="true" tabindex="-1"></a><span class="in">    newdata = Boston_test, </span></span>
<span id="cb1-587"><a href="#cb1-587" aria-hidden="true" tabindex="-1"></a><span class="in">    n.trees = 1:5000</span></span>
<span id="cb1-588"><a href="#cb1-588" aria-hidden="true" tabindex="-1"></a><span class="in">) %&gt;%</span></span>
<span id="cb1-589"><a href="#cb1-589" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb1-590"><a href="#cb1-590" aria-hidden="true" tabindex="-1"></a><span class="in">   # Testset-RMSE berechnen</span></span>
<span id="cb1-591"><a href="#cb1-591" aria-hidden="true" tabindex="-1"></a><span class="in">    as_tibble() %&gt;%</span></span>
<span id="cb1-592"><a href="#cb1-592" aria-hidden="true" tabindex="-1"></a><span class="in">    map_dbl(</span></span>
<span id="cb1-593"><a href="#cb1-593" aria-hidden="true" tabindex="-1"></a><span class="in">      .f = ~ sqrt(mean((.x - Boston_test$medv)^2))</span></span>
<span id="cb1-594"><a href="#cb1-594" aria-hidden="true" tabindex="-1"></a><span class="in">    ) %&gt;%</span></span>
<span id="cb1-595"><a href="#cb1-595" aria-hidden="true" tabindex="-1"></a><span class="in">    bind_cols(rmse = ., trees = 1:5000) %&gt;%</span></span>
<span id="cb1-596"><a href="#cb1-596" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb1-597"><a href="#cb1-597" aria-hidden="true" tabindex="-1"></a><span class="in">  # Plotten</span></span>
<span id="cb1-598"><a href="#cb1-598" aria-hidden="true" tabindex="-1"></a><span class="in">  ggplot(mapping = aes(x = trees, y = rmse)) +</span></span>
<span id="cb1-599"><a href="#cb1-599" aria-hidden="true" tabindex="-1"></a><span class="in">    geom_line() +</span></span>
<span id="cb1-600"><a href="#cb1-600" aria-hidden="true" tabindex="-1"></a><span class="in">    labs(</span></span>
<span id="cb1-601"><a href="#cb1-601" aria-hidden="true" tabindex="-1"></a><span class="in">      title = "Boosting: Testset-RMSE als Funktion von n.trees"</span></span>
<span id="cb1-602"><a href="#cb1-602" aria-hidden="true" tabindex="-1"></a><span class="in">    ) +</span></span>
<span id="cb1-603"><a href="#cb1-603" aria-hidden="true" tabindex="-1"></a><span class="in">    theme_cowplot()</span></span>
<span id="cb1-604"><a href="#cb1-604" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-605"><a href="#cb1-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-606"><a href="#cb1-606" aria-hidden="true" tabindex="-1"></a>Die Grafik zeigt eine schnelle Verbesserung des Out-of-sample-Fehlers mit der Größe des Ensembles. Für die gewählte Lernrate scheinen 5000 Bäume adäquat zu sein.</span>
<span id="cb1-607"><a href="#cb1-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-608"><a href="#cb1-608" aria-hidden="true" tabindex="-1"></a>Analog zu Bagging und Random Forests können wir die Relevanz der Regressoren in <span class="in">`Boston`</span> für die Vorhersage von <span class="in">`medv`</span> anhand der mit <span class="in">`summary()`</span> berechneten (relativen) Variable Importance für die Anpassung auf den Trainingsdatensatz einschätzen. </span>
<span id="cb1-609"><a href="#cb1-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-612"><a href="#cb1-612" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb1-613"><a href="#cb1-613" aria-hidden="true" tabindex="-1"></a><span class="in"># Variable Importance berechnen</span></span>
<span id="cb1-614"><a href="#cb1-614" aria-hidden="true" tabindex="-1"></a><span class="in">var_importance &lt;- summary(</span></span>
<span id="cb1-615"><a href="#cb1-615" aria-hidden="true" tabindex="-1"></a><span class="in">  object = gbm_model, </span></span>
<span id="cb1-616"><a href="#cb1-616" aria-hidden="true" tabindex="-1"></a><span class="in">  plotit = FALSE # k. graphische Ausgabe</span></span>
<span id="cb1-617"><a href="#cb1-617" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb1-618"><a href="#cb1-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-619"><a href="#cb1-619" aria-hidden="true" tabindex="-1"></a><span class="in"># ... und plotten</span></span>
<span id="cb1-620"><a href="#cb1-620" aria-hidden="true" tabindex="-1"></a><span class="in">var_importance &lt;- var_importance %&gt;%</span></span>
<span id="cb1-621"><a href="#cb1-621" aria-hidden="true" tabindex="-1"></a><span class="in">  as_tibble() %&gt;%</span></span>
<span id="cb1-622"><a href="#cb1-622" aria-hidden="true" tabindex="-1"></a><span class="in">  arrange(</span></span>
<span id="cb1-623"><a href="#cb1-623" aria-hidden="true" tabindex="-1"></a><span class="in">    desc(rel.inf)</span></span>
<span id="cb1-624"><a href="#cb1-624" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb1-625"><a href="#cb1-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-626"><a href="#cb1-626" aria-hidden="true" tabindex="-1"></a><span class="in">ggplot(</span></span>
<span id="cb1-627"><a href="#cb1-627" aria-hidden="true" tabindex="-1"></a><span class="in">  data = var_importance,</span></span>
<span id="cb1-628"><a href="#cb1-628" aria-hidden="true" tabindex="-1"></a><span class="in">  mapping = aes(</span></span>
<span id="cb1-629"><a href="#cb1-629" aria-hidden="true" tabindex="-1"></a><span class="in">    x = reorder(var, rel.inf), </span></span>
<span id="cb1-630"><a href="#cb1-630" aria-hidden="true" tabindex="-1"></a><span class="in">    y = rel.inf</span></span>
<span id="cb1-631"><a href="#cb1-631" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb1-632"><a href="#cb1-632" aria-hidden="true" tabindex="-1"></a><span class="in">) +</span></span>
<span id="cb1-633"><a href="#cb1-633" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_bar(stat = "identity") +</span></span>
<span id="cb1-634"><a href="#cb1-634" aria-hidden="true" tabindex="-1"></a><span class="in">  coord_flip() +</span></span>
<span id="cb1-635"><a href="#cb1-635" aria-hidden="true" tabindex="-1"></a><span class="in">  labs(</span></span>
<span id="cb1-636"><a href="#cb1-636" aria-hidden="true" tabindex="-1"></a><span class="in">    title = "Variable Importance für Gradient Boosting",</span></span>
<span id="cb1-637"><a href="#cb1-637" aria-hidden="true" tabindex="-1"></a><span class="in">    x = "Variable",</span></span>
<span id="cb1-638"><a href="#cb1-638" aria-hidden="true" tabindex="-1"></a><span class="in">    y = "Relativer Einfluss (%)"</span></span>
<span id="cb1-639"><a href="#cb1-639" aria-hidden="true" tabindex="-1"></a><span class="in">  ) +</span></span>
<span id="cb1-640"><a href="#cb1-640" aria-hidden="true" tabindex="-1"></a><span class="in">  theme_cowplot()</span></span>
<span id="cb1-641"><a href="#cb1-641" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-642"><a href="#cb1-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-643"><a href="#cb1-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-644"><a href="#cb1-644" aria-hidden="true" tabindex="-1"></a>Obwohl erneut <span class="in">`lstat`</span> und <span class="in">`rm`</span> als die wichtigsten Prädiktoren gelistet sind, identifiziert Gradient Boosting im Gegensatz zu Bagging und Random Forests <span class="in">`lstat`</span> als die Variable mit der größten Vorhersagekraft für <span class="in">`medv`</span>. </span>
<span id="cb1-645"><a href="#cb1-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-646"><a href="#cb1-646" aria-hidden="true" tabindex="-1"></a><span class="fu">## Zusammenfassung</span></span>
<span id="cb1-647"><a href="#cb1-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-648"><a href="#cb1-648" aria-hidden="true" tabindex="-1"></a>In diesem Kapitel haben wir die Anwendung baum-basierter Methoden in R diskutiert. Darunter Entscheidungsbäume, Bagging, Random Forests und Boosting. Entscheidungsbäume sind Modelle, die die Daten anhand binärer Entscheidungsregeln sukzessiv in kleinere, homogene Gruppen aufgeteilt werden. Baum-Modelle bieten intuitive Interpretierbarkeit, neigen jedoch zur Überanpassung, was durch Beschneiden (Pruning) vermieden werden kann. Die Vorhersage einzelner Bäume ist tendentiell mit hoher Varianz verbunden. Random Forests kombinieren mit Bagging viele Entscheidungsbäume, die auf zufälligen Teilmengen der Daten und Merkmale trainiert werden. Durch die Aggregation der Vorhersagen vieler Bäume reduziert der Random Forest die Varianz und verbessert so die Vorhersagegenauigkeit. Boosting-Methoden mit Entscheidungsbäumen trainieren kleine Bäume sukzessive, wobei jeder weitere Baum zur Korrektur der gegenwärtigen Fehler des Ensembles trainiert wird. Gradient Boosting nutzt den Gradienten der Verlustfunktion, um die Vorhersagequalität des Ensembles optimieren. Für alle Methoden wurden Implementierungen im <span class="in">`parsnip`</span>-Framework in R vorgestellt. Zudem wurde gezeigt, wie die Vorhersagegüte durch Testdatensätze beurteilt und die Bedeutung einzelner Variablen mit Variable-Importance-Metriken analysiert werden kann.</span>
</code><button title="In die Zwischenablage kopieren" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>