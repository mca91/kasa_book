<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="de" xml:lang="de"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="description" content="Kausalanalyse und Maschinelles Lernen mit R (KMLR) ist ein innovatives Online-Lehrbuch, das Studierende der Wirtschaftswissenschaften in moderne statistische Methoden und deren ökonometrische Anwendungen einführt. Das interaktive Kompendium für quantitative Lehrveranstaltungen verbindet theoretische Grundlagen mit praktischer Anwendung durch die Möglichkeit, R-Code direkt im Browser auszuführen. Von den statistischen Grundlagen über fortgeschrittene Methoden der Kausalanalyse bis hin zu Machine Learning-Techniken bietet das Buch einen umfassenden Einblick in fortschrittliche quantitative Forschungsmethoden. Besonders wertvoll für Studierende sind die zahlreichen interaktiven Visualisierungen und praxisnahen Fallstudien mit wirtschaftswissenschaftlichem Fokus, die ein tiefes Verständnis der ökonometrischen Methoden fördern und Kompetenzen in reproduzierbarer Forschung mit dem R-Ökosystem vermitteln.">
<title>15&nbsp; Baum-basierte Methoden – Kausalanalyse und maschinelles Lernen mit R</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Machine Learning.html" rel="next">
<link href="./svm.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/live-runtime/live-runtime.js" type="module"></script>
<link href="site_libs/quarto-contrib/live-runtime/live-runtime.css" rel="stylesheet"><script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Keine Treffer",
    "search-matching-documents-text": "Treffer",
    "search-copy-link-title": "Link in die Suche kopieren",
    "search-hide-matches-text": "Zusätzliche Treffer verbergen",
    "search-more-match-text": "weitere Treffer in diesem Dokument",
    "search-more-matches-text": "weitere Treffer in diesem Dokument",
    "search-clear-button-title": "Zurücksetzen",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Abbrechen",
    "search-submit-button-title": "Abschicken",
    "search-label": "Suchen"
  }
}</script><script type="module" src="site_libs/quarto-ojs/quarto-ojs-runtime.js"></script><link href="site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">
<script>
  MathJax = {
    tex: {
      tags: 'ams'  // should be 'ams', 'none', or 'all'
    }
  };
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.js" integrity="sha512-aoZChv+8imY/U1O7KIHXvO87EOzCuKO0GhFtpD6G2Cyjo/xPeTgdf3/bchB10iB+AojMTDkMHDPLKNxPJVqDcw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css">
<style>

  .panel-tabset .tab-content {
    border: 0;
    padding: 1em 0 0 0;
  }
  
  .panel-tabset .nav-item a {
    border-radius: 5px 5px 0 0;
  }
  
  .scientific_borders {
    border: 0;
    border-top: 2px solid black !important; 
    border-bottom: 2px solid black !important;
  }
  .table:not(.gt_table) > :not(caption)>*>* {
    border-bottom-width: 0;
  }
  .table:not(.gt_table) > thead {
    border-bottom: 1px solid black;
  }
  .soft-box-shadow {
    border: 1px solid rgba(233,236,239,.9) !important;
    border-radius: .5rem !important;
    background-color: rgba(250,250,250,.9) !important;
    box-shadow: 0px 1px 2px rgba(0,0,0,.1),
                0px 3px 7px rgba(0,0,0,.1),
                0px 12px 30px rgba(0,0,0,.08);
    margin-top: 2rem !important;
    margin-bottom: 2.5rem !important;
    padding: .25rem !important;
  }
  .obs-soft-box-shadow {
    border: 1px solid rgba(233,236,239,.9) !important;
    border-radius: .5rem !important;
    background-color: white !important;
    box-shadow: 0px 1px 2px rgba(0,0,0,.1),
                0px 3px 7px rgba(0,0,0,.1),
                0px 12px 30px rgba(0,0,0,.08);
    margin-top: 2rem !important;
    margin-bottom: 2.5rem !important;
    padding: .25rem !important;
  }
  
</style>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    
    var gt_tables = document.querySelectorAll(".gt_table");
    gt_tables.forEach(function(table) {
      table.classList.remove("table-striped");
    });
    
    var tables = document.querySelectorAll("table.table:not(.gt_table)");
    tables.forEach(function(table) {
      table.classList.remove("table-striped");
      table.classList.add("scientific_borders");
    });
    
    document.querySelectorAll("div.sourceCode").forEach(function(block) {
      block.classList.add("soft-box-shadow");
    });
    
    document.querySelectorAll("div.bg-white").forEach(function(block) {
      block.classList.remove("bg-white");
    });
    
const elements = document.querySelectorAll('[id^="qwebr-interactive-area"]');

    elements.forEach(element => {
        element.classList.add('box-shadow');
    });
    
    document.querySelectorAll('[id^="webr"]').forEach(function(block) {
      block.classList.add("box-shadow");
    });
    
        document.querySelectorAll('.card-header').forEach(function(block) {
      block.classList.add("box-shadow");
    });
    
  });
</script><style>
.qwebr-code-output-stdout {background-color: powderblue;}

.qwebr-button-run {
 width = 100%; 
}

.centered-caption {
   text-align: center;
}
</style>
<script src="https://cdn.jsdelivr.net/npm/quizdown@latest/public/build/quizdown.js"></script><script>quizdown.init();</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script><link rel="stylesheet" href="custom_styles.css">
<meta name="twitter:title" content="15&nbsp; Baum-basierte Methoden – Kausalanalyse und maschinelles Lernen mit R">
<meta name="twitter:description" content="Kausalanalyse und Maschinelles Lernen mit R (KMLR) ist ein innovatives Online-Lehrbuch, das Studierende der Wirtschaftswissenschaften in moderne statistische Methoden und deren ökonometrische Anwendungen einführt. Das interaktive Kompendium für quantitative Lehrveranstaltungen verbindet theoretische Grundlagen mit praktischer Anwendung durch die Möglichkeit, R-Code direkt im Browser auszuführen. Von den statistischen Grundlagen über fortgeschrittene Methoden der Kausalanalyse bis hin zu Machine Learning-Techniken bietet das Buch einen umfassenden Einblick in fortschrittliche quantitative Forschungsmethoden. Besonders wertvoll für Studierende sind die zahlreichen interaktiven Visualisierungen und praxisnahen Fallstudien mit wirtschaftswissenschaftlichem Fokus, die ein tiefes Verständnis der ökonometrischen Methoden fördern und Kompetenzen in reproduzierbarer Forschung mit dem R-Ökosystem vermitteln.">
<meta name="twitter:card" content="summary">
<meta name="citation_title" content="[[15]{.chapter-number}&nbsp; [Baum-basierte Methoden]{.chapter-title}]{#sec-trees .quarto-section-identifier}">
<meta name="citation_language" content="de">
<meta name="citation_reference" content="citation_title=The credibility revolution in empirical economics: How better research design is taking the con out of econometrics;,citation_author=Joshua D Angrist;,citation_author=Jörn-Steffen Pischke;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_issue=2;,citation_volume=24;,citation_journal_title=Journal of economic perspectives;,citation_publisher=American Economic Association;">
<meta name="citation_reference" content="citation_title=A new data set of educational attainment in the world, 1950–2010;,citation_author=Robert J. Barro;,citation_author=Jong Wha Lee;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_fulltext_html_url=https://www.sciencedirect.com/science/article/pii/S0304387812000855;,citation_doi=https://doi.org/10.1016/j.jdeveco.2012.10.001;,citation_issn=0304-3878;,citation_volume=104;,citation_journal_title=Journal of Development Economics;">
<meta name="citation_reference" content="citation_title=Beyond work ethic: Religion, individual, and political preferences;,citation_author=Christoph Basten;,citation_author=Frank Betz;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_issue=3;,citation_volume=5;,citation_journal_title=American Economic Journal: Economic Policy;,citation_publisher=American Economic Association;">
<meta name="citation_reference" content="citation_title=The effect of alcohol consumption on mortality: Regression discontinuity evidence from the minimum drinking age;,citation_author=Christopher Carpenter;,citation_author=Carlos Dobkin;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_issue=1;,citation_volume=1;,citation_journal_title=American Economic Journal: Applied Economics;,citation_publisher=American Economic Association;">
<meta name="citation_reference" content="citation_title=Optimal bandwidth choice for the regression discontinuity estimator;,citation_author=Guido Imbens;,citation_author=Karthik Kalyanaraman;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_issue=3;,citation_volume=79;,citation_journal_title=The Review of economic studies;,citation_publisher=Oxford University Press;">
<meta name="citation_reference" content="citation_title=Regression discontinuity designs: A guide to practice;,citation_author=G. W. Imbens;,citation_author=Thomas Lemieux;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_issue=2;,citation_volume=142;,citation_journal_title=Journal of econometrics;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=Why high-order polynomials should not be used in regression discontinuity designs;,citation_author=Andrew Gelman;,citation_author=Guido Imbens;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=3;,citation_volume=37;,citation_journal_title=Journal of Business &amp;amp;amp; Economic Statistics;,citation_publisher=Taylor &amp;amp; Francis;">
<meta name="citation_reference" content="citation_title=Randomized experiments from non-random selection in US house elections;,citation_author=David S Lee;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_issue=2;,citation_volume=142;,citation_journal_title=Journal of Econometrics;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=Die protestantische ethik und der geist des kapitalismus;,citation_author=Max Weber;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_volume=1614;">
<meta name="citation_reference" content="citation_title=Regularization and confounding in linear regression for treatment effect estimation;,citation_author=P Richard Hahn;,citation_author=Carlos M Carvalho;,citation_author=David Puelz;,citation_author=Jingyu He;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=Regression shrinkage and selection via the lasso;,citation_author=Robert Tibshirani;,citation_publication_date=1996;,citation_cover_date=1996;,citation_year=1996;,citation_issue=1;,citation_volume=58;,citation_journal_title=Journal of the Royal Statistical Society Series B: Statistical Methodology;,citation_publisher=Oxford University Press;">
<meta name="citation_reference" content="citation_title=Least angle regression;,citation_author=Bradley Efron;,citation_author=Trevor Hastie;,citation_author=Iain Johnstone;,citation_author=Robert Tibshirani;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;">
<meta name="citation_reference" content="citation_title=Sparse models and methods for optimal instruments with an application to eminent domain;,citation_author=Alexandre Belloni;,citation_author=Daniel Chen;,citation_author=Victor Chernozhukov;,citation_author=Christian Hansen;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_issue=6;,citation_volume=80;,citation_journal_title=Econometrica;,citation_publisher=Wiley Online Library;">
<meta name="citation_reference" content="citation_title=Least squares after model selection in high-dimensional sparse models;,citation_author=Alexandre Belloni;,citation_author=Victor Chernozhukov;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_journal_title=Bernoulli;,citation_publisher=JSTOR;">
<meta name="citation_reference" content="citation_title=High-dimensional methods and inference on structural and treatment effects;,citation_author=Alexandre Belloni;,citation_author=Victor Chernozhukov;,citation_author=Christian Hansen;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_issue=2;,citation_volume=28;,citation_journal_title=Journal of Economic Perspectives;,citation_publisher=American Economic Association 2014 Broadway, Suite 305, Nashville, TN 37203-2418;">
<meta name="citation_reference" content="citation_title=Ridge regression: Biased estimation for nonorthogonal problems;,citation_author=Arthur E Hoerl;,citation_author=Robert W Kennard;,citation_publication_date=1970;,citation_cover_date=1970;,citation_year=1970;,citation_issue=1;,citation_volume=12;,citation_journal_title=Technometrics;,citation_publisher=Taylor &amp;amp;amp; Francis;">
<meta name="citation_reference" content="citation_title=Using data mining to predict secondary school student performance;,citation_author=Paulo Cortez;,citation_author=Alice Maria Gonçalves Silva;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_publisher=EUROSIS-ETI;">
<meta name="citation_reference" content="citation_title=Manipulation of the running variable in the regression discontinuity design: A density test;,citation_author=Justin McCrary;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_issue=2;,citation_volume=142;,citation_journal_title=Journal of Econometrics;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=Simple local polynomial density estimators;,citation_author=Matias D Cattaneo;,citation_author=Michael Jansson;,citation_author=Xinwei Ma;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=531;,citation_volume=115;,citation_journal_title=Journal of the American Statistical Association;,citation_publisher=Taylor &amp;amp;amp; Francis;">
<meta name="citation_reference" content="citation_title=An introduction to propensity score methods for reducing the effects of confounding in observational studies;,citation_author=P. Austin;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=3;,citation_doi=10.1080/00273171.2011.568786;,citation_issn=0027-3171;,citation_pmid=21818162;,citation_volume=46;,citation_journal_title=Multivariate Behavioral Research;,citation_publisher=Informa UK Limited;">
<meta name="citation_reference" content="citation_title=The central role of the propensity score in observational studies for causal effects;,citation_author=Paul R. Rosenbaum;,citation_author=Donald R. Rubin;,citation_publication_date=1983;,citation_cover_date=1983;,citation_year=1983;,citation_issue=1;,citation_doi=10.1017/cbo9780511810725.016;,citation_issn=0006-3444;,citation_volume=70;,citation_journal_title=Biometrika;,citation_publisher=Cambridge University Press;">
<meta name="citation_reference" content="citation_title=Efficient estimation of average treatment effects using the estimated propensity score.;,citation_author=Keisuke Hirano;,citation_author=Guido Imbens;,citation_author=Geert Ridder;,citation_publication_date=2003;,citation_cover_date=2003;,citation_year=2003;,citation_issue=4;,citation_doi=10.1111/1468-0262.00442;,citation_volume=71;,citation_journal_title=Econometrica;,citation_publisher=The Econometric Society;">
<meta name="citation_reference" content="citation_title=Comment on “an essay on the logical foundations of survey sampling” by basu, d;,citation_author=J Hájek;,citation_publication_date=1971;,citation_cover_date=1971;,citation_year=1971;,citation_volume=236;,citation_journal_title=Foundations of Statistical Inference;">
<meta name="citation_reference" content="citation_title=Graphical display of covariate balance;,citation_author=Thomas Love;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_publisher=Presentation;">
<meta name="citation_reference" content="citation_title=The use of bootstrapping when using propensity-score matching without replacement: A simulation study.;,citation_author=Peter C. Austin;,citation_author=Dylan S. Small;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_issue=24;,citation_doi=10.1002/sim.6276;,citation_issn=0277-6715;,citation_volume=33;,citation_journal_title=Statistics in Medicine;,citation_publisher=Wiley;">
<meta name="citation_reference" content="citation_title=Robust post-matching inference.;,citation_author=Alberto Abadie;,citation_author=Jann Spiess;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_issue=538;,citation_doi=10.1080/01621459.2020.1840383;,citation_issn=0162-1459;,citation_volume=117;,citation_journal_title=Journal of the American Statistical Association;,citation_publisher=Informa UK Limited;">
<meta name="citation_reference" content="citation_title=Matching on the estimated propensity score.;,citation_author=undefined Imbens;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_issue=2;,citation_doi=10.3982/ecta11293;,citation_issn=0012-9682;,citation_volume=84;,citation_journal_title=Econometrica;,citation_publisher=The Econometric Society;">
<meta name="citation_reference" content="citation_title=The finite sample performance of inference methods for propensity score matching and weighting estimators.;,citation_author=Hugo Bodory;,citation_author=Lorenzo Camponovo;,citation_author=Martin Huber;,citation_author=Michael Lechner;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_doi=10.2139/ssrn.2731969;,citation_issn=1556-5068;,citation_journal_title=Journal of Business &amp;amp;amp; Economic Statistics;,citation_publisher=Elsevier BV;">
<meta name="citation_reference" content="citation_title=Interval estimation for treatment effects using propensity score matching. Statistics in medicine;,citation_author=Jennifer Hill;,citation_author=Jerome P. Reiter;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_issue=13;,citation_doi=10.1002/sim.2277;,citation_issn=0277-6715;,citation_volume=25;,citation_journal_title=Statistics in Medicine;,citation_publisher=Wiley;">
<meta name="citation_reference" content="citation_title=On the failure of the bootstrap for matching estimators.;,citation_author=Alberto Abadie;,citation_author=Guido W. Imbens;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_issue=6;,citation_doi=10.3982/ECTA6474;,citation_issn=0012-9682,1468-0262;,citation_volume=76;,citation_journal_title=Econometrica. Journal of the Econometric Society;">
<meta name="citation_reference" content="citation_title=Estimating the effect of treatment on binary outcomes using full matching on the propensity score.;,citation_author=Peter C. Austin;,citation_author=Elizabeth A. Stuart;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=6;,citation_doi=10.1177/0962280215601134;,citation_issn=0962-2802,1477-0334;,citation_volume=26;,citation_journal_title=Statistical Methods in Medical Research;">
<meta name="citation_reference" content="citation_title=Entropy balancing for causal effects: A multivariate reweighting method to produce balanced samples in observational studies;,citation_author=Jens Hainmueller;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_issue=1;,citation_doi=10.1093/pan/mpr025;,citation_issn=1047-1987;,citation_volume=20;,citation_journal_title=Political Analysis;,citation_publisher=Cambridge University Press (CUP);">
<meta name="citation_reference" content="citation_title=Econometric analysis of cross section and panel data;,citation_author=Jeffrey Wooldridge;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_isbn=9780262232586;">
<meta name="citation_reference" content="citation_title=Statistical inference;,citation_author=George Casella;,citation_author=Roger L. Berger;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;,citation_isbn=9780534243128;">
<meta name="citation_reference" content="citation_title=Synthetic control methods for comparative case studies: Estimating the effect of california’s tobacco control program.;,citation_author=Alexis Diamond Abadie;,citation_author=Jens Hainmueller;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_issue=490;,citation_doi=10.1198/jasa.2009.ap08746;,citation_volume=105;,citation_journal_title=Journal of the American Statistical Association;,citation_publisher=Informa UK Limited;">
<meta name="citation_reference" content="citation_title=Comparative politics and the synthetic control method: COMPARATIVE POLITICS AND THE SYNTHETIC CONTROL METHOD;,citation_author=Alberto Abadie;,citation_author=Alexis Diamond;,citation_author=Jens Hainmueller;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_issue=2;,citation_doi=10.1111/ajps.12116;,citation_volume=59;,citation_journal_title=American Journal of Political Science;,citation_publisher=Wiley;">
<meta name="citation_reference" content="citation_title=The costs of economic nationalism: Evidence from the brexit experiment*;,citation_author=Benjamin Born;,citation_author=Gernot J Müller;,citation_author=Moritz Schularick;,citation_author=Petr Sedláček;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=623;,citation_doi=10.1093/ej/uez020;,citation_volume=129;,citation_journal_title=The Economic Journal;,citation_publisher=Oxford University Press (OUP);">
<meta name="citation_reference" content="citation_title=End-of-sample instability tests;,citation_author=D. W. K. Andrews;,citation_publication_date=2003;,citation_cover_date=2003;,citation_year=2003;,citation_issue=6;,citation_doi=10.1111/1468-0262.00466;,citation_volume=71;,citation_journal_title=Econometrica;,citation_publisher=The Econometric Society;">
<meta name="citation_reference" content="citation_title=The effects of the 1993 earned income tax credit expansion on the labor supply of unmarried women;,citation_author=Kampon Adireksombat;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_issue=1;,citation_doi=https://doi.org/10.1177/1091142109358626;,citation_issn=1552-7530;,citation_volume=38;,citation_journal_title=Public Finance Review;,citation_publisher=SAGE Publications;">
<meta name="citation_reference" content="citation_title=Labor supply response to the earned income tax credit;,citation_author=N. Eissa;,citation_author=J. B. Liebman;,citation_publication_date=1996;,citation_cover_date=1996;,citation_year=1996;,citation_issue=2;,citation_doi=10.2307/2946689;,citation_issn=1531-4650;,citation_volume=111;,citation_journal_title=The Quarterly Journal of Economics;,citation_publisher=Oxford University Press (OUP);">
<meta name="citation_reference" content="citation_title=Difference-in-differences with multiple time periods.;,citation_author=Brantly Callaway;,citation_author=Pedro H. C. Sant’Anna;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=2;,citation_doi=10.1016/j.jeconom.2020.12.001;,citation_volume=225;,citation_journal_title=Journal of Econometrics;">
<meta name="citation_reference" content="citation_title=Difference-in-differences with variation in treatment timing.;,citation_author=Andrew Goodman-Bacon;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=2;,citation_doi=10.1016/j.jeconom.2021.03.014;,citation_volume=225;,citation_journal_title=Journal of Econometrics;,citation_publisher=Elsevier BV;">
<meta name="citation_reference" content="citation_title=Do police reduce crime? Estimates using the allocation of police forces after a terrorist attack;,citation_author=Rafael Di Tella;,citation_author=Ernesto Schargrodsky;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_issue=1;,citation_doi=10.1257/000282804322970733;,citation_volume=94;,citation_journal_title=American Economic Review;,citation_publisher=American Economic Association;">
<meta name="citation_reference" content="citation_title=Economic shocks and civil conflict: An instrumental variables approach;,citation_author=Edward Miguel;,citation_author=Shanker Satyanath;,citation_author=Ernest Sergenti;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_issue=4;,citation_doi=10.1086/421174;,citation_volume=112;,citation_journal_title=Journal of Political Economy;,citation_publisher=University of Chicago Press;">
<meta name="citation_reference" content="citation_title=Ethnicity, insurgency, and civil war.;,citation_author=undefined Fearon;,citation_author=David D. Laitin;,citation_publication_date=2003;,citation_cover_date=2003;,citation_year=2003;,citation_issue=01;,citation_doi=10.1017/s0003055403000534;,citation_volume=97;,citation_journal_title=American Political Science Review;,citation_publisher=Cambridge University Press (CUP);">
<meta name="citation_reference" content="citation_title=Weak states: Causes and consequences of the sicilian mafia;,citation_author=Daron Acemoglu;,citation_author=Giuseppe De Feo;,citation_author=Giacomo Davide De Luca;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_doi=10.1093/restud/rdz009;,citation_journal_title=The Review of Economic Studies;,citation_publisher=Oxford University Press (OUP);">
<meta name="citation_reference" content="citation_title=Income and democracy;,citation_author=Daron Acemoglu;,citation_author=Simon Johnson;,citation_author=James A. Robinson;,citation_author=Pierre Yared;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_issue=3;,citation_doi=10.1257/aer.98.3.808;,citation_volume=98;,citation_journal_title=American Economic Review;,citation_publisher=American Economic Association;">
<meta name="citation_reference" content="citation_title=Polyarchy: Participation and opposition: Participation and opposition;,citation_author=Robert Alan Dahl;,citation_publication_date=1971;,citation_cover_date=1971;,citation_year=1971;,citation_isbn=0300015658;">
<meta name="citation_reference" content="citation_title=The third wave: Democratization in the late twentieth century: Democratization in the late twentieth century;,citation_author=Samuel P. Huntington;,citation_publication_date=1991;,citation_cover_date=1991;,citation_year=1991;,citation_isbn=0-8061-2516-0;">
<meta name="citation_reference" content="citation_title=Capitalist development and democracy;,citation_author=Dietrich Rueschemeyer;,citation_author=Evelyne H. Stephens;,citation_author=John D. Stephens;,citation_publication_date=1992;,citation_cover_date=1992;,citation_year=1992;,citation_isbn=074560398X;">
<meta name="citation_reference" content="citation_title=The effect: An introduction to research design and causality;,citation_author=Nick Huntington-Klein;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_doi=10.1201/9781003226055;,citation_isbn=9781003226055;">
<meta name="citation_reference" content="citation_title=Synth: An r package for synthetic control methods in comparative case studies;,citation_author=Jens Hainmueller;,citation_author=Alexis Diamond;,citation_author=Alberto Abadie;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_fulltext_html_url=https://www.jstatsoft.org/v42/i13/;,citation_issue=13;,citation_volume=42;,citation_journal_title=Journal of Statistical Software;">
<meta name="citation_reference" content="citation_title=Replication data for: Income and democracy;,citation_author=Daron Acemoglu;,citation_author=Simon Johnson;,citation_author=James A. Robinson;,citation_author=Pierre Yared;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_doi=10.3886/E113251V1;,citation_publisher=ICPSR - Interuniversity Consortium for Political; Social Research;">
<meta name="citation_reference" content="citation_title=Biases in dynamic models with fixed effects;,citation_author=Stephen Nickell;,citation_publication_date=1981;,citation_cover_date=1981;,citation_year=1981;,citation_issue=6;,citation_doi=10.2307/1911408;,citation_volume=49;,citation_journal_title=Econometrica;,citation_publisher=JSTOR;">
<meta name="citation_reference" content="citation_title=Mostly harmless econometrics: An empiricist’s companion;,citation_author=J. Angrist;,citation_author=Jörn-Steffen Pischke;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_isbn=9780691120355;">
<meta name="citation_reference" content="citation_title=Some tests of specification for panel data: Monte carlo evidence and an application to employment equations;,citation_author=Manuel Arellano;,citation_author=Stephen Bond;,citation_publication_date=1991;,citation_cover_date=1991;,citation_year=1991;,citation_issue=2;,citation_doi=10.2307/2297968;,citation_volume=58;,citation_journal_title=The Review of Economic Studies;,citation_publisher=Oxford University Press (OUP);">
<meta name="citation_reference" content="citation_title=Estimation of dynamic models with error components;,citation_author=T. W. Anderson;,citation_author=Cheng Hsiao;,citation_publication_date=1981;,citation_cover_date=1981;,citation_year=1981;,citation_issue=375;,citation_doi=10.2307/2287517;,citation_volume=76;,citation_journal_title=Journal of the American Statistical Association;,citation_publisher=Informa UK Limited;">
<meta name="citation_reference" content="citation_title=The estimation of economic relationships using instrumental variables;,citation_author=J. D. Sargan;,citation_publication_date=1958;,citation_cover_date=1958;,citation_year=1958;,citation_issue=3;,citation_doi=10.2307/1907619;,citation_volume=26;,citation_journal_title=Econometrica;,citation_publisher=JSTOR;">
<meta name="citation_reference" content="citation_title=Large sample properties of generalized method of moments estimators;,citation_author=Lars Peter Hansen;,citation_publication_date=1982;,citation_cover_date=1982;,citation_year=1982;,citation_issue=4;,citation_doi=10.2307/1912775;,citation_volume=50;,citation_journal_title=Econometrica;,citation_publisher=JSTOR;">
<meta name="citation_reference" content="citation_title=Bootstrap procedures under some non-i.i.d. models;,citation_author=Regina Y. Liu;,citation_publication_date=1988;,citation_cover_date=1988;,citation_year=1988;,citation_issue=4;,citation_doi=10.1214/aos/1176351062;,citation_volume=16;,citation_journal_title=The Annals of Statistics;,citation_publisher=Institute of Mathematical Statistics;">
<meta name="citation_reference" content="citation_title=Bootstrap-based improvements for inference with clustered errors;,citation_author=A. Colin Cameron;,citation_author=Jonah B. Gelbach;,citation_author=Douglas L. Miller;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_issue=3;,citation_doi=10.1162/rest.90.3.414;,citation_volume=90;,citation_journal_title=Review of Economics and Statistics;,citation_publisher=MIT Press - Journals;">
<meta name="citation_reference" content="citation_title=Robust inference with multiway clustering;,citation_author=A. Colin Cameron;,citation_author=Jonah B. Gelbach;,citation_author=Douglas L. Miller;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=2;,citation_doi=10.1198/jbes.2010.07136;,citation_volume=29;,citation_journal_title=Journal of Business &amp;amp;amp;amp; Economic Statistics;,citation_publisher=Informa UK Limited;">
<meta name="citation_reference" content="citation_title=La Mafia E I Mafiosi;,citation_author=Antonino Cutrera;,citation_publication_date=1900;,citation_cover_date=1900;,citation_year=1900;">
<meta name="citation_reference" content="citation_title=Estimating the deterrent effect of incarceration using sentencing enhancements;,citation_author=David S Abrams;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_issue=4;,citation_doi=10.1257/app.4.4.32;,citation_volume=4;,citation_journal_title=American Economic Journal: Applied Economics;,citation_publisher=American Economic Association;">
<meta name="citation_reference" content="citation_title=Replication data for: Estimating the deterrent effect of incarceration using sentencing enhancements;,citation_author=David S. Abrams;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_doi=10.3886/E113838V1;,citation_publisher=ICPSR - Interuniversity Consortium for Political; Social Research;">
<meta name="citation_reference" content="citation_title=Earnings losses of displaced workers;,citation_author=Louis S. Jacobson;,citation_author=Robert J. LaLonde;,citation_author=Daniel G. Sullivan;,citation_publication_date=1993;,citation_cover_date=1993;,citation_year=1993;,citation_fulltext_html_url=http://www.jstor.org/stable/2117574;,citation_issue=4;,citation_volume=83;,citation_journal_title=The American Economic Review;,citation_publisher=American Economic Association;">
<meta name="citation_reference" content="citation_title=Bootstrap methods: Another look at the jackknife;,citation_author=B. Efron;,citation_publication_date=1979;,citation_cover_date=1979;,citation_year=1979;,citation_issue=1;,citation_doi=10.1214/aos/1176344552;,citation_volume=7;,citation_journal_title=The Annals of Statistics;,citation_publisher=Institute of Mathematical Statistics;">
<meta name="citation_reference" content="citation_title=Pattern recognition and machine learning;,citation_author=Christopher M. Bishop;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_isbn=9780387310732;,citation_series_title=Information science and statistics;">
<meta name="citation_reference" content="citation_title=Deep learning;,citation_author=Ian Goodfellow;,citation_author=Yoshua Bengio;,citation_author=Aaron Courville;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;">
<meta name="citation_reference" content="citation_title=Deep learning with r;,citation_author=Francois Chollet;,citation_author=J. J. Allaire;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_isbn=9781617295546;">
<meta name="citation_reference" content="citation_title=Classification and regression trees;,citation_author=L. Breiman;,citation_author=J. Friedman;,citation_author=C. J. Stone;,citation_author=R. A. Olshen;,citation_publication_date=1984;,citation_cover_date=1984;,citation_year=1984;,citation_isbn=9780412048418;">
<meta name="citation_reference" content="citation_title=The elements of statistical learning: Data mining, inference, and prediction;,citation_author=T. Hastie;,citation_author=R. Tibshirani;,citation_author=J. Friedman;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_isbn=9780387216065;,citation_series_title=Springer series in statistics;">
<meta name="citation_reference" content="citation_title=ISLR: Data for an introduction to statistical learning with applications in r;,citation_author=Gareth James;,citation_author=Daniela Witten;,citation_author=Trevor Hastie;,citation_author=Rob Tibshirani;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://CRAN.R-project.org/package=ISLR;">
<meta name="citation_reference" content="citation_title=An introduction to statistical learning: With applications in r;,citation_author=Gareth James;,citation_author=Daniela Witten;,citation_author=Trevor Hastie;,citation_author=Robert Tibshirani;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_isbn=9781461471370;,citation_series_title=Springer texts in statistics;">
<meta name="citation_reference" content="citation_title=R markdown: The definitive guide: The definitive guide;,citation_author=Yihui Xie;,citation_author=J. J. Allaire;,citation_author=Garrett Grolemund;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_doi=10.1201/9781138359444;">
<meta name="citation_reference" content="citation_title=R markdown cookbook;,citation_author=Yihui Xie;,citation_author=Christophe Dervieux;,citation_author=Emily Riederer;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://bookdown.org/yihui/rmarkdown-cookbook;,citation_isbn=9780367563837;">
<meta name="citation_reference" content="citation_title=Shiny: Web application framework for r;,citation_author=Winston Chang;,citation_author=Joe Cheng;,citation_author=JJ Allaire;,citation_author=Carson Sievert;,citation_author=Barret Schloerke;,citation_author=Yihui Xie;,citation_author=Jeff Allen;,citation_author=Jonathan McPherson;,citation_author=Alan Dipert;,citation_author=Barbara Borges;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://CRAN.R-project.org/package=shiny;">
<meta name="citation_reference" content="citation_title=Rmarkdown: Dynamic documents for r;,citation_author=JJ Allaire;,citation_author=Yihui Xie;,citation_author=Christophe Dervieux;,citation_author=Jonathan McPherson;,citation_author=Javier Luraschi;,citation_author=Kevin Ushey;,citation_author=Aron Atkins;,citation_author=Hadley Wickham;,citation_author=Joe Cheng;,citation_author=Winston Chang;,citation_author=Richard Iannone;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://github.com/rstudio/rmarkdown;">
<meta name="citation_reference" content="citation_title=Mastering shiny;,citation_author=H. Wickham;,citation_author=an O’Reilly Media Company Safari;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://books.google.de/books?id=ha1CzgEACAAJ;,citation_isbn=9781492047377;">
<meta name="citation_reference" content="citation_title=Recursive partitioning for heterogeneous causal effects;,citation_author=Susan Athey;,citation_author=Guido Imbens;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_issue=27;,citation_doi=10.1073/pnas.1510489113;,citation_volume=113;,citation_journal_title=Proceedings of the National Academy of Sciences;,citation_publisher=Proceedings of the National Academy of Sciences;">
<meta name="citation_reference" content="citation_title=Grf: Generalized random forests;,citation_author=Julie Tibshirani;,citation_author=Susan Athey;,citation_author=Erik Sverdrup;,citation_author=Stefan Wager;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://CRAN.R-project.org/package=grf;">
<meta name="citation_reference" content="citation_title=Statistical inference in two-sample summary-data mendelian randomization using robust adjusted profile score;,citation_author=Qingyuan Zhao;,citation_author=Jingshu Wang;,citation_author=Gibran Hemani;,citation_author=Jack Bowden;,citation_author=Dylan S. Small;,citation_publication_date=2020-06;,citation_cover_date=2020-06;,citation_year=2020;,citation_issue=3;,citation_doi=0.1214/18-AOS1709;,citation_issn=0090-5364;,citation_volume=48;,citation_journal_title=The Annals of Statistics;,citation_publisher=Institute of Mathematical Statistics;">
<meta name="citation_reference" content="citation_title=Generalized random forests;,citation_author=Susan Athey;,citation_author=Julie Tibshirani;,citation_author=Stefan Wager;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=2;,citation_doi=10.1214/18-aos1709;,citation_volume=47;,citation_journal_title=The Annals of Statistics;,citation_publisher=Institute of Mathematical Statistics;">
</head>
<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="navbar navbar-expand-lg " data-bs-theme="dark"><div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Kausalanalyse und maschinelles Lernen mit R</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/mca91/kasa_book" title="Quellcode" class="quarto-navigation-tool px-1" aria-label="Quellcode"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
<li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=%7Curl%7C">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=%7Curl%7C">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.linkedin.com/sharing/share-offsite/?url=%7Curl%7C">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
</div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Lesemodus umschalten">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          <div id="quarto-search" class="" title="Suchen"></div>
      </div> <!-- /container-fluid -->
    </nav><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Seitenleiste umschalten" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./RegReg.html">Machine Learning</a></li><li class="breadcrumb-item"><a href="./trees.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Baum-basierte Methoden</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Seitenleiste umschalten" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Einleitung</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Grundlagen</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Abschnitt umschalten">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R_Einfuehrung.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistische Programmierung mit R</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./RR.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Reproduzierbarkeit</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Reg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Simulation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Simulation</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Kausale Inferenz</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Abschnitt umschalten">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Matching.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./FixedEffects.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Panel-Daten</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./IV.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">IV-Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./DiD.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Difference-in-Differences</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./EventStudies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Event Studies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./RDD.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Regression Discontinuity Designs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./SyntheticControl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Synthetic Control</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Abschnitt umschalten">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./RegReg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Regularisierte Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./svm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./trees.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Baum-basierte Methoden</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Machine Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Neuronale Netzwerke</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Literatur.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Literatur</span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title"><a href="./index.html">Übersicht</a></h2>
   
  <ul>
<li><a href="#sec-simpletrees" id="toc-sec-simpletrees" class="nav-link active" data-scroll-target="#sec-simpletrees"><span class="header-section-number">15.1</span> Entscheidungsbäume</a></li>
  <li><a href="#training-von-b%C3%A4umen" id="toc-training-von-bäumen" class="nav-link" data-scroll-target="#training-von-b%C3%A4umen"><span class="header-section-number">15.2</span> Training von Bäumen</a></li>
  <li><a href="#bagging" id="toc-bagging" class="nav-link" data-scroll-target="#bagging"><span class="header-section-number">15.3</span> Bagging</a></li>
  <li><a href="#sec-brf" id="toc-sec-brf" class="nav-link" data-scroll-target="#sec-brf"><span class="header-section-number">15.4</span> Random Forests</a></li>
  <li><a href="#sec-boosting" id="toc-sec-boosting" class="nav-link" data-scroll-target="#sec-boosting"><span class="header-section-number">15.5</span> Boosting</a></li>
  <li>
<a href="#causal-trees-und-causal-forests" id="toc-causal-trees-und-causal-forests" class="nav-link" data-scroll-target="#causal-trees-und-causal-forests"><span class="header-section-number">15.6</span> Causal Trees und Causal Forests</a>
  <ul>
<li><a href="#causal-trees" id="toc-causal-trees" class="nav-link" data-scroll-target="#causal-trees"><span class="header-section-number">15.6.1</span> Causal Trees</a></li>
  <li>
<a href="#causal-forests" id="toc-causal-forests" class="nav-link" data-scroll-target="#causal-forests"><span class="header-section-number">15.6.2</span> Causal Forests</a>
  <ul class="collapse">
<li><a href="#inferenz-f%C3%BCr-effekt-sch%C3%A4tzungen" id="toc-inferenz-für-effekt-schätzungen" class="nav-link" data-scroll-target="#inferenz-f%C3%BCr-effekt-sch%C3%A4tzungen"><span class="header-section-number">15.6.2.1</span> Inferenz für Effekt-Schätzungen</a></li>
  </ul>
</li>
  </ul>
</li>
  <li><a href="#zusammenfassung" id="toc-zusammenfassung" class="nav-link" data-scroll-target="#zusammenfassung"><span class="header-section-number">15.7</span> Zusammenfassung</a></li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/mca91/kasa_book/edit/main/trees.qmd" class="toc-action"><i class="bi bi-github"></i>Seite editieren</a></li><li><a href="https://github.com/mca91/kasa_book/issues/new" class="toc-action"><i class="bi empty"></i>Problem melden</a></li><li><a href="https://github.com/mca91/kasa_book/blob/main/trees.qmd" class="toc-action"><i class="bi empty"></i>Quellcode anzeigen</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./RegReg.html">Machine Learning</a></li><li class="breadcrumb-item"><a href="./trees.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Baum-basierte Methoden</span></a></li></ol></nav><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-trees" class="quarto-section-identifier"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Baum-basierte Methoden</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<p><i class="callout-icon"></i></p>
</div>
<div class="callout-body-container">
<p>
<strong>Hinweis</strong>
</p>
<p>
Für eine optimale technische Stabilität empfehlen wir, dieses Online-Lehrbuch auf einem Notebook oder Desktop-Computer zu nutzen. Die interaktiven Komponente, insbesondere die R-Konsole (WebR), sind rechenintensiv und funktionieren auf mobilen Geräten nur eingeschränkt.
</p>
</div>
</div>
</div>
</div>
</div><p>Baum-basierte Methoden bieten eine vielseitige und leistungsstarke Herangehensweise für Vorhersage und Klassifikation in komplexen Datensätzen mit nicht-linearen Zusammenhängen. Ein Vorteil baum-basierter Methoden ist ihre inhärente Fähigkeit, die Bedeutung einzelner Variablen für die Vorhersage zu quantifizieren – eine Eigenschaft, die viele Machine-Learning-Modelle nicht ohne weiteres bieten und insbesondere in hoch-dimensionalen Anwendungen (mit vielen potentiellen Regressoren) nicht trivial ist. Dies ermöglicht es, tiefere Einblicke in den Einfluss einzelner Merkmale auf die Vorhersagen des Modells zu erhalten, was besonders in empirischen Anwendungen für die Entscheidungsstützung mit Machine Learning hilfreich sein kann.</p>
<p><em>Entscheidungsbäume</em> stellen die Grundlage dieser Methoden dar. Sie ermöglichen die Aufteilung der Daten in immer kleinere, homogenere Gruppen, basierend auf <em>binären</em> Entscheidungsregeln, die aus den Prädiktoren abgleitet werden. Die trainierten Regeln eines solchen Modells lassen sich anhand eines Binärbaums visualisieren, was eine intuitive Interpretierbarkeit der Ergebnisse erlaubt.</p>
<p><em>Random Forests</em> ist ein Ensemble-Ansatz, bei dem viele Entscheidungsbäume kombiniert werden. Jeder Baum wird auf einer zufälligen Teilmenge der Daten trainiert (<em>Bagging</em>), und bei jedem Knoten wird zusätzlich eine zufällige Teilmenge der Merkmale berücksichtigt. Die finale Vorhersage des Random Forests basiert auf der Aggregation der Vorhersagen aller Bäume (Mehrheitsvotum für Klassifikation, Durchschnitt für Regression). Dieses Verfahren reduziert das Risiko einer Überanpassung und erhöht oft die Vorhersagegenauigkeit im Vergleich zu einzelnen Entscheidungsbäumen.</p>
<p><em>Boosting</em> ist eine weitere Ensemble-Methode zur Anpassung von Modellen mit hoher Vorhersagegüte durch Kombination einfacher Modelle (<em>Base learner</em>), wobei Regressions- oder Klassifikationsbäume eingesetzt werden können. Alternativ zu Random Forests trainieren Boosting-Algorithmen sukzessiv einfache (Klassifikations- oder Regressions-)Bäume, wobei jeder nachfolgende Baum das Ziel hat, die Vorhersagefehler der vorherigen Bäume zu korrigieren.</p>
<p>In diesem Kapitel erläutern wir die Anwendung baum-basierter Methoden in R anhand von Beispieldatensätzen. Wir zeigen, wie Regressionsbäume, Random Forests und Boosting-Modelle im <code>parsnip</code>-Framework trainiert werden und wie die Vorhersageleistung durch die Wahl geeigneter Hyperparameter mit Cross-Validation und Out-of-Sample-Evaluierungsmethoden optimiert werden kann.</p>
<section id="sec-simpletrees" class="level2 page-columns page-full" data-number="15.1"><h2 data-number="15.1" class="anchored" data-anchor-id="sec-simpletrees">
<span class="header-section-number">15.1</span> Entscheidungsbäume</h2>
<p>Ein Entscheidungsbaum ist ein Modell, das auf der Basis von hierarchischen Bedingungen bzgl. der Regressoren Vorhersagen für die Outcome-Variable trifft. Jeder Baum beginnt mit einem Wurzelknoten (<em>root node</em>) und verzweigt sich binär. Jede Verzweigung (<em>split</em>) stellt eine Bedingung dar, die auf einem bestimmten Regressor basiert. Der Baum trifft Entscheidungen, indem er diese Bedingungen sukzessive überprüft, bis er zu einem Blattknoten (<em>leaf node</em> / <em>terminal node</em>) gelangt, der die finale Vorhersage liefert. Hierbei handelt es sich eine Mehrheitsentscheidung für Klassifikation und einen Mittelwert, jeweils gebildet anhand Beobachten des Trainingsdatensatzes im leaf node.</p>
<p><a href="#fig-exdectree" class="quarto-xref">Abbildung&nbsp;<span>15.1</span></a> zeigt ein einfaches Beispiel eines Entscheidungsbaums zur Klassifikation der Kreditwürdigkeit einer Person. Die Klassfikation erfolgt, in dem die Beobachtung basierend auf den Merkmalen Alter, Einkommen und Eigentum durch den Baum geleitet wird. Zunächst wird geprüft, die Person 30 Jahre oder jünger ist. Fall ja, entscheidet der Baum anhand des Einkommens: Bei einem Jahreseinkommen von 40.000 oder weniger wird die Person als wenig kreditwürdig klassifiziert, bei höherem Einkommen als mäßig kreditwürdig. Für Personen älter als 30 Jahre überprüft das Modell lediglich, ob die Person eine Immobilie besitzt, um zwischen mäßiger Kreditwürdigkeit und guter Bonität zu unterscheiden.</p>
<div class="cell page-columns page-full" data-fig-width="6" data-fig-height="5" data-layout-align="default">
<div class="cell-output-display page-columns page-full">
<div id="fig-exdectree" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-exdectree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<svg width="576" height="480" viewbox="10.80 10.80 571.96 304.40" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none"><g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(14.8 300.4)"><title>exdectree</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-289.6 557.16,-289.6 557.16,4 -4,4"></polygon><!-- 1 --><g id="node1" class="node"><title>1</title>
<polygon fill="none" stroke="black" points="314.96,-285.6 227.42,-285.6 227.42,-249.6 314.96,-249.6 314.96,-285.6"></polygon><text text-anchor="middle" x="271.19" y="-263.4" font-family="Times,serif" font-size="14.00">Alter &lt;= 30?</text></g><!-- 2 --><g id="node2" class="node"><title>2</title>
<polygon fill="none" stroke="black" points="229.13,-160.8 73.25,-160.8 73.25,-124.8 229.13,-124.8 229.13,-160.8"></polygon><text text-anchor="middle" x="151.19" y="-138.6" font-family="Times,serif" font-size="14.00">Einkommen &lt;= 40 Tsd.?</text></g><!-- 1&#45;&gt;2 --><g id="edge1" class="edge"><title>1-&gt;2</title>
<path fill="none" stroke="black" d="M254.51,-249.53C233.93,-228.47 198.82,-192.54 175.31,-168.48"></path><polygon fill="black" stroke="black" points="177.61,-165.83 168.12,-161.13 172.61,-170.72 177.61,-165.83"></polygon><text text-anchor="middle" x="206.02" y="-201" font-family="Times,serif" font-size="14.00">Ja</text></g><!-- 3 --><g id="node3" class="node"><title>3</title>
<polygon fill="none" stroke="black" points="431.34,-160.8 355.04,-160.8 355.04,-124.8 431.34,-124.8 431.34,-160.8"></polygon><text text-anchor="middle" x="393.19" y="-138.6" font-family="Times,serif" font-size="14.00">Eigentum?</text></g><!-- 1&#45;&gt;3 --><g id="edge2" class="edge"><title>1-&gt;3</title>
<path fill="none" stroke="black" d="M288.15,-249.53C309.07,-228.47 344.77,-192.54 368.67,-168.48"></path><polygon fill="black" stroke="black" points="371.41,-170.69 375.98,-161.13 366.45,-165.75 371.41,-170.69"></polygon><text text-anchor="middle" x="353.8" y="-201" font-family="Times,serif" font-size="14.00">Nein</text></g><!-- 4 --><g id="node4" class="node"><title>4</title>
<polygon fill="none" stroke="black" points="100.57,-36 -0.19,-36 -0.19,0 100.57,0 100.57,-36"></polygon><text text-anchor="middle" x="50.19" y="-13.8" font-family="Times,serif" font-size="14.00">Status: Niedrig</text></g><!-- 2&#45;&gt;4 --><g id="edge3" class="edge"><title>2-&gt;4</title>
<path fill="none" stroke="black" d="M137.15,-124.73C119.98,-103.86 90.79,-68.36 71.01,-44.31"></path><polygon fill="black" stroke="black" points="73.5,-41.83 64.44,-36.33 68.09,-46.27 73.5,-41.83"></polygon><text text-anchor="middle" x="55.02" y="-76.2" font-family="Times,serif" font-size="14.00">Ja</text></g><!-- 5 --><g id="node5" class="node"><title>5</title>
<polygon fill="none" stroke="black" points="329.03,-36 237.35,-36 237.35,0 329.03,0 329.03,-36"></polygon><text text-anchor="middle" x="283.19" y="-13.8" font-family="Times,serif" font-size="14.00">Status: Mittel</text></g><!-- 2&#45;&gt;5 --><g id="edge4" class="edge"><title>2-&gt;5</title>
<path fill="none" stroke="black" d="M169.55,-124.73C192.27,-103.58 231.14,-67.43 256.99,-43.37"></path><polygon fill="black" stroke="black" points="259.63,-45.7 264.57,-36.33 254.86,-40.57 259.63,-45.7"></polygon><text text-anchor="middle" x="238.8" y="-76.2" font-family="Times,serif" font-size="14.00">Nein</text></g><!-- 3&#45;&gt;5 --><g id="edge6" class="edge"><title>3-&gt;5</title>
<path fill="none" stroke="black" d="M377.9,-124.73C359.12,-103.77 327.13,-68.05 305.58,-44"></path><polygon fill="black" stroke="black" points="307.99,-41.44 298.71,-36.33 302.78,-46.11 307.99,-41.44"></polygon><text text-anchor="middle" x="347.8" y="-76.2" font-family="Times,serif" font-size="14.00">Nein</text></g><!-- 6 --><g id="node6" class="node"><title>6</title>
<polygon fill="none" stroke="black" points="553.13,-36 465.25,-36 465.25,0 553.13,0 553.13,-36"></polygon><text text-anchor="middle" x="509.19" y="-13.8" font-family="Times,serif" font-size="14.00">Status: Hoch</text></g><!-- 3&#45;&gt;6 --><g id="edge5" class="edge"><title>3-&gt;6</title>
<path fill="none" stroke="black" d="M409.32,-124.73C429.21,-103.67 463.15,-67.74 485.87,-43.68"></path><polygon fill="black" stroke="black" points="488.5,-46 492.83,-36.33 483.41,-41.19 488.5,-46"></polygon><text text-anchor="middle" x="464.02" y="-76.2" font-family="Times,serif" font-size="14.00">Ja</text></g></g></svg>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-exdectree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Abbildung&nbsp;15.1: Entscheidungsbaum: Klassifikation von Kreditwürdigkeit
</figcaption></figure>
</div>
</div>
</div>
</section><section id="training-von-bäumen" class="level2 page-columns page-full" data-number="15.2"><h2 data-number="15.2" class="anchored" data-anchor-id="training-von-bäumen">
<span class="header-section-number">15.2</span> Training von Bäumen</h2>
<p>Zur Konstruktion von Binär-Bäumen werden etablierte Algorithmen wie <em>Classification and Regression Trees</em> (<a href="https://de.wikipedia.org/wiki/CART_(Algorithmus)">CART</a> von <span class="citation" data-cites="Breimanetal1984">Breiman u.&nbsp;a. (<a href="Literatur.html#ref-Breimanetal1984" role="doc-biblioref">1984</a>)</span> verwendet. Die wesentliche Vorgehensweise für das Training eines Baums <span class="math inline">\(T\)</span> ist wie folgt:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
CART-Algorithmus
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>
<p><strong>Splitting</strong>: Beginnend am root node sucht der Algorithmus nach der “besten” Regel, die Daten anhand eines Merkmals in zwei Gruppen zu teilen. Die Qualität des Splits wird in Abhängigkeit der Definition der Outcome-Variable beurteilt:</p>
<ul>
<li><p><strong>Bei Klassifikation</strong>: Die Reinheit (<em>purtity</em>) der Klassen in den unmittelbar nachfolgen nodes wird maximiert. Ein gängiges Kriterium hierfür ist der <a href="https://de.wikipedia.org/wiki/Gini-Koeffizient"><em>Gini-Koeffizient</em></a>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p></li>
<li><p><strong>Bei Regression</strong>: Der MSE bei Vorhersage des Outcomes durch Mittelwertbildung für Beobachtungen in den unmittelbar nachfolgenden nodes wird minimiert.</p></li>
</ul>
</li>
<li>
<p><strong>Rekursion</strong>: Der Prozess wird rekursiv fortgesetzt, bis Abbruchkriterien greifen eine weitere Verzewigung verhindern:</p>
<ul>
<li>Die maximale Baumtiefe (<em>tree depth</em>) ist erreicht</li>
<li>Die leaf nodes sind hinreichend “rein”: Alle Beobachtungen in einem leaf node gehören zur gleichen Klasse oder die Verbesserung des Loss durch weitere Splits fällt unter einen festgelegten Schwellenwert</li>
<li>Weitere Splits führen zu leaf nodes, die eine Mindestanzahl an Beobachtungen (<em>minimum split</em>) unterschreiten würden</li>
</ul>
</li>
<li>
<p><strong>Pruning</strong>: Um Überanpassung an die Trainingsdaten zu vermeiden, kann der Baum beschnitten werden (<em>pruning</em>). Der Grundgedanke ist, dass tief verzweigte Bäume die Trainingsdaten zwar gut modellieren können, aber schlecht auf neue, unbekannte Daten generalisieren.</p>
<p>Bei <em>cost complexity (CP) pruning</em> werden, beginnend auf Ebene der leaf nodes sukuzessive Äste entfernt, und eine Balance zwischen Komplexität des Baums und dem Anpassungsfehler zu finden. Ähnlich wie bei regularisierter KQ-Schätzung (<a href="RegReg.html" class="quarto-xref"><span>Kapitel 13</span></a>), wird die Verlustfunktion <span class="math inline">\(L\)</span> um einen Strafterm für die Komplexität erweitert. Der Effekt der Strafe wird durch den CP-Parameter <span class="math inline">\(\alpha\in[0,1]\)</span> geregelt,</p>
<p><span class="math display">\[\begin{align*}
   L_{\alpha}(T) = L(T) + \alpha \lvert T\rvert,
\end{align*}\]</span></p>
</li>
</ol>
<p>für einen Baum <span class="math inline">\(T\)</span> mit Komplexitätsmaß <span class="math inline">\(\lvert T\rvert\)</span> (Anzahl der leaf nodes) <span class="citation" data-cites="Hastieetal2013">(<a href="Literatur.html#ref-Hastieetal2013" role="doc-biblioref">Hastie, Tibshirani, und Friedman 2013</a>)</span>.</p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Der Gini-Koeffizient <span class="math inline">\(0\leq G\leq1\)</span> misst die Homogenität der Outcome-Variable für die Beobachtungen eines Knotens. <span class="math inline">\(G=0\)</span> ergibt sich bei vollständiger “Reinheit” (alle Beobachtungen im Knoten gehören zur gleichen Klasse). <span class="math inline">\(G &gt; 0\)</span> zeigt Heterogenität der Klassen an, die mit <span class="math inline">\(G\)</span> zunimmt</p></div></div><p>Zur Demonstation der Schätzung von Regressionsbäumen mit R betrachten wir nachfolgend den Datensatz <code>MASS::Bosten</code>. Ziel hierbei ist es, mittlere Hauswerte <code>medv</code> in Stadteilen von Boston, MA vorherzusagen. Wir verwenden hierzu Funktionen aus dem Paket <code>parsnip</code>.</p>
<p>Zunächst transformieren wir den Datensatz in ein <code>tibble</code>-Objekt und definieren Trainings- und Test-Daten.</p>
<div class="cell">
<div>
<div id="webr-1">

</div>
<script type="webr-1-contents">
eyJhdHRyIjp7Indhcm5pbmciOmZhbHNlLCJlZGl0Ijp0cnVlLCJmaWctd2lkdGgiOiI4IiwiZXZhbCI6dHJ1ZSwidGltZWxpbWl0IjoiNjAiLCJtZXNzYWdlIjpmYWxzZSwiY29tcGxldGlvbiI6dHJ1ZX0sImNvZGUiOiJsaWJyYXJ5KHBhcnNuaXApXG5saWJyYXJ5KGNvd3Bsb3QpXG5cbiMgU2VlZCBzZXR6ZW5cbnNldC5zZWVkKDEyMzQpXG5cbiMgRGF0ZW5zYXR6IGFscyB0aWJibGVcbkJvc3RvbiA8LSBhc190aWJibGUoTUFTUzo6Qm9zdG9uKVxuXG4jIFNwbGl0dGluZyBpbiBUcmFpbmluZy0gdW5kIFRlc3QtRGF0ZW5cbkJvc3Rvbl9zcGxpdCA8LSBpbml0aWFsX3NwbGl0KFxuICBkYXRhID0gQm9zdG9uLCBcbiAgcHJvcCA9IDAuOCwgXG4gIHN0cmF0YSA9IG1lZHZcbiAgKVxuXG5Cb3N0b25fdHJhaW4gPC0gdHJhaW5pbmcoQm9zdG9uX3NwbGl0KVxuQm9zdG9uX3Rlc3QgPC0gdGVzdGluZyhCb3N0b25fc3BsaXQpXG5cbnNsaWNlX2hlYWQoQm9zdG9uX3RyYWluLCBuID0gMTApIn0=
</script>
</div>
</div>
<p><code>parsnip</code> bietet eine vereinheitlichetes Framework für das Training von Modellen mit R und eine flexible API für Machine Learning. Wir definieren zunächst mit <code><a href="https://parsnip.tidymodels.org/reference/decision_tree.html">parsnip::decision_tree()</a></code> eine Spezifikation zum Training von Entschieundgsmodellen und übergeben beispielhaft einen CP-Parameter <span class="math inline">\(\alpha=.1\)</span>. Mit <code><a href="https://parsnip.tidymodels.org/reference/set_engine.html">parsnip::set_engine</a></code> wählen wir das Paket <code>raprt</code>. Der hier implementierte Agorithmus ist CART. Zuletzt legen wir mit <code><a href="https://parsnip.tidymodels.org/reference/set_args.html">parsnip::set_mode()</a></code> fest, dass der Algorithmus für Regression durchgeführt werden soll.</p>
<div class="cell">
<div>
<div id="webr-2">

</div>
<script type="webr-2-contents">
eyJhdHRyIjp7Indhcm5pbmciOmZhbHNlLCJlZGl0Ijp0cnVlLCJmaWctd2lkdGgiOiI4IiwiZXZhbCI6dHJ1ZSwidGltZWxpbWl0IjoiNjAiLCJtZXNzYWdlIjpmYWxzZSwiY29tcGxldGlvbiI6dHJ1ZX0sImNvZGUiOiIjIFNwZXppZmlrYXRpb24gZmVzdGxlZ2VuXG50cmVlX3NwZWMgPC0gZGVjaXNpb25fdHJlZShcbiAgY29zdF9jb21wbGV4aXR5ID0gMC4xXG4gICkgJT4lXG4gIHNldF9lbmdpbmUoXCJycGFydFwiKSAlPiVcbiAgc2V0X21vZGUoXCJyZWdyZXNzaW9uXCIpXG5cbiMgTW9kZWxsIHRyYWluaWVyZW5cbnRyZWVfZml0IDwtIHRyZWVfc3BlYyAlPiVcbiAgZml0KFxuICAgIGZvcm11bGEgPSBtZWR2IH4gLiwgXG4gICAgZGF0YSA9IEJvc3Rvbl90cmFpbiwgXG4gICAgbW9kZWwgPSBUUlVFXG4gIClcblxuIyBUcmFpbmllcnRlbiBCYXVtIGluIEtvbnNvbGUgYXVzZ2ViZW5cbnRyZWVfZml0JGZpdCJ9
</script>
</div>
</div>
<p>Der Output in <code>tree_fit$fit</code> zeigt, dass CP-Pruning zu einem kleinen Baum mit 3 Hierarchie-Ebenen geführt hat. Die Struktur zeigt, dass <code>lstat</code> und <code>rm</code> für Splitting-Regeln (<code>split</code>) verwendet werden, wie viele Beobachtungen den nodes zugeordnet sind (<code>n</code>), den Wert der Verlustfunktion (<code>deviance</code>) sowie den Durchschnitt von <code>medv</code> für jede node (<code>yval</code>). Für die drei leaf nodes (gekennzeichnet mit <code>*</code>) ist <code>yval</code> die Vorhersage der Outcome-Varibale für entsprechend gruppierte Beobachtungen.</p>
<p>Eine leichter interpretierbare Darstellung der Entscheidungsregeln des angepassten Baums in <code>tree_fit$fit</code> erhalten wir mit <code><a href="https://rdrr.io/pkg/rattle/man/fancyRpartPlot.html">rattle::fancyRpartPlot()</a></code>.</p>
<div class="cell">
<div>
<div id="webr-3">

</div>
<script type="webr-3-contents">
eyJhdHRyIjp7Indhcm5pbmciOmZhbHNlLCJlZGl0Ijp0cnVlLCJldmFsIjp0cnVlLCJmaWctd2lkdGgiOjgsImZpZy1oZWlnaHQiOjgsInRpbWVsaW1pdCI6IjYwIiwibWVzc2FnZSI6ZmFsc2UsImNvbXBsZXRpb24iOnRydWV9LCJjb2RlIjoibGlicmFyeShyYXR0bGUpXG5cbiMgUGxvdCB0aGUgZGVjaXNpb24gdHJlZVxuZmFuY3lScGFydFBsb3QoXG4gIHRyZWVfZml0JGZpdCxcbiAgc3BsaXQuY29sID0gXCJibGFja1wiLCBcbiAgbm4uY29sID0gXCJibGFja1wiLCBcbiAgY2FwdGlvbiA9IFwiVHJhaW5pZXJ0ZXIgRW50c2NoZWlkdW5nc2JhdW0gZsO8ciBjcCA9IDAuMVwiLFxuICBwYWxldHRlID0gXCJTZXQxXCIsXG4gIGJyYW5jaC5jb2wgPSBcImJsYWNrXCIsXG4gIGRpZ2l0cyA9IDNcbikifQ==
</script>
</div>
</div>
<p><a href="#fig-rpartregspace" class="quarto-xref">Abbildung&nbsp;<span>15.2</span></a> zeigt Beobachtungen von <code>rm</code> und <code>lstat</code>, die hinsichtlich ihrer in drei Klassen eingeteilten Ausprägung von <code>medv</code> eingefärbt sind. Die durch den CART-Algorithmus gelernten Entscheidungsregeln sind als farbige Paritionen des Regressorraums dargestellt.</p>
<div class="cell page-columns page-full">
<div class="cell-output-display page-columns page-full">
<div id="fig-rpartregspace" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-rpartregspace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="trees_files/figure-html/fig-rpartregspace-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-rpartregspace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Abbildung&nbsp;15.2: Partitionierung des Regressorraums für <code>lstat</code> und <code>rm</code> durch Regressionsbaum
</figcaption></figure>
</div>
</div>
</div>
<p>Für eine datengetriebene Wahl des CP-Parameters <span class="math inline">\(\alpha\)</span> kann Cross Validation (CV) verwendet werden. Hierzu erstellen wir zunächst eine <code>parsnip</code>-Spezifikation mit <code>cost_complexity = tune::tune()</code> in <code>decision_tree()</code> und erstellen einen <em>workflow</em> mit <code>parsnip::workflow()</code></p>
<div class="cell">
<div>
<div id="webr-4">

</div>
<script type="webr-4-contents">
eyJhdHRyIjp7Indhcm5pbmciOmZhbHNlLCJlZGl0Ijp0cnVlLCJmaWctd2lkdGgiOiI4IiwiZXZhbCI6dHJ1ZSwidGltZWxpbWl0IjoiNjAiLCJtZXNzYWdlIjpmYWxzZSwiY29tcGxldGlvbiI6dHJ1ZX0sImNvZGUiOiIjIFNwZXppZmlrYXRpb24gZsO8ciBDViB2b24gY29zdF9jb21wbGV4aXR5XG50cmVlX3NwZWNfY3YgPC0gZGVjaXNpb25fdHJlZShcbiAgY29zdF9jb21wbGV4aXR5ID0gdHVuZSgpXG4gICkgJT4lXG4gIHNldF9lbmdpbmUoXCJycGFydFwiKSAlPiVcbiAgc2V0X21vZGUoXCJyZWdyZXNzaW9uXCIpXG5cbiMgV29ya2Zsb3cgZGVmaW5pZXJlblxudHJlZV93Zl9jdiA8LSB3b3JrZmxvdygpICU+JVxuICBhZGRfbW9kZWwodHJlZV9zcGVjX2N2KSAlPiVcbiAgYWRkX2Zvcm11bGEobWVkdiB+IC4pIn0=
</script>
</div>
</div>
<p>Mit <code><a href="https://rsample.tidymodels.org/reference/vfold_cv.html">rsample::vfold_cv()</a></code> definieren wir den CV-Prozess: 10-fold CV mit 2 Wiederholungen. <code><a href="https://tune.tidymodels.org/reference/tune_grid.html">tune::tune_grid()</a></code> führt CV anhand des in <code>tree_wf_cv</code> definierten workflows durch. Hierbei werden in <code>cp_grid</code> festgelegte Werte von <code>cost_complexity</code> berücksichtigt. Die mit <code>yardstick::metric_set(rmse)</code> festgelegte Verlustfunktion ist der mittlere quadratische Fehler (RMSE).<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Die hier verwedete Funktion ist <code><a href="https://yardstick.tidymodels.org/reference/rmse.html">yardstick::rmse()</a></code>.</p></div></div><div class="cell">
<div>
<div id="webr-5">

</div>
<script type="webr-5-contents">
eyJhdHRyIjp7Indhcm5pbmciOmZhbHNlLCJlZGl0Ijp0cnVlLCJmaWctd2lkdGgiOiI4IiwiZXZhbCI6dHJ1ZSwidGltZWxpbWl0IjoiNjAiLCJtZXNzYWdlIjpmYWxzZSwiY29tcGxldGlvbiI6dHJ1ZX0sImNvZGUiOiIjIENWLVByb3plc3MgZGVmaW5pZXJlblxuY3ZfZm9sZHMgPC0gdmZvbGRfY3YoXG4gIGRhdGEgPSBCb3N0b25fdHJhaW4sIFxuICB2ID0gMTAsIFxuICByZXBlYXRzID0gMlxuKVxuXG4jIENWIGR1cmNoZsO8aHJlbjpcbnNldC5zZWVkKDEyMzQpXG5cbiMgR3JpZCBkZWZpbmllcmVuXG5jcF9ncmlkIDwtIHRpYmJsZShcbiAgY29zdF9jb21wbGV4aXR5ID0gYyhcbiAgICAwLjEsIC4wNzUsIDAuMDUsIDAuMDEsIDAuMDAxLCAwLjAwMDFcbiAgICApXG4gIClcblxuIyBUdW5pbmcgbWl0IENWXG50cmVlX2ZpdF9jdiA8LSB0cmVlX3dmX2N2ICU+JVxuICAgIHR1bmVfZ3JpZChcbiAgICAgICAgcmVzYW1wbGVzID0gY3ZfZm9sZHMsIFxuICAgICAgICBncmlkID0gY3BfZ3JpZCxcbiAgICAgICAgbWV0cmljcyA9IG1ldHJpY19zZXQocm1zZSlcbiAgICApXG5cbiMgQ1YtRXJnZWJuaXNzZVxudHJlZV9maXRfY3YifQ==
</script>
</div>
</div>
<p>Mit <code><a href="https://ggplot2.tidyverse.org/reference/autoplot.html">workflowsets::autoplot()</a></code> kann der CV-RMSE für als Funktion des CP-Parameter leicht grafisch betrachtet dargestellt werden.</p>
<div class="cell">
<div>
<div id="webr-6">

</div>
<script type="webr-6-contents">
eyJhdHRyIjp7Indhcm5pbmciOmZhbHNlLCJlZGl0Ijp0cnVlLCJmaWctd2lkdGgiOiI4IiwiZXZhbCI6dHJ1ZSwidGltZWxpbWl0IjoiNjAiLCJtZXNzYWdlIjpmYWxzZSwiY29tcGxldGlvbiI6dHJ1ZX0sImNvZGUiOiIjIENWLUVyZ2Vibmlzc2UgdmlzdWFsaXNpZXJlblxuYXV0b3Bsb3QodHJlZV9maXRfY3YpICtcbiAgbGFicyhcbiAgICB0aXRsZSA9IFwiQ1YgZsO8ciBDUC1QYXJhbWV0ZXI6IFJNU0UgdnMuIEtvbXBsZXhpdMOkdFwiXG4gICkgK1xuICB0aGVtZV9jb3dwbG90KCkifQ==
</script>
</div>
</div>
<p>Für eine tabellierte Übersicht der besten Modelle kann <code><a href="https://tune.tidymodels.org/reference/show_best.html">tune::show_best()</a></code> verwendet werden. <code><a href="https://tune.tidymodels.org/reference/show_best.html">tune::select_best()</a></code> liest die beste Parameter-Kombination aus.</p>
<div class="cell">
<div>
<div id="webr-7">

</div>
<script type="webr-7-contents">
eyJhdHRyIjp7Indhcm5pbmciOmZhbHNlLCJlZGl0Ijp0cnVlLCJmaWctd2lkdGgiOiI4IiwiZXZhbCI6dHJ1ZSwidGltZWxpbWl0IjoiNjAiLCJtZXNzYWdlIjpmYWxzZSwiY29tcGxldGlvbiI6dHJ1ZX0sImNvZGUiOiIjIFRhYmVsbGFyaXNjaGUgw5xiZXJzaWNodFxuc2hvd19iZXN0KFxuICB4ID0gdHJlZV9maXRfY3YsIFxuICBtZXRyaWMgPSBcInJtc2VcIlxuKVxuXG4jIEdldHVudGVyIFBhcmVtYXRlclxuYmVzdF90cmVlX2ZpdCA8LSBzZWxlY3RfYmVzdChcbiAgeCA9IHRyZWVfZml0X2N2LCBcbiAgbWV0cmljID0gXCJybXNlXCJcbilcblxuYmVzdF90cmVlX2ZpdCJ9
</script>
</div>
</div>
<p>Anhand <code>tree_fit_cv</code> trainieren wir die finale Spezifikation.</p>
<div class="cell">
<div>
<div id="webr-8">

</div>
<script type="webr-8-contents">
eyJhdHRyIjp7Indhcm5pbmciOmZhbHNlLCJlZGl0Ijp0cnVlLCJmaWctd2lkdGgiOiI4IiwiZXZhbCI6dHJ1ZSwidGltZWxpbWl0IjoiNjAiLCJtZXNzYWdlIjpmYWxzZSwiY29tcGxldGlvbiI6dHJ1ZX0sImNvZGUiOiIjIEZpbmFsZXMgTW9kZWxsIHNjaMOkdHplblxuZmluYWxfdHJlZV9zcGVjIDwtIGRlY2lzaW9uX3RyZWUoXG4gIGNvc3RfY29tcGxleGl0eSA9IGJlc3RfdHJlZV9maXQkY29zdF9jb21wbGV4aXR5XG4gICkgJT4lXG4gIHNldF9lbmdpbmUoXCJycGFydFwiKSAlPiVcbiAgc2V0X21vZGUoXCJyZWdyZXNzaW9uXCIpXG5cbmZpbmFsX3RyZWVfZml0IDwtIGZpbmFsX3RyZWVfc3BlYyAlPiVcbiAgZml0KFxuICAgIGZvcm11bGEgPSBtZWR2IH4gLiwgXG4gICAgZGF0YSA9IEJvc3Rvbl90cmFpblxuICApXG5cbiMgZmluYWxfdHJlZV9maXQifQ==
</script>
</div>
</div>
<p>Der geringe CP-Parameter führt zu einem großen Entscheidungsbaum.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Die Dimension der Grafik wurde hier zwecks Darstellung des gesamten Baums gewählt. <code>print(final_tree_fit$fit)</code> druckt die Entscheidungsregeln in die R-Konsole (hierzu die letzte Zeile ausführen).</p></div></div><div class="cell">
<div>
<div id="webr-9">

</div>
<script type="webr-9-contents">
eyJhdHRyIjp7Indhcm5pbmciOmZhbHNlLCJlZGl0Ijp0cnVlLCJldmFsIjp0cnVlLCJmaWctd2lkdGgiOjgsImZpZy1oZWlnaHQiOjgsInRpbWVsaW1pdCI6IjYwIiwibWVzc2FnZSI6ZmFsc2UsImNvbXBsZXRpb24iOnRydWV9LCJjb2RlIjoiXG4jIENWLUZpdCBwbG90dGVuXG5mYW5jeVJwYXJ0UGxvdChcbiAgZmluYWxfdHJlZV9maXQkZml0LFxuICBzcGxpdC5jb2wgPSBcImJsYWNrXCIsIFxuICBubi5jb2wgPSBcImJsYWNrXCIsIFxuICBjYXB0aW9uID0gXCJNaXQgQ1YgZXJtaXR0ZWx0ZXIgRW50c2NoZWlkdW5nc2JhdW1cIixcbiAgcGFsZXR0ZSA9IFwiU2V0MVwiLFxuICBicmFuY2guY29sID0gXCJibGFja1wiXG4pIn0=
</script>
</div>
</div>
<p>Zur Beurteilung der Relevanz von Variablen für die Reduktion des Anpassungsfehlers (<em>variable importance</em>) kann der Eintrag <code>variable.importance</code> des <code>rpart</code>-Objekts herangezogen werden. Variable importance misst hier die Gesamtreduktion der Fehlerquadratsumem über alle Knoten, an denen die jeweilige Variable für Splits verwendet wird.</p>
<div class="cell">
<div>
<div id="webr-10">

</div>
<script type="webr-10-contents">
eyJhdHRyIjp7Indhcm5pbmciOmZhbHNlLCJlZGl0Ijp0cnVlLCJmaWctd2lkdGgiOiI4IiwiZXZhbCI6dHJ1ZSwidGltZWxpbWl0IjoiNjAiLCJtZXNzYWdlIjpmYWxzZSwiY29tcGxldGlvbiI6dHJ1ZX0sImNvZGUiOiIjIFZhcmlhYmxlLUltcG9ydGFuY2UgYXVzbGVzZW5cbmZpbmFsX3RyZWVfZml0JGZpdCR2YXJpYWJsZS5pbXBvcnRhbmNlIn0=
</script>
</div>
</div>
<p>Die Werte von Variable Importance zeigen, dass der mit CV ermittelte Baum <em>alle</em> Regressoren in <code>boston_train</code> für Splits nutzt, wobei <code>lstat</code> und <code>rm</code> die relevantesten Variablen sind.</p>
<p>Anhand von Vorhersagen für <code>medv</code> mit dem Test-Datensatz <code>boston_test</code> können wir das naive Baum-Modell <code>tree_fit</code> mit dem durch CV ermittelten Modell <code>tree_fit_cv</code> hinsichtlich des Vorhersagefehlers für ungesehene Beobachtungen vergleich. <code>yardstick::metric()</code> berechnet hierzu automatisch gängige Statistiken für Regressionsprobleme.</p>
<div class="cell">
<div>
<div id="webr-11">

</div>
<script type="webr-11-contents">
eyJhdHRyIjp7Indhcm5pbmciOmZhbHNlLCJlZGl0Ijp0cnVlLCJmaWctd2lkdGgiOiI4IiwiZXZhbCI6dHJ1ZSwidGltZWxpbWl0IjoiNjAiLCJtZXNzYWdlIjpmYWxzZSwiY29tcGxldGlvbiI6dHJ1ZX0sImNvZGUiOiIjIFZvcmhlcnNhZ2Vnw7x0ZSBuYWl2ZXMgTW9kZWxsXG50cmVlX3ByZWQgPC0gcHJlZGljdChcbiAgb2JqZWN0ID0gdHJlZV9maXQsIFxuICBuZXdfZGF0YSA9IEJvc3Rvbl90ZXN0XG4pICU+JVxuICBiaW5kX2NvbHMoQm9zdG9uX3Rlc3QpICU+JVxuICBtZXRyaWNzKHRydXRoID0gbWVkdiwgZXN0aW1hdGUgPSAucHJlZClcblxuIyBWb3JoZXJzYWdlZ8O8dGUgYmVpIENWXG50cmVlX3ByZWRfY3YgPC0gcHJlZGljdChcbiAgb2JqZWN0ID0gZmluYWxfdHJlZV9maXQsIFxuICBuZXdfZGF0YSA9IEJvc3Rvbl90ZXN0XG4pICU+JVxuICBiaW5kX2NvbHMoQm9zdG9uX3Rlc3QpICU+JVxuICBtZXRyaWNzKHRydXRoID0gbWVkdiwgZXN0aW1hdGUgPSAucHJlZClcblxudHJlZV9wcmVkXG50cmVlX3ByZWRfY3YifQ==
</script>
</div>
</div>
<p>Der Vergleich zeigt eine bessere Vorsageleistung des großen Baums in <code>tree_fit_cv</code>. In diesem Fall scheint CP-Pruning wenig hilfreich zu sein. Tatsächlich liefert ein Baum mit <span class="math inline">\(\alpha=0\)</span> bessere Vorhersagen als <code>tree_fit_cv</code> (überprüfe dies!).</p>
</section><section id="bagging" class="level2" data-number="15.3"><h2 data-number="15.3" class="anchored" data-anchor-id="bagging">
<span class="header-section-number">15.3</span> Bagging</h2>
<p><em>Bagging</em> ist eine Ensemble-Modelle, die durch aus einer Kombination von vielen Entscheidungsbäumen bestehen. Bagging steht für <em>Bootstrap Aggregating</em> und nutzt einen Algorithmus, bei dem Bäume auf <em>zufälligen</em> Stichproben aus dem Trainingsdatensatz angepasst werden: Jeder Baum wird auf einer <em>Bootstrap-Stichprobe</em> (siehe <a href="Simulation.html" class="quarto-xref"><span>Kapitel 5</span></a>) trainiert, die durch zufällige Züge (mit Zurücklegen) erstellt wird. Nach dem Training aggregiert Bagging die Vorhersagen aller Bäume des Ensembles.</p>
<p>Der Vorteil von Bagging gegenüber einem einzelnen Entscheidungsbaum ist, dass die Varianz der Vorhersage deutlich reduziert werden kann: Einzelne Entscheidungsbäume neigen dazu, Muster in den Trainingsdaten zu lernen, die sich zufällig aus der Zusammensetzung der Stichprobe ergeben und nicht repräsentativ für Zusammenhänge zwischen den Regressoren und der Outcome-Variable sind. Diese Überanpassung führt zu hoher Varianz auf von Vorhersagen für ungesehene Daten. Durch das Training vieler Bäume auf unterschiedlichen <em>zufälligen</em> Stichproben aus den Trainingsdaten und das anschließende Aggregieren kann der negative Effekt der Überanpassung auf die Unsicherheit der Vorhersage einzelner Bäume reduziert werden.</p>
<p>Eine Bagging-Spezifikation kann mit <code><a href="https://parsnip.tidymodels.org/reference/bag_tree.html">parsnip::bag_tree()</a></code> festgelegt werden. Mit <code>times = 500</code> wird definiert, dass der Bagging-Algorithmus ein Ensemble mit 500 Bäumen (mit CART) anpassen soll. Das Training und die Vorhersage auf den Testdaten erfolgt analog zur Vorgehensweise in <a href="#sec-simpletrees" class="quarto-xref"><span>Kapitel 15.1</span></a>.</p>
<div class="cell">
<div>
<div id="webr-12">

</div>
<script type="webr-12-contents">
eyJhdHRyIjp7Indhcm5pbmciOmZhbHNlLCJlZGl0Ijp0cnVlLCJmaWctd2lkdGgiOiI4IiwiZXZhbCI6dHJ1ZSwidGltZWxpbWl0IjoiNjAiLCJtZXNzYWdlIjpmYWxzZSwiY29tcGxldGlvbiI6dHJ1ZX0sImNvZGUiOiIjIFNwZXppZmlrYXRpb24gZsO8ciBCYWdnaW5nXG5iYWdnaW5nX3NwZWMgPC0gYmFnX3RyZWUoKSAlPiVcbiAgc2V0X2VuZ2luZShcbiAgICBlbmdpbmUgPSBcInJwYXJ0XCIsXG4gICAgdGltZXMgPSA1MDBcbiAgKSAlPiVcbiAgc2V0X21vZGUoXCJyZWdyZXNzaW9uXCIpXG5cblxuIyBUcmFpbmluZyBkdXJjaGbDvGhyZW5cbnNldC5zZWVkKDEyMzQpXG5cbmJhZ2dpbmdfZml0IDwtIGJhZ2dpbmdfc3BlYyAlPiVcbiAgZml0KFxuICAgIGZvcm11bGEgPSBtZWR2IH4gLiwgXG4gICAgZGF0YSA9IEJvc3Rvbl90cmFpblxuICApXG5cbiMgQXVzd2VydHVuZ1xuYmFnZ2luZ19wcmVkIDwtIHByZWRpY3QoXG4gIG9iamVjdCA9IGJhZ2dpbmdfZml0LCBcbiAgbmV3X2RhdGEgPSBCb3N0b25fdGVzdFxuICApICU+JVxuICBiaW5kX2NvbHMoQm9zdG9uX3Rlc3QpICU+JVxuICBtZXRyaWNzKFxuICAgIHRydXRoID0gbWVkdixcbiAgICBlc3RpbWF0ZSA9IC5wcmVkXG4gIClcblxuYmFnZ2luZ19wcmVkIn0=
</script>
</div>
</div>
<p>Die Auswertung auf den Testdatensatz ergibt eine deutliche Verbesserung der Vorhersageleistung gegenüber einem einfachen Regressionsbaum.</p>
<p>Obwohl die Bäume beim Bagging auf unterschiedlichen Stichproben trainiert werden, kann innerhalb des Ensembles dennoch eine deutliche Korrelation vorliegen: Da jeder Baum auf alle Regressoren für Splits zugreift, können trotz Bootstrapping ähnliche (unverteilhafte) Muster aus dem Datensatz erlernt werden, was sich nachteilig auf die Generalisierungsfähigkeit auswirken kann. Diese Korrelation mindert die Effektivität von Bagging, da stark korrelierte Bäume dazu neigen, ähnliche Fehler zu machen.</p>
</section><section id="sec-brf" class="level2 page-columns page-full" data-number="15.4"><h2 data-number="15.4" class="anchored" data-anchor-id="sec-brf">
<span class="header-section-number">15.4</span> Random Forests</h2>
<p><em>Random Forests</em> erweitern Bagging, indem zusätzlich bei jedem Knoten innerhalb jedes Baumes eine <em>zufällige Teilmenge der Regressoren</em> als potentielle Variable für die Split-Regel ausgewählt wird. Dies führt zu einer Reduktion der Korrelation zwischen den Bäumen, was die Genauigkeit verbessert und das Risiko von Overfitting weiter verringert.</p>
<p>In R erstellen wir die Spezifikation mit <code><a href="https://parsnip.tidymodels.org/reference/rand_forest.html">parsnip::rand_forest()</a></code>. Der Parameter <code>mtry</code> legt fest, wie viele Regressoren <span class="math inline">\(m\)</span> zufällig für jeden Split zur Verfügung stehen. Wir nutzen den im <code>randomForest</code>-Paket implementierten Algorithmus und legen in <code>set_engine()</code> fest, dass die von <code><a href="https://rdrr.io/pkg/randomForest/man/randomForest.html">randomForest::randomForest()</a></code> berechnete Fehler-Metrik im Output-Objekt ausgegeben wird (<code>tree.err = TRUE</code>). Um die Spezifikation für verschiedene Werte von <code>mtry</code> anwenden zu können, implementieren wir die Spezifikation innerhalb einer Wrapper-Funktion <code>rf_spec_mtry()</code>. Mit <code><a href="https://purrr.tidyverse.org/reference/map.html">purrr::map()</a></code> iterieren wir <code>rf_spec_mtry()</code> über drei verschiedene Werte für den Tuning-Parameter <code>mtry</code> (4, 6 und 10 Variablen).<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;Eine Faustregel für die Wahl von <span class="math inline">\(m\)</span> bei <span class="math inline">\(k\)</span> verfügbaren Regressoren ist <span class="math inline">\(m\approx\sqrt{k}\)</span>.</p></div></div><div class="cell">
<div>
<div id="webr-13">

</div>
<script type="webr-13-contents">
eyJhdHRyIjp7Indhcm5pbmciOmZhbHNlLCJlZGl0Ijp0cnVlLCJmaWctd2lkdGgiOiI4IiwiZXZhbCI6dHJ1ZSwidGltZWxpbWl0IjoiNjAiLCJtZXNzYWdlIjpmYWxzZSwiY29tcGxldGlvbiI6dHJ1ZX0sImNvZGUiOiJzZXQuc2VlZCgxMjM0KVxuXG4jIFdlcnRlIGbDvHIgbXRyeVxubXRyeV92YWx1ZXMgPC0gYyg0LCA2LCAxMClcblxuIyBGdW5rdGlvbjogUmFuZG9tIEZvcmVzdCBmw7xyIG10cnkgPSBtXG5yZl9zcGVjX210cnkgPC0gZnVuY3Rpb24obSkge1xuICByYW5kX2ZvcmVzdChtdHJ5ID0gbSwgdHJlZXMgPSA1MDApICU+JVxuICAgIHNldF9lbmdpbmUoXG4gICAgICBlbmdpbmUgPSBcInJhbmRvbUZvcmVzdFwiLCBcbiAgICAgIHRyZWUuZXJyID0gVFJVRVxuICAgICkgJT4lXG4gICAgc2V0X21vZGUoXCJyZWdyZXNzaW9uXCIpXG59XG5cbiMgTW9kZWxsZSBmw7xyIHZlcnNjaGllZGVuZSBtdHJ5LVdlcnRlIHRyYWluaWVyZW5cbnJmX2ZpdHMgPC0gbWFwKFxuICAueCA9IG10cnlfdmFsdWVzLCBcbiAgLmYgPSB+IHJmX3NwZWNfbXRyeSgueCkgJT4lXG4gICAgZml0KFxuICAgICAgZm9ybXVsYSA9IG1lZHYgfiAuLCBcbiAgICAgIGRhdGEgPSBCb3N0b25fdHJhaW5cbiAgICApXG4pXG5cbnJmX2ZpdHMgPC0gc2V0X25hbWVzKFxuICB4ID0gcmZfZml0cyxcbiAgbm0gPSAgcGFzdGUwKFwicmZfbXRyeVwiLCBtdHJ5X3ZhbHVlcywgXCJfZml0XCIpXG4pXG5cbiMgQXVzZ2FiZSBkZXIgRXJnZWJuaXNzZVxucmZfZml0cyJ9
</script>
</div>
</div>
<p>Für eine Beurteilung des Vorhersageleistung dieser drei Modelle können wir den <em>Out-of-Bag</em>-Fehler (OOB) verwenden:</p>
<p>Der OOB-Fehler ist eine Schätzung des Generalisierungsfehlers ohne einen separaten Testdatensatzes. Bei Random Forests (und Bagging) ist dies aufgrund der Berechnung des Ensembles für Bootstrap-Stichproben möglich: Grob ein Drittel der Beobachtungen des Datensatzes sind nicht Teil der Stichprobe, die für das Training jedes Baums im Ensemble genereiert werden.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> Diese nicht gezogenen Datenpunkte sind OOB-Beobachtungen. Der OOB-Fehler des Ensembles ist der durchschnittliche Fehler für die aggregierten Vorhersagen der Bäume des Forests.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;Beachte, dass beim Bootstrap <span class="math inline">\(n\)</span> aus <span class="math inline">\(n\)</span> Beobachtungen mit Zurücklegen gezogen werden. Die Wahrscheinlicht, dass eine Beobachtung <em>nicht</em> gezogen wird (“Out-of-Bag”), ist <span class="math inline">\((1-1/n)^n\approx37\%\)</span>.</p></div></div><p>Der OOB-Fehler kann auch verwendet werden, um die erforderliche Größe des Random Forests zu beurteilen: Eine größere Anzahl von Bäumen reduziert tendenziell die Varianz der Vorhersagen und verbessert die Generalisierungsfähigkeit. Allerdings nimmt dieser Effekt ab, und ab einer bestimmten Baumanzahl sind weitere Verbesserungen marginal. Obwohl das Risiko von Überanpassung durch viele Bäume aufgrund des Bagging minimal ist, kann es bei großen Datensätzen sinnvoll sein, kleinere Wälder zu trainieren, um den Rechenaufwand zu verringern. Wir plotten hierfür den OOB-Fehler für das Modell mit <code>mtry = 10</code> gegen die Anzahl der Bäume.</p>
<div class="cell">
<div>
<div id="webr-14">

</div>
<script type="webr-14-contents">
eyJhdHRyIjp7Indhcm5pbmciOmZhbHNlLCJlZGl0Ijp0cnVlLCJmaWctd2lkdGgiOiI4IiwiZXZhbCI6dHJ1ZSwidGltZWxpbWl0IjoiNjAiLCJtZXNzYWdlIjpmYWxzZSwiY29tcGxldGlvbiI6dHJ1ZX0sImNvZGUiOiJsaWJyYXJ5KGdnUmFuZG9tRm9yZXN0cylcblxuIyBPT0ItRmVobGVyIGFscyBGdW5rdGlvbiBkZXIgQmF1bWFuemFobFxucmZfZml0cyRyZl9tdHJ5MTBfZml0JGZpdCAlPiUgXG4gIGdnX2Vycm9yKCkgJT4lIFxuICBcbiAgcGxvdCgpICsgXG4gIGxhYnMoXG4gICAgdGl0bGUgPSBcIlJhbmRvbSBGb3Jlc3Q6IEVuc2VtYmxlZ3LDtsOfZSB2cy4gT09CLUZlaGxlciAobXRyeSA9IDEwKVwiXG4gICkgKyBcbiAgdGhlbWVfY293cGxvdCgpIn0=
</script>
</div>
</div>
<p>Die Grafik zeigt, dass die Verbesserung des OOB-Fehlers jenseits von 250 Beobachtungen deutlich nachlässt, sodass ein Training von 500 Bäumen ausreichend scheint.</p>
<p>Zur Beurteiliung der Vorhersagegüte mit dem Testdatensatz gehen wir analog zum Training vor und iterieren mit <code>map()</code> über <code>rf_fits</code>, die Liste der angepassten Modelle.</p>
<div class="cell">
<div>
<div id="webr-15">

</div>
<script type="webr-15-contents">
eyJhdHRyIjp7Indhcm5pbmciOmZhbHNlLCJlZGl0Ijp0cnVlLCJmaWctd2lkdGgiOiI4IiwiZXZhbCI6dHJ1ZSwidGltZWxpbWl0IjoiNjAiLCJtZXNzYWdlIjpmYWxzZSwiY29tcGxldGlvbiI6dHJ1ZX0sImNvZGUiOiIjIFZvcmhlcnNhZ2UgdW5kIEJlcmVjaG51bmcgdi4gTWV0cmlrZW4gZsO8ciBqZWRlbiBSRlxucmZfcHJlZGljdGlvbnMgPC0gbWFwKFxuICAueCA9IHJmX2ZpdHMsIFxuICAuZiA9ICB+IHByZWRpY3QoLngsIEJvc3Rvbl90ZXN0KSAlPiVcbiAgICBiaW5kX2NvbHMoQm9zdG9uX3Rlc3QpICU+JVxuICAgIG1ldHJpY3MoXG4gICAgICB0cnV0aCA9IG1lZHYsIFxuICAgICAgZXN0aW1hdGUgPSAucHJlZFxuICAgIClcbilcblxuIyBFaW50csOkZ2UgYmVuZW5uZW5cbnJmX3ByZWRpY3Rpb25zIDwtIHNldF9uYW1lcyhcbiAgeCA9IHJmX3ByZWRpY3Rpb25zLFxuICBubSA9ICBwYXN0ZTAoXCJyZl9tdHJ5XCIsIG10cnlfdmFsdWVzLCBcIl9wcmVkXCIpXG4pXG5cbnJmX3ByZWRpY3Rpb25zIn0=
</script>
</div>
</div>
<p>Ähnlich wie für einen einzelnen Baum kann die Relevanz von Variablen anhand der Reduktion der Loss-Funktion durch das Ensemble beurteilt werden. Für einen einfachen Vergleich der Variable Importance für den Random Forests mit <code>mtry = 10</code> in <code>rf_fits$rf_mtry10_fit</code> nutzen wir <code><a href="https://rdrr.io/pkg/ggRandomForests/man/gg_vimp.html">ggRandomForests::gg_vimp()</a></code>.</p>
<div class="cell">
<div>
<div id="webr-16">

</div>
<script type="webr-16-contents">
eyJhdHRyIjp7Indhcm5pbmciOmZhbHNlLCJlZGl0Ijp0cnVlLCJmaWctd2lkdGgiOiI4IiwiZXZhbCI6dHJ1ZSwidGltZWxpbWl0IjoiNjAiLCJtZXNzYWdlIjpmYWxzZSwiY29tcGxldGlvbiI6dHJ1ZX0sImNvZGUiOiIjIFZhcmlhYmxlIGltcG9ydGFuY2UgZsO8ciBtdHJ5ID0gMTBcbnJmX2ZpdHMkcmZfbXRyeTEwX2ZpdCRmaXQgJT4lXG4gIGdnX3ZpbXAoKSAgJT4lXG4gIHBsb3QoKSArXG4gICAgbGFicyhcbiAgICAgIHRpdGxlID0gXCJWYXJpYWJsZSBJbXBvcnRhbmNlIGbDvHIgUmFuZG9tIEZvcmVzdCAobXRyeSA9IDEwKVwiXG4gICAgKSArXG4gICAgdGhlbWVfY293cGxvdCgpIn0=
</script>
</div>
</div>
<p>Die Grafik bestärkt unsere Schlussfolgerung aus der Analyse des (mit CART trainierten) einzelnen Entscheidungsbaums in <a href="#sec-simpletrees" class="quarto-xref"><span>Kapitel 15.1</span></a>, dass <code>rm</code> und <code>lstat</code> die wichtigsten Regressoren für die Vorhersage von <code>medv</code> sind.</p>
</section><section id="sec-boosting" class="level2 page-columns page-full" data-number="15.5"><h2 data-number="15.5" class="anchored" data-anchor-id="sec-boosting">
<span class="header-section-number">15.5</span> Boosting</h2>
<p>Boosting ist eine leistungsstarke Ensemble-Methode für Vorhersagen, die kleine Modelle (oft Entscheidungsbäume geringer Tiefe) sukzessiv trainiert und zu einem starken Modell kombiniert. Anders als bei Random Forests, bei denen viele Bäume unabhängig voneinander auf zufälligen Stichproben der Daten trainiert werden, geht ein Boosting-Algorithmuss sequentiell vor: Jeder nachfolgende Baum wird darauf optimiert, die Fehler des vorherigen Modells zu reduzieren. Die Idee hierbei ist es, iterativ “schwache” Modelle zu erzeugen, die eine gute Anpassung für Datenpunkte liefern, die in den vorherigen Durchläufen schlecht vorhergesagt wurden.</p>
<p>Für einen Trainingsdatensatz <span class="math inline">\(\{(x_i, y_i)\}_{i=1}^n\)</span>, wobei <span class="math inline">\(x_i\)</span> die Input-Features und <span class="math inline">\(y_i\)</span> Beobachtungen des Outcomes sind, kann Boosting wiefolgt durchgeführt werden.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Boosting für Regression
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><p><strong>Initialisierung</strong>: Initialisiere das Boosting-Modell als <span class="math inline">\(\widehat{F}_0(x)\)</span>. Setze die Residuen <span class="math inline">\(r^0_i=y_i\)</span> für alle <span class="math inline">\(i\)</span></p></li>
<li>
<p><strong>Iteration</strong>: Wiederhole die folgenden Schritte für <span class="math inline">\(b = 1,2,\dots,B\)</span> mit <span class="math inline">\(B\)</span> hinreichend groß:</p>
<p>2.1 <strong>Base Learner</strong>: Trainiere Baum <span class="math inline">\(T_b\)</span> mit <span class="math inline">\(\{(\boldsymbol{x}_i, r^{b-1}_i)\}_{i=1}^n\)</span> für die Vorhersage des <em>Fehlers</em> der vorherigen Iteration <span class="math inline">\(r^{b-1}\)</span>.</p>
<p>2.2 <strong>Aktualisierung</strong>: Aktualisiere das Boosting-Modell,</p>
<p><span class="math display">\[\begin{align*}
   \widehat{F}_{b}(\boldsymbol{x}) = \widehat{F}_{b-1}(\boldsymbol{x}) + \eta \cdot T_{b}(\boldsymbol{x}),
   \end{align*}\]</span></p>
<p>wobei <span class="math inline">\(\eta\)</span> die (oft klein gewählte) <em>Lernrate</em> ist.</p>
<p>2.3 <strong>Fehlerberechnung</strong>: Berechne die Residuen <span class="math inline">\(r^b_i\)</span> als Differenzen zwischen dem tatsächlichen Werten <span class="math inline">\(y_i\)</span> und den Vorhersage des aktuellen Modells <span class="math inline">\(\widehat{F}_m(\boldsymbol{x}_i)\)</span>,</p>
<p><span class="math display">\[\begin{align*}
   r^b_i = y_i - \widehat{F}_b(\boldsymbol{x}_i).
   \end{align*}\]</span></p>
</li>
<li>
<p><strong>Output</strong>: Gib das finale Modell aus:</p>
<p><span class="math display">\[\begin{align*}
   \widehat{F}(\boldsymbol{x}) := \sum_{b=1}^B \eta\cdot \widehat{F}^b(\boldsymbol{x})
\end{align*}\]</span></p>
</li>
</ol>
</div>
</div>
<p>Der Parameter <span class="math inline">\(0\leq\eta\leq0\)</span> steuert, wie stark der Einfluss jedes neuen Baumes auf das Modell ist. Eine kleine Lernrate führt dazu, dass viele Bäume benötigt werden, was Vorhersagen (ähnlich wie bei Bagging) stabiler macht. Beachte die sequentielle Natur des Trainings: Die <span class="math inline">\(r^b_i\)</span> in Schritt 2.3 sind die zu vorhersagenden Outcome-Variable für den nächsten Baum. <span class="math inline">\(T_{b+1}\)</span> wird trainiert wird, um den <em>Fehler des bisherigen Modells</em> <span class="math inline">\(\widehat{F}_b\)</span> zu erklären.</p>
<p>Für die Anwendung auf <code><a href="https://rdrr.io/pkg/MASS/man/Boston.html">MASS::Boston</a></code> in R nutzen wir den im Paket <code>gbm</code> implementierten <em>Gradient-Boosting</em>-Algorithmus. Bei Gradient Boosting wird jeder Baum so trainiert, dass er den negativen Gradienten einer Verlustfunktion approximiert, also die Richtung des größten Fehlers. Das Modell wird schrittweise verbessert, indem es entlang des Gradienten aktualisiert wird, um die Vorhersagegüe zu optimieren; siehe <span class="citation" data-cites="Hastieetal2013">Hastie, Tibshirani, und Friedman (<a href="Literatur.html#ref-Hastieetal2013" role="doc-biblioref">2013</a>)</span> für eine detaillierte Erläuterung.</p>
<p>Mit dem nachfolgenden Code-Chunk trainieren wir ein Boosting-Modell für Regression mit 5000 einfachen Bäumen (<code>n.trees = 5000</code>) mit einer maximalen Tiefe von 2 (<code>interaction.depth = 2</code>), d.h. es folgen maximal 2 Entscheidungs-Regeln nacheinander. Um das Risiko von Overfitting gering zu halten, erlauben wir nur Splits, die zu mindestens zwei Beobachtungen in resultierenden nodes führen (<code>n.minobsinnode = 2</code>). Die Lernrate (Beitrag der Base Learner zum Ensemble) wird typischerweise klein (und in Abhängigkeit von <code>n.trees</code>) gewählt (<code>shrinkage = 0.001</code>).<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;Je kleiner die Lernrate, desto größer sollte <code>n.trees</code> gewählt werden.</p></div></div><div class="cell">
<div>
<div id="webr-17">

</div>
<script type="webr-17-contents">
eyJhdHRyIjp7Indhcm5pbmciOmZhbHNlLCJlZGl0Ijp0cnVlLCJmaWctd2lkdGgiOiI4IiwiZXZhbCI6dHJ1ZSwidGltZWxpbWl0IjoiNjAiLCJtZXNzYWdlIjpmYWxzZSwiY29tcGxldGlvbiI6dHJ1ZX0sImNvZGUiOiJzZXQuc2VlZCgxMjM0KVxuXG4jIEdyYWRpZW50IEJvb3N0aW5nIGR1cmNoZsO8aHJlblxuZ2JtX21vZGVsIDwtIGdibShcbiAgZm9ybXVsYSA9IG1lZHYgfiAuLCBcbiAgZGF0YSA9IEJvc3Rvbl90cmFpbiwgXG4gIGRpc3RyaWJ1dGlvbiA9IFwiZ2F1c3NpYW5cIiwgIyBmw7xyIFJlZ3Jlc3Npb25cbiAgbi50cmVlcyA9IDUwMDAsICAgICAgICAgICAjIEFuei4gQsOkdW1lXG4gIGludGVyYWN0aW9uLmRlcHRoID0gMiwgICAgICMgTWF4aW1hbGUgVGllZmUgZGVyIGJhc2UgbGVhcm5lclxuICBzaHJpbmthZ2UgPSAwLjAxLCAgICAgICAgICMgTGVybnJhdGVcbiAgbi5taW5vYnNpbm5vZGUgPSAyICAgICAgICAgIyBNaW4uIEJlb2JhY2h0dW5nZW4gaW4gbm9kZXNcbilcblxuZ2JtX21vZGVsICJ9
</script>
</div>
</div>
<p>Für die Vorhersagen auf dem Test-Datensatz legen wir mit <code>n.trees = gbm_model$n.trees</code> fest, dass das gesamte Ensemble genutzt werden soll.</p>
<div class="cell">
<div>
<div id="webr-18">

</div>
<script type="webr-18-contents">
eyJhdHRyIjp7Indhcm5pbmciOmZhbHNlLCJlZGl0Ijp0cnVlLCJmaWctd2lkdGgiOiI4IiwiZXZhbCI6dHJ1ZSwidGltZWxpbWl0IjoiNjAiLCJtZXNzYWdlIjpmYWxzZSwiY29tcGxldGlvbiI6dHJ1ZX0sImNvZGUiOiIjIFZvcmhlcnNhZ2VuIFRlc3QtRGF0ZW5zYXR6XG5nYm1fcHJlZGljdGlvbnMgPC0gcHJlZGljdChcbiAgb2JqZWN0ID0gZ2JtX21vZGVsLCBcbiAgbmV3ZGF0YSA9IEJvc3Rvbl90ZXN0LCBcbiAgbi50cmVlcyA9IGdibV9tb2RlbCRuLnRyZWVzICMgZ2VzYW10ZXMgRW5zZW1ibGUgbnV0emVuXG4pXG5cbiMgQXVzd2VydHVuZyBUZXN0LURhdGVuc2F0elxucmVzdWx0cyA8LSBCb3N0b25fdGVzdCAlPiVcbiAgbXV0YXRlKHByZWRpY3Rpb25zID0gZ2JtX3ByZWRpY3Rpb25zKSAlPiVcbiAgbWV0cmljcyhcbiAgICB0cnV0aCA9IG1lZHYsIFxuICAgIGVzdGltYXRlID0gcHJlZGljdGlvbnNcbiAgKVxuXG5yZXN1bHRzIn0=
</script>
</div>
</div>
<p>Die Ergebnisse zeigen, dass Gradient Boosting bereits für die naive Parameterwahl im Aufruf von <code><a href="https://rdrr.io/pkg/gbm/man/gbm.html">gbm::gbm()</a></code> zu einer Verbesserung der Vorhersageleistung gegenüber den Random-Forest-Modellen führt.</p>
<p>Anstatt <code>n.trees = 5000</code> können wir <code>n.trees</code> in <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> einen Vektor mit verschiedenen Ensemble-Größen übergeben. Für <code>n.trees = 5000</code> erhalten wir Vorhersagen für jeden Status, den das Boosting-Modell im Training nach seiner Initialisierung bis zu der in <code><a href="https://rdrr.io/pkg/gbm/man/gbm.html">gbm::gbm()</a></code> festgelgten Größe durchläuft. Anhand dieser Vorhersagen können wir die Generalisierungsfähigkeit des Modells in Abhängigkeit der gewählten Lernrate und der Größe beurteilen, in dem wir den RMSE für den gesamten Trainingsprozess berechnen. Für eine leichtere Interpretation erzeugen wir eine Grafik ählich wie bei der OOB-Analyse des Random-Forest-Modells.</p>
<div class="cell">
<div>
<div id="webr-19">

</div>
<script type="webr-19-contents">
eyJhdHRyIjp7Indhcm5pbmciOmZhbHNlLCJlZGl0Ijp0cnVlLCJmaWctd2lkdGgiOiI4IiwiZXZhbCI6dHJ1ZSwidGltZWxpbWl0IjoiNjAiLCJtZXNzYWdlIjpmYWxzZSwiY29tcGxldGlvbiI6dHJ1ZX0sImNvZGUiOiIjIFZvcmhlcnNhZ2VuIHN1a3plc3NpdiB0cmVmZmVuXG5wcmVkaWN0KFxuICAgIG9iamVjdCA9IGdibV9tb2RlbCwgXG4gICAgbmV3ZGF0YSA9IEJvc3Rvbl90ZXN0LCBcbiAgICBuLnRyZWVzID0gMTo1MDAwXG4pICU+JVxuICAgIFxuICAgIyBUZXN0c2V0LVJNU0UgYmVyZWNobmVuXG4gICAgYXNfdGliYmxlKCkgJT4lXG4gICAgbWFwX2RibChcbiAgICAgIC5mID0gfiBzcXJ0KG1lYW4oKC54IC0gQm9zdG9uX3Rlc3QkbWVkdileMikpXG4gICAgKSAlPiVcbiAgICBiaW5kX2NvbHMocm1zZSA9IC4sIHRyZWVzID0gMTo1MDAwKSAlPiVcbiAgXG4gICMgUGxvdHRlblxuICBnZ3Bsb3QobWFwcGluZyA9IGFlcyh4ID0gdHJlZXMsIHkgPSBybXNlKSkgK1xuICAgIGdlb21fbGluZSgpICtcbiAgICBsYWJzKFxuICAgICAgdGl0bGUgPSBcIkJvb3N0aW5nOiBUZXN0c2V0LVJNU0UgYWxzIEZ1bmt0aW9uIHZvbiBuLnRyZWVzXCJcbiAgICApICtcbiAgICB0aGVtZV9jb3dwbG90KCkifQ==
</script>
</div>
</div>
<p>Die Grafik zeigt eine schnelle Verbesserung des Out-of-sample-Fehlers mit der Größe des Ensembles. Für die gewählte Lernrate scheinen 5000 Bäume adäquat zu sein.</p>
<p>Analog zu Bagging und Random Forests können wir die Relevanz der Regressoren in <code>Boston</code> für die Vorhersage von <code>medv</code> anhand der mit <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> berechneten (relativen) Variable Importance für die Anpassung auf den Trainingsdatensatz einschätzen.</p>
<div class="cell">
<div>
<div id="webr-20">

</div>
<script type="webr-20-contents">
eyJhdHRyIjp7Indhcm5pbmciOmZhbHNlLCJlZGl0Ijp0cnVlLCJmaWctd2lkdGgiOiI4IiwiZXZhbCI6dHJ1ZSwidGltZWxpbWl0IjoiNjAiLCJtZXNzYWdlIjpmYWxzZSwiY29tcGxldGlvbiI6dHJ1ZX0sImNvZGUiOiIjIFZhcmlhYmxlIEltcG9ydGFuY2UgYmVyZWNobmVuXG52YXJfaW1wb3J0YW5jZSA8LSBzdW1tYXJ5KFxuICBvYmplY3QgPSBnYm1fbW9kZWwsIFxuICBwbG90aXQgPSBGQUxTRSAjIGsuIGdyYXBoaXNjaGUgQXVzZ2FiZVxuKVxuXG4jIC4uLiB1bmQgcGxvdHRlblxudmFyX2ltcG9ydGFuY2UgPC0gdmFyX2ltcG9ydGFuY2UgJT4lXG4gIGFzX3RpYmJsZSgpICU+JVxuICBhcnJhbmdlKFxuICAgIGRlc2MocmVsLmluZilcbiAgKVxuXG5nZ3Bsb3QoXG4gIGRhdGEgPSB2YXJfaW1wb3J0YW5jZSxcbiAgbWFwcGluZyA9IGFlcyhcbiAgICB4ID0gcmVvcmRlcih2YXIsIHJlbC5pbmYpLCBcbiAgICB5ID0gcmVsLmluZlxuICApXG4pICtcbiAgZ2VvbV9iYXIoc3RhdCA9IFwiaWRlbnRpdHlcIikgK1xuICBjb29yZF9mbGlwKCkgK1xuICBsYWJzKFxuICAgIHRpdGxlID0gXCJWYXJpYWJsZSBJbXBvcnRhbmNlIGbDvHIgR3JhZGllbnQgQm9vc3RpbmdcIixcbiAgICB4ID0gXCJWYXJpYWJsZVwiLFxuICAgIHkgPSBcIlJlbGF0aXZlciBFaW5mbHVzcyAoJSlcIlxuICApICtcbiAgdGhlbWVfY293cGxvdCgpIn0=
</script>
</div>
</div>
<p>Obwohl erneut <code>lstat</code> und <code>rm</code> als die wichtigsten Prädiktoren gelistet sind, identifiziert Gradient Boosting im Gegensatz zu Bagging und Random Forests <code>lstat</code> als die Variable mit der größten Vorhersagekraft für <code>medv</code>.</p>
</section><section id="causal-trees-und-causal-forests" class="level2 page-columns page-full" data-number="15.6"><h2 data-number="15.6" class="anchored" data-anchor-id="causal-trees-und-causal-forests">
<span class="header-section-number">15.6</span> Causal Trees und Causal Forests</h2>
<p>Baum-Algorithmen sind vielversprechende Ansätze zur Schätzung kausaler Effekte, insbesondere in Situationen, in denen die Bestimmung heterogener Effekte gewünscht ist: Der Vorteil von Baum-Methoden liegt darin, dass sie nicht-parametrisch sind: Der Regressorraum wird adaptiv in Partitionen unterteilt, um auf Basis dieser Aufteilung differenzierte Vorhersagen für die Zielvariable zu treffen. Diese Eigenschaft kann für Kausalanalysen hilfreich sein, da wir in vielen empirischen Anwendungen die Effekte einer Behandlung nicht nur im Durchschnitt für die betrachtete Population, sondern <em>differenzierter</em> schätzen möchten: Ein durchschnittlicher Behandlungseffekt (engl. <em>average treatment effect</em>, ATE) kann nicht ausreichend informativ für unsere Forschungsfrage sein, bspw. wenn wir erwarten, dass eine politische Intervention unterschiedliche Auswirkungen auf verschiedene Bevölkerungsgruppen hat. Idealerweise möchten wir <span class="math inline">\(\tau_i\)</span> bestimmen, den individuellen Behandlungseffekt einer Beobachtung <span class="math inline">\(i\)</span>. Das fundamentale Problem der Kausalinferenz ist, dass <span class="math inline">\(\tau_i\)</span> nicht ermittelt werden kann (s. u.), sodass wir unser Ziel abschwächen müssen. Statt <span class="math inline">\(\tau_i\)</span> suchen wir einen Behandlungseffekt in Abhängigkeit von beobachtbaren Charakteristiken <span class="math inline">\(\boldsymbol{X}\)</span> für Untergruppen der Population, einen <em>conditional average treatment effect</em> (CATE). Im Potential-Outcomes-Framework ist der CATE definiert als</p>
<p><span class="math display">\[\begin{align*}
  \tau(\boldsymbol{x}) = \textup{E}\big(Y^{(1)} - Y^{(0)}\big\vert \boldsymbol{X} = \boldsymbol{x}\big),
\end{align*}\]</span></p>
<p>wobei <span class="math inline">\(Y^{(1)}\)</span> und <span class="math inline">\(Y^{(0)}\)</span> die potenziellen Outcomes darstellen, wenn eine Behandlung erfolgt bzw. nicht erfolgt. In der Praxis beobachten wir jedoch nur <span class="math inline">\(Y_i = Y_i^{(B_i)}\)</span>, wobei <span class="math inline">\(B_i\)</span> der Behandlungsindikator für die Beobachtung <span class="math inline">\(i\)</span> ist, sodass <span class="math inline">\(\tau(\boldsymbol{x}_i)\)</span> nicht direkt beobachtet werden kann. Unter der Annahme, dass nach Kontrolle für (beobachtbare) <span class="math inline">\(\boldsymbol{X}\)</span> die Zuordnung zur Behandlung quasi-zufällig ist (<em>unconfoundedness</em>), formal</p>
<p><span class="math display">\[\begin{align*}
Y_i^{(0)},\,Y_i^{(1)} \perp B_i \vert \boldsymbol{X}_i,
\end{align*}\]</span></p>
<p>kann <span class="math inline">\(\tau(\boldsymbol{x})\)</span> geschätzt werden: Wir können Outcome-Differenzen zwischen behandelten und nicht behandelten Beobachtungen als kausal interpretieren, da unbeobachtete Faktoren die Ergebnisse nicht verzerren.</p>
<p>CART und andere traditionelle Entscheidungsbaum-Algorithmen sind für die Schätzung heterogener Behandlungseffekte jedoch ungeeignet. Dafür gibt es zwei wesentliche Ursachen:</p>
<ul>
<li>
<p><strong>Das Splitting-Kriterium</strong></p>
<p>Das Splitting-Kriterium des CART-Algorithmus optimiert die Aufteilungen der Beobachtungen in jedem Knoten, um die Genauigkeit von Vorhersagen für die Outcome-Variable <span class="math inline">\(Y\)</span> durch Minimierung der Heterogienität (Klassifikation) oder des MSE (Regression) zu optimieren. Diese Kriterien zielen also darauf ab, die <em>Homogenität innerhalb der Blätter hinsichtlich</em> <span class="math inline">\(Y\)</span> zu maximieren.</p>
<p>Für die Schätzung heterogener kausaler Effekte ist ein solches Splitting jedoch nicht zielführend. Statt Knoten zu formen, in denen <span class="math inline">\(Y\)</span> möglichst homogen ist, benötigen wir für die Schätzung von Behandlungseffekten grundsätzlich Aufteilungen, bei denen sich <span class="math inline">\(Y\)</span> zwischen den behandelten und unbehandelten Individuen innerhalb der Knoten unterscheidet.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> Das Splitting sollte zu Blättern führen, die hinsichtlich des <em>geschätzten Behandlungseffekts</em> möglichst heterogen sind.</p>
<p>Die Wahl des Splitting-Kriterium für die Schätzung kausaler Effekte mit Bäumen ist nicht trivial: Ein natürliches Kriterium ist der mittlere quadratische Fehler bei der Vorhersage von <span class="math inline">\(\tau\)</span>,</p>
<p><span class="math display">\[\begin{align*}
    \textup{MSE}_\tau = \frac{1}{n} \sum_{i=1}^n (\tau_i - \widehat{\tau}_i(\boldsymbol{X}_i))^2.
  \end{align*}\]</span></p>
<p><span class="math inline">\(\textup{MSE}_\tau\)</span> ist jedoch nicht direkt berechenbar: Aufgrund der nicht-beobachtbaren individuellen Behandlungseffekte <span class="math inline">\(\tau_i\)</span> müsste <span class="math inline">\(\textup{MSE}_\tau\)</span> selbst geschätzt werden!<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
</li>
<li>
<p><strong>Data leakage</strong></p>
<p>Data leakage tritt auf, wenn Informationen aus dem Trainingsprozess in den Modellvalidierungs- oder Schätzprozess einfließen. Bei der Anpassung des Baums berücksichtigt der Algorithmus idealerweise Informationen über <span class="math inline">\(Y\)</span> <em>und</em> <span class="math inline">\(B\)</span> im Splitting-Prozess, um die besten Aufteilungen zu finden. Die hiezu verwendeten Datenpunkte definieren damit <em>den zu schätzten CATE</em> anhand der durch Partionierung gebildeten Blätter. Wenn dieselben Datenpunkte auch für die tatsächliche Schätzung des CATE mit dem trainierten Baum verwendet werden, besteht die Gefahr von Überanpassung und somit verzerrten Schätzungen.</p>
</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;Wenn die Kontroll- und Behandlungsbeobachtungen in einem Blatt sehr ähnliche Outcomes <span class="math inline">\(Y\)</span> haben, können wir den Effekt nicht schätzen.</p></div><div id="fn8"><p><sup>8</sup>&nbsp;Bei “herkömmlichen” Regressionsbäumen besteht dieses Problem nicht, weil das Splitting-Kriterium Abweichungen von den wahren, <em>beobachteten</em> Werten von <span class="math inline">\(Y\)</span> misst.</p></div></div><section id="causal-trees" class="level3 page-columns page-full" data-number="15.6.1"><h3 data-number="15.6.1" class="anchored" data-anchor-id="causal-trees">
<span class="header-section-number">15.6.1</span> Causal Trees</h3>
<p>Der Causal Tree Algorithmus von <span class="citation" data-cites="AtheyImbens2016">Athey und Imbens (<a href="Literatur.html#ref-AtheyImbens2016" role="doc-biblioref">2016</a>)</span> modifiziert den CART-Algorithmus für die Schätzung heterogener Behandlungseffekte. In diesem Kontext wird die Vorgehensweise als „ehrlich“ (<em>honest</em>) bezeichnet, wenn nicht dieselben Informationen sowohl zur Auswahl des Modells (die Partitionierung des Regressorraums durch Splits) als auch zur Schätzung anhand dieses Modells verwendet werden. <span class="citation" data-cites="AtheyImbens2016">Athey und Imbens (<a href="Literatur.html#ref-AtheyImbens2016" role="doc-biblioref">2016</a>)</span> adressieren das Data-Leakage-Problem durch zufällige Aufteilung des Datensatzes in eine Teilmenge <span class="math inline">\(\mathcal{S}^{tr}\)</span> für das <em>Training des Baums</em> und eine Teilmenge <span class="math inline">\(\mathcal{S}^{est}\)</span> für die <em>Schätzung der Behandlungseffekte</em>.</p>
<p>Für die Erläuterung von <em>honest splitting</em> führen wir folgende Notation aus <span class="citation" data-cites="AtheyImbens2016">Athey und Imbens (<a href="Literatur.html#ref-AtheyImbens2016" role="doc-biblioref">2016</a>)</span> ein:</p>
<ul>
<li><p><span class="math inline">\(\mathcal{S}^{te}\)</span> ist ein hypothetischer <em>Testdatensatz</em></p></li>
<li><p><span class="math inline">\(\Pi\)</span> ist eine <em>Partition</em>, d.h. eine Aufteilung des Regressorraums von <span class="math inline">\(\boldsymbol{X}\)</span><a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p></li>
<li><p>Wir definieren die Schätzung des CATE anhand der Beobachtungen <span class="math inline">\(\mathcal{S}^{est}\)</span>: Der CATE <span class="math inline">\(\widehat{\tau}(\boldsymbol{X}_i,\mathcal{S}^{est},\Pi)\)</span> ist die Differenz der Mittelwerte von <span class="math inline">\(Y_i\)</span> für Behandlungs- und Kontrollbeobachtungen in dem aus <span class="math inline">\(\Pi\)</span> resultierenden Blatt für <span class="math inline">\(\boldsymbol{X}_i\)</span>.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;<span class="math inline">\(\Pi\)</span> sammelt also die Entschidungsregeln eines Baums und ist äquivalent zu <span class="math inline">\(T\)</span> in den füheren Kapiteln.</p></div><div id="fn10"><p><sup>10</sup>&nbsp;Die Notation <span class="math inline">\(\textup{E}_{\mathcal{S}^{est},\,\mathcal{S}^{te}}\)</span> meint, dass die Erwartung über <span class="math inline">\(\mathcal{S}^{est}\)</span>, und <span class="math inline">\(\mathcal{S}^{te}\)</span> gebildet wird.</p></div></div><p>Für die Wahl der Splits (die Partitionierung <span class="math inline">\(\Pi\)</span>) für den Causal Tree schlagen <span class="citation" data-cites="AtheyImbens2016">Athey und Imbens (<a href="Literatur.html#ref-AtheyImbens2016" role="doc-biblioref">2016</a>)</span> statt der Minimierung des MSE der Vorhersagen <span class="math inline">\(\widehat{Y}\)</span> (wie bei Regressionsbäumen) die Minimierung des MSE für den CATE vor. Das Vorgehen hierbei ist <em>honest</em> in dem Sinn, dass der erwartete Schätzfehler für ungesehene Beobachtungen <span class="math inline">\(\mathcal{S}^{te}\)</span> anhand einer Paritionierung <span class="math inline">\(\Pi\)</span> und entsprechenden Schätzungen der Behandlungseffekte <span class="math inline">\(\widehat\tau\)</span> mit unabhängigen Datensätzen <span class="math inline">\(\mathcal{S}^{tr}\)</span> bzw. <span class="math inline">\(\mathcal{S}^{est}\)</span> minimiert wird. Das hierzu verwendete Splitting-Kriterium ist eine Schätzung des <em>Erwartungswerts</em> von <span class="math display">\[\begin{align*}
  \textup{MSE}(\mathcal{S}^{est},\mathcal{S}^{te},\Pi) = \frac{1}{n^{te}} \sum_{i=1}^{n^{te}} \big(\tau_i - \widehat{\tau}(\boldsymbol{X}_i,\mathcal{S}^{est},\Pi)\big)^2,
\end{align*}\]</span> der <em>erwartete</em><a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> mittlere quadratische Fehler der heterogenen Behandlungseffekte,</p>
<p><span class="math display">\[\begin{align*}
  \textup{EMSE}(\Pi) = \textup{E}_{\mathcal{S}^{est},\,\mathcal{S}^{te}}\big[\textup{MSE}(\mathcal{S}^{est},\mathcal{S}^{te},\,\Pi)\big].
\end{align*}\]</span></p>
<p>Eine hilfreiche Umformung für <span class="math inline">\(\textup{EMSE}\)</span> ist</p>
<p><span class="math display">\[\begin{align*}
  \textup{EMSE}(\Pi) = \textup{Var}_{\mathcal{S}^{est},\boldsymbol{X}_i} \big[\widehat\tau(\boldsymbol{X}_i,\mathcal{S}^{est},\Pi)\big] - \textup{E}_{\boldsymbol{X}_i}\big[\tau^2(\boldsymbol{X}_i,\Pi)\big] + \textup{E}[\tau_i^2],
\end{align*}\]</span></p>
<p>denn <span class="citation" data-cites="AtheyImbens2016">Athey und Imbens (<a href="Literatur.html#ref-AtheyImbens2016" role="doc-biblioref">2016</a>)</span> zeigen, wie die ersten beiden Summanden empirisch geschätzt werden können. Der Term <span class="math inline">\(\textup{E}[\tau_i^2]\)</span> ist nicht schätzbar (unbeobachteter individueller Behandlungseffekt <span class="math inline">\(\tau_i\)</span>), kann aber vernachlässigt werden, da er nicht von <span class="math inline">\(\Pi\)</span> oder den Daten abhängt und somit eine <em>Konstante</em> ist, die sich beim Vergleich des geschätzen EMSE für verschiedene <span class="math inline">\(\Pi\)</span> rauskürzt.</p>
<p>Dies sorgt für konsistente Schätzungen. <span class="citation" data-cites="AtheyImbens2016">Athey und Imbens (<a href="Literatur.html#ref-AtheyImbens2016" role="doc-biblioref">2016</a>)</span> zeigen, dass die Minimierung des EMSE sowohl eine ausgewogene Verteilung der behandelten und unbehandelten Individuen als auch eine genaue Schätzung des Behandlungseffekts innerhalb jedes Knotens gewährleistet.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Algorithmus: Causal Tree
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>Passe den Baum an: teile den Regressorraum mit binären Entscheidungsregeln rekursiv in Partitionen <span class="math inline">\(\Pi\)</span>:
<ol type="a">
<li>An jedem Knoten wird die Aufteilung so gewählt, dass die Schätzung des <em>erwarteten mittleren quadratischen Fehlers</em> () über alle möglichen binären Aufteilungen <span class="math inline">\(\Pi\)</span> minimiert wird.</li>
<li>Stelle sicher, dass eine Mindestanzahl von behandelten und Kontroll-Einheiten in jedem Blatt des so angepassten Baums nicht unterschritten wird.</li>
</ol>
</li>
<li>Bestimmte mit Cross-Validation die Tiefe <span class="math inline">\(d^*\)</span> der Partition, die eine Schätzung des MSE der Behandlungseffekte minimiert.</li>
<li>Ehalte die Partition <span class="math inline">\(\Pi^*\)</span> durch das Beschneiden von <span class="math inline">\(\Pi\)</span> auf die Tiefe <span class="math inline">\(d^*\)</span>: Entferne Blätter, die die geringste Verbesserung der Anpassung bieten. Dieser Schritt liefert den finalen Baum.</li>
<li>Schätze die Behandlungseffekte in jedem Blatt von <span class="math inline">\(\Pi^*\)</span> mit den Beobachtungen in <span class="math inline">\(\mathcal{S}^{est}\)</span>.</li>
</ol>
</div>
</div>
<p>Zur Illustration der Anpassung eines Causal Trees mit R lesen wir zunächst den Datensatz <code>nl_effects</code> ein. Der Datensatz enhält 10000 Beobachtungen für</p>
<ul>
<li>10 Regressoren <code>X1</code>, <code>X2</code>, …, <code>X10</code>
</li>
<li>die Behandlungsvariable <code>B</code>
</li>
<li>das Outcome <code>Y</code>
</li>
<li>tatsächliche individuelle Behandlungseffekte <code>tau</code>
</li>
</ul>
<p>Die Daten wurden so erzeugt, dass lediglich die Regressoren <code>X1</code>, <code>X2</code> und <code>X3</code> Vorhersagekraft für <code>Y</code> haben <em>und</em> mit der Behandlungsvariable <code>B</code> korreliert sind. Der durchschnittliche Behandlungseffekt (ATE) beträgt 2.5.</p>
<p>Wie lesen Datensatz zunächst ein und verschaffen uns einen Überblick.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn11"><p><sup>11</sup>&nbsp;Aus technischen Gründen verzichten wir in diesem Kapitel zur Zeit auf die Einbindung der WebR-Konsole.</p></div></div><div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'dplyr'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following objects are masked from 'package:stats':

    filter, lag</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following objects are masked from 'package:base':

    intersect, setdiff, setequal, union</code></pre>
</div>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># 'nl_effects' einlesen</span></span>
<span><span class="va">nl_effects</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/readRDS.html">readRDS</a></span><span class="op">(</span></span>
<span>  file <span class="op">=</span> <span class="st">"datasets/nl_effects.Rds"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Überblick</span></span>
<span><span class="fu"><a href="https://pillar.r-lib.org/reference/glimpse.html">glimpse</a></span><span class="op">(</span><span class="va">nl_effects</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 10,000
Columns: 14
$ X1  &lt;dbl&gt; -0.17189611, -0.55391939, 0.10451867, 0.04195350, -1.76622921, 0.5…
$ X2  &lt;dbl&gt; 0.10969737, 0.37320810, -0.46970463, 0.35885349, -1.66904480, -0.6…
$ X3  &lt;dbl&gt; -0.02993756, -0.41947892, -0.32323825, -1.18389717, 0.37425641, 1.…
$ X4  &lt;dbl&gt; 0.71681636, -0.80359882, -0.54967605, -0.11371422, -2.16508303, 0.…
$ X5  &lt;dbl&gt; 1.5304038, 0.3064168, -0.3336049, 0.1159094, 0.1128305, 1.5083073,…
$ X6  &lt;dbl&gt; 0.60233011, 0.23672672, -0.50799015, 1.54354856, 1.77343598, 0.102…
$ X7  &lt;dbl&gt; 0.805828304, 0.018675533, 0.986624988, -1.125443184, -0.767981003,…
$ X8  &lt;dbl&gt; 1.0294563, 0.2367033, 0.6623974, -0.7134651, 0.2517959, -1.2147408…
$ X9  &lt;dbl&gt; -1.0318085, 0.2527040, 0.5715453, 0.4919105, -0.6422120, 0.4883813…
$ X10 &lt;dbl&gt; -1.1753465, 0.1958151, -1.8581124, -0.1841951, -0.1020165, -1.3933…
$ Y   &lt;dbl&gt; 2.9837154, 1.0474000, 2.1831497, 0.5972527, -1.7904405, 2.5689629,…
$ B   &lt;int&gt; 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, …
$ tau &lt;dbl&gt; 1.8041831, 1.0962438, 1.8915916, 0.9224442, 2.0008825, 4.3721214, …
$ ps  &lt;dbl&gt; 0.4785262, 0.4311993, 0.5130619, 0.5052440, 0.2925328, 0.5669275, …</code></pre>
</div>
</div>
<p>Für die Schätzung nutzen wir das R-Paket <code>grf</code> <span class="citation" data-cites="grfPackage">(<a href="Literatur.html#ref-grfPackage" role="doc-biblioref">Tibshirani u.&nbsp;a. 2024</a>)</span>. Mit <code><a href="https://rdrr.io/pkg/grf/man/causal_forest.html">grf::causal_forest()</a></code> kann ein Ensemble-Modell mit vielen Causal Trees (Causal Forest) geschätzt werden. Über die Argumente <code>num.trees = 1</code> und <code>ci.group.size = 1</code> legen wir fest, dass lediglich ein Causal Tree angepasst werden soll. Beachte, dass die Regressoren dem Argument <code>X</code> als <code>matrix</code>-Objekt übergeben werden müssen. Mit <code>min.node.size = 50</code> legen wir eine Mindestanzahl an Beobachtungen für die Knoten des Baums fest. Da die Aufteilung von <code>nl_effects</code> in Trainings-, Schätz- und Validierungsdatensatz zufällig generiert und Cross-Validation zu Bestimmung der Baum-Tiefe eingesetzt wird, setzen wir mit <code>seed = 1234</code> einen Seed für Reproduzierbarkeit.</p>
<div class="cell">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/grf-labs/grf">grf</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Causal Tree Anpassen</span></span>
<span><span class="va">causal_tree</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/grf/man/causal_forest.html">causal_forest</a></span><span class="op">(</span></span>
<span>  X <span class="op">=</span> <span class="va">nl_effects</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>    <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">Y</span>, <span class="op">-</span><span class="va">B</span>, <span class="op">-</span><span class="va">tau</span>, <span class="op">-</span><span class="va">ps</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  Y <span class="op">=</span> <span class="va">nl_effects</span><span class="op">$</span><span class="va">Y</span>,</span>
<span>  W <span class="op">=</span> <span class="va">nl_effects</span><span class="op">$</span><span class="va">B</span>, </span>
<span>  min.node.size <span class="op">=</span> <span class="fl">100</span>,</span>
<span>  num.trees <span class="op">=</span> <span class="fl">1</span>, </span>
<span>  ci.group.size <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  seed <span class="op">=</span> <span class="fl">1234</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Zusammenfassung:</span></span>
<span><span class="va">causal_tree</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>GRF forest object of type causal_forest 
Number of trees: 1 
Number of training samples: 10000 
Variable importance: 
    1     2     3     4     5     6     7     8     9    10 
0.158 0.000 0.842 0.000 0.000 0.000 0.000 0.000 0.000 0.000 </code></pre>
</div>
</div>
<p>Den angepassten Causal Tree lesen wir mit <code><a href="https://rdrr.io/pkg/grf/man/get_tree.html">grf::get_tree()</a></code> aus und nutzen die zugehörige <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code>-Methode für eine grafische Darstellung, siehe <a href="#fig-ct" class="quarto-xref">Abbildung&nbsp;<span>15.3</span></a>.</p>
<div class="cell">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Trainierten Causal Tree auslesen</span></span>
<span><span class="va">the_tree</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/grf/man/get_tree.html">get_tree</a></span><span class="op">(</span><span class="va">causal_tree</span>, index <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Tree plotten</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">the_tree</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="fig-ct" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-ct-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/causal_tree.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-ct-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Abbildung&nbsp;15.3: Mit <code><a href="https://rdrr.io/pkg/grf/man/causal_forest.html">grf::causal_forest</a></code> geschätzter Causal Tree
</figcaption></figure>
</div>
</section><section id="causal-forests" class="level3 page-columns page-full" data-number="15.6.2"><h3 data-number="15.6.2" class="anchored" data-anchor-id="causal-forests">
<span class="header-section-number">15.6.2</span> Causal Forests</h3>
<p><em>Causal Forests</em> erweitern das Konzept der Causal Trees zu einem Ensemble-Verfahren, ähnlich wie Random Forests klassische Regressionsbäume erweitern. Bei Causal Forests werden viele Causal Trees auf Bootstrap-Stichproben der Daten trainiert, wobei für jeden Baum nur eine zufällige Teilmenge der Kovariaten für potentielle Splits berücksichtigt wird. Das “Honest Splitting”-Prinzip wird beibehalten: Für jeden Baum wird die Bootstrap-Stichprobe in eine Trainings- und eine Schätzstichprobe aufgeteilt. Der finale geschätzte Behandlungseffekt für eine neue Beobachtung ergibt sich aus dem Durchschnitt der Vorhersagen aller Bäume. Diese Ensemble-Methode reduziert die Varianz der Schätzungen im Vergleich zu einzelnen Causal Trees. Zusätzlich ermöglicht die Forest-Struktur die Berechnung von Konfidenzintervallen für die geschätzten Behandlungseffekte durch die Analyse der Verteilung der Vorhersagen über alle Bäume. Causal Forests zielen dabei primär darauf ab, den Conditional Average Treatment Effect (CATE), auch bekannt als heterogener Behandlungseffekt <span class="math inline">\(\tau(x)\)</span>, zu schätzen. Der CATE ist definiert als <span class="math display">\[\begin{align*}
  \tau(x) = \textup{E}[Y(1) - Y(0)|X = x],
\end{align*}\]</span> wobei <span class="math inline">\(Y(1)\)</span> und <span class="math inline">\(Y(0)\)</span> die potentiellen Outcomes unter Behandlung bzw. Kontrolle sind, und <span class="math inline">\(X\)</span> die verfügbaren Kovariablen darstellt. Im Gegensatz zum durchschnittlichen Behandlungseffekt (ATE), der über die gesamte Population gemittelt wird, oder zum durchschnittlichen Behandlungseffekt der Behandelten (ATT), ermöglicht der CATE die <em>Schätzung individualisierter Behandlungseffekte für spezifische Kovariatenwerte</em>. Causal Forests sind dabei besonders geeignet, nicht-lineare und komplexe Heterogenitätsmuster in den Behandlungseffekten zu erfassen. Sie erlauben es uns:</p>
<ul>
<li>Für jede individuelle Beobachtung <span class="math inline">\(i\)</span> einen spezifischen bedingten Behandlungseffekt <span class="math inline">\(\tau(x_i)\)</span> zu schätzen</li>
<li>Diese individuellen Schätzungen zu validen Gruppendurchschnitten zu aggregieren</li>
<li>Die Unsicherheit dieser Schätzungen durch Konfidenzintervalle zu quantifizieren</li>
</ul>
<p>Die Ensemble-Struktur ermöglicht dabei nicht nur die Punktschätzung des CATE, sondern auch die Berechnung asymptotisch valider Konfidenzintervalle für diese bedingten Effekte: <span class="citation" data-cites="Atheyetal2019">Athey, Tibshirani, und Wager (<a href="Literatur.html#ref-Atheyetal2019" role="doc-biblioref">2019</a>)</span> zeigen, dass Causal Forests unter bestimmten Regularitätsbedingungen asymptotisch normalverteilte und konsistente Schätzer für die bedingten durchschnittlichen Behandlungseffekte liefern.</p>
<p>Causal Forests eignen sich insbesondere für polit-ökonomische Fragestellungen, bei denen die Wirkung von Maßnahmen zwischen verschiedenen Gruppen oder Regionen variiert. Die Berechnung von CATEs mit Causal Forests könnte beispielsweise ein besseres Verständnis darüber ermöglichen, welche Arbeitssuchenden besonders von bestimmten Weiterbildungsprogrammen profitieren oder wie Subventionen oder Infrastrukturinvestitionen je nach lokalen Gegebenheiten unterschiedlich wirken.</p>
<p><a href="#tbl-rfcfcomp" class="quarto-xref">Tabelle&nbsp;<span>15.1</span></a> vergleicht die wesentlichen Aspekte von Random Forests und Causal Forests.</p>
<div id="tbl-rfcfcomp" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-tbl figure page-columns page-full"><div aria-describedby="tbl-rfcfcomp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 20%">
<col style="width: 38%">
<col style="width: 41%">
</colgroup>
<thead><tr class="header">
<th>Aspekt</th>
<th>Random Forests</th>
<th>Causal Forests</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Zielfunktion</strong></td>
<td>Minimierung MSE der Outcome-Vorhersage</td>
<td>Minimierung MSE der gesch. Behandlungseffekte</td>
</tr>
<tr class="even">
<td><strong>Split-Kriterium</strong></td>
<td>Minimierung der Varianz von Y in Knoten</td>
<td>Maximierung der Behandlungseffekt-Heterogenität zwischen Knoten</td>
</tr>
<tr class="odd">
<td><strong>Datennutzung</strong></td>
<td>Gleiche Daten für Anpassung und Vorhersage</td>
<td>“Honest Splitting”: Separate Daten für Baumstruktur und Effektschätzung</td>
</tr>
<tr class="even">
<td><strong>Schätzung</strong></td>
<td>Bedingte Erwartung <span class="math inline">\(\textup{E}[Y|X]\)</span>
</td>
<td>Bedingter Behandlungseffekt <span class="math inline">\(\textup{E}[Y(1) - Y(0)|X]\)</span>
</td>
</tr>
<tr class="odd">
<td><strong>Inferenz</strong></td>
<td>Punktschätzungen</td>
<td>Asymptotische Verteilung und Konfidenzintervalle</td>
</tr>
<tr class="even">
<td><strong>Balancierung</strong></td>
<td>Keine Behandlung-Kontroll-Balancierung</td>
<td>Mindestanzahl von Behandlungs- und Kontrolleinheiten pro Knoten</td>
</tr>
<tr class="odd">
<td><strong>Bootstrap</strong></td>
<td>Zufällige Stichprobe mit Zurücklegen</td>
<td>Doppelte Stichprobe: Split-Sample und Estimation-Sample</td>
</tr>
<tr class="even">
<td><strong>Modellkomplexität</strong></td>
<td>Durch Cross-Validation optimiert</td>
<td>Durch “honest” Cross-Validation mit separaten Schätzstichproben optimiert</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-tbl margin-caption" id="tbl-rfcfcomp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabelle&nbsp;15.1: Vergleich von Random Forests und Causal Forests
</figcaption></figure>
</div>
<p>In diesem Beispiel verwenden wir das Paket <code>grf</code> (Generalized Random Forests) in R, um einen Causal Forest zu trainieren und die individuellen Behandlungseffekte zu schätzen.</p>
<div class="cell">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/grf-labs/grf">grf</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.stat.berkeley.edu/~breiman/RandomForests/">randomForest</a></span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>randomForest 4.7-1.1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Type rfNews() to see new features/changes/bug fixes.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'randomForest'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following object is masked from 'package:dplyr':

    combine</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following object is masked from 'package:ggplot2':

    margin</code></pre>
</div>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidymodels.tidymodels.org">tidymodels</a></span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>✔ broom        1.0.6      ✔ rsample      1.2.1 
✔ dials        1.3.0      ✔ tibble       3.2.1 
✔ infer        1.0.7      ✔ tidyr        1.3.1 
✔ modeldata    1.4.0      ✔ tune         1.2.1 
✔ parsnip      1.2.1      ✔ workflows    1.1.4 
✔ purrr        1.0.2      ✔ workflowsets 1.1.0 
✔ recipes      1.0.10     ✔ yardstick    1.3.1 </code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'broom' was built under R version 4.3.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'modeldata' was built under R version 4.3.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──
✖ randomForest::combine() masks dplyr::combine()
✖ purrr::discard()        masks scales::discard()
✖ dplyr::filter()         masks stats::filter()
✖ dplyr::lag()            masks stats::lag()
✖ randomForest::margin()  masks ggplot2::margin()
✖ dials::prune()          masks rpart::prune()
✖ recipes::step()         masks stats::step()
• Use tidymodels_prefer() to resolve common conflicts.</code></pre>
</div>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">the_split</span> <span class="op">&lt;-</span> <span class="fu">initial_split</span><span class="op">(</span>data <span class="op">=</span> <span class="va">nl_effects</span>, prop <span class="op">=</span> <span class="fl">.8</span><span class="op">)</span></span>
<span><span class="va">nl_effects_train</span> <span class="op">&lt;-</span> <span class="fu">training</span><span class="op">(</span><span class="va">the_split</span><span class="op">)</span></span>
<span><span class="va">nl_effects_test</span> <span class="op">&lt;-</span> <span class="fu">testing</span><span class="op">(</span><span class="va">the_split</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Variablen in Matrizen / Vektoren überführen</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="va">nl_effects_train</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="fu"><a href="https://tidyselect.r-lib.org/reference/starts_with.html">starts_with</a></span><span class="op">(</span><span class="st">"X"</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">B</span> <span class="op">&lt;-</span> <span class="va">nl_effects_train</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">B</span><span class="op">)</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="va">nl_effects_train</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span></span>
<span><span class="va">tau</span> <span class="op">&lt;-</span> <span class="va">nl_effects_train</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">tau</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Bevor wir den Causal Forest trainieren, schätzen wir zunächst den Propensity Score und das Outcome-Modell. Diese Vorschätzungen werden verwendet, um die Effizienz des Causal Forests zu verbessern. Der Propensity Score modelliert die Wahrscheinlichkeit der Behandlungszuweisung, während das Outcome-Modell den grundlegenden Zusammenhang zwischen den Kovariablen und der Zielvariable erfasst.</p>
<p>Während der Propensity Score zur Verbesserung der Balancierung der Behandlungs- und Kontrollgruppen im Causal Forest beiträgt, ermöglicht die Outcome-Schätzung dem Causal Forest, sich auf die <em>Heterogenität der Behandlungseffekte</em> zu konzentrieren, statt den gemeinsamen Effekt der Kovariaten auf <span class="math inline">\(Y\)</span> mitschätzen zu müssen. Dies ist analog zur Regression mit Kontrolle für Kovariablen: Der “gemeinsame” Effekt der Regressoren wird durch die Schätzung des Outcomes mit einem Regression Forest bereits herausgerechnet.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> Obwohl <code>grf()</code> diese Schätzungen auch intern vornehmen kann, bietet die explizite Schätzung mehr Kontrolle über den Prozess. Wir nutzen hierzu <code><a href="https://rdrr.io/pkg/grf/man/regression_forest.html">grf::regression_forest()</a></code>, die Implementierung eines Random-Forest-Algorithmus für Regression.</p>
<div class="no-row-height column-margin column-container"><div id="fn12"><p><sup>12</sup>&nbsp;Dies wird auch als <em>Orthogonoalisierung</em> bezeichnet.</p></div></div><div class="cell">
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Propensity Score Schätzen</span></span>
<span><span class="va">B_hat_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/grf/man/regression_forest.html">regression_forest</a></span><span class="op">(</span></span>
<span>  X <span class="op">=</span> <span class="va">X</span>,</span>
<span>  Y <span class="op">=</span> <span class="va">B</span>,</span>
<span>  num.trees <span class="op">=</span> <span class="fl">4000</span>,</span>
<span>  tune.parameters <span class="op">=</span> <span class="st">"all"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">B_hat</span> <span class="op">&lt;-</span> <span class="va">B_hat_mod</span><span class="op">$</span><span class="va">predictions</span></span>
<span></span>
<span><span class="co"># Outcome Schätzen</span></span>
<span><span class="va">Y_hat_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/grf/man/regression_forest.html">regression_forest</a></span><span class="op">(</span></span>
<span>  X <span class="op">=</span> <span class="va">X</span>,</span>
<span>  Y <span class="op">=</span> <span class="va">Y</span>,</span>
<span>  num.trees <span class="op">=</span> <span class="fl">4000</span>,</span>
<span>  tune.parameters <span class="op">=</span> <span class="st">"all"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">Y_hat</span> <span class="op">&lt;-</span> <span class="va">Y_hat_mod</span><span class="op">$</span><span class="va">predictions</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Den Causal Forest schätzen wir mit <code><a href="https://rdrr.io/pkg/grf/man/causal_forest.html">grf::causal_forest()</a></code> unter Übergabe der zuvor geschätzten Propensity-Scores und Outcomes.</p>
<div class="cell">
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Causal Forest trainieren</span></span>
<span><span class="va">cf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/grf/man/causal_forest.html">causal_forest</a></span><span class="op">(</span></span>
<span>  X <span class="op">=</span> <span class="va">X</span>,</span>
<span>  Y <span class="op">=</span> <span class="va">Y</span>, </span>
<span>  W <span class="op">=</span> <span class="va">B</span>, </span>
<span>  Y.hat <span class="op">=</span> <span class="va">Y_hat</span>,</span>
<span>  W.hat <span class="op">=</span> <span class="va">B_hat</span>, </span>
<span>  num.trees <span class="op">=</span> <span class="fl">4000</span>,   </span>
<span>  tune.parameters <span class="op">=</span> <span class="st">"all"</span></span>
<span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Als ersten Analyseschritt berechnen wir den durchschnittlichen Behandlungseffekt (ATE). Dieser gibt uns einen ersten Eindruck der Genauigkeit der Schätzung des Behandlungseffekts</p>
<div class="cell">
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Vorhersage des durchschnittlichen Behandlungseffekts</span></span>
<span><span class="co"># (ATT) mit Causal Forest</span></span>
<span><span class="op">(</span></span>
<span>  <span class="va">tau.cf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/grf/man/average_treatment_effect.html">average_treatment_effect</a></span><span class="op">(</span><span class="va">cf</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  estimate    std.err 
2.52632012 0.02334527 </code></pre>
</div>
</div>
<p>Die geschätzen heterogenen Behandlungseffekte erhalten wir mit <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code>.</p>
<div class="cell">
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Schätzungen der bedingte Behandlungseffekte (CATE)</span></span>
<span><span class="co"># auslesen</span></span>
<span><span class="va">tau.hat.cf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">cf</span><span class="op">)</span><span class="op">$</span><span class="va">predictions</span></span>
<span></span>
<span><span class="co"># Überblick</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">tau.hat.cf</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 5.121895 3.061950 3.270362 4.464369 3.056366 4.288634</code></pre>
</div>
</div>
<p>Da wir in diesem simulierten Datensatz die wahren Behandlungseffekte kennen, können wir die Qualität unserer Schätzungen visuell überprüfen. In der folgenden Grafik vergleichen wir die tatsächlichen individuellen Behandlungseffelte mit dem CATE-Schätzungen. Die rote Linie ist die Referenz für eine perfekte Übereinstimmung zwischen geschätzten und wahren Effekten. Je näher die Punkte an dieser Linie liegen, desto besser ist unsere Schätzung.</p>
<div class="cell">
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># CATE-Schätzungen vs. ITE plotten</span></span>
<span><span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span></span>
<span>    true_effect <span class="op">=</span> <span class="va">nl_effects_train</span><span class="op">$</span><span class="va">tau</span>,</span>
<span>    estimated_effect <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">cf</span><span class="op">)</span><span class="op">$</span><span class="va">predictions</span>,</span>
<span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">true_effect</span>, y <span class="op">=</span> <span class="va">estimated_effect</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">geom_point</span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.3</span>, size <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">geom_abline</span><span class="op">(</span>intercept <span class="op">=</span> <span class="fl">0</span>, slope <span class="op">=</span> <span class="fl">1</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">labs</span><span class="op">(</span></span>
<span>        x <span class="op">=</span> <span class="st">"Wahrer Behandlungseffekt (ITE)"</span>,</span>
<span>        y <span class="op">=</span> <span class="st">"Geschätzter Behandlungseffekt (CATE)"</span>,</span>
<span>        title <span class="op">=</span> <span class="st">"Vergleich von wahren und geschätzten Behandlungseffekten"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">theme_minimal</span><span class="op">(</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="trees_files/figure-html/unnamed-chunk-32-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Anschließend quantifizieren wir die Genauigkeit unserer Schätzungen durch den RMSE (Root Mean Square Error). Wir berechnen diesen sowohl für die Trainings- als auch für die Testdaten, um die Generalisierbarkeit unseres Modells zu überprüfen.</p>
<div class="cell">
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># RMSE für Causal Forest</span></span>
<span><span class="op">(</span></span>
<span>  <span class="va">mse.cf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">tau.hat.cf</span> <span class="op">-</span> <span class="va">tau</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5006396</code></pre>
</div>
</div>
<p>Der RMSE auf den Trainingsdaten zeigt die Schätzgenauigkeit unseres Modells an. Durch das “honest splitting” Verfahren des Causal Forests, bei dem separate Teilstichproben für die Anpassung des Baums und die Effektschätzung verwendet werden, erwarten wir keine substantielle Verschlechterung der Performanz auf den Testdaten <code>nl_effects_test</code>.</p>
<div class="cell">
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># RMSE für separaten Test-Datensatz</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span></span>
<span>    <span class="op">(</span></span>
<span>      <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">cf</span>, newdata <span class="op">=</span> <span class="va">nl_effects_test</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">Y</span>, <span class="op">-</span><span class="va">tau</span>, <span class="op">-</span><span class="va">B</span>, <span class="op">-</span><span class="va">ps</span><span class="op">)</span><span class="op">)</span><span class="op">$</span><span class="va">predictions</span> </span>
<span>      <span class="op">-</span> <span class="va">nl_effects_test</span><span class="op">$</span><span class="va">tau</span></span>
<span>      <span class="op">)</span><span class="op">^</span><span class="fl">2</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.4947249</code></pre>
</div>
</div>
<p>Ein ähnlicher RMSE auf den Testdaten bestätigt die inhärente Generalisierungsfähigkeit des Causal Forest Algorithmus.</p>
<section id="inferenz-für-effekt-schätzungen" class="level4" data-number="15.6.2.1"><h4 data-number="15.6.2.1" class="anchored" data-anchor-id="inferenz-für-effekt-schätzungen">
<span class="header-section-number">15.6.2.1</span> Inferenz für Effekt-Schätzungen</h4>
<p>Zunächst berechnen wir die Konfidenzintervalle für die geschätzten Behandlungseffekte in <code>cf</code>. Die <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code>-Funktion für <code>grf</code>-Objekte kann diese direkt mit ausgeben.</p>
<div class="cell">
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Vorhersage mit Konfidenzintervallen</span></span>
<span><span class="va">predictions_with_ci</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">cf</span>, estimate.variance <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># In tibble umwandeln </span></span>
<span><span class="va">cate_estimates</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span></span>
<span>  cate <span class="op">=</span> <span class="va">predictions_with_ci</span><span class="op">$</span><span class="va">predictions</span>,</span>
<span>  stderr <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">predictions_with_ci</span><span class="op">$</span><span class="va">variance.estimates</span><span class="op">)</span>,</span>
<span>  lower_ci <span class="op">=</span> <span class="va">cate</span> <span class="op">-</span> <span class="fl">1.96</span> <span class="op">*</span> <span class="va">stderr</span>,</span>
<span>  upper_ci <span class="op">=</span> <span class="va">cate</span> <span class="op">+</span> <span class="fl">1.96</span> <span class="op">*</span> <span class="va">stderr</span>,</span>
<span>  true_effect <span class="op">=</span> <span class="va">nl_effects_train</span><span class="op">$</span><span class="va">tau</span></span>
<span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Um die Präzision unserer Schätzungen zu visualisieren, erstellen wir einen Plot, der die Konfidenzintervalle für eine Stichprobe von Beobachtungen zeigt. Wir sortieren nach der Größe der geschätzten Effekte, um eine übersichtlichere Darstellung zu erhalten.</p>
<div class="cell">
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Sample von 100 Beobachtungen für übersichtlichere Visualisierung</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="va">cate_estimates</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/sample_n.html">sample_n</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>id <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/row_number.html">row_number</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/arrange.html">arrange</a></span><span class="op">(</span><span class="va">cate</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">id</span>, y <span class="op">=</span> <span class="va">cate</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_errorbar</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>ymin <span class="op">=</span> <span class="va">lower_ci</span>, ymax <span class="op">=</span> <span class="va">upper_ci</span><span class="op">)</span>, width <span class="op">=</span> <span class="fl">0.2</span>, alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_point</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>y <span class="op">=</span> <span class="va">true_effect</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"red"</span>, size <span class="op">=</span> <span class="fl">1</span>, alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="st">"Beobachtung (sortiert nach geschätztem Effekt)"</span>,</span>
<span>    y <span class="op">=</span> <span class="st">"Behandlungseffekt"</span>,</span>
<span>    title <span class="op">=</span> <span class="st">"Geschätzte Behandlungseffekte mit 95% Konfidenzintervallen"</span>,</span>
<span>    subtitle <span class="op">=</span> <span class="st">"Rote Punkte zeigen wahre Effekte"</span></span>
<span>  <span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme_minimal</span><span class="op">(</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="trees_files/figure-html/unnamed-chunk-36-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Die roten Punkte zeigen die wahren Effekte, während die schwarzen Punkte mit den Konfidenzintervallen unsere Schätzungen darstellen.</p>
<!-- Ein gut kalibriertes Modell sollte etwa 95% der wahren Effekte innerhalb der Konfidenzintervalle haben.^[In der englischsprachigen Literatur wird dies als *coverage* bezeichnet.] Wir können dies mit R-Code prüfen. -->
<!-- ```{r} -->
<!-- # Überprüfung der Coverage der Konfidenzintervalle -->
<!-- ( -->
<!--   coverage <- cate_estimates %>% -->
<!--   summarise( -->
<!--     coverage = mean(true_effect >= lower_ci & true_effect <= upper_ci), -->
<!--     avg_ci_width = mean(upper_ci - lower_ci) -->
<!--   ) -->
<!-- ) -->
<!-- ``` -->
</section></section></section><section id="zusammenfassung" class="level2" data-number="15.7"><h2 data-number="15.7" class="anchored" data-anchor-id="zusammenfassung">
<span class="header-section-number">15.7</span> Zusammenfassung</h2>
<p>In diesem Kapitel haben wir die Anwendung baum-basierter Methoden in R diskutiert. Darunter Entscheidungsbäume, Bagging, Random Forests, Boosting und Causal Forests. Entscheidungsbäume sind Modelle, die die Daten anhand binärer Entscheidungsregeln sukzessiv in kleinere, homogene Gruppen aufgeteilt werden. Baum-Modelle bieten intuitive Interpretierbarkeit, neigen jedoch zur Überanpassung, was durch Beschneiden (Pruning) vermieden werden kann. Die Vorhersage einzelner Bäume ist tendentiell mit hoher Varianz verbunden. Random Forests kombinieren mit Bagging viele Entscheidungsbäume, die auf zufälligen Teilmengen der Daten und Merkmale trainiert werden. Durch die Aggregation der Vorhersagen vieler Bäume reduziert der Random Forest die Varianz und verbessert so die Vorhersagegenauigkeit. Boosting-Methoden mit Entscheidungsbäumen trainieren kleine Bäume sukzessive, wobei jeder weitere Baum zur Korrektur der gegenwärtigen Fehler des Ensembles trainiert wird. Gradient Boosting nutzt den Gradienten der Verlustfunktion, um die Vorhersagequalität des Ensembles zu optimieren. Diese prädiktiven Methoden wurden im <code>parsnip</code>-Framework in R implementiert.</p>
<p>Causal Forests erweitern das Random Forest-Konzept für die Schätzung heterogener Behandlungseffekte. Dabei wird das “Honest Splitting”-Prinzip angewandt, bei dem separate Datenstichproben für die Strukturbestimmung der Bäume und die Effektschätzung verwendet werden. Die Implementierung erfolgt mit dem <code>grf</code>-Paket, wobei zur Effizienzsteigerung Propensity Scores und Outcomes vorgeschätzt werden. Diese Orthogonalisierung ermöglicht es dem Causal Forest, sich auf die Heterogenität der Behandlungseffekte zu konzentrieren.</p>
<p>Für alle Methoden wurde gezeigt, wie die Vorhersagegüte durch Testdatensätze beurteilt und die Bedeutung einzelner Variablen mit Variable-Importance-Metriken analysiert werden kann.</p>


<!-- -->

<script type="webr-data">
eyJvcHRpb25zIjp7ImJhc2VVcmwiOiJodHRwczovL3dlYnIuci13YXNtLm9yZy92MC40LjEvIn0sInBhY2thZ2VzIjp7InJlcG9zIjpbXSwicGtncyI6WyJldmFsdWF0ZSIsImtuaXRyIiwiaHRtbHRvb2xzIiwiYmFndWV0dGUiLCJjb3dwbG90IiwiZ2JtIiwiZHBseXIiLCJnZ3Bsb3QyIiwiZ2dSYW5kb21Gb3Jlc3RzIiwiTUFTUyIsInB1cnJyIiwicmFuZG9tRm9yZXN0IiwicmF0dGxlIiwidGlkeW1vZGVscyIsInRpZHlyIl19LCJyZW5kZXJfZGYiOiJkZWZhdWx0In0=
</script><script type="ojs-module-contents">
{"contents":[{"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_20;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n","methodName":"interpretQuiet","inline":false,"cellName":"webr-widget-20"},{"source":"viewof _webr_editor_20 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-20-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-20-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_20 = webROjs.process(_webr_editor_20, {});\n","methodName":"interpret","inline":false,"cellName":"webr-20"},{"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_19;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n","methodName":"interpretQuiet","inline":false,"cellName":"webr-widget-19"},{"source":"viewof _webr_editor_19 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-19-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-19-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_19 = webROjs.process(_webr_editor_19, {});\n","methodName":"interpret","inline":false,"cellName":"webr-19"},{"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_18;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n","methodName":"interpretQuiet","inline":false,"cellName":"webr-widget-18"},{"source":"viewof _webr_editor_18 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-18-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-18-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_18 = webROjs.process(_webr_editor_18, {});\n","methodName":"interpret","inline":false,"cellName":"webr-18"},{"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_17;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n","methodName":"interpretQuiet","inline":false,"cellName":"webr-widget-17"},{"source":"viewof _webr_editor_17 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-17-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-17-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_17 = webROjs.process(_webr_editor_17, {});\n","methodName":"interpret","inline":false,"cellName":"webr-17"},{"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_16;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n","methodName":"interpretQuiet","inline":false,"cellName":"webr-widget-16"},{"source":"viewof _webr_editor_16 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-16-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-16-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_16 = webROjs.process(_webr_editor_16, {});\n","methodName":"interpret","inline":false,"cellName":"webr-16"},{"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_15;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n","methodName":"interpretQuiet","inline":false,"cellName":"webr-widget-15"},{"source":"viewof _webr_editor_15 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-15-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-15-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_15 = webROjs.process(_webr_editor_15, {});\n","methodName":"interpret","inline":false,"cellName":"webr-15"},{"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_14;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n","methodName":"interpretQuiet","inline":false,"cellName":"webr-widget-14"},{"source":"viewof _webr_editor_14 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-14-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-14-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_14 = webROjs.process(_webr_editor_14, {});\n","methodName":"interpret","inline":false,"cellName":"webr-14"},{"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_13;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n","methodName":"interpretQuiet","inline":false,"cellName":"webr-widget-13"},{"source":"viewof _webr_editor_13 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-13-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-13-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_13 = webROjs.process(_webr_editor_13, {});\n","methodName":"interpret","inline":false,"cellName":"webr-13"},{"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_12;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n","methodName":"interpretQuiet","inline":false,"cellName":"webr-widget-12"},{"source":"viewof _webr_editor_12 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-12-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-12-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_12 = webROjs.process(_webr_editor_12, {});\n","methodName":"interpret","inline":false,"cellName":"webr-12"},{"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_11;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n","methodName":"interpretQuiet","inline":false,"cellName":"webr-widget-11"},{"source":"viewof _webr_editor_11 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-11-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-11-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_11 = webROjs.process(_webr_editor_11, {});\n","methodName":"interpret","inline":false,"cellName":"webr-11"},{"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_10;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n","methodName":"interpretQuiet","inline":false,"cellName":"webr-widget-10"},{"source":"viewof _webr_editor_10 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-10-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-10-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_10 = webROjs.process(_webr_editor_10, {});\n","methodName":"interpret","inline":false,"cellName":"webr-10"},{"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_9;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n","methodName":"interpretQuiet","inline":false,"cellName":"webr-widget-9"},{"source":"viewof _webr_editor_9 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-9-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-9-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_9 = webROjs.process(_webr_editor_9, {});\n","methodName":"interpret","inline":false,"cellName":"webr-9"},{"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_8;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n","methodName":"interpretQuiet","inline":false,"cellName":"webr-widget-8"},{"source":"viewof _webr_editor_8 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-8-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-8-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_8 = webROjs.process(_webr_editor_8, {});\n","methodName":"interpret","inline":false,"cellName":"webr-8"},{"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_7;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n","methodName":"interpretQuiet","inline":false,"cellName":"webr-widget-7"},{"source":"viewof _webr_editor_7 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-7-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-7-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_7 = webROjs.process(_webr_editor_7, {});\n","methodName":"interpret","inline":false,"cellName":"webr-7"},{"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_6;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n","methodName":"interpretQuiet","inline":false,"cellName":"webr-widget-6"},{"source":"viewof _webr_editor_6 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-6-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-6-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_6 = webROjs.process(_webr_editor_6, {});\n","methodName":"interpret","inline":false,"cellName":"webr-6"},{"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_5;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n","methodName":"interpretQuiet","inline":false,"cellName":"webr-widget-5"},{"source":"viewof _webr_editor_5 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-5-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-5-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_5 = webROjs.process(_webr_editor_5, {});\n","methodName":"interpret","inline":false,"cellName":"webr-5"},{"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_4;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n","methodName":"interpretQuiet","inline":false,"cellName":"webr-widget-4"},{"source":"viewof _webr_editor_4 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-4-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-4-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_4 = webROjs.process(_webr_editor_4, {});\n","methodName":"interpret","inline":false,"cellName":"webr-4"},{"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_3;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n","methodName":"interpretQuiet","inline":false,"cellName":"webr-widget-3"},{"source":"viewof _webr_editor_3 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-3-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-3-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_3 = webROjs.process(_webr_editor_3, {});\n","methodName":"interpret","inline":false,"cellName":"webr-3"},{"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_2;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n","methodName":"interpretQuiet","inline":false,"cellName":"webr-widget-2"},{"source":"viewof _webr_editor_2 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-2-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-2-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_2 = webROjs.process(_webr_editor_2, {});\n","methodName":"interpret","inline":false,"cellName":"webr-2"},{"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_1;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n","methodName":"interpretQuiet","inline":false,"cellName":"webr-widget-1"},{"source":"viewof _webr_editor_1 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-1-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-1-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_1 = webROjs.process(_webr_editor_1, {});\n","methodName":"interpret","inline":false,"cellName":"webr-1"},{"source":"webROjs = {\n  const { WebR } = window._exercise_ojs_runtime.WebR;\n  const {\n    WebREvaluator,\n    WebREnvironmentManager,\n    setupR,\n    b64Decode,\n    collapsePath\n  } = window._exercise_ojs_runtime;\n\n  const statusContainer = document.getElementById(\"exercise-loading-status\");\n  const indicatorContainer = document.getElementById(\"exercise-loading-indicator\");\n  indicatorContainer.classList.remove(\"d-none\");\n\n  let statusText = document.createElement(\"div\")\n  statusText.classList = \"exercise-loading-details\";\n  statusText = statusContainer.appendChild(statusText);\n  statusText.textContent = `Initialise`;\n\n  // Hoist indicator out from final slide when running under reveal\n  const revealStatus = document.querySelector(\".reveal .exercise-loading-indicator\");\n  if (revealStatus) {\n    revealStatus.remove();\n    document.querySelector(\".reveal > .slides\").appendChild(revealStatus);\n  }\n\n  // webR supplemental data and options\n  const dataContent = document.querySelector(`script[type=\\\"webr-data\\\"]`).textContent;\n  const data = JSON.parse(b64Decode(dataContent));\n\n  // Grab list of resources to be downloaded\n  const filesContent = document.querySelector(`script[type=\\\"vfs-file\\\"]`).textContent;\n  const files = JSON.parse(b64Decode(filesContent));\n\n  // Initialise webR and setup for R code evaluation\n  let webRPromise = (async (webR) => {\n    statusText.textContent = `Downloading webR`;\n    await webR.init();\n\n    // Install provided list of packages\n    // Ensure webR default repo is included\n    data.packages.repos.push(\"https://repo.r-wasm.org\")\n    await data.packages.pkgs.map((pkg) => () => {\n      statusText.textContent = `Downloading package: ${pkg}`;\n      return webR.evalRVoid(`\n        webr::install(pkg, repos = repos)\n        library(pkg, character.only = TRUE)\n      `, { env: {\n        pkg: pkg,\n        repos: data.packages.repos,\n      }});\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n\n    // Download and install resources\n    await files.map((file) => async () => {\n      const name = file.substring(file.lastIndexOf('/') + 1);\n      statusText.textContent = `Downloading resource: ${name}`;\n      const response = await fetch(file);\n      if (!response.ok) {\n        throw new Error(`Can't download \\`${file}\\`. Error ${response.status}: \"${response.statusText}\".`);\n      }\n      const data = await response.arrayBuffer();\n\n      // Store URLs in the cwd without any subdirectory structure\n      if (file.includes(\"://\")) {\n        file = name;\n      }\n\n      // Collapse higher directory structure\n      file = collapsePath(file);\n\n      // Create directory tree, ignoring \"directory exists\" VFS errors\n      const parts = file.split('/').slice(0, -1);\n      let path = '';\n      while (parts.length > 0) {\n        path += parts.shift() + '/';\n        try {\n          await webR.FS.mkdir(path);\n        } catch (e) {\n          if (!e.message.includes(\"FS error\")) {\n            throw e;\n          }\n        }\n      }\n\n      // Write this file to the VFS\n      return await webR.FS.writeFile(file, new Uint8Array(data));\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n\n    statusText.textContent = `Installing webR shims`;\n    await webR.evalRVoid(`webr::shim_install()`);\n\n    statusText.textContent = `WebR environment setup`;\n    await setupR(webR, data);\n\n    statusText.remove();\n    if (statusContainer.children.length == 0) {\n      statusContainer.parentNode.remove();\n    }\n    return webR;\n  })(new WebR(data.options));\n\n  // Keep track of initial OJS block render\n  const renderedOjs = {};\n\n  const process = async (context, inputs) => {\n    const webR = await webRPromise;\n    const evaluator = new WebREvaluator(webR, context)\n    await evaluator.process(inputs);\n    return evaluator.container;\n  }\n\n  return {\n    process,\n    webRPromise,\n    renderedOjs,\n  };\n}\n","methodName":"interpretQuiet","inline":false,"cellName":"webr-prelude"}]}
</script><div id="exercise-loading-indicator" class="exercise-loading-indicator d-none d-flex align-items-center gap-2">
<div id="exercise-loading-status" class="d-flex gap-2">

</div>
<div class="spinner-grow spinner-grow-sm">

</div>
</div>
<script type="vfs-file">
W10=
</script><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-AtheyImbens2016" class="csl-entry" role="listitem">
Athey, Susan, und Guido Imbens. 2016. <span>„Recursive partitioning for heterogeneous causal effects“</span>. <em>Proceedings of the National Academy of Sciences</em> 113 (27): 7353–60. <a href="https://doi.org/10.1073/pnas.1510489113">https://doi.org/10.1073/pnas.1510489113</a>.
</div>
<div id="ref-Atheyetal2019" class="csl-entry" role="listitem">
Athey, Susan, Julie Tibshirani, und Stefan Wager. 2019. <span>„Generalized random forests“</span>. <em>The Annals of Statistics</em> 47 (2). <a href="https://doi.org/10.1214/18-aos1709">https://doi.org/10.1214/18-aos1709</a>.
</div>
<div id="ref-Breimanetal1984" class="csl-entry" role="listitem">
Breiman, L., J. Friedman, C. J. Stone, und R. A. Olshen. 1984. <em>Classification and Regression Trees</em>. Taylor &amp; Francis.
</div>
<div id="ref-Hastieetal2013" class="csl-entry" role="listitem">
Hastie, T., R. Tibshirani, und J. Friedman. 2013. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Springer Series in Statistics. Springer New York.
</div>
<div id="ref-grfPackage" class="csl-entry" role="listitem">
Tibshirani, Julie, Susan Athey, Erik Sverdrup, und Stefan Wager. 2024. <em>grf: Generalized Random Forests</em>. <a href="https://CRAN.R-project.org/package=grf">https://CRAN.R-project.org/package=grf</a>.
</div>
</div>
</section></main><!-- /main --><script type="ojs-module-contents">
eyJjb250ZW50cyI6W119
</script><script type="module">
if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
window._ojs.paths.runtimeToDoc = "../..";
window._ojs.paths.runtimeToRoot = "../..";
window._ojs.paths.docToRoot = "";
window._ojs.selfContained = false;
window._ojs.runtime.interpretFromScriptTags();
</script><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Kopiert");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Kopiert");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./svm.html" class="pagination-link" aria-label="Support Vector Machines">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Machine Learning.html" class="pagination-link" aria-label="Neuronale Netzwerke">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Neuronale Netzwerke</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Quellcode</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb36" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span><span class="co"> </span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="co">  live-html:</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="co">    webr: </span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="co">      packages:</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'baguette'</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'cowplot'</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'gbm'</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'dplyr'</span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'ggplot2'</span></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'ggRandomForests'</span></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'MASS'</span></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'purrr'</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'randomForest'</span></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'rattle'</span></span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'tidymodels'</span></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'tidyr'</span></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a><span class="co">      cell-options:</span></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a><span class="co">        fig-width: 8</span></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a><span class="an">engine:</span><span class="co"> knitr</span></span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>{{&lt; include ./_extensions/r-wasm/live/_knitr.qmd &gt;}}</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a><span class="fu"># Baum-basierte Methoden {#sec-trees}</span></span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a>Baum-basierte Methoden bieten eine vielseitige und leistungsstarke Herangehensweise für Vorhersage und Klassifikation in komplexen Datensätzen mit nicht-linearen Zusammenhängen. Ein Vorteil baum-basierter Methoden ist ihre inhärente Fähigkeit, die Bedeutung einzelner Variablen für die Vorhersage zu quantifizieren – eine Eigenschaft, die viele Machine-Learning-Modelle nicht ohne weiteres bieten und insbesondere in hoch-dimensionalen Anwendungen (mit vielen potentiellen Regressoren) nicht trivial ist. Dies ermöglicht es, tiefere Einblicke in den Einfluss einzelner Merkmale auf die Vorhersagen des Modells zu erhalten, was besonders in empirischen Anwendungen für die Entscheidungsstützung mit Machine Learning hilfreich sein kann.</span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a>*Entscheidungsbäume* stellen die Grundlage dieser Methoden dar. Sie ermöglichen die Aufteilung der Daten in immer kleinere, homogenere Gruppen, basierend auf *binären* Entscheidungsregeln, die aus den Prädiktoren abgleitet werden. Die trainierten Regeln eines solchen Modells lassen sich anhand eines Binärbaums visualisieren, was eine intuitive Interpretierbarkeit der Ergebnisse erlaubt. </span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a>*Random Forests* ist ein Ensemble-Ansatz, bei dem viele Entscheidungsbäume kombiniert werden. Jeder Baum wird auf einer zufälligen Teilmenge der Daten trainiert (*Bagging*), und bei jedem Knoten wird zusätzlich eine zufällige Teilmenge der Merkmale berücksichtigt. Die finale Vorhersage des Random Forests basiert auf der Aggregation der Vorhersagen aller Bäume (Mehrheitsvotum für Klassifikation, Durchschnitt für Regression). Dieses Verfahren reduziert das Risiko einer Überanpassung und erhöht oft die Vorhersagegenauigkeit im Vergleich zu einzelnen Entscheidungsbäumen.</span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a>*Boosting* ist eine weitere Ensemble-Methode zur Anpassung von Modellen mit hoher Vorhersagegüte durch Kombination einfacher Modelle (*Base learner*), wobei Regressions- oder Klassifikationsbäume eingesetzt werden können. Alternativ zu Random Forests trainieren Boosting-Algorithmen sukzessiv einfache (Klassifikations- oder Regressions-)Bäume, wobei jeder nachfolgende Baum das Ziel hat, die Vorhersagefehler der vorherigen Bäume zu korrigieren. </span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a>In diesem Kapitel erläutern wir die Anwendung baum-basierter Methoden in R anhand von Beispieldatensätzen. Wir zeigen, wie Regressionsbäume, Random Forests und Boosting-Modelle im <span class="in">`parsnip`</span>-Framework trainiert werden und wie die Vorhersageleistung durch die Wahl geeigneter Hyperparameter mit Cross-Validation und Out-of-Sample-Evaluierungsmethoden optimiert werden kann.</span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a><span class="fu">## Entscheidungsbäume {#sec-simpletrees}</span></span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a>Ein Entscheidungsbaum ist ein Modell, das auf der Basis von hierarchischen Bedingungen bzgl. der Regressoren Vorhersagen für die Outcome-Variable trifft. Jeder Baum beginnt mit einem Wurzelknoten (*root node*) und verzweigt sich binär. Jede Verzweigung (*split*) stellt eine Bedingung dar, die auf einem bestimmten Regressor basiert. Der Baum trifft Entscheidungen, indem er diese Bedingungen sukzessive überprüft, bis er zu einem Blattknoten (*leaf node* / *terminal node*) gelangt, der die finale Vorhersage liefert. Hierbei handelt es sich eine Mehrheitsentscheidung für Klassifikation und einen Mittelwert, jeweils gebildet anhand Beobachten des Trainingsdatensatzes im leaf node.</span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-42"><a href="#cb36-42" aria-hidden="true" tabindex="-1"></a>@fig-exdectree zeigt ein einfaches Beispiel eines Entscheidungsbaums zur Klassifikation der Kreditwürdigkeit einer Person. Die Klassfikation erfolgt, in dem die Beobachtung basierend auf den Merkmalen Alter, Einkommen und Eigentum durch den Baum geleitet wird. Zunächst wird geprüft, die Person 30 Jahre oder jünger ist. Fall ja, entscheidet der Baum anhand des Einkommens: Bei einem Jahreseinkommen von 40.000 oder weniger wird die Person als wenig kreditwürdig klassifiziert, bei höherem Einkommen als mäßig kreditwürdig. Für Personen älter als 30 Jahre überprüft das Modell lediglich, ob die Person eine Immobilie besitzt, um zwischen mäßiger Kreditwürdigkeit und guter Bonität zu unterscheiden.</span>
<span id="cb36-43"><a href="#cb36-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-46"><a href="#cb36-46" aria-hidden="true" tabindex="-1"></a><span class="in">```{dot}</span></span>
<span id="cb36-47"><a href="#cb36-47" aria-hidden="true" tabindex="-1"></a><span class="in">//| fig-width: 6</span></span>
<span id="cb36-48"><a href="#cb36-48" aria-hidden="true" tabindex="-1"></a><span class="in">//| fig-height: 5</span></span>
<span id="cb36-49"><a href="#cb36-49" aria-hidden="true" tabindex="-1"></a><span class="in">//| fig-cap: "Entscheidungsbaum: Klassifikation von Kreditwürdigkeit"</span></span>
<span id="cb36-50"><a href="#cb36-50" aria-hidden="true" tabindex="-1"></a><span class="in">//| label: fig-exdectree</span></span>
<span id="cb36-51"><a href="#cb36-51" aria-hidden="true" tabindex="-1"></a><span class="in">digraph exdectree {</span></span>
<span id="cb36-52"><a href="#cb36-52" aria-hidden="true" tabindex="-1"></a><span class="in">    node [shape=box];</span></span>
<span id="cb36-53"><a href="#cb36-53" aria-hidden="true" tabindex="-1"></a><span class="in">    splines=false;</span></span>
<span id="cb36-54"><a href="#cb36-54" aria-hidden="true" tabindex="-1"></a><span class="in">    ranksep = 1;</span></span>
<span id="cb36-55"><a href="#cb36-55" aria-hidden="true" tabindex="-1"></a><span class="in">    nodesep = 1.75;</span></span>
<span id="cb36-56"><a href="#cb36-56" aria-hidden="true" tabindex="-1"></a><span class="in">    margin = 0.15;</span></span>
<span id="cb36-57"><a href="#cb36-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-58"><a href="#cb36-58" aria-hidden="true" tabindex="-1"></a><span class="in">    1 [label="Alter &lt;= 30?"];</span></span>
<span id="cb36-59"><a href="#cb36-59" aria-hidden="true" tabindex="-1"></a><span class="in">    2 [label="Einkommen &lt;= 40 Tsd.?"];</span></span>
<span id="cb36-60"><a href="#cb36-60" aria-hidden="true" tabindex="-1"></a><span class="in">    3 [label="Eigentum?"];</span></span>
<span id="cb36-61"><a href="#cb36-61" aria-hidden="true" tabindex="-1"></a><span class="in">    4 [label="Status: Niedrig"];</span></span>
<span id="cb36-62"><a href="#cb36-62" aria-hidden="true" tabindex="-1"></a><span class="in">    5 [label="Status: Mittel"];</span></span>
<span id="cb36-63"><a href="#cb36-63" aria-hidden="true" tabindex="-1"></a><span class="in">    6 [label="Status: Hoch"];</span></span>
<span id="cb36-64"><a href="#cb36-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-65"><a href="#cb36-65" aria-hidden="true" tabindex="-1"></a><span class="in">    1 -&gt; 2 [label="Ja"];</span></span>
<span id="cb36-66"><a href="#cb36-66" aria-hidden="true" tabindex="-1"></a><span class="in">    1 -&gt; 3 [label="Nein"];</span></span>
<span id="cb36-67"><a href="#cb36-67" aria-hidden="true" tabindex="-1"></a><span class="in">    2 -&gt; 4 [label="Ja"];</span></span>
<span id="cb36-68"><a href="#cb36-68" aria-hidden="true" tabindex="-1"></a><span class="in">    2 -&gt; 5 [label="Nein"];</span></span>
<span id="cb36-69"><a href="#cb36-69" aria-hidden="true" tabindex="-1"></a><span class="in">    3 -&gt; 6 [label="Ja"];</span></span>
<span id="cb36-70"><a href="#cb36-70" aria-hidden="true" tabindex="-1"></a><span class="in">    3 -&gt; 5 [label="Nein"];</span></span>
<span id="cb36-71"><a href="#cb36-71" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb36-72"><a href="#cb36-72" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-73"><a href="#cb36-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-74"><a href="#cb36-74" aria-hidden="true" tabindex="-1"></a><span class="fu">## Training von Bäumen</span></span>
<span id="cb36-75"><a href="#cb36-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-76"><a href="#cb36-76" aria-hidden="true" tabindex="-1"></a>Zur Konstruktion von Binär-Bäumen werden etablierte Algorithmen wie *Classification and Regression Trees* (<span class="co">[</span><span class="ot">CART</span><span class="co">]</span>(https://de.wikipedia.org/wiki/CART_(Algorithmus)) von @Breimanetal1984 verwendet. Die wesentliche Vorgehensweise für das Training eines Baums $T$ ist wie folgt:</span>
<span id="cb36-77"><a href="#cb36-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-78"><a href="#cb36-78" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb36-79"><a href="#cb36-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-80"><a href="#cb36-80" aria-hidden="true" tabindex="-1"></a><span class="fu">## CART-Algorithmus</span></span>
<span id="cb36-81"><a href="#cb36-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-82"><a href="#cb36-82" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Splitting**: Beginnend am root node sucht der Algorithmus nach der "besten" Regel, die Daten anhand eines Merkmals in zwei Gruppen zu teilen. Die Qualität des Splits wird in Abhängigkeit der Definition der Outcome-Variable beurteilt:</span>
<span id="cb36-83"><a href="#cb36-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-84"><a href="#cb36-84" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>**Bei Klassifikation**: Die Reinheit (*purtity*) der Klassen in den unmittelbar nachfolgen nodes wird maximiert. Ein gängiges Kriterium hierfür ist der [*Gini-Koeffizient*](https://de.wikipedia.org/wiki/Gini-Koeffizient).^<span class="co">[</span><span class="ot">Der Gini-Koeffizient $0\leq G\leq1$ misst die Homogenität der Outcome-Variable für die Beobachtungen eines Knotens. $G=0$ ergibt sich bei vollständiger "Reinheit" (alle Beobachtungen im Knoten gehören zur gleichen Klasse). $G &gt; 0$ zeigt Heterogenität der Klassen an, die mit $G$ zunimmt</span><span class="co">]</span></span>
<span id="cb36-85"><a href="#cb36-85" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-86"><a href="#cb36-86" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>**Bei Regression**: Der MSE bei Vorhersage des Outcomes durch Mittelwertbildung für Beobachtungen in den unmittelbar nachfolgenden nodes wird minimiert.</span>
<span id="cb36-87"><a href="#cb36-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-88"><a href="#cb36-88" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Rekursion**: Der Prozess wird rekursiv fortgesetzt, bis Abbruchkriterien greifen eine weitere Verzewigung verhindern:</span>
<span id="cb36-89"><a href="#cb36-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-90"><a href="#cb36-90" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Die maximale Baumtiefe (*tree depth*) ist erreicht </span>
<span id="cb36-91"><a href="#cb36-91" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Die leaf nodes sind hinreichend "rein": Alle Beobachtungen in einem leaf node gehören zur gleichen Klasse oder die Verbesserung des Loss durch weitere Splits fällt unter einen festgelegten Schwellenwert</span>
<span id="cb36-92"><a href="#cb36-92" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Weitere Splits führen zu leaf nodes, die eine Mindestanzahl an Beobachtungen (*minimum split*) unterschreiten würden</span>
<span id="cb36-93"><a href="#cb36-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-94"><a href="#cb36-94" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Pruning**: Um Überanpassung an die Trainingsdaten zu vermeiden, kann der Baum beschnitten werden (*pruning*). Der Grundgedanke ist, dass tief verzweigte Bäume die Trainingsdaten zwar gut modellieren können, aber schlecht auf neue, unbekannte Daten generalisieren. </span>
<span id="cb36-95"><a href="#cb36-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-96"><a href="#cb36-96" aria-hidden="true" tabindex="-1"></a>    Bei *cost complexity (CP) pruning* werden, beginnend auf Ebene der leaf nodes sukuzessive Äste entfernt, und eine Balance zwischen Komplexität des Baums und dem Anpassungsfehler zu finden. Ähnlich wie bei regularisierter KQ-Schätzung (@sec-regreg), wird die Verlustfunktion $L$ um einen Strafterm für die Komplexität erweitert. Der Effekt der Strafe wird durch den CP-Parameter $\alpha\in<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ geregelt,</span>
<span id="cb36-97"><a href="#cb36-97" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-98"><a href="#cb36-98" aria-hidden="true" tabindex="-1"></a>    \begin{align*}</span>
<span id="cb36-99"><a href="#cb36-99" aria-hidden="true" tabindex="-1"></a>      L_{\alpha}(T) = L(T) + \alpha \lvert T\rvert,</span>
<span id="cb36-100"><a href="#cb36-100" aria-hidden="true" tabindex="-1"></a>    \end{align*}</span>
<span id="cb36-101"><a href="#cb36-101" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb36-102"><a href="#cb36-102" aria-hidden="true" tabindex="-1"></a>  für einen Baum $T$ mit Komplexitätsmaß $\lvert T\rvert$ (Anzahl der leaf nodes) <span class="co">[</span><span class="ot">@Hastieetal2013</span><span class="co">]</span>.</span>
<span id="cb36-103"><a href="#cb36-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-104"><a href="#cb36-104" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb36-105"><a href="#cb36-105" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb36-106"><a href="#cb36-106" aria-hidden="true" tabindex="-1"></a>Zur Demonstation der Schätzung von Regressionsbäumen mit R betrachten wir nachfolgend den Datensatz <span class="in">`MASS::Bosten`</span>. Ziel hierbei ist es, mittlere Hauswerte <span class="in">`medv`</span> in Stadteilen von Boston, MA vorherzusagen. Wir verwenden hierzu Funktionen aus dem Paket <span class="in">`parsnip`</span>. </span>
<span id="cb36-107"><a href="#cb36-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-108"><a href="#cb36-108" aria-hidden="true" tabindex="-1"></a>Zunächst transformieren wir den Datensatz in ein <span class="in">`tibble`</span>-Objekt und definieren Trainings- und Test-Daten.</span>
<span id="cb36-109"><a href="#cb36-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-112"><a href="#cb36-112" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb36-113"><a href="#cb36-113" aria-hidden="true" tabindex="-1"></a><span class="in">library(parsnip)</span></span>
<span id="cb36-114"><a href="#cb36-114" aria-hidden="true" tabindex="-1"></a><span class="in">library(cowplot)</span></span>
<span id="cb36-115"><a href="#cb36-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-116"><a href="#cb36-116" aria-hidden="true" tabindex="-1"></a><span class="in"># Seed setzen</span></span>
<span id="cb36-117"><a href="#cb36-117" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(1234)</span></span>
<span id="cb36-118"><a href="#cb36-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-119"><a href="#cb36-119" aria-hidden="true" tabindex="-1"></a><span class="in"># Datensatz als tibble</span></span>
<span id="cb36-120"><a href="#cb36-120" aria-hidden="true" tabindex="-1"></a><span class="in">Boston &lt;- as_tibble(MASS::Boston)</span></span>
<span id="cb36-121"><a href="#cb36-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-122"><a href="#cb36-122" aria-hidden="true" tabindex="-1"></a><span class="in"># Splitting in Training- und Test-Daten</span></span>
<span id="cb36-123"><a href="#cb36-123" aria-hidden="true" tabindex="-1"></a><span class="in">Boston_split &lt;- initial_split(</span></span>
<span id="cb36-124"><a href="#cb36-124" aria-hidden="true" tabindex="-1"></a><span class="in">  data = Boston, </span></span>
<span id="cb36-125"><a href="#cb36-125" aria-hidden="true" tabindex="-1"></a><span class="in">  prop = 0.8, </span></span>
<span id="cb36-126"><a href="#cb36-126" aria-hidden="true" tabindex="-1"></a><span class="in">  strata = medv</span></span>
<span id="cb36-127"><a href="#cb36-127" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb36-128"><a href="#cb36-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-129"><a href="#cb36-129" aria-hidden="true" tabindex="-1"></a><span class="in">Boston_train &lt;- training(Boston_split)</span></span>
<span id="cb36-130"><a href="#cb36-130" aria-hidden="true" tabindex="-1"></a><span class="in">Boston_test &lt;- testing(Boston_split)</span></span>
<span id="cb36-131"><a href="#cb36-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-132"><a href="#cb36-132" aria-hidden="true" tabindex="-1"></a><span class="in">slice_head(Boston_train, n = 10)</span></span>
<span id="cb36-133"><a href="#cb36-133" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-134"><a href="#cb36-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-135"><a href="#cb36-135" aria-hidden="true" tabindex="-1"></a><span class="in">`parsnip`</span> bietet eine vereinheitlichetes Framework für das Training von Modellen mit R und eine flexible API für Machine Learning. Wir definieren zunächst mit <span class="in">`parsnip::decision_tree()`</span> eine Spezifikation zum Training von Entschieundgsmodellen und übergeben beispielhaft einen CP-Parameter $\alpha=.1$. Mit <span class="in">`parsnip::set_engine`</span> wählen wir das Paket <span class="in">`raprt`</span>. Der hier implementierte Agorithmus ist CART. Zuletzt legen wir mit <span class="in">` parsnip::set_mode()`</span> fest, dass der Algorithmus für Regression durchgeführt werden soll.</span>
<span id="cb36-136"><a href="#cb36-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-139"><a href="#cb36-139" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb36-140"><a href="#cb36-140" aria-hidden="true" tabindex="-1"></a><span class="in"># Spezifikation festlegen</span></span>
<span id="cb36-141"><a href="#cb36-141" aria-hidden="true" tabindex="-1"></a><span class="in">tree_spec &lt;- decision_tree(</span></span>
<span id="cb36-142"><a href="#cb36-142" aria-hidden="true" tabindex="-1"></a><span class="in">  cost_complexity = 0.1</span></span>
<span id="cb36-143"><a href="#cb36-143" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb36-144"><a href="#cb36-144" aria-hidden="true" tabindex="-1"></a><span class="in">  set_engine("rpart") %&gt;%</span></span>
<span id="cb36-145"><a href="#cb36-145" aria-hidden="true" tabindex="-1"></a><span class="in">  set_mode("regression")</span></span>
<span id="cb36-146"><a href="#cb36-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-147"><a href="#cb36-147" aria-hidden="true" tabindex="-1"></a><span class="in"># Modell trainieren</span></span>
<span id="cb36-148"><a href="#cb36-148" aria-hidden="true" tabindex="-1"></a><span class="in">tree_fit &lt;- tree_spec %&gt;%</span></span>
<span id="cb36-149"><a href="#cb36-149" aria-hidden="true" tabindex="-1"></a><span class="in">  fit(</span></span>
<span id="cb36-150"><a href="#cb36-150" aria-hidden="true" tabindex="-1"></a><span class="in">    formula = medv ~ ., </span></span>
<span id="cb36-151"><a href="#cb36-151" aria-hidden="true" tabindex="-1"></a><span class="in">    data = Boston_train, </span></span>
<span id="cb36-152"><a href="#cb36-152" aria-hidden="true" tabindex="-1"></a><span class="in">    model = TRUE</span></span>
<span id="cb36-153"><a href="#cb36-153" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb36-154"><a href="#cb36-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-155"><a href="#cb36-155" aria-hidden="true" tabindex="-1"></a><span class="in"># Trainierten Baum in Konsole ausgeben</span></span>
<span id="cb36-156"><a href="#cb36-156" aria-hidden="true" tabindex="-1"></a><span class="in">tree_fit$fit</span></span>
<span id="cb36-157"><a href="#cb36-157" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-158"><a href="#cb36-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-159"><a href="#cb36-159" aria-hidden="true" tabindex="-1"></a>Der Output in <span class="in">`tree_fit$fit`</span> zeigt, dass CP-Pruning zu einem kleinen Baum mit 3 Hierarchie-Ebenen geführt hat. Die Struktur zeigt, dass <span class="in">`lstat`</span> und <span class="in">`rm`</span> für Splitting-Regeln (<span class="in">`split`</span>) verwendet werden, wie viele Beobachtungen den  nodes zugeordnet sind (<span class="in">`n`</span>), den Wert der Verlustfunktion (<span class="in">`deviance`</span>) sowie den Durchschnitt von <span class="in">`medv`</span> für jede node (<span class="in">`yval`</span>). Für die drei leaf nodes (gekennzeichnet mit <span class="in">`*`</span>) ist <span class="in">`yval`</span> die Vorhersage der Outcome-Varibale für entsprechend gruppierte Beobachtungen.</span>
<span id="cb36-160"><a href="#cb36-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-161"><a href="#cb36-161" aria-hidden="true" tabindex="-1"></a>Eine leichter interpretierbare Darstellung der Entscheidungsregeln des angepassten Baums in <span class="in">`tree_fit$fit`</span>  erhalten wir mit <span class="in">`rattle::fancyRpartPlot()`</span>.</span>
<span id="cb36-162"><a href="#cb36-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-165"><a href="#cb36-165" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb36-166"><a href="#cb36-166" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-width: 8</span></span>
<span id="cb36-167"><a href="#cb36-167" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-height: 8</span></span>
<span id="cb36-168"><a href="#cb36-168" aria-hidden="true" tabindex="-1"></a><span class="in">library(rattle)</span></span>
<span id="cb36-169"><a href="#cb36-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-170"><a href="#cb36-170" aria-hidden="true" tabindex="-1"></a><span class="in"># Plot the decision tree</span></span>
<span id="cb36-171"><a href="#cb36-171" aria-hidden="true" tabindex="-1"></a><span class="in">fancyRpartPlot(</span></span>
<span id="cb36-172"><a href="#cb36-172" aria-hidden="true" tabindex="-1"></a><span class="in">  tree_fit$fit,</span></span>
<span id="cb36-173"><a href="#cb36-173" aria-hidden="true" tabindex="-1"></a><span class="in">  split.col = "black", </span></span>
<span id="cb36-174"><a href="#cb36-174" aria-hidden="true" tabindex="-1"></a><span class="in">  nn.col = "black", </span></span>
<span id="cb36-175"><a href="#cb36-175" aria-hidden="true" tabindex="-1"></a><span class="in">  caption = "Trainierter Entscheidungsbaum für cp = 0.1",</span></span>
<span id="cb36-176"><a href="#cb36-176" aria-hidden="true" tabindex="-1"></a><span class="in">  palette = "Set1",</span></span>
<span id="cb36-177"><a href="#cb36-177" aria-hidden="true" tabindex="-1"></a><span class="in">  branch.col = "black",</span></span>
<span id="cb36-178"><a href="#cb36-178" aria-hidden="true" tabindex="-1"></a><span class="in">  digits = 3</span></span>
<span id="cb36-179"><a href="#cb36-179" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb36-180"><a href="#cb36-180" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-181"><a href="#cb36-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-182"><a href="#cb36-182" aria-hidden="true" tabindex="-1"></a>@fig-rpartregspace zeigt Beobachtungen von <span class="in">`rm`</span> und <span class="in">`lstat`</span>, die hinsichtlich ihrer in drei Klassen eingeteilten Ausprägung von <span class="in">`medv`</span> eingefärbt sind. Die durch den CART-Algorithmus gelernten Entscheidungsregeln sind als farbige Paritionen des Regressorraums dargestellt. </span>
<span id="cb36-183"><a href="#cb36-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-186"><a href="#cb36-186" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-187"><a href="#cb36-187" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb36-188"><a href="#cb36-188" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Partitionierung des Regressorraums für `lstat` und `rm` durch Regressionsbaum"</span></span>
<span id="cb36-189"><a href="#cb36-189" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rpartregspace</span></span>
<span id="cb36-190"><a href="#cb36-190" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb36-191"><a href="#cb36-191" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb36-192"><a href="#cb36-192" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cowplot)</span>
<span id="cb36-193"><a href="#cb36-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-194"><a href="#cb36-194" aria-hidden="true" tabindex="-1"></a><span class="co"># Seed setzen</span></span>
<span id="cb36-195"><a href="#cb36-195" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb36-196"><a href="#cb36-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-197"><a href="#cb36-197" aria-hidden="true" tabindex="-1"></a><span class="co"># Datensatz als tibble</span></span>
<span id="cb36-198"><a href="#cb36-198" aria-hidden="true" tabindex="-1"></a>Boston <span class="ot">&lt;-</span> tibble<span class="sc">::</span><span class="fu">as_tibble</span>(MASS<span class="sc">::</span>Boston)</span>
<span id="cb36-199"><a href="#cb36-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-200"><a href="#cb36-200" aria-hidden="true" tabindex="-1"></a><span class="co"># Splitting in Training- und Test-Daten</span></span>
<span id="cb36-201"><a href="#cb36-201" aria-hidden="true" tabindex="-1"></a>Boston_split <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">initial_split</span>(</span>
<span id="cb36-202"><a href="#cb36-202" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> Boston, </span>
<span id="cb36-203"><a href="#cb36-203" aria-hidden="true" tabindex="-1"></a>  <span class="at">prop =</span> <span class="fl">0.8</span>, </span>
<span id="cb36-204"><a href="#cb36-204" aria-hidden="true" tabindex="-1"></a>  <span class="at">strata =</span> medv</span>
<span id="cb36-205"><a href="#cb36-205" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb36-206"><a href="#cb36-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-207"><a href="#cb36-207" aria-hidden="true" tabindex="-1"></a>Boston_train <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">training</span>(Boston_split)</span>
<span id="cb36-208"><a href="#cb36-208" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb36-209"><a href="#cb36-209" aria-hidden="true" tabindex="-1"></a>tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(<span class="at">formula =</span> medv <span class="sc">~</span> rm <span class="sc">+</span> lstat, <span class="at">data =</span> Boston_train, <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">cp =</span> .<span class="dv">1</span>))</span>
<span id="cb36-210"><a href="#cb36-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-211"><a href="#cb36-211" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a grid of values for rm and lstat (the two features of interest)</span></span>
<span id="cb36-212"><a href="#cb36-212" aria-hidden="true" tabindex="-1"></a>grid_rm <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(Boston_train<span class="sc">$</span>rm), <span class="fu">max</span>(Boston_train<span class="sc">$</span>rm), <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb36-213"><a href="#cb36-213" aria-hidden="true" tabindex="-1"></a>grid_lstat <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(Boston_train<span class="sc">$</span>lstat), <span class="fu">max</span>(Boston_train<span class="sc">$</span>lstat), <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb36-214"><a href="#cb36-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-215"><a href="#cb36-215" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate all combinations of grid points</span></span>
<span id="cb36-216"><a href="#cb36-216" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">rm =</span> grid_rm, <span class="at">lstat =</span> grid_lstat)</span>
<span id="cb36-217"><a href="#cb36-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-218"><a href="#cb36-218" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict the median value (medv) for each point in the grid</span></span>
<span id="cb36-219"><a href="#cb36-219" aria-hidden="true" tabindex="-1"></a>grid<span class="sc">$</span>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree, <span class="at">newdata =</span> grid)</span>
<span id="cb36-220"><a href="#cb36-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-221"><a href="#cb36-221" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a scatter plot of the original data points colored by the actual medv values</span></span>
<span id="cb36-222"><a href="#cb36-222" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(Boston_train, <span class="fu">aes</span>(<span class="at">x =</span> rm, <span class="at">y =</span> lstat)) <span class="sc">+</span></span>
<span id="cb36-223"><a href="#cb36-223" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Anz. Räume (rm)"</span>, <span class="at">y =</span> <span class="st">"Anteil Bev. niedriger Status  (lstat)"</span>) <span class="sc">+</span></span>
<span id="cb36-224"><a href="#cb36-224" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-225"><a href="#cb36-225" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add the predicted decision boundary</span></span>
<span id="cb36-226"><a href="#cb36-226" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_tile</span>(<span class="at">data =</span> grid, <span class="fu">aes</span>(<span class="at">x =</span> rm, <span class="at">y =</span> lstat, <span class="at">fill =</span> <span class="fu">as.factor</span>(<span class="fu">round</span>(pred,<span class="dv">2</span>))), <span class="at">alpha =</span> <span class="fl">0.3</span>) <span class="sc">+</span></span>
<span id="cb36-227"><a href="#cb36-227" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_fill_viridis_d</span>(<span class="st">"Vorhersage in Partition"</span>) <span class="sc">+</span>  <span class="co"># Fill scale for predicted medv</span></span>
<span id="cb36-228"><a href="#cb36-228" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">color =</span> <span class="fu">cut</span>(medv, <span class="dv">3</span>))) <span class="sc">+</span>  <span class="co"># Original data points</span></span>
<span id="cb36-229"><a href="#cb36-229" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_color_viridis_d</span>(<span class="st">"medv"</span>) <span class="sc">+</span>  <span class="co"># Color scale for the median value of homes</span></span>
<span id="cb36-230"><a href="#cb36-230" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_cowplot</span>() <span class="sc">+</span>  <span class="co"># Use a clean theme'</span></span>
<span id="cb36-231"><a href="#cb36-231" aria-hidden="true" tabindex="-1"></a>    <span class="fu">coord_cartesian</span>(<span class="at">expand =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb36-232"><a href="#cb36-232" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"top"</span>, <span class="at">legend.direction =</span> <span class="st">"vertical"</span>)</span>
<span id="cb36-233"><a href="#cb36-233" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-234"><a href="#cb36-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-235"><a href="#cb36-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-236"><a href="#cb36-236" aria-hidden="true" tabindex="-1"></a>Für eine datengetriebene Wahl des CP-Parameters $\alpha$ kann Cross Validation (CV) verwendet werden. Hierzu erstellen wir zunächst eine <span class="in">`parsnip`</span>-Spezifikation mit <span class="in">`cost_complexity = tune::tune()`</span> in <span class="in">`decision_tree()`</span> und erstellen einen *workflow* mit <span class="in">`parsnip::workflow()`</span></span>
<span id="cb36-237"><a href="#cb36-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-240"><a href="#cb36-240" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb36-241"><a href="#cb36-241" aria-hidden="true" tabindex="-1"></a><span class="in"># Spezifikation für CV von cost_complexity</span></span>
<span id="cb36-242"><a href="#cb36-242" aria-hidden="true" tabindex="-1"></a><span class="in">tree_spec_cv &lt;- decision_tree(</span></span>
<span id="cb36-243"><a href="#cb36-243" aria-hidden="true" tabindex="-1"></a><span class="in">  cost_complexity = tune()</span></span>
<span id="cb36-244"><a href="#cb36-244" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb36-245"><a href="#cb36-245" aria-hidden="true" tabindex="-1"></a><span class="in">  set_engine("rpart") %&gt;%</span></span>
<span id="cb36-246"><a href="#cb36-246" aria-hidden="true" tabindex="-1"></a><span class="in">  set_mode("regression")</span></span>
<span id="cb36-247"><a href="#cb36-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-248"><a href="#cb36-248" aria-hidden="true" tabindex="-1"></a><span class="in"># Workflow definieren</span></span>
<span id="cb36-249"><a href="#cb36-249" aria-hidden="true" tabindex="-1"></a><span class="in">tree_wf_cv &lt;- workflow() %&gt;%</span></span>
<span id="cb36-250"><a href="#cb36-250" aria-hidden="true" tabindex="-1"></a><span class="in">  add_model(tree_spec_cv) %&gt;%</span></span>
<span id="cb36-251"><a href="#cb36-251" aria-hidden="true" tabindex="-1"></a><span class="in">  add_formula(medv ~ .)</span></span>
<span id="cb36-252"><a href="#cb36-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-253"><a href="#cb36-253" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-254"><a href="#cb36-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-255"><a href="#cb36-255" aria-hidden="true" tabindex="-1"></a>Mit <span class="in">`rsample::vfold_cv()`</span> definieren wir den CV-Prozess: 10-fold CV mit 2 Wiederholungen. <span class="in">`tune::tune_grid()`</span> führt CV anhand des in <span class="in">`tree_wf_cv`</span> definierten workflows durch. Hierbei werden in <span class="in">`cp_grid`</span> festgelegte Werte von <span class="in">`cost_complexity`</span> berücksichtigt. Die mit <span class="in">`yardstick::metric_set(rmse)`</span> festgelegte Verlustfunktion ist der mittlere quadratische Fehler (RMSE).^<span class="co">[</span><span class="ot">Die hier verwedete Funktion ist `yardstick::rmse()`.</span><span class="co">]</span></span>
<span id="cb36-256"><a href="#cb36-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-259"><a href="#cb36-259" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb36-260"><a href="#cb36-260" aria-hidden="true" tabindex="-1"></a><span class="in"># CV-Prozess definieren</span></span>
<span id="cb36-261"><a href="#cb36-261" aria-hidden="true" tabindex="-1"></a><span class="in">cv_folds &lt;- vfold_cv(</span></span>
<span id="cb36-262"><a href="#cb36-262" aria-hidden="true" tabindex="-1"></a><span class="in">  data = Boston_train, </span></span>
<span id="cb36-263"><a href="#cb36-263" aria-hidden="true" tabindex="-1"></a><span class="in">  v = 10, </span></span>
<span id="cb36-264"><a href="#cb36-264" aria-hidden="true" tabindex="-1"></a><span class="in">  repeats = 2</span></span>
<span id="cb36-265"><a href="#cb36-265" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb36-266"><a href="#cb36-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-267"><a href="#cb36-267" aria-hidden="true" tabindex="-1"></a><span class="in"># CV durchführen:</span></span>
<span id="cb36-268"><a href="#cb36-268" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(1234)</span></span>
<span id="cb36-269"><a href="#cb36-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-270"><a href="#cb36-270" aria-hidden="true" tabindex="-1"></a><span class="in"># Grid definieren</span></span>
<span id="cb36-271"><a href="#cb36-271" aria-hidden="true" tabindex="-1"></a><span class="in">cp_grid &lt;- tibble(</span></span>
<span id="cb36-272"><a href="#cb36-272" aria-hidden="true" tabindex="-1"></a><span class="in">  cost_complexity = c(</span></span>
<span id="cb36-273"><a href="#cb36-273" aria-hidden="true" tabindex="-1"></a><span class="in">    0.1, .075, 0.05, 0.01, 0.001, 0.0001</span></span>
<span id="cb36-274"><a href="#cb36-274" aria-hidden="true" tabindex="-1"></a><span class="in">    )</span></span>
<span id="cb36-275"><a href="#cb36-275" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb36-276"><a href="#cb36-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-277"><a href="#cb36-277" aria-hidden="true" tabindex="-1"></a><span class="in"># Tuning mit CV</span></span>
<span id="cb36-278"><a href="#cb36-278" aria-hidden="true" tabindex="-1"></a><span class="in">tree_fit_cv &lt;- tree_wf_cv %&gt;%</span></span>
<span id="cb36-279"><a href="#cb36-279" aria-hidden="true" tabindex="-1"></a><span class="in">    tune_grid(</span></span>
<span id="cb36-280"><a href="#cb36-280" aria-hidden="true" tabindex="-1"></a><span class="in">        resamples = cv_folds, </span></span>
<span id="cb36-281"><a href="#cb36-281" aria-hidden="true" tabindex="-1"></a><span class="in">        grid = cp_grid,</span></span>
<span id="cb36-282"><a href="#cb36-282" aria-hidden="true" tabindex="-1"></a><span class="in">        metrics = metric_set(rmse)</span></span>
<span id="cb36-283"><a href="#cb36-283" aria-hidden="true" tabindex="-1"></a><span class="in">    )</span></span>
<span id="cb36-284"><a href="#cb36-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-285"><a href="#cb36-285" aria-hidden="true" tabindex="-1"></a><span class="in"># CV-Ergebnisse</span></span>
<span id="cb36-286"><a href="#cb36-286" aria-hidden="true" tabindex="-1"></a><span class="in">tree_fit_cv</span></span>
<span id="cb36-287"><a href="#cb36-287" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-288"><a href="#cb36-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-289"><a href="#cb36-289" aria-hidden="true" tabindex="-1"></a>Mit <span class="in">`workflowsets::autoplot()`</span> kann der CV-RMSE für als Funktion des CP-Parameter leicht grafisch betrachtet dargestellt werden.</span>
<span id="cb36-290"><a href="#cb36-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-293"><a href="#cb36-293" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb36-294"><a href="#cb36-294" aria-hidden="true" tabindex="-1"></a><span class="in"># CV-Ergebnisse visualisieren</span></span>
<span id="cb36-295"><a href="#cb36-295" aria-hidden="true" tabindex="-1"></a><span class="in">autoplot(tree_fit_cv) +</span></span>
<span id="cb36-296"><a href="#cb36-296" aria-hidden="true" tabindex="-1"></a><span class="in">  labs(</span></span>
<span id="cb36-297"><a href="#cb36-297" aria-hidden="true" tabindex="-1"></a><span class="in">    title = "CV für CP-Parameter: RMSE vs. Komplexität"</span></span>
<span id="cb36-298"><a href="#cb36-298" aria-hidden="true" tabindex="-1"></a><span class="in">  ) +</span></span>
<span id="cb36-299"><a href="#cb36-299" aria-hidden="true" tabindex="-1"></a><span class="in">  theme_cowplot()</span></span>
<span id="cb36-300"><a href="#cb36-300" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-301"><a href="#cb36-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-302"><a href="#cb36-302" aria-hidden="true" tabindex="-1"></a>Für eine tabellierte Übersicht der besten Modelle kann <span class="in">`tune::show_best()`</span> verwendet werden. <span class="in">`tune::select_best()`</span> liest die beste Parameter-Kombination aus. </span>
<span id="cb36-303"><a href="#cb36-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-306"><a href="#cb36-306" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb36-307"><a href="#cb36-307" aria-hidden="true" tabindex="-1"></a><span class="in"># Tabellarische Übersicht</span></span>
<span id="cb36-308"><a href="#cb36-308" aria-hidden="true" tabindex="-1"></a><span class="in">show_best(</span></span>
<span id="cb36-309"><a href="#cb36-309" aria-hidden="true" tabindex="-1"></a><span class="in">  x = tree_fit_cv, </span></span>
<span id="cb36-310"><a href="#cb36-310" aria-hidden="true" tabindex="-1"></a><span class="in">  metric = "rmse"</span></span>
<span id="cb36-311"><a href="#cb36-311" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb36-312"><a href="#cb36-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-313"><a href="#cb36-313" aria-hidden="true" tabindex="-1"></a><span class="in"># Getunter Paremater</span></span>
<span id="cb36-314"><a href="#cb36-314" aria-hidden="true" tabindex="-1"></a><span class="in">best_tree_fit &lt;- select_best(</span></span>
<span id="cb36-315"><a href="#cb36-315" aria-hidden="true" tabindex="-1"></a><span class="in">  x = tree_fit_cv, </span></span>
<span id="cb36-316"><a href="#cb36-316" aria-hidden="true" tabindex="-1"></a><span class="in">  metric = "rmse"</span></span>
<span id="cb36-317"><a href="#cb36-317" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb36-318"><a href="#cb36-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-319"><a href="#cb36-319" aria-hidden="true" tabindex="-1"></a><span class="in">best_tree_fit</span></span>
<span id="cb36-320"><a href="#cb36-320" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-321"><a href="#cb36-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-322"><a href="#cb36-322" aria-hidden="true" tabindex="-1"></a>Anhand <span class="in">`tree_fit_cv`</span> trainieren wir die finale Spezifikation.</span>
<span id="cb36-323"><a href="#cb36-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-326"><a href="#cb36-326" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb36-327"><a href="#cb36-327" aria-hidden="true" tabindex="-1"></a><span class="in"># Finales Modell schätzen</span></span>
<span id="cb36-328"><a href="#cb36-328" aria-hidden="true" tabindex="-1"></a><span class="in">final_tree_spec &lt;- decision_tree(</span></span>
<span id="cb36-329"><a href="#cb36-329" aria-hidden="true" tabindex="-1"></a><span class="in">  cost_complexity = best_tree_fit$cost_complexity</span></span>
<span id="cb36-330"><a href="#cb36-330" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb36-331"><a href="#cb36-331" aria-hidden="true" tabindex="-1"></a><span class="in">  set_engine("rpart") %&gt;%</span></span>
<span id="cb36-332"><a href="#cb36-332" aria-hidden="true" tabindex="-1"></a><span class="in">  set_mode("regression")</span></span>
<span id="cb36-333"><a href="#cb36-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-334"><a href="#cb36-334" aria-hidden="true" tabindex="-1"></a><span class="in">final_tree_fit &lt;- final_tree_spec %&gt;%</span></span>
<span id="cb36-335"><a href="#cb36-335" aria-hidden="true" tabindex="-1"></a><span class="in">  fit(</span></span>
<span id="cb36-336"><a href="#cb36-336" aria-hidden="true" tabindex="-1"></a><span class="in">    formula = medv ~ ., </span></span>
<span id="cb36-337"><a href="#cb36-337" aria-hidden="true" tabindex="-1"></a><span class="in">    data = Boston_train</span></span>
<span id="cb36-338"><a href="#cb36-338" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb36-339"><a href="#cb36-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-340"><a href="#cb36-340" aria-hidden="true" tabindex="-1"></a><span class="in"># final_tree_fit</span></span>
<span id="cb36-341"><a href="#cb36-341" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-342"><a href="#cb36-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-343"><a href="#cb36-343" aria-hidden="true" tabindex="-1"></a>Der geringe CP-Parameter führt zu einem großen Entscheidungsbaum.^<span class="co">[</span><span class="ot">Die Dimension der Grafik wurde hier zwecks Darstellung des gesamten Baums gewählt. `print(final_tree_fit$fit)` druckt die Entscheidungsregeln in die R-Konsole (hierzu die letzte Zeile ausführen).</span><span class="co">]</span> </span>
<span id="cb36-344"><a href="#cb36-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-347"><a href="#cb36-347" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb36-348"><a href="#cb36-348" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-width: 8</span></span>
<span id="cb36-349"><a href="#cb36-349" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-height: 8</span></span>
<span id="cb36-350"><a href="#cb36-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-351"><a href="#cb36-351" aria-hidden="true" tabindex="-1"></a><span class="in"># CV-Fit plotten</span></span>
<span id="cb36-352"><a href="#cb36-352" aria-hidden="true" tabindex="-1"></a><span class="in">fancyRpartPlot(</span></span>
<span id="cb36-353"><a href="#cb36-353" aria-hidden="true" tabindex="-1"></a><span class="in">  final_tree_fit$fit,</span></span>
<span id="cb36-354"><a href="#cb36-354" aria-hidden="true" tabindex="-1"></a><span class="in">  split.col = "black", </span></span>
<span id="cb36-355"><a href="#cb36-355" aria-hidden="true" tabindex="-1"></a><span class="in">  nn.col = "black", </span></span>
<span id="cb36-356"><a href="#cb36-356" aria-hidden="true" tabindex="-1"></a><span class="in">  caption = "Mit CV ermittelter Entscheidungsbaum",</span></span>
<span id="cb36-357"><a href="#cb36-357" aria-hidden="true" tabindex="-1"></a><span class="in">  palette = "Set1",</span></span>
<span id="cb36-358"><a href="#cb36-358" aria-hidden="true" tabindex="-1"></a><span class="in">  branch.col = "black"</span></span>
<span id="cb36-359"><a href="#cb36-359" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb36-360"><a href="#cb36-360" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-361"><a href="#cb36-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-362"><a href="#cb36-362" aria-hidden="true" tabindex="-1"></a>Zur Beurteilung der Relevanz von Variablen für die Reduktion des Anpassungsfehlers (*variable importance*) kann der Eintrag <span class="in">`variable.importance`</span> des <span class="in">`rpart`</span>-Objekts herangezogen werden. Variable importance misst hier die Gesamtreduktion der Fehlerquadratsumem über alle Knoten, an denen die jeweilige Variable für Splits verwendet wird. </span>
<span id="cb36-363"><a href="#cb36-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-366"><a href="#cb36-366" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb36-367"><a href="#cb36-367" aria-hidden="true" tabindex="-1"></a><span class="in"># Variable-Importance auslesen</span></span>
<span id="cb36-368"><a href="#cb36-368" aria-hidden="true" tabindex="-1"></a><span class="in">final_tree_fit$fit$variable.importance</span></span>
<span id="cb36-369"><a href="#cb36-369" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-370"><a href="#cb36-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-371"><a href="#cb36-371" aria-hidden="true" tabindex="-1"></a>Die Werte von Variable Importance zeigen, dass der mit CV ermittelte Baum *alle* Regressoren in <span class="in">`boston_train`</span> für Splits nutzt, wobei <span class="in">`lstat`</span> und <span class="in">`rm`</span> die relevantesten Variablen sind.</span>
<span id="cb36-372"><a href="#cb36-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-373"><a href="#cb36-373" aria-hidden="true" tabindex="-1"></a>Anhand von Vorhersagen für <span class="in">`medv`</span> mit dem Test-Datensatz <span class="in">`boston_test`</span> können wir das naive Baum-Modell <span class="in">`tree_fit`</span> mit dem durch CV ermittelten Modell <span class="in">`tree_fit_cv`</span> hinsichtlich des Vorhersagefehlers für ungesehene Beobachtungen vergleich. <span class="in">`yardstick::metric()`</span> berechnet hierzu automatisch gängige Statistiken für Regressionsprobleme. </span>
<span id="cb36-374"><a href="#cb36-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-377"><a href="#cb36-377" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb36-378"><a href="#cb36-378" aria-hidden="true" tabindex="-1"></a><span class="in"># Vorhersagegüte naives Modell</span></span>
<span id="cb36-379"><a href="#cb36-379" aria-hidden="true" tabindex="-1"></a><span class="in">tree_pred &lt;- predict(</span></span>
<span id="cb36-380"><a href="#cb36-380" aria-hidden="true" tabindex="-1"></a><span class="in">  object = tree_fit, </span></span>
<span id="cb36-381"><a href="#cb36-381" aria-hidden="true" tabindex="-1"></a><span class="in">  new_data = Boston_test</span></span>
<span id="cb36-382"><a href="#cb36-382" aria-hidden="true" tabindex="-1"></a><span class="in">) %&gt;%</span></span>
<span id="cb36-383"><a href="#cb36-383" aria-hidden="true" tabindex="-1"></a><span class="in">  bind_cols(Boston_test) %&gt;%</span></span>
<span id="cb36-384"><a href="#cb36-384" aria-hidden="true" tabindex="-1"></a><span class="in">  metrics(truth = medv, estimate = .pred)</span></span>
<span id="cb36-385"><a href="#cb36-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-386"><a href="#cb36-386" aria-hidden="true" tabindex="-1"></a><span class="in"># Vorhersagegüte bei CV</span></span>
<span id="cb36-387"><a href="#cb36-387" aria-hidden="true" tabindex="-1"></a><span class="in">tree_pred_cv &lt;- predict(</span></span>
<span id="cb36-388"><a href="#cb36-388" aria-hidden="true" tabindex="-1"></a><span class="in">  object = final_tree_fit, </span></span>
<span id="cb36-389"><a href="#cb36-389" aria-hidden="true" tabindex="-1"></a><span class="in">  new_data = Boston_test</span></span>
<span id="cb36-390"><a href="#cb36-390" aria-hidden="true" tabindex="-1"></a><span class="in">) %&gt;%</span></span>
<span id="cb36-391"><a href="#cb36-391" aria-hidden="true" tabindex="-1"></a><span class="in">  bind_cols(Boston_test) %&gt;%</span></span>
<span id="cb36-392"><a href="#cb36-392" aria-hidden="true" tabindex="-1"></a><span class="in">  metrics(truth = medv, estimate = .pred)</span></span>
<span id="cb36-393"><a href="#cb36-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-394"><a href="#cb36-394" aria-hidden="true" tabindex="-1"></a><span class="in">tree_pred</span></span>
<span id="cb36-395"><a href="#cb36-395" aria-hidden="true" tabindex="-1"></a><span class="in">tree_pred_cv</span></span>
<span id="cb36-396"><a href="#cb36-396" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-397"><a href="#cb36-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-398"><a href="#cb36-398" aria-hidden="true" tabindex="-1"></a>Der Vergleich zeigt eine bessere Vorsageleistung des großen Baums in <span class="in">`tree_fit_cv`</span>. In diesem Fall scheint CP-Pruning wenig hilfreich zu sein. Tatsächlich liefert ein Baum mit $\alpha=0$ bessere Vorhersagen als <span class="in">`tree_fit_cv`</span> (überprüfe dies!). </span>
<span id="cb36-399"><a href="#cb36-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-400"><a href="#cb36-400" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bagging</span></span>
<span id="cb36-401"><a href="#cb36-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-402"><a href="#cb36-402" aria-hidden="true" tabindex="-1"></a>*Bagging* ist eine Ensemble-Modelle, die durch aus einer Kombination von vielen Entscheidungsbäumen bestehen. Bagging steht für *Bootstrap Aggregating* und nutzt einen Algorithmus, bei dem Bäume auf *zufälligen* Stichproben aus dem Trainingsdatensatz angepasst werden: Jeder Baum wird auf einer *Bootstrap-Stichprobe* (siehe @sec-sim) trainiert, die durch zufällige Züge (mit Zurücklegen) erstellt wird. Nach dem Training aggregiert Bagging die Vorhersagen aller Bäume des Ensembles.</span>
<span id="cb36-403"><a href="#cb36-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-404"><a href="#cb36-404" aria-hidden="true" tabindex="-1"></a>Der Vorteil von Bagging gegenüber einem einzelnen Entscheidungsbaum ist, dass die Varianz der Vorhersage deutlich reduziert werden kann: Einzelne Entscheidungsbäume neigen dazu, Muster in den Trainingsdaten zu lernen, die sich zufällig aus der Zusammensetzung der Stichprobe ergeben und nicht repräsentativ für Zusammenhänge zwischen den Regressoren und der Outcome-Variable sind. Diese Überanpassung führt zu hoher Varianz auf von Vorhersagen für ungesehene Daten. Durch das Training vieler Bäume auf unterschiedlichen *zufälligen* Stichproben aus den Trainingsdaten und das anschließende Aggregieren kann der negative Effekt der Überanpassung auf die Unsicherheit der Vorhersage einzelner Bäume reduziert werden.</span>
<span id="cb36-405"><a href="#cb36-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-406"><a href="#cb36-406" aria-hidden="true" tabindex="-1"></a>Eine Bagging-Spezifikation kann mit <span class="in">`parsnip::bag_tree()`</span> festgelegt werden. Mit <span class="in">`times = 500`</span> wird definiert, dass der Bagging-Algorithmus ein Ensemble mit 500 Bäumen (mit CART) anpassen soll. Das Training und die Vorhersage auf den Testdaten erfolgt analog zur Vorgehensweise in @sec-simpletrees.</span>
<span id="cb36-407"><a href="#cb36-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-410"><a href="#cb36-410" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb36-411"><a href="#cb36-411" aria-hidden="true" tabindex="-1"></a><span class="in"># Spezifikation für Bagging</span></span>
<span id="cb36-412"><a href="#cb36-412" aria-hidden="true" tabindex="-1"></a><span class="in">bagging_spec &lt;- bag_tree() %&gt;%</span></span>
<span id="cb36-413"><a href="#cb36-413" aria-hidden="true" tabindex="-1"></a><span class="in">  set_engine(</span></span>
<span id="cb36-414"><a href="#cb36-414" aria-hidden="true" tabindex="-1"></a><span class="in">    engine = "rpart",</span></span>
<span id="cb36-415"><a href="#cb36-415" aria-hidden="true" tabindex="-1"></a><span class="in">    times = 500</span></span>
<span id="cb36-416"><a href="#cb36-416" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb36-417"><a href="#cb36-417" aria-hidden="true" tabindex="-1"></a><span class="in">  set_mode("regression")</span></span>
<span id="cb36-418"><a href="#cb36-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-419"><a href="#cb36-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-420"><a href="#cb36-420" aria-hidden="true" tabindex="-1"></a><span class="in"># Training durchführen</span></span>
<span id="cb36-421"><a href="#cb36-421" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(1234)</span></span>
<span id="cb36-422"><a href="#cb36-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-423"><a href="#cb36-423" aria-hidden="true" tabindex="-1"></a><span class="in">bagging_fit &lt;- bagging_spec %&gt;%</span></span>
<span id="cb36-424"><a href="#cb36-424" aria-hidden="true" tabindex="-1"></a><span class="in">  fit(</span></span>
<span id="cb36-425"><a href="#cb36-425" aria-hidden="true" tabindex="-1"></a><span class="in">    formula = medv ~ ., </span></span>
<span id="cb36-426"><a href="#cb36-426" aria-hidden="true" tabindex="-1"></a><span class="in">    data = Boston_train</span></span>
<span id="cb36-427"><a href="#cb36-427" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb36-428"><a href="#cb36-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-429"><a href="#cb36-429" aria-hidden="true" tabindex="-1"></a><span class="in"># Auswertung</span></span>
<span id="cb36-430"><a href="#cb36-430" aria-hidden="true" tabindex="-1"></a><span class="in">bagging_pred &lt;- predict(</span></span>
<span id="cb36-431"><a href="#cb36-431" aria-hidden="true" tabindex="-1"></a><span class="in">  object = bagging_fit, </span></span>
<span id="cb36-432"><a href="#cb36-432" aria-hidden="true" tabindex="-1"></a><span class="in">  new_data = Boston_test</span></span>
<span id="cb36-433"><a href="#cb36-433" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb36-434"><a href="#cb36-434" aria-hidden="true" tabindex="-1"></a><span class="in">  bind_cols(Boston_test) %&gt;%</span></span>
<span id="cb36-435"><a href="#cb36-435" aria-hidden="true" tabindex="-1"></a><span class="in">  metrics(</span></span>
<span id="cb36-436"><a href="#cb36-436" aria-hidden="true" tabindex="-1"></a><span class="in">    truth = medv,</span></span>
<span id="cb36-437"><a href="#cb36-437" aria-hidden="true" tabindex="-1"></a><span class="in">    estimate = .pred</span></span>
<span id="cb36-438"><a href="#cb36-438" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb36-439"><a href="#cb36-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-440"><a href="#cb36-440" aria-hidden="true" tabindex="-1"></a><span class="in">bagging_pred</span></span>
<span id="cb36-441"><a href="#cb36-441" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-442"><a href="#cb36-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-443"><a href="#cb36-443" aria-hidden="true" tabindex="-1"></a>Die Auswertung auf den Testdatensatz ergibt eine deutliche Verbesserung der Vorhersageleistung gegenüber einem einfachen Regressionsbaum.</span>
<span id="cb36-444"><a href="#cb36-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-445"><a href="#cb36-445" aria-hidden="true" tabindex="-1"></a>Obwohl die Bäume beim Bagging auf unterschiedlichen Stichproben trainiert werden, kann innerhalb des Ensembles dennoch eine deutliche Korrelation vorliegen: Da jeder Baum auf alle Regressoren für Splits zugreift, können trotz Bootstrapping ähnliche (unverteilhafte) Muster aus dem Datensatz erlernt werden, was sich nachteilig auf die Generalisierungsfähigkeit auswirken kann. Diese Korrelation mindert die Effektivität von Bagging, da stark korrelierte Bäume dazu neigen, ähnliche Fehler zu machen.</span>
<span id="cb36-446"><a href="#cb36-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-447"><a href="#cb36-447" aria-hidden="true" tabindex="-1"></a><span class="fu">## Random Forests {#sec-brf}</span></span>
<span id="cb36-448"><a href="#cb36-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-449"><a href="#cb36-449" aria-hidden="true" tabindex="-1"></a>*Random Forests* erweitern Bagging, indem zusätzlich bei jedem Knoten innerhalb jedes Baumes eine *zufällige Teilmenge der Regressoren* als potentielle Variable für die Split-Regel ausgewählt wird. Dies führt zu einer Reduktion der Korrelation zwischen den Bäumen, was die Genauigkeit verbessert und das Risiko von Overfitting weiter verringert.</span>
<span id="cb36-450"><a href="#cb36-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-451"><a href="#cb36-451" aria-hidden="true" tabindex="-1"></a>In R erstellen wir die Spezifikation mit <span class="in">`parsnip::rand_forest()`</span>. Der Parameter <span class="in">`mtry`</span> legt fest, wie viele Regressoren $m$ zufällig für jeden Split zur Verfügung stehen. Wir nutzen den im <span class="in">`randomForest`</span>-Paket implementierten Algorithmus und legen in <span class="in">`set_engine()`</span> fest, dass die von <span class="in">`randomForest::randomForest()`</span> berechnete Fehler-Metrik im Output-Objekt ausgegeben wird (<span class="in">`tree.err = TRUE`</span>). Um die Spezifikation für verschiedene Werte von <span class="in">`mtry`</span> anwenden zu können, implementieren wir die Spezifikation innerhalb einer Wrapper-Funktion <span class="in">`rf_spec_mtry()`</span>. Mit <span class="in">`purrr::map()`</span> iterieren wir <span class="in">`rf_spec_mtry()`</span> über drei verschiedene Werte für den Tuning-Parameter <span class="in">`mtry`</span> (4, 6 und 10 Variablen).^<span class="co">[</span><span class="ot">Eine Faustregel für die Wahl von $m$ bei $k$ verfügbaren Regressoren ist $m\approx\sqrt{k}$.</span><span class="co">]</span></span>
<span id="cb36-452"><a href="#cb36-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-455"><a href="#cb36-455" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb36-456"><a href="#cb36-456" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(1234)</span></span>
<span id="cb36-457"><a href="#cb36-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-458"><a href="#cb36-458" aria-hidden="true" tabindex="-1"></a><span class="in"># Werte für mtry</span></span>
<span id="cb36-459"><a href="#cb36-459" aria-hidden="true" tabindex="-1"></a><span class="in">mtry_values &lt;- c(4, 6, 10)</span></span>
<span id="cb36-460"><a href="#cb36-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-461"><a href="#cb36-461" aria-hidden="true" tabindex="-1"></a><span class="in"># Funktion: Random Forest für mtry = m</span></span>
<span id="cb36-462"><a href="#cb36-462" aria-hidden="true" tabindex="-1"></a><span class="in">rf_spec_mtry &lt;- function(m) {</span></span>
<span id="cb36-463"><a href="#cb36-463" aria-hidden="true" tabindex="-1"></a><span class="in">  rand_forest(mtry = m, trees = 500) %&gt;%</span></span>
<span id="cb36-464"><a href="#cb36-464" aria-hidden="true" tabindex="-1"></a><span class="in">    set_engine(</span></span>
<span id="cb36-465"><a href="#cb36-465" aria-hidden="true" tabindex="-1"></a><span class="in">      engine = "randomForest", </span></span>
<span id="cb36-466"><a href="#cb36-466" aria-hidden="true" tabindex="-1"></a><span class="in">      tree.err = TRUE</span></span>
<span id="cb36-467"><a href="#cb36-467" aria-hidden="true" tabindex="-1"></a><span class="in">    ) %&gt;%</span></span>
<span id="cb36-468"><a href="#cb36-468" aria-hidden="true" tabindex="-1"></a><span class="in">    set_mode("regression")</span></span>
<span id="cb36-469"><a href="#cb36-469" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb36-470"><a href="#cb36-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-471"><a href="#cb36-471" aria-hidden="true" tabindex="-1"></a><span class="in"># Modelle für verschiedene mtry-Werte trainieren</span></span>
<span id="cb36-472"><a href="#cb36-472" aria-hidden="true" tabindex="-1"></a><span class="in">rf_fits &lt;- map(</span></span>
<span id="cb36-473"><a href="#cb36-473" aria-hidden="true" tabindex="-1"></a><span class="in">  .x = mtry_values, </span></span>
<span id="cb36-474"><a href="#cb36-474" aria-hidden="true" tabindex="-1"></a><span class="in">  .f = ~ rf_spec_mtry(.x) %&gt;%</span></span>
<span id="cb36-475"><a href="#cb36-475" aria-hidden="true" tabindex="-1"></a><span class="in">    fit(</span></span>
<span id="cb36-476"><a href="#cb36-476" aria-hidden="true" tabindex="-1"></a><span class="in">      formula = medv ~ ., </span></span>
<span id="cb36-477"><a href="#cb36-477" aria-hidden="true" tabindex="-1"></a><span class="in">      data = Boston_train</span></span>
<span id="cb36-478"><a href="#cb36-478" aria-hidden="true" tabindex="-1"></a><span class="in">    )</span></span>
<span id="cb36-479"><a href="#cb36-479" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb36-480"><a href="#cb36-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-481"><a href="#cb36-481" aria-hidden="true" tabindex="-1"></a><span class="in">rf_fits &lt;- set_names(</span></span>
<span id="cb36-482"><a href="#cb36-482" aria-hidden="true" tabindex="-1"></a><span class="in">  x = rf_fits,</span></span>
<span id="cb36-483"><a href="#cb36-483" aria-hidden="true" tabindex="-1"></a><span class="in">  nm =  paste0("rf_mtry", mtry_values, "_fit")</span></span>
<span id="cb36-484"><a href="#cb36-484" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb36-485"><a href="#cb36-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-486"><a href="#cb36-486" aria-hidden="true" tabindex="-1"></a><span class="in"># Ausgabe der Ergebnisse</span></span>
<span id="cb36-487"><a href="#cb36-487" aria-hidden="true" tabindex="-1"></a><span class="in">rf_fits</span></span>
<span id="cb36-488"><a href="#cb36-488" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-489"><a href="#cb36-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-490"><a href="#cb36-490" aria-hidden="true" tabindex="-1"></a>Für eine Beurteilung des Vorhersageleistung dieser drei Modelle können wir den *Out-of-Bag*-Fehler (OOB) verwenden: </span>
<span id="cb36-491"><a href="#cb36-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-492"><a href="#cb36-492" aria-hidden="true" tabindex="-1"></a>Der OOB-Fehler ist eine Schätzung des Generalisierungsfehlers ohne einen separaten Testdatensatzes. Bei Random Forests (und Bagging) ist dies aufgrund der Berechnung des Ensembles für Bootstrap-Stichproben möglich: Grob ein Drittel der Beobachtungen des Datensatzes sind nicht Teil der Stichprobe, die für das Training jedes Baums im Ensemble genereiert werden.^<span class="co">[</span><span class="ot">Beachte, dass beim Bootstrap $n$ aus $n$  Beobachtungen mit Zurücklegen gezogen werden. Die Wahrscheinlicht, dass eine Beobachtung *nicht* gezogen wird ("Out-of-Bag"), ist $(1-1/n)^n\approx37\%$.</span><span class="co">]</span> Diese nicht gezogenen Datenpunkte sind OOB-Beobachtungen. Der OOB-Fehler des Ensembles ist der durchschnittliche Fehler für die aggregierten Vorhersagen der Bäume des Forests.</span>
<span id="cb36-493"><a href="#cb36-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-494"><a href="#cb36-494" aria-hidden="true" tabindex="-1"></a>Der OOB-Fehler kann auch verwendet werden, um die erforderliche Größe des Random Forests zu beurteilen: Eine größere Anzahl von Bäumen reduziert tendenziell die Varianz der Vorhersagen und verbessert die Generalisierungsfähigkeit. Allerdings nimmt dieser Effekt ab, und ab einer bestimmten Baumanzahl sind weitere Verbesserungen marginal. Obwohl das Risiko von Überanpassung durch viele Bäume aufgrund des Bagging minimal ist, kann es bei großen Datensätzen sinnvoll sein, kleinere Wälder zu trainieren, um den Rechenaufwand zu verringern. Wir plotten hierfür den OOB-Fehler für das Modell mit <span class="in">`mtry = 10`</span> gegen die Anzahl der Bäume.</span>
<span id="cb36-495"><a href="#cb36-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-498"><a href="#cb36-498" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb36-499"><a href="#cb36-499" aria-hidden="true" tabindex="-1"></a><span class="in">library(ggRandomForests)</span></span>
<span id="cb36-500"><a href="#cb36-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-501"><a href="#cb36-501" aria-hidden="true" tabindex="-1"></a><span class="in"># OOB-Fehler als Funktion der Baumanzahl</span></span>
<span id="cb36-502"><a href="#cb36-502" aria-hidden="true" tabindex="-1"></a><span class="in">rf_fits$rf_mtry10_fit$fit %&gt;% </span></span>
<span id="cb36-503"><a href="#cb36-503" aria-hidden="true" tabindex="-1"></a><span class="in">  gg_error() %&gt;% </span></span>
<span id="cb36-504"><a href="#cb36-504" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb36-505"><a href="#cb36-505" aria-hidden="true" tabindex="-1"></a><span class="in">  plot() + </span></span>
<span id="cb36-506"><a href="#cb36-506" aria-hidden="true" tabindex="-1"></a><span class="in">  labs(</span></span>
<span id="cb36-507"><a href="#cb36-507" aria-hidden="true" tabindex="-1"></a><span class="in">    title = "Random Forest: Ensemblegröße vs. OOB-Fehler (mtry = 10)"</span></span>
<span id="cb36-508"><a href="#cb36-508" aria-hidden="true" tabindex="-1"></a><span class="in">  ) + </span></span>
<span id="cb36-509"><a href="#cb36-509" aria-hidden="true" tabindex="-1"></a><span class="in">  theme_cowplot()</span></span>
<span id="cb36-510"><a href="#cb36-510" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-511"><a href="#cb36-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-512"><a href="#cb36-512" aria-hidden="true" tabindex="-1"></a>Die Grafik zeigt, dass die Verbesserung des OOB-Fehlers jenseits von 250 Beobachtungen deutlich nachlässt, sodass ein Training von 500 Bäumen ausreichend scheint.</span>
<span id="cb36-513"><a href="#cb36-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-514"><a href="#cb36-514" aria-hidden="true" tabindex="-1"></a>Zur Beurteiliung der Vorhersagegüte mit dem Testdatensatz gehen wir analog zum Training vor und iterieren mit <span class="in">`map()`</span> über <span class="in">`rf_fits`</span>, die Liste der angepassten Modelle.</span>
<span id="cb36-515"><a href="#cb36-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-518"><a href="#cb36-518" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb36-519"><a href="#cb36-519" aria-hidden="true" tabindex="-1"></a><span class="in"># Vorhersage und Berechnung v. Metriken für jeden RF</span></span>
<span id="cb36-520"><a href="#cb36-520" aria-hidden="true" tabindex="-1"></a><span class="in">rf_predictions &lt;- map(</span></span>
<span id="cb36-521"><a href="#cb36-521" aria-hidden="true" tabindex="-1"></a><span class="in">  .x = rf_fits, </span></span>
<span id="cb36-522"><a href="#cb36-522" aria-hidden="true" tabindex="-1"></a><span class="in">  .f =  ~ predict(.x, Boston_test) %&gt;%</span></span>
<span id="cb36-523"><a href="#cb36-523" aria-hidden="true" tabindex="-1"></a><span class="in">    bind_cols(Boston_test) %&gt;%</span></span>
<span id="cb36-524"><a href="#cb36-524" aria-hidden="true" tabindex="-1"></a><span class="in">    metrics(</span></span>
<span id="cb36-525"><a href="#cb36-525" aria-hidden="true" tabindex="-1"></a><span class="in">      truth = medv, </span></span>
<span id="cb36-526"><a href="#cb36-526" aria-hidden="true" tabindex="-1"></a><span class="in">      estimate = .pred</span></span>
<span id="cb36-527"><a href="#cb36-527" aria-hidden="true" tabindex="-1"></a><span class="in">    )</span></span>
<span id="cb36-528"><a href="#cb36-528" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb36-529"><a href="#cb36-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-530"><a href="#cb36-530" aria-hidden="true" tabindex="-1"></a><span class="in"># Einträge benennen</span></span>
<span id="cb36-531"><a href="#cb36-531" aria-hidden="true" tabindex="-1"></a><span class="in">rf_predictions &lt;- set_names(</span></span>
<span id="cb36-532"><a href="#cb36-532" aria-hidden="true" tabindex="-1"></a><span class="in">  x = rf_predictions,</span></span>
<span id="cb36-533"><a href="#cb36-533" aria-hidden="true" tabindex="-1"></a><span class="in">  nm =  paste0("rf_mtry", mtry_values, "_pred")</span></span>
<span id="cb36-534"><a href="#cb36-534" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb36-535"><a href="#cb36-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-536"><a href="#cb36-536" aria-hidden="true" tabindex="-1"></a><span class="in">rf_predictions</span></span>
<span id="cb36-537"><a href="#cb36-537" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-538"><a href="#cb36-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-539"><a href="#cb36-539" aria-hidden="true" tabindex="-1"></a>Ähnlich wie für einen einzelnen Baum kann die Relevanz von Variablen anhand der Reduktion der Loss-Funktion durch das Ensemble beurteilt werden. Für einen einfachen Vergleich der Variable Importance für den Random Forests mit <span class="in">`mtry = 10`</span> in <span class="in">`rf_fits$rf_mtry10_fit`</span> nutzen wir <span class="in">`ggRandomForests::gg_vimp()`</span>.</span>
<span id="cb36-540"><a href="#cb36-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-543"><a href="#cb36-543" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb36-544"><a href="#cb36-544" aria-hidden="true" tabindex="-1"></a><span class="in"># Variable importance für mtry = 10</span></span>
<span id="cb36-545"><a href="#cb36-545" aria-hidden="true" tabindex="-1"></a><span class="in">rf_fits$rf_mtry10_fit$fit %&gt;%</span></span>
<span id="cb36-546"><a href="#cb36-546" aria-hidden="true" tabindex="-1"></a><span class="in">  gg_vimp()  %&gt;%</span></span>
<span id="cb36-547"><a href="#cb36-547" aria-hidden="true" tabindex="-1"></a><span class="in">  plot() +</span></span>
<span id="cb36-548"><a href="#cb36-548" aria-hidden="true" tabindex="-1"></a><span class="in">    labs(</span></span>
<span id="cb36-549"><a href="#cb36-549" aria-hidden="true" tabindex="-1"></a><span class="in">      title = "Variable Importance für Random Forest (mtry = 10)"</span></span>
<span id="cb36-550"><a href="#cb36-550" aria-hidden="true" tabindex="-1"></a><span class="in">    ) +</span></span>
<span id="cb36-551"><a href="#cb36-551" aria-hidden="true" tabindex="-1"></a><span class="in">    theme_cowplot()</span></span>
<span id="cb36-552"><a href="#cb36-552" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-553"><a href="#cb36-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-554"><a href="#cb36-554" aria-hidden="true" tabindex="-1"></a>Die Grafik bestärkt unsere Schlussfolgerung aus der Analyse des (mit CART trainierten) einzelnen Entscheidungsbaums in @sec-simpletrees, dass <span class="in">`rm`</span> und <span class="in">`lstat`</span> die wichtigsten Regressoren für die Vorhersage von <span class="in">`medv`</span> sind.</span>
<span id="cb36-555"><a href="#cb36-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-556"><a href="#cb36-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-557"><a href="#cb36-557" aria-hidden="true" tabindex="-1"></a><span class="fu">## Boosting {#sec-boosting}</span></span>
<span id="cb36-558"><a href="#cb36-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-559"><a href="#cb36-559" aria-hidden="true" tabindex="-1"></a>Boosting ist eine leistungsstarke Ensemble-Methode für Vorhersagen, die kleine Modelle (oft Entscheidungsbäume geringer Tiefe) sukzessiv trainiert und zu einem starken Modell kombiniert. Anders als bei Random Forests, bei denen viele Bäume unabhängig voneinander auf zufälligen Stichproben der Daten trainiert werden, geht ein Boosting-Algorithmuss sequentiell vor: Jeder nachfolgende Baum wird darauf optimiert, die Fehler des vorherigen Modells zu reduzieren. Die Idee hierbei ist es, iterativ "schwache" Modelle zu erzeugen, die eine gute Anpassung für Datenpunkte liefern, die in den vorherigen Durchläufen schlecht vorhergesagt wurden.</span>
<span id="cb36-560"><a href="#cb36-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-561"><a href="#cb36-561" aria-hidden="true" tabindex="-1"></a>Für einen Trainingsdatensatz $<span class="sc">\{</span>(x_i, y_i)<span class="sc">\}</span>_{i=1}^n$, wobei $x_i$ die Input-Features und $y_i$ Beobachtungen des Outcomes sind, kann Boosting wiefolgt durchgeführt werden.</span>
<span id="cb36-562"><a href="#cb36-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-563"><a href="#cb36-563" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb36-564"><a href="#cb36-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-565"><a href="#cb36-565" aria-hidden="true" tabindex="-1"></a><span class="fu">## Boosting für Regression</span></span>
<span id="cb36-566"><a href="#cb36-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-567"><a href="#cb36-567" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Initialisierung**: Initialisiere das Boosting-Modell als $\widehat{F}_0(x)$. Setze die Residuen $r^0_i=y_i$ für alle $i$</span>
<span id="cb36-568"><a href="#cb36-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-569"><a href="#cb36-569" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Iteration**: Wiederhole die folgenden Schritte für $b = 1,2,\dots,B$ mit $B$ hinreichend groß:</span>
<span id="cb36-570"><a href="#cb36-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-571"><a href="#cb36-571" aria-hidden="true" tabindex="-1"></a>    2.1 **Base Learner**: Trainiere Baum $T_b$ mit $\{(\boldsymbol{x}_i, r^{b-1}_i)\}_{i=1}^n$ für die Vorhersage des *Fehlers* der vorherigen Iteration $r^{b-1}$.</span>
<span id="cb36-572"><a href="#cb36-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-573"><a href="#cb36-573" aria-hidden="true" tabindex="-1"></a>    2.2 **Aktualisierung**: Aktualisiere das Boosting-Modell,</span>
<span id="cb36-574"><a href="#cb36-574" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-575"><a href="#cb36-575" aria-hidden="true" tabindex="-1"></a>      \begin{align*}</span>
<span id="cb36-576"><a href="#cb36-576" aria-hidden="true" tabindex="-1"></a>      \widehat{F}_{b}(\boldsymbol{x}) = \widehat{F}_{b-1}(\boldsymbol{x}) + \eta \cdot T_{b}(\boldsymbol{x}),</span>
<span id="cb36-577"><a href="#cb36-577" aria-hidden="true" tabindex="-1"></a>      \end{align*}</span>
<span id="cb36-578"><a href="#cb36-578" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb36-579"><a href="#cb36-579" aria-hidden="true" tabindex="-1"></a>      wobei $\eta$ die (oft klein gewählte) *Lernrate* ist.</span>
<span id="cb36-580"><a href="#cb36-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-581"><a href="#cb36-581" aria-hidden="true" tabindex="-1"></a>    2.3 **Fehlerberechnung**: Berechne die Residuen $r^b_i$ als Differenzen zwischen dem tatsächlichen Werten $y_i$ und den Vorhersage des aktuellen Modells $\widehat{F}_m(\boldsymbol{x}_i)$,</span>
<span id="cb36-582"><a href="#cb36-582" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-583"><a href="#cb36-583" aria-hidden="true" tabindex="-1"></a>      \begin{align*}</span>
<span id="cb36-584"><a href="#cb36-584" aria-hidden="true" tabindex="-1"></a>      r^b_i = y_i - \widehat{F}_b(\boldsymbol{x}_i).</span>
<span id="cb36-585"><a href="#cb36-585" aria-hidden="true" tabindex="-1"></a>      \end{align*}</span>
<span id="cb36-586"><a href="#cb36-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-587"><a href="#cb36-587" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Output**: Gib das finale Modell aus:</span>
<span id="cb36-588"><a href="#cb36-588" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb36-589"><a href="#cb36-589" aria-hidden="true" tabindex="-1"></a>    \begin{align*}</span>
<span id="cb36-590"><a href="#cb36-590" aria-hidden="true" tabindex="-1"></a>      \widehat{F}(\boldsymbol{x}) := \sum_{b=1}^B \eta\cdot \widehat{F}^b(\boldsymbol{x})</span>
<span id="cb36-591"><a href="#cb36-591" aria-hidden="true" tabindex="-1"></a>    \end{align*}</span>
<span id="cb36-592"><a href="#cb36-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-593"><a href="#cb36-593" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb36-594"><a href="#cb36-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-595"><a href="#cb36-595" aria-hidden="true" tabindex="-1"></a>Der Parameter $0\leq\eta\leq0$ steuert, wie stark der Einfluss jedes neuen Baumes auf das Modell ist. Eine kleine Lernrate führt dazu, dass viele Bäume benötigt werden, was Vorhersagen (ähnlich wie bei Bagging) stabiler macht. Beachte die sequentielle Natur des Trainings: Die $r^b_i$ in Schritt 2.3 sind die zu vorhersagenden Outcome-Variable für den nächsten Baum. $T_{b+1}$ wird trainiert wird, um den *Fehler des bisherigen Modells* $\widehat{F}_b$ zu erklären.</span>
<span id="cb36-596"><a href="#cb36-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-597"><a href="#cb36-597" aria-hidden="true" tabindex="-1"></a>Für die Anwendung auf <span class="in">`MASS::Boston`</span> in R nutzen wir den im Paket <span class="in">`gbm`</span> implementierten *Gradient-Boosting*-Algorithmus. Bei Gradient Boosting wird jeder Baum so trainiert, dass er den negativen Gradienten einer Verlustfunktion approximiert, also die Richtung des größten Fehlers. Das Modell wird schrittweise verbessert, indem es entlang des Gradienten aktualisiert wird, um die Vorhersagegüe zu optimieren; siehe @Hastieetal2013 für eine detaillierte Erläuterung.</span>
<span id="cb36-598"><a href="#cb36-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-599"><a href="#cb36-599" aria-hidden="true" tabindex="-1"></a>Mit dem nachfolgenden Code-Chunk trainieren wir ein Boosting-Modell für Regression mit 5000 einfachen Bäumen (<span class="in">`n.trees = 5000`</span>) mit einer maximalen Tiefe von 2 (<span class="in">`interaction.depth = 2`</span>), d.h. es folgen maximal 2 Entscheidungs-Regeln nacheinander. Um das Risiko von Overfitting gering zu halten, erlauben wir nur Splits, die zu mindestens zwei Beobachtungen in resultierenden nodes führen (<span class="in">`n.minobsinnode = 2`</span>). Die Lernrate (Beitrag der Base Learner zum Ensemble) wird typischerweise klein (und in Abhängigkeit von <span class="in">`n.trees`</span>) gewählt (<span class="in">`shrinkage = 0.001`</span>).^<span class="co">[</span><span class="ot">Je kleiner die Lernrate, desto größer sollte `n.trees` gewählt werden.</span><span class="co">]</span></span>
<span id="cb36-600"><a href="#cb36-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-603"><a href="#cb36-603" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb36-604"><a href="#cb36-604" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(1234)</span></span>
<span id="cb36-605"><a href="#cb36-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-606"><a href="#cb36-606" aria-hidden="true" tabindex="-1"></a><span class="in"># Gradient Boosting durchführen</span></span>
<span id="cb36-607"><a href="#cb36-607" aria-hidden="true" tabindex="-1"></a><span class="in">gbm_model &lt;- gbm(</span></span>
<span id="cb36-608"><a href="#cb36-608" aria-hidden="true" tabindex="-1"></a><span class="in">  formula = medv ~ ., </span></span>
<span id="cb36-609"><a href="#cb36-609" aria-hidden="true" tabindex="-1"></a><span class="in">  data = Boston_train, </span></span>
<span id="cb36-610"><a href="#cb36-610" aria-hidden="true" tabindex="-1"></a><span class="in">  distribution = "gaussian", # für Regression</span></span>
<span id="cb36-611"><a href="#cb36-611" aria-hidden="true" tabindex="-1"></a><span class="in">  n.trees = 5000,           # Anz. Bäume</span></span>
<span id="cb36-612"><a href="#cb36-612" aria-hidden="true" tabindex="-1"></a><span class="in">  interaction.depth = 2,     # Maximale Tiefe der base learner</span></span>
<span id="cb36-613"><a href="#cb36-613" aria-hidden="true" tabindex="-1"></a><span class="in">  shrinkage = 0.01,         # Lernrate</span></span>
<span id="cb36-614"><a href="#cb36-614" aria-hidden="true" tabindex="-1"></a><span class="in">  n.minobsinnode = 2         # Min. Beobachtungen in nodes</span></span>
<span id="cb36-615"><a href="#cb36-615" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb36-616"><a href="#cb36-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-617"><a href="#cb36-617" aria-hidden="true" tabindex="-1"></a><span class="in">gbm_model </span></span>
<span id="cb36-618"><a href="#cb36-618" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-619"><a href="#cb36-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-620"><a href="#cb36-620" aria-hidden="true" tabindex="-1"></a>Für die Vorhersagen auf dem Test-Datensatz legen wir mit <span class="in">`n.trees = gbm_model$n.trees`</span> fest, dass das gesamte Ensemble genutzt werden soll.</span>
<span id="cb36-621"><a href="#cb36-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-624"><a href="#cb36-624" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb36-625"><a href="#cb36-625" aria-hidden="true" tabindex="-1"></a><span class="in"># Vorhersagen Test-Datensatz</span></span>
<span id="cb36-626"><a href="#cb36-626" aria-hidden="true" tabindex="-1"></a><span class="in">gbm_predictions &lt;- predict(</span></span>
<span id="cb36-627"><a href="#cb36-627" aria-hidden="true" tabindex="-1"></a><span class="in">  object = gbm_model, </span></span>
<span id="cb36-628"><a href="#cb36-628" aria-hidden="true" tabindex="-1"></a><span class="in">  newdata = Boston_test, </span></span>
<span id="cb36-629"><a href="#cb36-629" aria-hidden="true" tabindex="-1"></a><span class="in">  n.trees = gbm_model$n.trees # gesamtes Ensemble nutzen</span></span>
<span id="cb36-630"><a href="#cb36-630" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb36-631"><a href="#cb36-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-632"><a href="#cb36-632" aria-hidden="true" tabindex="-1"></a><span class="in"># Auswertung Test-Datensatz</span></span>
<span id="cb36-633"><a href="#cb36-633" aria-hidden="true" tabindex="-1"></a><span class="in">results &lt;- Boston_test %&gt;%</span></span>
<span id="cb36-634"><a href="#cb36-634" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(predictions = gbm_predictions) %&gt;%</span></span>
<span id="cb36-635"><a href="#cb36-635" aria-hidden="true" tabindex="-1"></a><span class="in">  metrics(</span></span>
<span id="cb36-636"><a href="#cb36-636" aria-hidden="true" tabindex="-1"></a><span class="in">    truth = medv, </span></span>
<span id="cb36-637"><a href="#cb36-637" aria-hidden="true" tabindex="-1"></a><span class="in">    estimate = predictions</span></span>
<span id="cb36-638"><a href="#cb36-638" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb36-639"><a href="#cb36-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-640"><a href="#cb36-640" aria-hidden="true" tabindex="-1"></a><span class="in">results</span></span>
<span id="cb36-641"><a href="#cb36-641" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-642"><a href="#cb36-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-643"><a href="#cb36-643" aria-hidden="true" tabindex="-1"></a>Die Ergebnisse zeigen, dass Gradient Boosting bereits für die naive Parameterwahl im Aufruf von <span class="in">`gbm::gbm()`</span> zu einer Verbesserung der Vorhersageleistung gegenüber den Random-Forest-Modellen führt.</span>
<span id="cb36-644"><a href="#cb36-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-645"><a href="#cb36-645" aria-hidden="true" tabindex="-1"></a>Anstatt <span class="in">`n.trees = 5000`</span> können wir <span class="in">`n.trees`</span> in <span class="in">`predict()`</span> einen Vektor mit verschiedenen Ensemble-Größen übergeben. Für <span class="in">`n.trees = 5000`</span> erhalten wir Vorhersagen für jeden Status, den das Boosting-Modell im Training nach seiner Initialisierung bis zu der in <span class="in">`gbm::gbm()`</span> festgelgten Größe durchläuft. Anhand dieser Vorhersagen können wir die Generalisierungsfähigkeit des Modells in Abhängigkeit der gewählten Lernrate und der Größe beurteilen, in dem wir den RMSE für den gesamten Trainingsprozess berechnen. Für eine leichtere Interpretation erzeugen wir eine Grafik ählich wie bei der OOB-Analyse des Random-Forest-Modells.</span>
<span id="cb36-646"><a href="#cb36-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-649"><a href="#cb36-649" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb36-650"><a href="#cb36-650" aria-hidden="true" tabindex="-1"></a><span class="in"># Vorhersagen sukzessiv treffen</span></span>
<span id="cb36-651"><a href="#cb36-651" aria-hidden="true" tabindex="-1"></a><span class="in">predict(</span></span>
<span id="cb36-652"><a href="#cb36-652" aria-hidden="true" tabindex="-1"></a><span class="in">    object = gbm_model, </span></span>
<span id="cb36-653"><a href="#cb36-653" aria-hidden="true" tabindex="-1"></a><span class="in">    newdata = Boston_test, </span></span>
<span id="cb36-654"><a href="#cb36-654" aria-hidden="true" tabindex="-1"></a><span class="in">    n.trees = 1:5000</span></span>
<span id="cb36-655"><a href="#cb36-655" aria-hidden="true" tabindex="-1"></a><span class="in">) %&gt;%</span></span>
<span id="cb36-656"><a href="#cb36-656" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb36-657"><a href="#cb36-657" aria-hidden="true" tabindex="-1"></a><span class="in">   # Testset-RMSE berechnen</span></span>
<span id="cb36-658"><a href="#cb36-658" aria-hidden="true" tabindex="-1"></a><span class="in">    as_tibble() %&gt;%</span></span>
<span id="cb36-659"><a href="#cb36-659" aria-hidden="true" tabindex="-1"></a><span class="in">    map_dbl(</span></span>
<span id="cb36-660"><a href="#cb36-660" aria-hidden="true" tabindex="-1"></a><span class="in">      .f = ~ sqrt(mean((.x - Boston_test$medv)^2))</span></span>
<span id="cb36-661"><a href="#cb36-661" aria-hidden="true" tabindex="-1"></a><span class="in">    ) %&gt;%</span></span>
<span id="cb36-662"><a href="#cb36-662" aria-hidden="true" tabindex="-1"></a><span class="in">    bind_cols(rmse = ., trees = 1:5000) %&gt;%</span></span>
<span id="cb36-663"><a href="#cb36-663" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb36-664"><a href="#cb36-664" aria-hidden="true" tabindex="-1"></a><span class="in">  # Plotten</span></span>
<span id="cb36-665"><a href="#cb36-665" aria-hidden="true" tabindex="-1"></a><span class="in">  ggplot(mapping = aes(x = trees, y = rmse)) +</span></span>
<span id="cb36-666"><a href="#cb36-666" aria-hidden="true" tabindex="-1"></a><span class="in">    geom_line() +</span></span>
<span id="cb36-667"><a href="#cb36-667" aria-hidden="true" tabindex="-1"></a><span class="in">    labs(</span></span>
<span id="cb36-668"><a href="#cb36-668" aria-hidden="true" tabindex="-1"></a><span class="in">      title = "Boosting: Testset-RMSE als Funktion von n.trees"</span></span>
<span id="cb36-669"><a href="#cb36-669" aria-hidden="true" tabindex="-1"></a><span class="in">    ) +</span></span>
<span id="cb36-670"><a href="#cb36-670" aria-hidden="true" tabindex="-1"></a><span class="in">    theme_cowplot()</span></span>
<span id="cb36-671"><a href="#cb36-671" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-672"><a href="#cb36-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-673"><a href="#cb36-673" aria-hidden="true" tabindex="-1"></a>Die Grafik zeigt eine schnelle Verbesserung des Out-of-sample-Fehlers mit der Größe des Ensembles. Für die gewählte Lernrate scheinen 5000 Bäume adäquat zu sein.</span>
<span id="cb36-674"><a href="#cb36-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-675"><a href="#cb36-675" aria-hidden="true" tabindex="-1"></a>Analog zu Bagging und Random Forests können wir die Relevanz der Regressoren in <span class="in">`Boston`</span> für die Vorhersage von <span class="in">`medv`</span> anhand der mit <span class="in">`summary()`</span> berechneten (relativen) Variable Importance für die Anpassung auf den Trainingsdatensatz einschätzen. </span>
<span id="cb36-676"><a href="#cb36-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-679"><a href="#cb36-679" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb36-680"><a href="#cb36-680" aria-hidden="true" tabindex="-1"></a><span class="in"># Variable Importance berechnen</span></span>
<span id="cb36-681"><a href="#cb36-681" aria-hidden="true" tabindex="-1"></a><span class="in">var_importance &lt;- summary(</span></span>
<span id="cb36-682"><a href="#cb36-682" aria-hidden="true" tabindex="-1"></a><span class="in">  object = gbm_model, </span></span>
<span id="cb36-683"><a href="#cb36-683" aria-hidden="true" tabindex="-1"></a><span class="in">  plotit = FALSE # k. graphische Ausgabe</span></span>
<span id="cb36-684"><a href="#cb36-684" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb36-685"><a href="#cb36-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-686"><a href="#cb36-686" aria-hidden="true" tabindex="-1"></a><span class="in"># ... und plotten</span></span>
<span id="cb36-687"><a href="#cb36-687" aria-hidden="true" tabindex="-1"></a><span class="in">var_importance &lt;- var_importance %&gt;%</span></span>
<span id="cb36-688"><a href="#cb36-688" aria-hidden="true" tabindex="-1"></a><span class="in">  as_tibble() %&gt;%</span></span>
<span id="cb36-689"><a href="#cb36-689" aria-hidden="true" tabindex="-1"></a><span class="in">  arrange(</span></span>
<span id="cb36-690"><a href="#cb36-690" aria-hidden="true" tabindex="-1"></a><span class="in">    desc(rel.inf)</span></span>
<span id="cb36-691"><a href="#cb36-691" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb36-692"><a href="#cb36-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-693"><a href="#cb36-693" aria-hidden="true" tabindex="-1"></a><span class="in">ggplot(</span></span>
<span id="cb36-694"><a href="#cb36-694" aria-hidden="true" tabindex="-1"></a><span class="in">  data = var_importance,</span></span>
<span id="cb36-695"><a href="#cb36-695" aria-hidden="true" tabindex="-1"></a><span class="in">  mapping = aes(</span></span>
<span id="cb36-696"><a href="#cb36-696" aria-hidden="true" tabindex="-1"></a><span class="in">    x = reorder(var, rel.inf), </span></span>
<span id="cb36-697"><a href="#cb36-697" aria-hidden="true" tabindex="-1"></a><span class="in">    y = rel.inf</span></span>
<span id="cb36-698"><a href="#cb36-698" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb36-699"><a href="#cb36-699" aria-hidden="true" tabindex="-1"></a><span class="in">) +</span></span>
<span id="cb36-700"><a href="#cb36-700" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_bar(stat = "identity") +</span></span>
<span id="cb36-701"><a href="#cb36-701" aria-hidden="true" tabindex="-1"></a><span class="in">  coord_flip() +</span></span>
<span id="cb36-702"><a href="#cb36-702" aria-hidden="true" tabindex="-1"></a><span class="in">  labs(</span></span>
<span id="cb36-703"><a href="#cb36-703" aria-hidden="true" tabindex="-1"></a><span class="in">    title = "Variable Importance für Gradient Boosting",</span></span>
<span id="cb36-704"><a href="#cb36-704" aria-hidden="true" tabindex="-1"></a><span class="in">    x = "Variable",</span></span>
<span id="cb36-705"><a href="#cb36-705" aria-hidden="true" tabindex="-1"></a><span class="in">    y = "Relativer Einfluss (%)"</span></span>
<span id="cb36-706"><a href="#cb36-706" aria-hidden="true" tabindex="-1"></a><span class="in">  ) +</span></span>
<span id="cb36-707"><a href="#cb36-707" aria-hidden="true" tabindex="-1"></a><span class="in">  theme_cowplot()</span></span>
<span id="cb36-708"><a href="#cb36-708" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-709"><a href="#cb36-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-710"><a href="#cb36-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-711"><a href="#cb36-711" aria-hidden="true" tabindex="-1"></a>Obwohl erneut <span class="in">`lstat`</span> und <span class="in">`rm`</span> als die wichtigsten Prädiktoren gelistet sind, identifiziert Gradient Boosting im Gegensatz zu Bagging und Random Forests <span class="in">`lstat`</span> als die Variable mit der größten Vorhersagekraft für <span class="in">`medv`</span>. </span>
<span id="cb36-712"><a href="#cb36-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-713"><a href="#cb36-713" aria-hidden="true" tabindex="-1"></a><span class="fu">## Causal Trees und Causal Forests</span></span>
<span id="cb36-714"><a href="#cb36-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-715"><a href="#cb36-715" aria-hidden="true" tabindex="-1"></a>Baum-Algorithmen sind vielversprechende Ansätze zur Schätzung kausaler Effekte, insbesondere in Situationen, in denen die Bestimmung heterogener Effekte gewünscht ist: Der Vorteil von Baum-Methoden liegt darin, dass sie nicht-parametrisch sind: Der Regressorraum wird adaptiv in Partitionen unterteilt, um auf Basis dieser Aufteilung differenzierte Vorhersagen für die Zielvariable zu treffen. Diese Eigenschaft kann für Kausalanalysen hilfreich sein, da wir in vielen empirischen Anwendungen die Effekte einer Behandlung nicht nur im Durchschnitt für die betrachtete Population, sondern *differenzierter* schätzen möchten: Ein durchschnittlicher Behandlungseffekt (engl. *average treatment effect*, ATE) kann nicht ausreichend informativ für unsere Forschungsfrage sein, bspw. wenn wir erwarten, dass eine politische Intervention unterschiedliche Auswirkungen auf verschiedene Bevölkerungsgruppen hat. Idealerweise möchten wir $\tau_i$ bestimmen, den individuellen Behandlungseffekt einer Beobachtung $i$. Das fundamentale Problem der Kausalinferenz ist, dass $\tau_i$ nicht ermittelt werden kann (s. u.), sodass wir unser Ziel abschwächen müssen. Statt $\tau_i$ suchen wir einen Behandlungseffekt in Abhängigkeit von beobachtbaren Charakteristiken $\boldsymbol{X}$ für Untergruppen der Population, einen *conditional average treatment effect* (CATE). Im Potential-Outcomes-Framework ist der CATE definiert als</span>
<span id="cb36-716"><a href="#cb36-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-717"><a href="#cb36-717" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb36-718"><a href="#cb36-718" aria-hidden="true" tabindex="-1"></a>  \tau(\boldsymbol{x}) = \textup{E}\big(Y^{(1)} - Y^{(0)}\big\vert \boldsymbol{X} = \boldsymbol{x}\big),</span>
<span id="cb36-719"><a href="#cb36-719" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb36-720"><a href="#cb36-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-721"><a href="#cb36-721" aria-hidden="true" tabindex="-1"></a>wobei $Y^{(1)}$ und $Y^{(0)}$ die potenziellen Outcomes darstellen, wenn eine Behandlung erfolgt bzw. nicht erfolgt. In der Praxis beobachten wir jedoch nur $Y_i = Y_i^{(B_i)}$, wobei $B_i$ der Behandlungsindikator für die Beobachtung $i$ ist, sodass $\tau(\boldsymbol{x}_i)$ nicht direkt beobachtet werden kann. Unter der Annahme, dass nach Kontrolle für (beobachtbare) $\boldsymbol{X}$ die Zuordnung zur Behandlung quasi-zufällig ist (*unconfoundedness*), formal</span>
<span id="cb36-722"><a href="#cb36-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-723"><a href="#cb36-723" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb36-724"><a href="#cb36-724" aria-hidden="true" tabindex="-1"></a>Y_i^{(0)},\,Y_i^{(1)} \perp B_i \vert \boldsymbol{X}_i,</span>
<span id="cb36-725"><a href="#cb36-725" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb36-726"><a href="#cb36-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-727"><a href="#cb36-727" aria-hidden="true" tabindex="-1"></a>kann $\tau(\boldsymbol{x})$ geschätzt werden: Wir können Outcome-Differenzen zwischen behandelten und nicht behandelten Beobachtungen als kausal interpretieren, da unbeobachtete Faktoren die Ergebnisse nicht verzerren.</span>
<span id="cb36-728"><a href="#cb36-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-729"><a href="#cb36-729" aria-hidden="true" tabindex="-1"></a>CART und andere traditionelle Entscheidungsbaum-Algorithmen sind für die Schätzung heterogener Behandlungseffekte jedoch ungeeignet. Dafür gibt es zwei wesentliche Ursachen:</span>
<span id="cb36-730"><a href="#cb36-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-731"><a href="#cb36-731" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Das Splitting-Kriterium**</span>
<span id="cb36-732"><a href="#cb36-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-733"><a href="#cb36-733" aria-hidden="true" tabindex="-1"></a>    Das Splitting-Kriterium des CART-Algorithmus optimiert die Aufteilungen der Beobachtungen in jedem Knoten, um die Genauigkeit von Vorhersagen für die Outcome-Variable $Y$ durch Minimierung der Heterogienität (Klassifikation) oder des MSE (Regression) zu optimieren. Diese Kriterien zielen also darauf ab, die *Homogenität innerhalb der Blätter hinsichtlich* $Y$ zu maximieren.</span>
<span id="cb36-734"><a href="#cb36-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-735"><a href="#cb36-735" aria-hidden="true" tabindex="-1"></a>    Für die Schätzung heterogener kausaler Effekte ist ein solches Splitting jedoch nicht zielführend. Statt Knoten zu formen, in denen $Y$ möglichst homogen ist, benötigen wir für die Schätzung von Behandlungseffekten grundsätzlich Aufteilungen, bei denen sich $Y$ zwischen den behandelten und unbehandelten Individuen innerhalb der Knoten unterscheidet.^<span class="co">[</span><span class="ot">Wenn die Kontroll- und Behandlungsbeobachtungen in einem Blatt sehr ähnliche Outcomes $Y$ haben, können wir den Effekt nicht schätzen.</span><span class="co">]</span> Das Splitting sollte zu Blättern führen, die hinsichtlich des *geschätzten Behandlungseffekts* möglichst heterogen sind.</span>
<span id="cb36-736"><a href="#cb36-736" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-737"><a href="#cb36-737" aria-hidden="true" tabindex="-1"></a>    Die Wahl des Splitting-Kriterium für die Schätzung kausaler Effekte mit Bäumen ist nicht trivial: Ein natürliches Kriterium ist der mittlere quadratische Fehler bei der Vorhersage von $\tau$,</span>
<span id="cb36-738"><a href="#cb36-738" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-739"><a href="#cb36-739" aria-hidden="true" tabindex="-1"></a>    \begin{align*}</span>
<span id="cb36-740"><a href="#cb36-740" aria-hidden="true" tabindex="-1"></a>      \textup{MSE}_\tau = \frac{1}{n} \sum_{i=1}^n (\tau_i - \widehat{\tau}_i(\boldsymbol{X}_i))^2.</span>
<span id="cb36-741"><a href="#cb36-741" aria-hidden="true" tabindex="-1"></a>    \end{align*}</span>
<span id="cb36-742"><a href="#cb36-742" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-743"><a href="#cb36-743" aria-hidden="true" tabindex="-1"></a>    $\textup{MSE}_\tau$ ist jedoch nicht direkt berechenbar: Aufgrund der nicht-beobachtbaren individuellen Behandlungseffekte $\tau_i$ müsste $\textup{MSE}_\tau$ selbst geschätzt werden!^<span class="co">[</span><span class="ot">Bei "herkömmlichen" Regressionsbäumen besteht dieses Problem nicht, weil das Splitting-Kriterium Abweichungen von den wahren, *beobachteten* Werten von $Y$ misst.</span><span class="co">]</span></span>
<span id="cb36-744"><a href="#cb36-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-745"><a href="#cb36-745" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Data leakage**</span>
<span id="cb36-746"><a href="#cb36-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-747"><a href="#cb36-747" aria-hidden="true" tabindex="-1"></a>    Data leakage tritt auf, wenn Informationen aus dem Trainingsprozess in den Modellvalidierungs- oder Schätzprozess einfließen. Bei der Anpassung des Baums berücksichtigt der Algorithmus idealerweise Informationen über $Y$ *und* $B$ im Splitting-Prozess, um die besten Aufteilungen zu finden. Die hiezu verwendeten Datenpunkte definieren damit *den zu schätzten CATE* anhand der durch Partionierung gebildeten Blätter. Wenn dieselben Datenpunkte auch für die tatsächliche Schätzung des CATE mit dem trainierten Baum verwendet werden, besteht die Gefahr von Überanpassung und somit verzerrten Schätzungen.</span>
<span id="cb36-748"><a href="#cb36-748" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-749"><a href="#cb36-749" aria-hidden="true" tabindex="-1"></a><span class="fu">### Causal Trees</span></span>
<span id="cb36-750"><a href="#cb36-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-751"><a href="#cb36-751" aria-hidden="true" tabindex="-1"></a>Der Causal Tree Algorithmus von @AtheyImbens2016 modifiziert den CART-Algorithmus für die Schätzung heterogener Behandlungseffekte. In diesem Kontext wird die Vorgehensweise als „ehrlich“ (*honest*) bezeichnet, wenn nicht dieselben Informationen sowohl zur Auswahl des Modells (die Partitionierung des Regressorraums durch Splits) als auch zur Schätzung anhand dieses Modells verwendet werden. @AtheyImbens2016 adressieren das Data-Leakage-Problem durch zufällige Aufteilung des Datensatzes in eine Teilmenge $\mathcal{S}^{tr}$ für das *Training des Baums* und eine Teilmenge $\mathcal{S}^{est}$ für die *Schätzung der Behandlungseffekte*. </span>
<span id="cb36-752"><a href="#cb36-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-753"><a href="#cb36-753" aria-hidden="true" tabindex="-1"></a>Für die Erläuterung von *honest splitting* führen wir folgende Notation aus @AtheyImbens2016 ein:</span>
<span id="cb36-754"><a href="#cb36-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-755"><a href="#cb36-755" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathcal{S}^{te}$ ist ein hypothetischer *Testdatensatz*</span>
<span id="cb36-756"><a href="#cb36-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-757"><a href="#cb36-757" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\Pi$ ist eine *Partition*, d.h. eine Aufteilung des Regressorraums von $\boldsymbol{X}$^<span class="co">[</span><span class="ot">$\Pi$ sammelt also die Entschidungsregeln eines Baums und ist äquivalent zu $T$ in den füheren Kapiteln.</span><span class="co">]</span></span>
<span id="cb36-758"><a href="#cb36-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-759"><a href="#cb36-759" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Wir definieren die Schätzung des CATE anhand der Beobachtungen $\mathcal{S}^{est}$: Der CATE  $\widehat{\tau}(\boldsymbol{X}_i,\mathcal{S}^{est},\Pi)$ ist die Differenz der Mittelwerte von $Y_i$ für Behandlungs- und Kontrollbeobachtungen in dem aus $\Pi$ resultierenden Blatt für $\boldsymbol{X}_i$.</span>
<span id="cb36-760"><a href="#cb36-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-761"><a href="#cb36-761" aria-hidden="true" tabindex="-1"></a>Für die Wahl der Splits (die Partitionierung $\Pi$) für den Causal Tree schlagen @AtheyImbens2016 statt der Minimierung des MSE der Vorhersagen $\widehat{Y}$ (wie bei Regressionsbäumen) die Minimierung des MSE für den CATE vor. Das Vorgehen hierbei ist *honest* in dem Sinn, dass der erwartete Schätzfehler für ungesehene Beobachtungen $\mathcal{S}^{te}$ anhand einer Paritionierung $\Pi$ und entsprechenden Schätzungen der Behandlungseffekte $\widehat\tau$ mit unabhängigen Datensätzen $\mathcal{S}^{tr}$ bzw. $\mathcal{S}^{est}$ minimiert wird. Das hierzu verwendete Splitting-Kriterium ist eine Schätzung des *Erwartungswerts* von </span>
<span id="cb36-762"><a href="#cb36-762" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb36-763"><a href="#cb36-763" aria-hidden="true" tabindex="-1"></a>  \textup{MSE}(\mathcal{S}^{est},\mathcal{S}^{te},\Pi) = \frac{1}{n^{te}} \sum_{i=1}^{n^{te}} \big(\tau_i - \widehat{\tau}(\boldsymbol{X}_i,\mathcal{S}^{est},\Pi)\big)^2,</span>
<span id="cb36-764"><a href="#cb36-764" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb36-765"><a href="#cb36-765" aria-hidden="true" tabindex="-1"></a>der *erwartete*^<span class="co">[</span><span class="ot">Die Notation $\textup{E}_{\mathcal{S}^{est},\,\mathcal{S}^{te}}$ meint, dass die Erwartung über $\mathcal{S}^{est}$, und $\mathcal{S}^{te}$ gebildet wird.</span><span class="co">]</span> mittlere quadratische Fehler der heterogenen Behandlungseffekte,</span>
<span id="cb36-766"><a href="#cb36-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-767"><a href="#cb36-767" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb36-768"><a href="#cb36-768" aria-hidden="true" tabindex="-1"></a>  \textup{EMSE}(\Pi) = \textup{E}_{\mathcal{S}^{est},\,\mathcal{S}^{te}}\big<span class="co">[</span><span class="ot">\textup{MSE}(\mathcal{S}^{est},\mathcal{S}^{te},\,\Pi)\big</span><span class="co">]</span>.</span>
<span id="cb36-769"><a href="#cb36-769" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb36-770"><a href="#cb36-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-771"><a href="#cb36-771" aria-hidden="true" tabindex="-1"></a>Eine hilfreiche Umformung für $\textup{EMSE}$ ist</span>
<span id="cb36-772"><a href="#cb36-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-773"><a href="#cb36-773" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb36-774"><a href="#cb36-774" aria-hidden="true" tabindex="-1"></a>  \textup{EMSE}(\Pi) = \textup{Var}_{\mathcal{S}^{est},\boldsymbol{X}_i} \big[\widehat\tau(\boldsymbol{X}_i,\mathcal{S}^{est},\Pi)\big] - \textup{E}_{\boldsymbol{X}_i}\big<span class="co">[</span><span class="ot">\tau^2(\boldsymbol{X}_i,\Pi)\big</span><span class="co">]</span> + \textup{E}<span class="co">[</span><span class="ot">\tau_i^2</span><span class="co">]</span>,</span>
<span id="cb36-775"><a href="#cb36-775" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb36-776"><a href="#cb36-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-777"><a href="#cb36-777" aria-hidden="true" tabindex="-1"></a>denn @AtheyImbens2016 zeigen, wie die ersten beiden Summanden empirisch geschätzt werden können. Der Term $\textup{E}<span class="co">[</span><span class="ot">\tau_i^2</span><span class="co">]</span>$ ist nicht schätzbar (unbeobachteter individueller Behandlungseffekt $\tau_i$), kann aber vernachlässigt werden, da er nicht von $\Pi$ oder den Daten abhängt und somit eine *Konstante* ist, die sich beim Vergleich des geschätzen EMSE für verschiedene $\Pi$ rauskürzt.</span>
<span id="cb36-778"><a href="#cb36-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-779"><a href="#cb36-779" aria-hidden="true" tabindex="-1"></a>Dies sorgt für konsistente Schätzungen. @AtheyImbens2016 zeigen, dass die Minimierung des EMSE sowohl eine ausgewogene Verteilung der behandelten und unbehandelten Individuen als auch eine genaue Schätzung des Behandlungseffekts innerhalb jedes Knotens gewährleistet.</span>
<span id="cb36-780"><a href="#cb36-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-781"><a href="#cb36-781" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb36-782"><a href="#cb36-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-783"><a href="#cb36-783" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithmus: Causal Tree</span></span>
<span id="cb36-784"><a href="#cb36-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-785"><a href="#cb36-785" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Passe den Baum an: teile den Regressorraum mit binären Entscheidungsregeln rekursiv in Partitionen $\Pi$:</span>
<span id="cb36-786"><a href="#cb36-786" aria-hidden="true" tabindex="-1"></a>    a. An jedem Knoten wird die Aufteilung so gewählt, dass die Schätzung des *erwarteten mittleren quadratischen Fehlers* \textup{EMSE}(\Pi) über alle möglichen binären Aufteilungen $\Pi$ minimiert wird.</span>
<span id="cb36-787"><a href="#cb36-787" aria-hidden="true" tabindex="-1"></a>    b. Stelle sicher, dass eine Mindestanzahl von behandelten und Kontroll-Einheiten in jedem Blatt des so angepassten Baums nicht unterschritten wird.</span>
<span id="cb36-788"><a href="#cb36-788" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Bestimmte mit Cross-Validation die Tiefe $d^*$ der Partition, die eine Schätzung des MSE der Behandlungseffekte minimiert.</span>
<span id="cb36-789"><a href="#cb36-789" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Ehalte die Partition $\Pi^*$ durch das Beschneiden von $\Pi$ auf die Tiefe $d^*$: Entferne Blätter, die die geringste Verbesserung der Anpassung bieten. Dieser Schritt liefert den finalen Baum.</span>
<span id="cb36-790"><a href="#cb36-790" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Schätze die Behandlungseffekte in jedem Blatt von $\Pi^*$ mit den Beobachtungen in $\mathcal{S}^{est}$.</span>
<span id="cb36-791"><a href="#cb36-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-792"><a href="#cb36-792" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb36-793"><a href="#cb36-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-794"><a href="#cb36-794" aria-hidden="true" tabindex="-1"></a>Zur Illustration der Anpassung eines Causal Trees mit R lesen wir zunächst den Datensatz <span class="in">`nl_effects`</span> ein. Der Datensatz enhält 10000 Beobachtungen für </span>
<span id="cb36-795"><a href="#cb36-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-796"><a href="#cb36-796" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>10 Regressoren <span class="in">`X1`</span>, <span class="in">`X2`</span>, ...,  <span class="in">`X10`</span></span>
<span id="cb36-797"><a href="#cb36-797" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>die Behandlungsvariable <span class="in">`B`</span></span>
<span id="cb36-798"><a href="#cb36-798" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>das Outcome <span class="in">`Y`</span></span>
<span id="cb36-799"><a href="#cb36-799" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>tatsächliche individuelle Behandlungseffekte <span class="in">`tau`</span></span>
<span id="cb36-800"><a href="#cb36-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-801"><a href="#cb36-801" aria-hidden="true" tabindex="-1"></a>Die Daten wurden so erzeugt, dass lediglich die Regressoren <span class="in">`X1`</span>, <span class="in">`X2`</span> und <span class="in">`X3`</span> Vorhersagekraft für <span class="in">`Y`</span> haben *und* mit der Behandlungsvariable <span class="in">`B`</span> korreliert sind. Der durchschnittliche Behandlungseffekt (ATE) beträgt 2.5.</span>
<span id="cb36-802"><a href="#cb36-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-803"><a href="#cb36-803" aria-hidden="true" tabindex="-1"></a>Wie lesen Datensatz zunächst ein und verschaffen uns einen Überblick.^<span class="co">[</span><span class="ot">Aus technischen Gründen verzichten wir in diesem Kapitel zur Zeit auf die Einbindung der WebR-Konsole.</span><span class="co">]</span></span>
<span id="cb36-804"><a href="#cb36-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-807"><a href="#cb36-807" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-808"><a href="#cb36-808" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb36-809"><a href="#cb36-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-810"><a href="#cb36-810" aria-hidden="true" tabindex="-1"></a><span class="co"># 'nl_effects' einlesen</span></span>
<span id="cb36-811"><a href="#cb36-811" aria-hidden="true" tabindex="-1"></a>nl_effects <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(</span>
<span id="cb36-812"><a href="#cb36-812" aria-hidden="true" tabindex="-1"></a>  <span class="at">file =</span> <span class="st">"datasets/nl_effects.Rds"</span></span>
<span id="cb36-813"><a href="#cb36-813" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb36-814"><a href="#cb36-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-815"><a href="#cb36-815" aria-hidden="true" tabindex="-1"></a><span class="co"># Überblick</span></span>
<span id="cb36-816"><a href="#cb36-816" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(nl_effects)</span>
<span id="cb36-817"><a href="#cb36-817" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-818"><a href="#cb36-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-819"><a href="#cb36-819" aria-hidden="true" tabindex="-1"></a>Für die Schätzung nutzen wir das R-Paket <span class="in">`grf`</span> <span class="co">[</span><span class="ot">@grfPackage</span><span class="co">]</span>. Mit <span class="in">`grf::causal_forest()`</span> kann ein Ensemble-Modell mit vielen Causal Trees (Causal Forest) geschätzt werden. Über die Argumente <span class="in">`num.trees = 1`</span> und <span class="in">`ci.group.size = 1`</span> legen wir fest, dass lediglich ein Causal Tree angepasst werden soll. Beachte, dass die Regressoren dem Argument <span class="in">`X`</span> als <span class="in">`matrix`</span>-Objekt übergeben werden müssen. Mit <span class="in">`min.node.size = 50`</span> legen wir eine Mindestanzahl an Beobachtungen für die Knoten des Baums fest. Da die Aufteilung von <span class="in">`nl_effects`</span> in Trainings-, Schätz- und Validierungsdatensatz zufällig generiert und Cross-Validation zu Bestimmung der Baum-Tiefe eingesetzt wird, setzen wir mit <span class="in">`seed = 1234`</span> einen Seed für Reproduzierbarkeit.</span>
<span id="cb36-820"><a href="#cb36-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-823"><a href="#cb36-823" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-824"><a href="#cb36-824" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb36-825"><a href="#cb36-825" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(grf)</span>
<span id="cb36-826"><a href="#cb36-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-827"><a href="#cb36-827" aria-hidden="true" tabindex="-1"></a><span class="co"># Causal Tree Anpassen</span></span>
<span id="cb36-828"><a href="#cb36-828" aria-hidden="true" tabindex="-1"></a>causal_tree <span class="ot">&lt;-</span> <span class="fu">causal_forest</span>(</span>
<span id="cb36-829"><a href="#cb36-829" aria-hidden="true" tabindex="-1"></a>  <span class="at">X =</span> nl_effects <span class="sc">%&gt;%</span> </span>
<span id="cb36-830"><a href="#cb36-830" aria-hidden="true" tabindex="-1"></a>    dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>Y, <span class="sc">-</span>B, <span class="sc">-</span>tau, <span class="sc">-</span>ps) <span class="sc">%&gt;%</span> </span>
<span id="cb36-831"><a href="#cb36-831" aria-hidden="true" tabindex="-1"></a>    <span class="fu">as.matrix</span>(),</span>
<span id="cb36-832"><a href="#cb36-832" aria-hidden="true" tabindex="-1"></a>  <span class="at">Y =</span> nl_effects<span class="sc">$</span>Y,</span>
<span id="cb36-833"><a href="#cb36-833" aria-hidden="true" tabindex="-1"></a>  <span class="at">W =</span> nl_effects<span class="sc">$</span>B, </span>
<span id="cb36-834"><a href="#cb36-834" aria-hidden="true" tabindex="-1"></a>  <span class="at">min.node.size =</span> <span class="dv">100</span>,</span>
<span id="cb36-835"><a href="#cb36-835" aria-hidden="true" tabindex="-1"></a>  <span class="at">num.trees =</span> <span class="dv">1</span>, </span>
<span id="cb36-836"><a href="#cb36-836" aria-hidden="true" tabindex="-1"></a>  <span class="at">ci.group.size =</span> <span class="dv">1</span>,</span>
<span id="cb36-837"><a href="#cb36-837" aria-hidden="true" tabindex="-1"></a>  <span class="at">seed =</span> <span class="dv">1234</span></span>
<span id="cb36-838"><a href="#cb36-838" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb36-839"><a href="#cb36-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-840"><a href="#cb36-840" aria-hidden="true" tabindex="-1"></a><span class="co"># Zusammenfassung:</span></span>
<span id="cb36-841"><a href="#cb36-841" aria-hidden="true" tabindex="-1"></a>causal_tree</span>
<span id="cb36-842"><a href="#cb36-842" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-843"><a href="#cb36-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-844"><a href="#cb36-844" aria-hidden="true" tabindex="-1"></a>Den angepassten Causal Tree lesen wir mit <span class="in">`grf::get_tree()`</span> aus und nutzen die zugehörige <span class="in">`plot()`</span>-Methode für eine grafische Darstellung, siehe @fig-ct.</span>
<span id="cb36-845"><a href="#cb36-845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-848"><a href="#cb36-848" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-849"><a href="#cb36-849" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb36-850"><a href="#cb36-850" aria-hidden="true" tabindex="-1"></a><span class="co"># Trainierten Causal Tree auslesen</span></span>
<span id="cb36-851"><a href="#cb36-851" aria-hidden="true" tabindex="-1"></a>the_tree <span class="ot">&lt;-</span> <span class="fu">get_tree</span>(causal_tree, <span class="at">index =</span> <span class="dv">1</span>)</span>
<span id="cb36-852"><a href="#cb36-852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-853"><a href="#cb36-853" aria-hidden="true" tabindex="-1"></a><span class="co"># Tree plotten</span></span>
<span id="cb36-854"><a href="#cb36-854" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(the_tree)</span>
<span id="cb36-855"><a href="#cb36-855" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-856"><a href="#cb36-856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-857"><a href="#cb36-857" aria-hidden="true" tabindex="-1"></a><span class="al">![Mit `grf::causal_forest` geschätzter Causal Tree](img/causal_tree.png)</span>{#fig-ct width=70%}</span>
<span id="cb36-858"><a href="#cb36-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-859"><a href="#cb36-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-860"><a href="#cb36-860" aria-hidden="true" tabindex="-1"></a><span class="fu">### Causal Forests</span></span>
<span id="cb36-861"><a href="#cb36-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-862"><a href="#cb36-862" aria-hidden="true" tabindex="-1"></a>*Causal Forests* erweitern das Konzept der Causal Trees zu einem Ensemble-Verfahren, ähnlich wie Random Forests klassische Regressionsbäume erweitern. Bei Causal Forests werden viele Causal Trees auf Bootstrap-Stichproben der Daten trainiert, wobei für jeden Baum nur eine zufällige Teilmenge der Kovariaten für potentielle Splits berücksichtigt wird. Das "Honest Splitting"-Prinzip wird beibehalten: Für jeden Baum wird die Bootstrap-Stichprobe in eine Trainings- und eine Schätzstichprobe aufgeteilt. Der finale geschätzte Behandlungseffekt für eine neue Beobachtung ergibt sich aus dem Durchschnitt der Vorhersagen aller Bäume. Diese Ensemble-Methode reduziert die Varianz der Schätzungen im Vergleich zu einzelnen Causal Trees. Zusätzlich ermöglicht die Forest-Struktur die Berechnung von Konfidenzintervallen für die geschätzten Behandlungseffekte durch die Analyse der Verteilung der Vorhersagen über alle Bäume. Causal Forests zielen dabei primär darauf ab, den Conditional Average Treatment Effect (CATE), auch bekannt als heterogener Behandlungseffekt $\tau(x)$, zu schätzen. Der CATE ist definiert als</span>
<span id="cb36-863"><a href="#cb36-863" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb36-864"><a href="#cb36-864" aria-hidden="true" tabindex="-1"></a>  \tau(x) = \textup{E}<span class="co">[</span><span class="ot">Y(1) - Y(0)|X = x</span><span class="co">]</span>,</span>
<span id="cb36-865"><a href="#cb36-865" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb36-866"><a href="#cb36-866" aria-hidden="true" tabindex="-1"></a>wobei $Y(1)$ und $Y(0)$ die potentiellen Outcomes unter Behandlung bzw. Kontrolle sind, und $X$ die verfügbaren Kovariablen darstellt. Im Gegensatz zum durchschnittlichen Behandlungseffekt (ATE), der über die gesamte Population gemittelt wird, oder zum durchschnittlichen Behandlungseffekt der Behandelten (ATT), ermöglicht der CATE die *Schätzung individualisierter Behandlungseffekte für spezifische Kovariatenwerte*. Causal Forests sind dabei besonders geeignet, nicht-lineare und komplexe Heterogenitätsmuster in den Behandlungseffekten zu erfassen. Sie erlauben es uns:</span>
<span id="cb36-867"><a href="#cb36-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-868"><a href="#cb36-868" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Für jede individuelle Beobachtung $i$ einen spezifischen bedingten Behandlungseffekt $\tau(x_i)$ zu schätzen</span>
<span id="cb36-869"><a href="#cb36-869" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Diese individuellen Schätzungen zu validen Gruppendurchschnitten zu aggregieren</span>
<span id="cb36-870"><a href="#cb36-870" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Die Unsicherheit dieser Schätzungen durch Konfidenzintervalle zu quantifizieren</span>
<span id="cb36-871"><a href="#cb36-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-872"><a href="#cb36-872" aria-hidden="true" tabindex="-1"></a>Die Ensemble-Struktur ermöglicht dabei nicht nur die Punktschätzung des CATE, sondern auch die Berechnung asymptotisch valider Konfidenzintervalle für diese bedingten Effekte: @Atheyetal2019 zeigen, dass Causal Forests unter bestimmten Regularitätsbedingungen asymptotisch normalverteilte und konsistente Schätzer für die bedingten durchschnittlichen Behandlungseffekte liefern.</span>
<span id="cb36-873"><a href="#cb36-873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-874"><a href="#cb36-874" aria-hidden="true" tabindex="-1"></a>Causal Forests eignen sich insbesondere für polit-ökonomische Fragestellungen, bei denen die Wirkung von Maßnahmen zwischen verschiedenen Gruppen oder Regionen variiert. Die Berechnung von CATEs mit Causal Forests könnte beispielsweise ein besseres Verständnis darüber ermöglichen, welche Arbeitssuchenden besonders von bestimmten Weiterbildungsprogrammen profitieren oder wie Subventionen oder Infrastrukturinvestitionen je nach lokalen Gegebenheiten unterschiedlich wirken.</span>
<span id="cb36-875"><a href="#cb36-875" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-876"><a href="#cb36-876" aria-hidden="true" tabindex="-1"></a>@tbl-rfcfcomp vergleicht die wesentlichen Aspekte von Random Forests und Causal Forests.</span>
<span id="cb36-877"><a href="#cb36-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-878"><a href="#cb36-878" aria-hidden="true" tabindex="-1"></a>| Aspekt | Random Forests | Causal Forests |</span>
<span id="cb36-879"><a href="#cb36-879" aria-hidden="true" tabindex="-1"></a>|--------|---------------|----------------|</span>
<span id="cb36-880"><a href="#cb36-880" aria-hidden="true" tabindex="-1"></a>| **Zielfunktion** | Minimierung MSE der Outcome-Vorhersage | Minimierung MSE der gesch. Behandlungseffekte |</span>
<span id="cb36-881"><a href="#cb36-881" aria-hidden="true" tabindex="-1"></a>| **Split-Kriterium** | Minimierung der Varianz von Y in Knoten | Maximierung der Behandlungseffekt-Heterogenität zwischen Knoten |</span>
<span id="cb36-882"><a href="#cb36-882" aria-hidden="true" tabindex="-1"></a>| **Datennutzung** | Gleiche Daten für Anpassung und Vorhersage | "Honest Splitting": Separate Daten für Baumstruktur und Effektschätzung |</span>
<span id="cb36-883"><a href="#cb36-883" aria-hidden="true" tabindex="-1"></a>| **Schätzung** | Bedingte Erwartung $\textup{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$ | Bedingter Behandlungseffekt $\textup{E}<span class="co">[</span><span class="ot">Y(1) - Y(0)|X</span><span class="co">]</span>$ |</span>
<span id="cb36-884"><a href="#cb36-884" aria-hidden="true" tabindex="-1"></a>| **Inferenz** | Punktschätzungen | Asymptotische Verteilung und Konfidenzintervalle |</span>
<span id="cb36-885"><a href="#cb36-885" aria-hidden="true" tabindex="-1"></a>| **Balancierung** | Keine Behandlung-Kontroll-Balancierung | Mindestanzahl von Behandlungs- und Kontrolleinheiten pro Knoten |</span>
<span id="cb36-886"><a href="#cb36-886" aria-hidden="true" tabindex="-1"></a>| **Bootstrap** | Zufällige Stichprobe mit Zurücklegen | Doppelte Stichprobe: Split-Sample und Estimation-Sample |</span>
<span id="cb36-887"><a href="#cb36-887" aria-hidden="true" tabindex="-1"></a>| **Modellkomplexität** | Durch Cross-Validation optimiert | Durch "honest" Cross-Validation mit separaten Schätzstichproben optimiert |</span>
<span id="cb36-888"><a href="#cb36-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-889"><a href="#cb36-889" aria-hidden="true" tabindex="-1"></a>Table: Vergleich von Random Forests und Causal Forests {#tbl-rfcfcomp}</span>
<span id="cb36-890"><a href="#cb36-890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-891"><a href="#cb36-891" aria-hidden="true" tabindex="-1"></a>In diesem Beispiel verwenden wir das Paket <span class="in">`grf`</span> (Generalized Random Forests) in R, um einen Causal Forest zu trainieren und die individuellen Behandlungseffekte zu schätzen.</span>
<span id="cb36-892"><a href="#cb36-892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-895"><a href="#cb36-895" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-896"><a href="#cb36-896" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(grf)</span>
<span id="cb36-897"><a href="#cb36-897" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb36-898"><a href="#cb36-898" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb36-899"><a href="#cb36-899" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-900"><a href="#cb36-900" aria-hidden="true" tabindex="-1"></a>the_split <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(<span class="at">data =</span> nl_effects, <span class="at">prop =</span> .<span class="dv">8</span>)</span>
<span id="cb36-901"><a href="#cb36-901" aria-hidden="true" tabindex="-1"></a>nl_effects_train <span class="ot">&lt;-</span> <span class="fu">training</span>(the_split)</span>
<span id="cb36-902"><a href="#cb36-902" aria-hidden="true" tabindex="-1"></a>nl_effects_test <span class="ot">&lt;-</span> <span class="fu">testing</span>(the_split)</span>
<span id="cb36-903"><a href="#cb36-903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-904"><a href="#cb36-904" aria-hidden="true" tabindex="-1"></a><span class="co"># Variablen in Matrizen / Vektoren überführen</span></span>
<span id="cb36-905"><a href="#cb36-905" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> nl_effects_train <span class="sc">%&gt;%</span> </span>
<span id="cb36-906"><a href="#cb36-906" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">starts_with</span>(<span class="st">"X"</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb36-907"><a href="#cb36-907" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb36-908"><a href="#cb36-908" aria-hidden="true" tabindex="-1"></a>B <span class="ot">&lt;-</span> nl_effects_train <span class="sc">%&gt;%</span> <span class="fu">pull</span>(B)</span>
<span id="cb36-909"><a href="#cb36-909" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> nl_effects_train <span class="sc">%&gt;%</span> <span class="fu">pull</span>(Y)</span>
<span id="cb36-910"><a href="#cb36-910" aria-hidden="true" tabindex="-1"></a>tau <span class="ot">&lt;-</span> nl_effects_train <span class="sc">%&gt;%</span> <span class="fu">pull</span>(tau)</span>
<span id="cb36-911"><a href="#cb36-911" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-912"><a href="#cb36-912" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-913"><a href="#cb36-913" aria-hidden="true" tabindex="-1"></a>Bevor wir den Causal Forest trainieren, schätzen wir zunächst den Propensity Score und das Outcome-Modell. Diese Vorschätzungen werden verwendet, um die Effizienz des Causal Forests zu verbessern. Der Propensity Score modelliert die Wahrscheinlichkeit der Behandlungszuweisung, während das Outcome-Modell den grundlegenden Zusammenhang zwischen den Kovariablen und der Zielvariable erfasst. </span>
<span id="cb36-914"><a href="#cb36-914" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-915"><a href="#cb36-915" aria-hidden="true" tabindex="-1"></a>Während der Propensity Score zur Verbesserung der Balancierung der Behandlungs- und Kontrollgruppen im Causal Forest beiträgt, ermöglicht die Outcome-Schätzung dem Causal Forest, sich auf die *Heterogenität der Behandlungseffekte* zu konzentrieren, statt den gemeinsamen Effekt der Kovariaten auf $Y$ mitschätzen zu müssen. Dies ist analog zur Regression mit Kontrolle für Kovariablen: Der "gemeinsame" Effekt der Regressoren wird durch die Schätzung des Outcomes mit einem Regression Forest bereits herausgerechnet.^[Dies wird auch als *Orthogonoalisierung* bezeichnet.] Obwohl <span class="in">`grf()`</span> diese Schätzungen auch intern vornehmen kann, bietet die explizite Schätzung mehr Kontrolle über den Prozess. Wir nutzen hierzu <span class="in">`grf::regression_forest()`</span>, die Implementierung eines Random-Forest-Algorithmus für Regression.</span>
<span id="cb36-916"><a href="#cb36-916" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-917"><a href="#cb36-917" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, cache=T}</span></span>
<span id="cb36-918"><a href="#cb36-918" aria-hidden="true" tabindex="-1"></a><span class="in"># Propensity Score Schätzen</span></span>
<span id="cb36-919"><a href="#cb36-919" aria-hidden="true" tabindex="-1"></a><span class="in">B_hat_mod &lt;- regression_forest(</span></span>
<span id="cb36-920"><a href="#cb36-920" aria-hidden="true" tabindex="-1"></a><span class="in">  X = X,</span></span>
<span id="cb36-921"><a href="#cb36-921" aria-hidden="true" tabindex="-1"></a><span class="in">  Y = B,</span></span>
<span id="cb36-922"><a href="#cb36-922" aria-hidden="true" tabindex="-1"></a><span class="in">  num.trees = 4000,</span></span>
<span id="cb36-923"><a href="#cb36-923" aria-hidden="true" tabindex="-1"></a><span class="in">  tune.parameters = "all"</span></span>
<span id="cb36-924"><a href="#cb36-924" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb36-925"><a href="#cb36-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-926"><a href="#cb36-926" aria-hidden="true" tabindex="-1"></a><span class="in">B_hat &lt;- B_hat_mod$predictions</span></span>
<span id="cb36-927"><a href="#cb36-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-928"><a href="#cb36-928" aria-hidden="true" tabindex="-1"></a><span class="in"># Outcome Schätzen</span></span>
<span id="cb36-929"><a href="#cb36-929" aria-hidden="true" tabindex="-1"></a><span class="in">Y_hat_mod &lt;- regression_forest(</span></span>
<span id="cb36-930"><a href="#cb36-930" aria-hidden="true" tabindex="-1"></a><span class="in">  X = X,</span></span>
<span id="cb36-931"><a href="#cb36-931" aria-hidden="true" tabindex="-1"></a><span class="in">  Y = Y,</span></span>
<span id="cb36-932"><a href="#cb36-932" aria-hidden="true" tabindex="-1"></a><span class="in">  num.trees = 4000,</span></span>
<span id="cb36-933"><a href="#cb36-933" aria-hidden="true" tabindex="-1"></a><span class="in">  tune.parameters = "all"</span></span>
<span id="cb36-934"><a href="#cb36-934" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb36-935"><a href="#cb36-935" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-936"><a href="#cb36-936" aria-hidden="true" tabindex="-1"></a><span class="in">Y_hat &lt;- Y_hat_mod$predictions</span></span>
<span id="cb36-937"><a href="#cb36-937" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-938"><a href="#cb36-938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-939"><a href="#cb36-939" aria-hidden="true" tabindex="-1"></a>Den Causal Forest schätzen wir mit <span class="in">`grf::causal_forest()`</span> unter Übergabe der zuvor geschätzten Propensity-Scores und Outcomes.</span>
<span id="cb36-940"><a href="#cb36-940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-941"><a href="#cb36-941" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, cache=TRUE}</span></span>
<span id="cb36-942"><a href="#cb36-942" aria-hidden="true" tabindex="-1"></a><span class="in"># Causal Forest trainieren</span></span>
<span id="cb36-943"><a href="#cb36-943" aria-hidden="true" tabindex="-1"></a><span class="in">cf &lt;- causal_forest(</span></span>
<span id="cb36-944"><a href="#cb36-944" aria-hidden="true" tabindex="-1"></a><span class="in">  X = X,</span></span>
<span id="cb36-945"><a href="#cb36-945" aria-hidden="true" tabindex="-1"></a><span class="in">  Y = Y, </span></span>
<span id="cb36-946"><a href="#cb36-946" aria-hidden="true" tabindex="-1"></a><span class="in">  W = B, </span></span>
<span id="cb36-947"><a href="#cb36-947" aria-hidden="true" tabindex="-1"></a><span class="in">  Y.hat = Y_hat,</span></span>
<span id="cb36-948"><a href="#cb36-948" aria-hidden="true" tabindex="-1"></a><span class="in">  W.hat = B_hat, </span></span>
<span id="cb36-949"><a href="#cb36-949" aria-hidden="true" tabindex="-1"></a><span class="in">  num.trees = 4000,   </span></span>
<span id="cb36-950"><a href="#cb36-950" aria-hidden="true" tabindex="-1"></a><span class="in">  tune.parameters = "all"</span></span>
<span id="cb36-951"><a href="#cb36-951" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb36-952"><a href="#cb36-952" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-953"><a href="#cb36-953" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-954"><a href="#cb36-954" aria-hidden="true" tabindex="-1"></a>Als ersten Analyseschritt berechnen wir den durchschnittlichen Behandlungseffekt (ATE). Dieser gibt uns einen ersten Eindruck der Genauigkeit der Schätzung des Behandlungseffekts</span>
<span id="cb36-955"><a href="#cb36-955" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-958"><a href="#cb36-958" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-959"><a href="#cb36-959" aria-hidden="true" tabindex="-1"></a><span class="co"># Vorhersage des durchschnittlichen Behandlungseffekts</span></span>
<span id="cb36-960"><a href="#cb36-960" aria-hidden="true" tabindex="-1"></a><span class="co"># (ATT) mit Causal Forest</span></span>
<span id="cb36-961"><a href="#cb36-961" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb36-962"><a href="#cb36-962" aria-hidden="true" tabindex="-1"></a>  tau.cf <span class="ot">&lt;-</span> <span class="fu">average_treatment_effect</span>(cf)</span>
<span id="cb36-963"><a href="#cb36-963" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb36-964"><a href="#cb36-964" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-965"><a href="#cb36-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-966"><a href="#cb36-966" aria-hidden="true" tabindex="-1"></a>Die geschätzen heterogenen Behandlungseffekte erhalten wir mit <span class="in">`predict()`</span>.</span>
<span id="cb36-967"><a href="#cb36-967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-970"><a href="#cb36-970" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-971"><a href="#cb36-971" aria-hidden="true" tabindex="-1"></a><span class="co"># Schätzungen der bedingte Behandlungseffekte (CATE)</span></span>
<span id="cb36-972"><a href="#cb36-972" aria-hidden="true" tabindex="-1"></a><span class="co"># auslesen</span></span>
<span id="cb36-973"><a href="#cb36-973" aria-hidden="true" tabindex="-1"></a>tau.hat.cf <span class="ot">&lt;-</span> <span class="fu">predict</span>(cf)<span class="sc">$</span>predictions</span>
<span id="cb36-974"><a href="#cb36-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-975"><a href="#cb36-975" aria-hidden="true" tabindex="-1"></a><span class="co"># Überblick</span></span>
<span id="cb36-976"><a href="#cb36-976" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(tau.hat.cf)</span>
<span id="cb36-977"><a href="#cb36-977" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-978"><a href="#cb36-978" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-979"><a href="#cb36-979" aria-hidden="true" tabindex="-1"></a>Da wir in diesem simulierten Datensatz die wahren Behandlungseffekte kennen, können wir die Qualität unserer Schätzungen visuell überprüfen. In der folgenden Grafik vergleichen wir die tatsächlichen individuellen Behandlungseffelte mit dem CATE-Schätzungen. Die rote Linie ist die Referenz für eine perfekte Übereinstimmung zwischen geschätzten und wahren Effekten. Je näher die Punkte an dieser Linie liegen, desto besser ist unsere Schätzung.</span>
<span id="cb36-980"><a href="#cb36-980" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-983"><a href="#cb36-983" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-984"><a href="#cb36-984" aria-hidden="true" tabindex="-1"></a><span class="co"># CATE-Schätzungen vs. ITE plotten</span></span>
<span id="cb36-985"><a href="#cb36-985" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb36-986"><a href="#cb36-986" aria-hidden="true" tabindex="-1"></a>    <span class="at">true_effect =</span> nl_effects_train<span class="sc">$</span>tau,</span>
<span id="cb36-987"><a href="#cb36-987" aria-hidden="true" tabindex="-1"></a>    <span class="at">estimated_effect =</span> <span class="fu">predict</span>(cf)<span class="sc">$</span>predictions,</span>
<span id="cb36-988"><a href="#cb36-988" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb36-989"><a href="#cb36-989" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> true_effect, <span class="at">y =</span> estimated_effect)) <span class="sc">+</span></span>
<span id="cb36-990"><a href="#cb36-990" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.3</span>, <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb36-991"><a href="#cb36-991" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb36-992"><a href="#cb36-992" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(</span>
<span id="cb36-993"><a href="#cb36-993" aria-hidden="true" tabindex="-1"></a>        <span class="at">x =</span> <span class="st">"Wahrer Behandlungseffekt (ITE)"</span>,</span>
<span id="cb36-994"><a href="#cb36-994" aria-hidden="true" tabindex="-1"></a>        <span class="at">y =</span> <span class="st">"Geschätzter Behandlungseffekt (CATE)"</span>,</span>
<span id="cb36-995"><a href="#cb36-995" aria-hidden="true" tabindex="-1"></a>        <span class="at">title =</span> <span class="st">"Vergleich von wahren und geschätzten Behandlungseffekten"</span></span>
<span id="cb36-996"><a href="#cb36-996" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">+</span></span>
<span id="cb36-997"><a href="#cb36-997" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_minimal</span>()</span>
<span id="cb36-998"><a href="#cb36-998" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-999"><a href="#cb36-999" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-1000"><a href="#cb36-1000" aria-hidden="true" tabindex="-1"></a>Anschließend quantifizieren wir die Genauigkeit unserer Schätzungen durch den RMSE (Root Mean Square Error). Wir berechnen diesen sowohl für die Trainings- als auch für die Testdaten, um die Generalisierbarkeit unseres Modells zu überprüfen.</span>
<span id="cb36-1001"><a href="#cb36-1001" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-1004"><a href="#cb36-1004" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-1005"><a href="#cb36-1005" aria-hidden="true" tabindex="-1"></a><span class="co"># RMSE für Causal Forest</span></span>
<span id="cb36-1006"><a href="#cb36-1006" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb36-1007"><a href="#cb36-1007" aria-hidden="true" tabindex="-1"></a>  mse.cf <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((tau.hat.cf <span class="sc">-</span> tau)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb36-1008"><a href="#cb36-1008" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb36-1009"><a href="#cb36-1009" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-1010"><a href="#cb36-1010" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-1011"><a href="#cb36-1011" aria-hidden="true" tabindex="-1"></a>Der RMSE auf den Trainingsdaten zeigt die Schätzgenauigkeit unseres Modells an. Durch das "honest splitting" Verfahren des Causal Forests, bei dem separate Teilstichproben für die Anpassung des Baums und die Effektschätzung verwendet werden, erwarten wir keine substantielle Verschlechterung der Performanz auf den Testdaten <span class="in">`nl_effects_test`</span>.</span>
<span id="cb36-1012"><a href="#cb36-1012" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-1015"><a href="#cb36-1015" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-1016"><a href="#cb36-1016" aria-hidden="true" tabindex="-1"></a><span class="co"># RMSE für separaten Test-Datensatz</span></span>
<span id="cb36-1017"><a href="#cb36-1017" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(</span>
<span id="cb36-1018"><a href="#cb36-1018" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(</span>
<span id="cb36-1019"><a href="#cb36-1019" aria-hidden="true" tabindex="-1"></a>    (</span>
<span id="cb36-1020"><a href="#cb36-1020" aria-hidden="true" tabindex="-1"></a>      <span class="fu">predict</span>(<span class="at">object =</span> cf, <span class="at">newdata =</span> nl_effects_test <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>Y, <span class="sc">-</span>tau, <span class="sc">-</span>B, <span class="sc">-</span>ps))<span class="sc">$</span>predictions </span>
<span id="cb36-1021"><a href="#cb36-1021" aria-hidden="true" tabindex="-1"></a>      <span class="sc">-</span> nl_effects_test<span class="sc">$</span>tau</span>
<span id="cb36-1022"><a href="#cb36-1022" aria-hidden="true" tabindex="-1"></a>      )<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb36-1023"><a href="#cb36-1023" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb36-1024"><a href="#cb36-1024" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb36-1025"><a href="#cb36-1025" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-1026"><a href="#cb36-1026" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-1027"><a href="#cb36-1027" aria-hidden="true" tabindex="-1"></a>Ein ähnlicher RMSE auf den Testdaten bestätigt die inhärente Generalisierungsfähigkeit des Causal Forest Algorithmus.</span>
<span id="cb36-1028"><a href="#cb36-1028" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-1029"><a href="#cb36-1029" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Inferenz für Effekt-Schätzungen</span></span>
<span id="cb36-1030"><a href="#cb36-1030" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-1031"><a href="#cb36-1031" aria-hidden="true" tabindex="-1"></a>Zunächst berechnen wir die Konfidenzintervalle für die geschätzten Behandlungseffekte in <span class="in">`cf`</span>. Die <span class="in">`predict()`</span>-Funktion für <span class="in">`grf`</span>-Objekte kann diese direkt mit ausgeben.</span>
<span id="cb36-1032"><a href="#cb36-1032" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-1035"><a href="#cb36-1035" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-1036"><a href="#cb36-1036" aria-hidden="true" tabindex="-1"></a><span class="co"># Vorhersage mit Konfidenzintervallen</span></span>
<span id="cb36-1037"><a href="#cb36-1037" aria-hidden="true" tabindex="-1"></a>predictions_with_ci <span class="ot">&lt;-</span> <span class="fu">predict</span>(cf, <span class="at">estimate.variance =</span> <span class="cn">TRUE</span>)</span>
<span id="cb36-1038"><a href="#cb36-1038" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-1039"><a href="#cb36-1039" aria-hidden="true" tabindex="-1"></a><span class="co"># In tibble umwandeln </span></span>
<span id="cb36-1040"><a href="#cb36-1040" aria-hidden="true" tabindex="-1"></a>cate_estimates <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb36-1041"><a href="#cb36-1041" aria-hidden="true" tabindex="-1"></a>  <span class="at">cate =</span> predictions_with_ci<span class="sc">$</span>predictions,</span>
<span id="cb36-1042"><a href="#cb36-1042" aria-hidden="true" tabindex="-1"></a>  <span class="at">stderr =</span> <span class="fu">sqrt</span>(predictions_with_ci<span class="sc">$</span>variance.estimates),</span>
<span id="cb36-1043"><a href="#cb36-1043" aria-hidden="true" tabindex="-1"></a>  <span class="at">lower_ci =</span> cate <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> stderr,</span>
<span id="cb36-1044"><a href="#cb36-1044" aria-hidden="true" tabindex="-1"></a>  <span class="at">upper_ci =</span> cate <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> stderr,</span>
<span id="cb36-1045"><a href="#cb36-1045" aria-hidden="true" tabindex="-1"></a>  <span class="at">true_effect =</span> nl_effects_train<span class="sc">$</span>tau</span>
<span id="cb36-1046"><a href="#cb36-1046" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb36-1047"><a href="#cb36-1047" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-1048"><a href="#cb36-1048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-1049"><a href="#cb36-1049" aria-hidden="true" tabindex="-1"></a>Um die Präzision unserer Schätzungen zu visualisieren, erstellen wir einen Plot, der die Konfidenzintervalle für eine Stichprobe von Beobachtungen zeigt. Wir sortieren nach der Größe der geschätzten Effekte, um eine übersichtlichere Darstellung zu erhalten.</span>
<span id="cb36-1050"><a href="#cb36-1050" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-1053"><a href="#cb36-1053" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-1054"><a href="#cb36-1054" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample von 100 Beobachtungen für übersichtlichere Visualisierung</span></span>
<span id="cb36-1055"><a href="#cb36-1055" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb36-1056"><a href="#cb36-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-1057"><a href="#cb36-1057" aria-hidden="true" tabindex="-1"></a>cate_estimates <span class="sc">%&gt;%</span></span>
<span id="cb36-1058"><a href="#cb36-1058" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sample_n</span>(<span class="dv">100</span>) <span class="sc">%&gt;%</span></span>
<span id="cb36-1059"><a href="#cb36-1059" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">id =</span> <span class="fu">row_number</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb36-1060"><a href="#cb36-1060" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(cate) <span class="sc">%&gt;%</span></span>
<span id="cb36-1061"><a href="#cb36-1061" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> id, <span class="at">y =</span> cate)) <span class="sc">+</span></span>
<span id="cb36-1062"><a href="#cb36-1062" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb36-1063"><a href="#cb36-1063" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_errorbar</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> lower_ci, <span class="at">ymax =</span> upper_ci), <span class="at">width =</span> <span class="fl">0.2</span>, <span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb36-1064"><a href="#cb36-1064" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y =</span> true_effect), <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb36-1065"><a href="#cb36-1065" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb36-1066"><a href="#cb36-1066" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Beobachtung (sortiert nach geschätztem Effekt)"</span>,</span>
<span id="cb36-1067"><a href="#cb36-1067" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Behandlungseffekt"</span>,</span>
<span id="cb36-1068"><a href="#cb36-1068" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Geschätzte Behandlungseffekte mit 95% Konfidenzintervallen"</span>,</span>
<span id="cb36-1069"><a href="#cb36-1069" aria-hidden="true" tabindex="-1"></a>    <span class="at">subtitle =</span> <span class="st">"Rote Punkte zeigen wahre Effekte"</span></span>
<span id="cb36-1070"><a href="#cb36-1070" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb36-1071"><a href="#cb36-1071" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb36-1072"><a href="#cb36-1072" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-1073"><a href="#cb36-1073" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-1074"><a href="#cb36-1074" aria-hidden="true" tabindex="-1"></a>Die roten Punkte zeigen die wahren Effekte, während die schwarzen Punkte mit den Konfidenzintervallen unsere Schätzungen darstellen. </span>
<span id="cb36-1075"><a href="#cb36-1075" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-1076"><a href="#cb36-1076" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Ein gut kalibriertes Modell sollte etwa 95% der wahren Effekte innerhalb der Konfidenzintervalle haben.^[In der englischsprachigen Literatur wird dies als *coverage* bezeichnet.] Wir können dies mit R-Code prüfen. --&gt;</span></span>
<span id="cb36-1077"><a href="#cb36-1077" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-1078"><a href="#cb36-1078" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ```{r} --&gt;</span></span>
<span id="cb36-1079"><a href="#cb36-1079" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # Überprüfung der Coverage der Konfidenzintervalle --&gt;</span></span>
<span id="cb36-1080"><a href="#cb36-1080" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ( --&gt;</span></span>
<span id="cb36-1081"><a href="#cb36-1081" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   coverage &lt;- cate_estimates %&gt;% --&gt;</span></span>
<span id="cb36-1082"><a href="#cb36-1082" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   summarise( --&gt;</span></span>
<span id="cb36-1083"><a href="#cb36-1083" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     coverage = mean(true_effect &gt;= lower_ci &amp; true_effect &lt;= upper_ci), --&gt;</span></span>
<span id="cb36-1084"><a href="#cb36-1084" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     avg_ci_width = mean(upper_ci - lower_ci) --&gt;</span></span>
<span id="cb36-1085"><a href="#cb36-1085" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   ) --&gt;</span></span>
<span id="cb36-1086"><a href="#cb36-1086" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ) --&gt;</span></span>
<span id="cb36-1087"><a href="#cb36-1087" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ``` --&gt;</span></span>
<span id="cb36-1088"><a href="#cb36-1088" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-1089"><a href="#cb36-1089" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-1090"><a href="#cb36-1090" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-1091"><a href="#cb36-1091" aria-hidden="true" tabindex="-1"></a><span class="fu">## Zusammenfassung</span></span>
<span id="cb36-1092"><a href="#cb36-1092" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-1093"><a href="#cb36-1093" aria-hidden="true" tabindex="-1"></a>In diesem Kapitel haben wir die Anwendung baum-basierter Methoden in R diskutiert. Darunter Entscheidungsbäume, Bagging, Random Forests, Boosting und Causal Forests. Entscheidungsbäume sind Modelle, die die Daten anhand binärer Entscheidungsregeln sukzessiv in kleinere, homogene Gruppen aufgeteilt werden. Baum-Modelle bieten intuitive Interpretierbarkeit, neigen jedoch zur Überanpassung, was durch Beschneiden (Pruning) vermieden werden kann. Die Vorhersage einzelner Bäume ist tendentiell mit hoher Varianz verbunden. Random Forests kombinieren mit Bagging viele Entscheidungsbäume, die auf zufälligen Teilmengen der Daten und Merkmale trainiert werden. Durch die Aggregation der Vorhersagen vieler Bäume reduziert der Random Forest die Varianz und verbessert so die Vorhersagegenauigkeit. Boosting-Methoden mit Entscheidungsbäumen trainieren kleine Bäume sukzessive, wobei jeder weitere Baum zur Korrektur der gegenwärtigen Fehler des Ensembles trainiert wird. Gradient Boosting nutzt den Gradienten der Verlustfunktion, um die Vorhersagequalität des Ensembles zu optimieren. Diese prädiktiven Methoden wurden im <span class="in">`parsnip`</span>-Framework in R implementiert.</span>
<span id="cb36-1094"><a href="#cb36-1094" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-1095"><a href="#cb36-1095" aria-hidden="true" tabindex="-1"></a>Causal Forests erweitern das Random Forest-Konzept für die Schätzung heterogener Behandlungseffekte. Dabei wird das "Honest Splitting"-Prinzip angewandt, bei dem separate Datenstichproben für die Strukturbestimmung der Bäume und die Effektschätzung verwendet werden. Die Implementierung erfolgt mit dem <span class="in">`grf`</span>-Paket, wobei zur Effizienzsteigerung Propensity Scores und Outcomes vorgeschätzt werden. Diese Orthogonalisierung ermöglicht es dem Causal Forest, sich auf die Heterogenität der Behandlungseffekte zu konzentrieren.</span>
<span id="cb36-1096"><a href="#cb36-1096" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-1097"><a href="#cb36-1097" aria-hidden="true" tabindex="-1"></a>Für alle Methoden wurde gezeigt, wie die Vorhersagegüte durch Testdatensätze beurteilt und die Bedeutung einzelner Variablen mit Variable-Importance-Metriken analysiert werden kann.</span>
</code><button title="In die Zwischenablage kopieren" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/mca91/kasa_book/edit/main/trees.qmd" class="toc-action"><i class="bi bi-github"></i>Seite editieren</a></li><li><a href="https://github.com/mca91/kasa_book/issues/new" class="toc-action"><i class="bi empty"></i>Problem melden</a></li><li><a href="https://github.com/mca91/kasa_book/blob/main/trees.qmd" class="toc-action"><i class="bi empty"></i>Quellcode anzeigen</a></li></ul></div></div></div></footer></body></html>