<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="de" xml:lang="de"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>15&nbsp; Baum-basierte Methoden – Kausalanalyse und maschinelles Lernen mit R</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Machine Learning.html" rel="next">
<link href="./svm.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/live-runtime/live-runtime.js" type="module"></script>
<link href="site_libs/quarto-contrib/live-runtime/live-runtime.css" rel="stylesheet"><script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "Keine Treffer",
    "search-matching-documents-text": "Treffer",
    "search-copy-link-title": "Link in die Suche kopieren",
    "search-hide-matches-text": "Zusätzliche Treffer verbergen",
    "search-more-match-text": "weitere Treffer in diesem Dokument",
    "search-more-matches-text": "weitere Treffer in diesem Dokument",
    "search-clear-button-title": "Zurücksetzen",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Abbrechen",
    "search-submit-button-title": "Abschicken",
    "search-label": "Suchen"
  }
}</script><script type="module" src="site_libs/quarto-ojs/quarto-ojs-runtime.js"></script><link href="site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">
<meta name="robots" content="noindex">
<script>
  MathJax = {
    tex: {
      tags: 'ams'  // should be 'ams', 'none', or 'all'
    }
  };
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.js" integrity="sha512-aoZChv+8imY/U1O7KIHXvO87EOzCuKO0GhFtpD6G2Cyjo/xPeTgdf3/bchB10iB+AojMTDkMHDPLKNxPJVqDcw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css">
<style>
  .panel-tabset .tab-content {
    border: 0;
    padding: 1em 0 0 0;
  }
  
  .panel-tabset .nav-item a {
    border-radius: 5px 5px 0 0;
  }
  
  .scientific_borders {
    border: 0;
    border-top: 2px solid black !important; 
    border-bottom: 2px solid black !important;
  }
  .table:not(.gt_table) > :not(caption)>*>* {
    border-bottom-width: 0;
  }
  .table:not(.gt_table) > thead {
    border-bottom: 1px solid black;
  }
  .soft-box-shadow {
    border: 1px solid rgba(233,236,239,.9) !important;
    border-radius: .5rem !important;
    background-color: rgba(250,250,250,.9) !important;
    box-shadow: 0px 1px 2px rgba(0,0,0,.1),
                0px 3px 7px rgba(0,0,0,.1),
                0px 12px 30px rgba(0,0,0,.08);
    margin-top: 2rem !important;
    margin-bottom: 2.5rem !important;
    padding: .25rem !important;
  }
  .obs-soft-box-shadow {
    border: 1px solid rgba(233,236,239,.9) !important;
    border-radius: .5rem !important;
    background-color: white !important;
    box-shadow: 0px 1px 2px rgba(0,0,0,.1),
                0px 3px 7px rgba(0,0,0,.1),
                0px 12px 30px rgba(0,0,0,.08);
    margin-top: 2rem !important;
    margin-bottom: 2.5rem !important;
    padding: .25rem !important;
  }
  
</style>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    
    var gt_tables = document.querySelectorAll(".gt_table");
    gt_tables.forEach(function(table) {
      table.classList.remove("table-striped");
    });
    
    var tables = document.querySelectorAll("table.table:not(.gt_table)");
    tables.forEach(function(table) {
      table.classList.remove("table-striped");
      table.classList.add("scientific_borders");
    });
    
    document.querySelectorAll("div.sourceCode").forEach(function(block) {
      block.classList.add("soft-box-shadow");
    });
    
    document.querySelectorAll("div.bg-white").forEach(function(block) {
      block.classList.remove("bg-white");
    });
    
const elements = document.querySelectorAll('[id^="qwebr-interactive-area"]');

    elements.forEach(element => {
        element.classList.add('box-shadow');
    });
    
    document.querySelectorAll('[id^="webr"]').forEach(function(block) {
      block.classList.add("box-shadow");
    });
    
        document.querySelectorAll('.card-header').forEach(function(block) {
      block.classList.add("box-shadow");
    });
    
  });
</script><style>
.qwebr-code-output-stdout {background-color: powderblue;}

.qwebr-button-run {
 width = 100%; 
}

.centered-caption {
   text-align: center;
}
</style>
<script src="https://cdn.jsdelivr.net/npm/quizdown@latest/public/build/quizdown.js"></script><script>quizdown.init();</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script><link rel="stylesheet" href="custom_styles.css">
</head>
<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Seitenleiste umschalten" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./RegReg.html">Machine Learning</a></li><li class="breadcrumb-item"><a href="./trees.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Baum-basierte Methoden</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Seitenleiste umschalten" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./"></a><a href="./index.html">Kausalanalyse und machinelles Lernen mit R</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Lesemodus umschalten">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Einleitung</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Grundlagen</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Abschnitt umschalten">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R_Einfuehrung.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistische Programmierung mit R</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./RR.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Reproduzierbarkeit</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Reg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Simulation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Simulation</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Kausale Inferenz</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Abschnitt umschalten">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Matching.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./FixedEffects.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Panel-Daten</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./IV.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">IV-Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./DiD.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Difference-in-Differences</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./EventStudies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Event Studies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./RDD.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Regression Discontiniuty Designs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./SyntheticControl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Synthetic Control</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Abschnitt umschalten">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./RegReg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Regularisierte Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./svm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./trees.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Baum-basierte Methoden</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Machine Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Neuronale Netzwerke</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Literatur.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Literatur</span></a>
  </div>
</li>
    </ul>
</div>
    <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title"><a href="./index.html">Übersicht</a></h2>
   
  <ul>
<li><a href="#sec-simpletrees" id="toc-sec-simpletrees" class="nav-link active" data-scroll-target="#sec-simpletrees"><span class="header-section-number">15.1</span> Entscheidungsbäume</a></li>
  <li><a href="#training-von-b%C3%A4umen" id="toc-training-von-bäumen" class="nav-link" data-scroll-target="#training-von-b%C3%A4umen"><span class="header-section-number">15.2</span> Training von Bäumen</a></li>
  <li><a href="#bagging" id="toc-bagging" class="nav-link" data-scroll-target="#bagging"><span class="header-section-number">15.3</span> Bagging</a></li>
  <li><a href="#sec-brf" id="toc-sec-brf" class="nav-link" data-scroll-target="#sec-brf"><span class="header-section-number">15.4</span> Random Forests</a></li>
  <li><a href="#sec-boosting" id="toc-sec-boosting" class="nav-link" data-scroll-target="#sec-boosting"><span class="header-section-number">15.5</span> Boosting</a></li>
  <li>
<a href="#causal-trees-und-causal-forests" id="toc-causal-trees-und-causal-forests" class="nav-link" data-scroll-target="#causal-trees-und-causal-forests"><span class="header-section-number">15.6</span> Causal Trees und Causal Forests</a>
  <ul>
<li><a href="#causal-trees" id="toc-causal-trees" class="nav-link" data-scroll-target="#causal-trees"><span class="header-section-number">15.6.1</span> Causal Trees</a></li>
  <li><a href="#causal-forests" id="toc-causal-forests" class="nav-link" data-scroll-target="#causal-forests"><span class="header-section-number">15.6.2</span> Causal Forests</a></li>
  </ul>
</li>
  <li><a href="#zusammenfassung" id="toc-zusammenfassung" class="nav-link" data-scroll-target="#zusammenfassung"><span class="header-section-number">15.7</span> Zusammenfassung</a></li>
  </ul></nav>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./RegReg.html">Machine Learning</a></li><li class="breadcrumb-item"><a href="./trees.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Baum-basierte Methoden</span></a></li></ol></nav><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-trees" class="quarto-section-identifier"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Baum-basierte Methoden</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><p>Baum-basierte Methoden bieten eine vielseitige und leistungsstarke Herangehensweise für Vorhersage und Klassifikation in komplexen Datensätzen mit nicht-linearen Zusammenhängen. Ein Vorteil baum-basierter Methoden ist ihre inhärente Fähigkeit, die Bedeutung einzelner Variablen für die Vorhersage zu quantifizieren – eine Eigenschaft, die viele Machine-Learning-Modelle nicht ohne weiteres bieten und insbesondere in hoch-dimensionalen Anwendungen (mit vielen potentiellen Regressoren) nicht trivial ist. Dies ermöglicht es, tiefere Einblicke in den Einfluss einzelner Merkmale auf die Vorhersagen des Modells zu erhalten, was besonders in empirischen Anwendungen für die Entscheidungsstützung mit Machine Learning hilfreich sein kann.</p>
<p><em>Entscheidungsbäume</em> stellen die Grundlage dieser Methoden dar. Sie ermöglichen die Aufteilung der Daten in immer kleinere, homogenere Gruppen, basierend auf <em>binären</em> Entscheidungsregeln, die aus den Prädiktoren abgleitet werden. Die trainierten Regeln eines solchen Modells lassen sich anhand eines Binärbaums visualisieren, was eine intuitive Interpretierbarkeit der Ergebnisse erlaubt.</p>
<p><em>Random Forests</em> ist ein Ensemble-Ansatz, bei dem viele Entscheidungsbäume kombiniert werden. Jeder Baum wird auf einer zufälligen Teilmenge der Daten trainiert (<em>Bagging</em>), und bei jedem Knoten wird zusätzlich eine zufällige Teilmenge der Merkmale berücksichtigt. Die finale Vorhersage des Random Forests basiert auf der Aggregation der Vorhersagen aller Bäume (Mehrheitsvotum für Klassifikation, Durchschnitt für Regression). Dieses Verfahren reduziert das Risiko einer Überanpassung und erhöht oft die Vorhersagegenauigkeit im Vergleich zu einzelnen Entscheidungsbäumen.</p>
<p><em>Boosting</em> ist eine weitere Ensemble-Methode zur Anpassung von Modellen mit hoher Vorhersagegüte durch Kombination einfacher Modelle (<em>Base learner</em>), wobei Regressions- oder Klassifikationsbäume eingesetzt werden können. Alternativ zu Random Forests trainieren Boosting-Algorithmen sukzessiv einfache (Klassifikations- oder Regressions-)Bäume, wobei jeder nachfolgende Baum das Ziel hat, die Vorhersagefehler der vorherigen Bäume zu korrigieren.</p>
<p>In diesem Kapitel erläutern wir die Anwendung baum-basierter Methoden in R anhand von Beispieldatensätzen. Wir zeigen, wie Regressionsbäume, Random Forests und Boosting-Modelle im <code>parsnip</code>-Framework trainiert werden und wie die Vorhersageleistung durch die Wahl geeigneter Hyperparameter mit Cross-Validation und Out-of-Sample-Evaluierungsmethoden optimiert werden kann.</p>
<section id="sec-simpletrees" class="level2 page-columns page-full" data-number="15.1"><h2 data-number="15.1" class="anchored" data-anchor-id="sec-simpletrees">
<span class="header-section-number">15.1</span> Entscheidungsbäume</h2>
<p>Ein Entscheidungsbaum ist ein Modell, das auf der Basis von hierarchischen Bedingungen bzgl. der Regressoren Vorhersagen für die Outcome-Variable trifft. Jeder Baum beginnt mit einem Wurzelknoten (<em>root node</em>) und verzweigt sich binär. Jede Verzweigung (<em>split</em>) stellt eine Bedingung dar, die auf einem bestimmten Regressor basiert. Der Baum trifft Entscheidungen, indem er diese Bedingungen sukzessive überprüft, bis er zu einem Blattknoten (<em>leaf node</em> / <em>terminal node</em>) gelangt, der die finale Vorhersage liefert. Hierbei handelt es sich eine Mehrheitsentscheidung für Klassifikation und einen Mittelwert, jeweils gebildet anhand Beobachten des Trainingsdatensatzes im leaf node.</p>
<p><a href="#fig-exdectree" class="quarto-xref">Abbildung&nbsp;<span>15.1</span></a> zeigt ein einfaches Beispiel eines Entscheidungsbaums zur Klassifikation der Kreditwürdigkeit einer Person. Die Klassfikation erfolgt, in dem die Beobachtung basierend auf den Merkmalen Alter, Einkommen und Eigentum durch den Baum geleitet wird. Zunächst wird geprüft, die Person 30 Jahre oder jünger ist. Fall ja, entscheidet der Baum anhand des Einkommens: Bei einem Jahreseinkommen von 40.000 oder weniger wird die Person als wenig kreditwürdig klassifiziert, bei höherem Einkommen als mäßig kreditwürdig. Für Personen älter als 30 Jahre überprüft das Modell lediglich, ob die Person eine Immobilie besitzt, um zwischen mäßiger Kreditwürdigkeit und guter Bonität zu unterscheiden.</p>
<div class="cell page-columns page-full" data-fig-width="6" data-fig-height="5" data-layout-align="default">
<div class="cell-output-display page-columns page-full">
<div id="fig-exdectree" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-exdectree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<svg width="576" height="480" viewbox="10.80 10.80 571.96 304.40" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none"><g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(14.8 300.4)"><title>exdectree</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-289.6 557.16,-289.6 557.16,4 -4,4"></polygon><!-- 1 --><g id="node1" class="node"><title>1</title>
<polygon fill="none" stroke="black" points="314.96,-285.6 227.42,-285.6 227.42,-249.6 314.96,-249.6 314.96,-285.6"></polygon><text text-anchor="middle" x="271.19" y="-263.4" font-family="Times,serif" font-size="14.00">Alter &lt;= 30?</text></g><!-- 2 --><g id="node2" class="node"><title>2</title>
<polygon fill="none" stroke="black" points="229.13,-160.8 73.25,-160.8 73.25,-124.8 229.13,-124.8 229.13,-160.8"></polygon><text text-anchor="middle" x="151.19" y="-138.6" font-family="Times,serif" font-size="14.00">Einkommen &lt;= 40 Tsd.?</text></g><!-- 1&#45;&gt;2 --><g id="edge1" class="edge"><title>1-&gt;2</title>
<path fill="none" stroke="black" d="M254.51,-249.53C233.93,-228.47 198.82,-192.54 175.31,-168.48"></path><polygon fill="black" stroke="black" points="177.61,-165.83 168.12,-161.13 172.61,-170.72 177.61,-165.83"></polygon><text text-anchor="middle" x="206.02" y="-201" font-family="Times,serif" font-size="14.00">Ja</text></g><!-- 3 --><g id="node3" class="node"><title>3</title>
<polygon fill="none" stroke="black" points="431.34,-160.8 355.04,-160.8 355.04,-124.8 431.34,-124.8 431.34,-160.8"></polygon><text text-anchor="middle" x="393.19" y="-138.6" font-family="Times,serif" font-size="14.00">Eigentum?</text></g><!-- 1&#45;&gt;3 --><g id="edge2" class="edge"><title>1-&gt;3</title>
<path fill="none" stroke="black" d="M288.15,-249.53C309.07,-228.47 344.77,-192.54 368.67,-168.48"></path><polygon fill="black" stroke="black" points="371.41,-170.69 375.98,-161.13 366.45,-165.75 371.41,-170.69"></polygon><text text-anchor="middle" x="353.8" y="-201" font-family="Times,serif" font-size="14.00">Nein</text></g><!-- 4 --><g id="node4" class="node"><title>4</title>
<polygon fill="none" stroke="black" points="100.57,-36 -0.19,-36 -0.19,0 100.57,0 100.57,-36"></polygon><text text-anchor="middle" x="50.19" y="-13.8" font-family="Times,serif" font-size="14.00">Status: Niedrig</text></g><!-- 2&#45;&gt;4 --><g id="edge3" class="edge"><title>2-&gt;4</title>
<path fill="none" stroke="black" d="M137.15,-124.73C119.98,-103.86 90.79,-68.36 71.01,-44.31"></path><polygon fill="black" stroke="black" points="73.5,-41.83 64.44,-36.33 68.09,-46.27 73.5,-41.83"></polygon><text text-anchor="middle" x="55.02" y="-76.2" font-family="Times,serif" font-size="14.00">Ja</text></g><!-- 5 --><g id="node5" class="node"><title>5</title>
<polygon fill="none" stroke="black" points="329.03,-36 237.35,-36 237.35,0 329.03,0 329.03,-36"></polygon><text text-anchor="middle" x="283.19" y="-13.8" font-family="Times,serif" font-size="14.00">Status: Mittel</text></g><!-- 2&#45;&gt;5 --><g id="edge4" class="edge"><title>2-&gt;5</title>
<path fill="none" stroke="black" d="M169.55,-124.73C192.27,-103.58 231.14,-67.43 256.99,-43.37"></path><polygon fill="black" stroke="black" points="259.63,-45.7 264.57,-36.33 254.86,-40.57 259.63,-45.7"></polygon><text text-anchor="middle" x="238.8" y="-76.2" font-family="Times,serif" font-size="14.00">Nein</text></g><!-- 3&#45;&gt;5 --><g id="edge6" class="edge"><title>3-&gt;5</title>
<path fill="none" stroke="black" d="M377.9,-124.73C359.12,-103.77 327.13,-68.05 305.58,-44"></path><polygon fill="black" stroke="black" points="307.99,-41.44 298.71,-36.33 302.78,-46.11 307.99,-41.44"></polygon><text text-anchor="middle" x="347.8" y="-76.2" font-family="Times,serif" font-size="14.00">Nein</text></g><!-- 6 --><g id="node6" class="node"><title>6</title>
<polygon fill="none" stroke="black" points="553.13,-36 465.25,-36 465.25,0 553.13,0 553.13,-36"></polygon><text text-anchor="middle" x="509.19" y="-13.8" font-family="Times,serif" font-size="14.00">Status: Hoch</text></g><!-- 3&#45;&gt;6 --><g id="edge5" class="edge"><title>3-&gt;6</title>
<path fill="none" stroke="black" d="M409.32,-124.73C429.21,-103.67 463.15,-67.74 485.87,-43.68"></path><polygon fill="black" stroke="black" points="488.5,-46 492.83,-36.33 483.41,-41.19 488.5,-46"></polygon><text text-anchor="middle" x="464.02" y="-76.2" font-family="Times,serif" font-size="14.00">Ja</text></g></g></svg>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-exdectree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Abbildung&nbsp;15.1: Entscheidungsbaum: Klassifikation von Kreditwürdigkeit
</figcaption></figure>
</div>
</div>
</div>
</section><section id="training-von-bäumen" class="level2 page-columns page-full" data-number="15.2"><h2 data-number="15.2" class="anchored" data-anchor-id="training-von-bäumen">
<span class="header-section-number">15.2</span> Training von Bäumen</h2>
<p>Zur Konstruktion von Binär-Bäumen werden etablierte Algorithmen wie <em>Classification and Regression Trees</em> (<a href="https://de.wikipedia.org/wiki/CART_(Algorithmus)">CART</a> von <span class="citation" data-cites="Breimanetal1984">Breiman u.&nbsp;a. (<a href="Literatur.html#ref-Breimanetal1984" role="doc-biblioref">1984</a>)</span> verwendet. Die wesentliche Vorgehensweise für das Training eines Baums <span class="math inline">\(T\)</span> ist wie folgt:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
CART-Algorithmus
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>
<p><strong>Splitting</strong>: Beginnend am root node sucht der Algorithmus nach der “besten” Regel, die Daten anhand eines Merkmals in zwei Gruppen zu teilen. Die Qualität des Splits wird in Abhängigkeit der Definition der Outcome-Variable beurteilt:</p>
<ul>
<li><p><strong>Bei Klassifikation</strong>: Die Reinheit (<em>purtity</em>) der Klassen in den unmittelbar nachfolgen nodes wird maximiert. Ein gängiges Kriterium hierfür ist der <a href="https://de.wikipedia.org/wiki/Gini-Koeffizient"><em>Gini-Koeffizient</em></a>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p></li>
<li><p><strong>Bei Regression</strong>: Der MSE bei Vorhersage des Outcomes durch Mittelwertbildung für Beobachtungen in den unmittelbar nachfolgenden nodes wird minimiert.</p></li>
</ul>
</li>
<li>
<p><strong>Rekursion</strong>: Der Prozess wird rekursiv fortgesetzt, bis Abbruchkriterien greifen eine weitere Verzewigung verhindern:</p>
<ul>
<li>Die maximale Baumtiefe (<em>tree depth</em>) ist erreicht</li>
<li>Die leaf nodes sind hinreichend “rein”: Alle Beobachtungen in einem leaf node gehören zur gleichen Klasse oder die Verbesserung des Loss durch weitere Splits fällt unter einen festgelegten Schwellenwert</li>
<li>Weitere Splits führen zu leaf nodes, die eine Mindestanzahl an Beobachtungen (<em>minimum split</em>) unterschreiten würden</li>
</ul>
</li>
<li>
<p><strong>Pruning</strong>: Um Überanpassung an die Trainingsdaten zu vermeiden, kann der Baum beschnitten werden (<em>pruning</em>). Der Grundgedanke ist, dass tief verzweigte Bäume die Trainingsdaten zwar gut modellieren können, aber schlecht auf neue, unbekannte Daten generalisieren.</p>
<p>Bei <em>cost complexity (CP) pruning</em> werden, beginnend auf Ebene der leaf nodes sukuzessive Äste entfernt, und eine Balance zwischen Komplexität des Baums und dem Anpassungsfehler zu finden. Ähnlich wie bei regularisierter KQ-Schätzung (<a href="RegReg.html" class="quarto-xref"><span>Kapitel 13</span></a>), wird die Verlustfunktion <span class="math inline">\(L\)</span> um einen Strafterm für die Komplexität erweitert. Der Effekt der Strafe wird durch den CP-Parameter <span class="math inline">\(\alpha\in[0,1]\)</span> geregelt,</p>
<p><span class="math display">\[\begin{align*}
   L_{\alpha}(T) = L(T) + \alpha \lvert T\rvert,
\end{align*}\]</span></p>
</li>
</ol>
<p>für einen Baum <span class="math inline">\(T\)</span> mit Komplexitätsmaß <span class="math inline">\(\lvert T\rvert\)</span> (Anzahl der leaf nodes) <span class="citation" data-cites="Hastieetal2013">(<a href="Literatur.html#ref-Hastieetal2013" role="doc-biblioref">Hastie, Tibshirani, und Friedman 2013</a>)</span>.</p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Der Gini-Koeffizient <span class="math inline">\(0\leq G\leq1\)</span> misst die Homogenität der Outcome-Variable für die Beobachtungen eines Knotens. <span class="math inline">\(G=0\)</span> ergibt sich bei vollständiger “Reinheit” (alle Beobachtungen im Knoten gehören zur gleichen Klasse). <span class="math inline">\(G &gt; 0\)</span> zeigt Heterogenität der Klassen an, die mit <span class="math inline">\(G\)</span> zunimmt</p></div></div><p>Zur Demonstation der Schätzung von Regressionsbäumen mit R betrachten wir nachfolgend den Datensatz <code>MASS::Bosten</code>. Ziel hierbei ist es, mittlere Hauswerte <code>medv</code> in Stadteilen von Boston, MA vorherzusagen. Wir verwenden hierzu Funktionen aus dem Paket <code>parsnip</code>.</p>
<p>Zunächst transformieren wir den Datensatz in ein <code>tibble</code>-Objekt und definieren Trainings- und Test-Daten.</p>
<div class="cell">
<div>
<div id="webr-1">

</div>
<script type="webr-1-contents">
eyJjb2RlIjoibGlicmFyeShwYXJzbmlwKVxubGlicmFyeShjb3dwbG90KVxuXG4jIFNlZWQgc2V0emVuXG5zZXQuc2VlZCgxMjM0KVxuXG4jIERhdGVuc2F0eiBhbHMgdGliYmxlXG5Cb3N0b24gPC0gYXNfdGliYmxlKE1BU1M6OkJvc3RvbilcblxuIyBTcGxpdHRpbmcgaW4gVHJhaW5pbmctIHVuZCBUZXN0LURhdGVuXG5Cb3N0b25fc3BsaXQgPC0gaW5pdGlhbF9zcGxpdChcbiAgZGF0YSA9IEJvc3RvbiwgXG4gIHByb3AgPSAwLjgsIFxuICBzdHJhdGEgPSBtZWR2XG4gIClcblxuQm9zdG9uX3RyYWluIDwtIHRyYWluaW5nKEJvc3Rvbl9zcGxpdClcbkJvc3Rvbl90ZXN0IDwtIHRlc3RpbmcoQm9zdG9uX3NwbGl0KVxuXG5zbGljZV9oZWFkKEJvc3Rvbl90cmFpbiwgbiA9IDEwKSIsImF0dHIiOnsiZWRpdCI6dHJ1ZSwiZXZhbCI6dHJ1ZSwiZmlnLXdpZHRoIjoiOCJ9fQ==
</script>
</div>
</div>
<p><code>parsnip</code> bietet eine vereinheitlichetes Framework für das Training von Modellen mit R und eine flexible API für Machine Learning. Wir definieren zunächst mit <code><a href="https://parsnip.tidymodels.org/reference/decision_tree.html">parsnip::decision_tree()</a></code> eine Spezifikation zum Training von Entschieundgsmodellen und übergeben beispielhaft einen CP-Parameter <span class="math inline">\(\alpha=.1\)</span>. Mit <code><a href="https://parsnip.tidymodels.org/reference/set_engine.html">parsnip::set_engine</a></code> wählen wir das Paket <code>raprt</code>. Der hier implementierte Agorithmus ist CART. Zuletzt legen wir mit <code><a href="https://parsnip.tidymodels.org/reference/set_args.html">parsnip::set_mode()</a></code> fest, dass der Algorithmus für Regression durchgeführt werden soll.</p>
<div class="cell">
<div>
<div id="webr-2">

</div>
<script type="webr-2-contents">
eyJjb2RlIjoiIyBTcGV6aWZpa2F0aW9uIGZlc3RsZWdlblxudHJlZV9zcGVjIDwtIGRlY2lzaW9uX3RyZWUoXG4gIGNvc3RfY29tcGxleGl0eSA9IDAuMVxuICApICU+JVxuICBzZXRfZW5naW5lKFwicnBhcnRcIikgJT4lXG4gIHNldF9tb2RlKFwicmVncmVzc2lvblwiKVxuXG4jIE1vZGVsbCB0cmFpbmllcmVuXG50cmVlX2ZpdCA8LSB0cmVlX3NwZWMgJT4lXG4gIGZpdChcbiAgICBmb3JtdWxhID0gbWVkdiB+IC4sIFxuICAgIGRhdGEgPSBCb3N0b25fdHJhaW4sIFxuICAgIG1vZGVsID0gVFJVRVxuICApXG5cbiMgVHJhaW5pZXJ0ZW4gQmF1bSBpbiBLb25zb2xlIGF1c2dlYmVuXG50cmVlX2ZpdCRmaXQiLCJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWUsImZpZy13aWR0aCI6IjgifX0=
</script>
</div>
</div>
<p>Der Output in <code>tree_fit$fit</code> zeigt, dass CP-Pruning zu einem kleinen Baum mit 3 Hierarchie-Ebenen geführt hat. Die Struktur zeigt, dass <code>lstat</code> und <code>rm</code> für Splitting-Regeln (<code>split</code>) verwendet werden, wie viele Beobachtungen den nodes zugeordnet sind (<code>n</code>), den Wert der Verlustfunktion (<code>deviance</code>) sowie den Durchschnitt von <code>medv</code> für jede node (<code>yval</code>). Für die drei leaf nodes (gekennzeichnet mit <code>*</code>) ist <code>yval</code> die Vorhersage der Outcome-Varibale für entsprechend gruppierte Beobachtungen.</p>
<p>Eine leichter interpretierbare Darstellung der Entscheidungsregeln des angepassten Baums in <code>tree_fit$fit</code> erhalten wir mit <code><a href="https://rdrr.io/pkg/rattle/man/fancyRpartPlot.html">rattle::fancyRpartPlot()</a></code>.</p>
<div class="cell">
<div>
<div id="webr-3">

</div>
<script type="webr-3-contents">
eyJjb2RlIjoibGlicmFyeShyYXR0bGUpXG5cbiMgUGxvdCB0aGUgZGVjaXNpb24gdHJlZVxuZmFuY3lScGFydFBsb3QoXG4gIHRyZWVfZml0JGZpdCxcbiAgc3BsaXQuY29sID0gXCJibGFja1wiLCBcbiAgbm4uY29sID0gXCJibGFja1wiLCBcbiAgY2FwdGlvbiA9IFwiVHJhaW5pZXJ0ZXIgRW50c2NoZWlkdW5nc2JhdW0gZsO8ciBjcCA9IDAuMVwiLFxuICBwYWxldHRlID0gXCJTZXQxXCIsXG4gIGJyYW5jaC5jb2wgPSBcImJsYWNrXCIsXG4gIGRpZ2l0cyA9IDNcbikiLCJhdHRyIjp7ImVkaXQiOnRydWUsImZpZy1oZWlnaHQiOjgsImV2YWwiOnRydWUsImZpZy13aWR0aCI6OH19
</script>
</div>
</div>
<p><a href="#fig-rpartregspace" class="quarto-xref">Abbildung&nbsp;<span>15.2</span></a> zeigt Beobachtungen von <code>rm</code> und <code>lstat</code>, die hinsichtlich ihrer in drei Klassen eingeteilten Ausprägung von <code>medv</code> eingefärbt sind. Die durch den CART-Algorithmus gelernten Entscheidungsregeln sind als farbige Paritionen des Regressorraums dargestellt.</p>
<div class="cell page-columns page-full">
<div class="cell-output-display page-columns page-full">
<div id="fig-rpartregspace" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-rpartregspace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="trees_files/figure-html/fig-rpartregspace-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-rpartregspace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Abbildung&nbsp;15.2: Partitionierung des Regressorraums für <code>lstat</code> und <code>rm</code> durch Regressionsbaum
</figcaption></figure>
</div>
</div>
</div>
<p>Für eine datengetriebene Wahl des CP-Parameters <span class="math inline">\(\alpha\)</span> kann Cross Validation (CV) verwendet werden. Hierzu erstellen wir zunächst eine <code>parsnip</code>-Spezifikation mit <code>cost_complexity = tune::tune()</code> in <code>decision_tree()</code> und erstellen einen <em>workflow</em> mit <code>parsnip::workflow()</code></p>
<div class="cell">
<div>
<div id="webr-4">

</div>
<script type="webr-4-contents">
eyJjb2RlIjoiIyBTcGV6aWZpa2F0aW9uIGbDvHIgQ1Ygdm9uIGNvc3RfY29tcGxleGl0eVxudHJlZV9zcGVjX2N2IDwtIGRlY2lzaW9uX3RyZWUoXG4gIGNvc3RfY29tcGxleGl0eSA9IHR1bmUoKVxuICApICU+JVxuICBzZXRfZW5naW5lKFwicnBhcnRcIikgJT4lXG4gIHNldF9tb2RlKFwicmVncmVzc2lvblwiKVxuXG4jIFdvcmtmbG93IGRlZmluaWVyZW5cbnRyZWVfd2ZfY3YgPC0gd29ya2Zsb3coKSAlPiVcbiAgYWRkX21vZGVsKHRyZWVfc3BlY19jdikgJT4lXG4gIGFkZF9mb3JtdWxhKG1lZHYgfiAuKSIsImF0dHIiOnsiZWRpdCI6dHJ1ZSwiZXZhbCI6dHJ1ZSwiZmlnLXdpZHRoIjoiOCJ9fQ==
</script>
</div>
</div>
<p>Mit <code><a href="https://rsample.tidymodels.org/reference/vfold_cv.html">rsample::vfold_cv()</a></code> definieren wir den CV-Prozess: 10-fold CV mit 2 Wiederholungen. <code><a href="https://tune.tidymodels.org/reference/tune_grid.html">tune::tune_grid()</a></code> führt CV anhand des in <code>tree_wf_cv</code> definierten workflows durch. Hierbei werden in <code>cp_grid</code> festgelegte Werte von <code>cost_complexity</code> berücksichtigt. Die mit <code>yardstick::metric_set(rmse)</code> festgelegte Verlustfunktion ist der mittlere quadratische Fehler (RMSE).<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Die hier verwedete Funktion ist <code><a href="https://yardstick.tidymodels.org/reference/rmse.html">yardstick::rmse()</a></code>.</p></div></div><div class="cell">
<div>
<div id="webr-5">

</div>
<script type="webr-5-contents">
eyJjb2RlIjoiIyBDVi1Qcm96ZXNzIGRlZmluaWVyZW5cbmN2X2ZvbGRzIDwtIHZmb2xkX2N2KFxuICBkYXRhID0gQm9zdG9uX3RyYWluLCBcbiAgdiA9IDEwLCBcbiAgcmVwZWF0cyA9IDJcbilcblxuIyBDViBkdXJjaGbDvGhyZW46XG5zZXQuc2VlZCgxMjM0KVxuXG4jIEdyaWQgZGVmaW5pZXJlblxuY3BfZ3JpZCA8LSB0aWJibGUoXG4gIGNvc3RfY29tcGxleGl0eSA9IGMoXG4gICAgMC4xLCAuMDc1LCAwLjA1LCAwLjAxLCAwLjAwMSwgMC4wMDAxXG4gICAgKVxuICApXG5cbiMgVHVuaW5nIG1pdCBDVlxudHJlZV9maXRfY3YgPC0gdHJlZV93Zl9jdiAlPiVcbiAgICB0dW5lX2dyaWQoXG4gICAgICAgIHJlc2FtcGxlcyA9IGN2X2ZvbGRzLCBcbiAgICAgICAgZ3JpZCA9IGNwX2dyaWQsXG4gICAgICAgIG1ldHJpY3MgPSBtZXRyaWNfc2V0KHJtc2UpXG4gICAgKVxuXG4jIENWLUVyZ2Vibmlzc2VcbnRyZWVfZml0X2N2IiwiYXR0ciI6eyJlZGl0Ijp0cnVlLCJldmFsIjp0cnVlLCJmaWctd2lkdGgiOiI4In19
</script>
</div>
</div>
<p>Mit <code><a href="https://ggplot2.tidyverse.org/reference/autoplot.html">workflowsets::autoplot()</a></code> kann der CV-RMSE für als Funktion des CP-Parameter leicht grafisch betrachtet dargestellt werden.</p>
<div class="cell">
<div>
<div id="webr-6">

</div>
<script type="webr-6-contents">
eyJjb2RlIjoiIyBDVi1FcmdlYm5pc3NlIHZpc3VhbGlzaWVyZW5cbmF1dG9wbG90KHRyZWVfZml0X2N2KSArXG4gIGxhYnMoXG4gICAgdGl0bGUgPSBcIkNWIGbDvHIgQ1AtUGFyYW1ldGVyOiBSTVNFIHZzLiBLb21wbGV4aXTDpHRcIlxuICApICtcbiAgdGhlbWVfY293cGxvdCgpIiwiYXR0ciI6eyJlZGl0Ijp0cnVlLCJldmFsIjp0cnVlLCJmaWctd2lkdGgiOiI4In19
</script>
</div>
</div>
<p>Für eine tabellierte Übersicht der besten Modelle kann <code><a href="https://tune.tidymodels.org/reference/show_best.html">tune::show_best()</a></code> verwendet werden. <code><a href="https://tune.tidymodels.org/reference/show_best.html">tune::select_best()</a></code> liest die beste Parameter-Kombination aus.</p>
<div class="cell">
<div>
<div id="webr-7">

</div>
<script type="webr-7-contents">
eyJjb2RlIjoiIyBUYWJlbGxhcmlzY2hlIMOcYmVyc2ljaHRcbnNob3dfYmVzdChcbiAgeCA9IHRyZWVfZml0X2N2LCBcbiAgbWV0cmljID0gXCJybXNlXCJcbilcblxuIyBHZXR1bnRlciBQYXJlbWF0ZXJcbmJlc3RfdHJlZV9maXQgPC0gc2VsZWN0X2Jlc3QoXG4gIHggPSB0cmVlX2ZpdF9jdiwgXG4gIG1ldHJpYyA9IFwicm1zZVwiXG4pXG5cbmJlc3RfdHJlZV9maXQiLCJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWUsImZpZy13aWR0aCI6IjgifX0=
</script>
</div>
</div>
<p>Anhand <code>tree_fit_cv</code> trainieren wir die finale Spezifikation.</p>
<div class="cell">
<div>
<div id="webr-8">

</div>
<script type="webr-8-contents">
eyJjb2RlIjoiIyBGaW5hbGVzIE1vZGVsbCBzY2jDpHR6ZW5cbmZpbmFsX3RyZWVfc3BlYyA8LSBkZWNpc2lvbl90cmVlKFxuICBjb3N0X2NvbXBsZXhpdHkgPSBiZXN0X3RyZWVfZml0JGNvc3RfY29tcGxleGl0eVxuICApICU+JVxuICBzZXRfZW5naW5lKFwicnBhcnRcIikgJT4lXG4gIHNldF9tb2RlKFwicmVncmVzc2lvblwiKVxuXG5maW5hbF90cmVlX2ZpdCA8LSBmaW5hbF90cmVlX3NwZWMgJT4lXG4gIGZpdChcbiAgICBmb3JtdWxhID0gbWVkdiB+IC4sIFxuICAgIGRhdGEgPSBCb3N0b25fdHJhaW5cbiAgKVxuXG4jIGZpbmFsX3RyZWVfZml0IiwiYXR0ciI6eyJlZGl0Ijp0cnVlLCJldmFsIjp0cnVlLCJmaWctd2lkdGgiOiI4In19
</script>
</div>
</div>
<p>Der geringe CP-Parameter führt zu einem großen Entscheidungsbaum.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Die Dimension der Grafik wurde hier zwecks Darstellung des gesamten Baums gewählt. <code>print(final_tree_fit$fit)</code> druckt die Entscheidungsregeln in die R-Konsole (hierzu die letzte Zeile ausführen).</p></div></div><div class="cell">
<div>
<div id="webr-9">

</div>
<script type="webr-9-contents">
eyJjb2RlIjoiXG4jIENWLUZpdCBwbG90dGVuXG5mYW5jeVJwYXJ0UGxvdChcbiAgZmluYWxfdHJlZV9maXQkZml0LFxuICBzcGxpdC5jb2wgPSBcImJsYWNrXCIsIFxuICBubi5jb2wgPSBcImJsYWNrXCIsIFxuICBjYXB0aW9uID0gXCJNaXQgQ1YgZXJtaXR0ZWx0ZXIgRW50c2NoZWlkdW5nc2JhdW1cIixcbiAgcGFsZXR0ZSA9IFwiU2V0MVwiLFxuICBicmFuY2guY29sID0gXCJibGFja1wiXG4pIiwiYXR0ciI6eyJlZGl0Ijp0cnVlLCJmaWctaGVpZ2h0Ijo4LCJldmFsIjp0cnVlLCJmaWctd2lkdGgiOjh9fQ==
</script>
</div>
</div>
<p>Zur Beurteilung der Relevanz von Variablen für die Reduktion des Anpassungsfehlers (<em>variable importance</em>) kann der Eintrag <code>variable.importance</code> des <code>rpart</code>-Objekts herangezogen werden. Variable importance misst hier die Gesamtreduktion der Fehlerquadratsumem über alle Knoten, an denen die jeweilige Variable für Splits verwendet wird.</p>
<div class="cell">
<div>
<div id="webr-10">

</div>
<script type="webr-10-contents">
eyJjb2RlIjoiIyBWYXJpYWJsZS1JbXBvcnRhbmNlIGF1c2xlc2VuXG5maW5hbF90cmVlX2ZpdCRmaXQkdmFyaWFibGUuaW1wb3J0YW5jZSIsImF0dHIiOnsiZWRpdCI6dHJ1ZSwiZXZhbCI6dHJ1ZSwiZmlnLXdpZHRoIjoiOCJ9fQ==
</script>
</div>
</div>
<p>Die Werte von Variable Importance zeigen, dass der mit CV ermittelte Baum <em>alle</em> Regressoren in <code>boston_train</code> für Splits nutzt, wobei <code>lstat</code> und <code>rm</code> die relevantesten Variablen sind.</p>
<p>Anhand von Vorhersagen für <code>medv</code> mit dem Test-Datensatz <code>boston_test</code> können wir das naive Baum-Modell <code>tree_fit</code> mit dem durch CV ermittelten Modell <code>tree_fit_cv</code> hinsichtlich des Vorhersagefehlers für ungesehene Beobachtungen vergleich. <code>yardstick::metric()</code> berechnet hierzu automatisch gängige Statistiken für Regressionsprobleme.</p>
<div class="cell">
<div>
<div id="webr-11">

</div>
<script type="webr-11-contents">
eyJjb2RlIjoiIyBWb3JoZXJzYWdlZ8O8dGUgbmFpdmVzIE1vZGVsbFxudHJlZV9wcmVkIDwtIHByZWRpY3QoXG4gIG9iamVjdCA9IHRyZWVfZml0LCBcbiAgbmV3X2RhdGEgPSBCb3N0b25fdGVzdFxuKSAlPiVcbiAgYmluZF9jb2xzKEJvc3Rvbl90ZXN0KSAlPiVcbiAgbWV0cmljcyh0cnV0aCA9IG1lZHYsIGVzdGltYXRlID0gLnByZWQpXG5cbiMgVm9yaGVyc2FnZWfDvHRlIGJlaSBDVlxudHJlZV9wcmVkX2N2IDwtIHByZWRpY3QoXG4gIG9iamVjdCA9IGZpbmFsX3RyZWVfZml0LCBcbiAgbmV3X2RhdGEgPSBCb3N0b25fdGVzdFxuKSAlPiVcbiAgYmluZF9jb2xzKEJvc3Rvbl90ZXN0KSAlPiVcbiAgbWV0cmljcyh0cnV0aCA9IG1lZHYsIGVzdGltYXRlID0gLnByZWQpXG5cbnRyZWVfcHJlZFxudHJlZV9wcmVkX2N2IiwiYXR0ciI6eyJlZGl0Ijp0cnVlLCJldmFsIjp0cnVlLCJmaWctd2lkdGgiOiI4In19
</script>
</div>
</div>
<p>Der Vergleich zeigt eine bessere Vorsageleistung des großen Baums in <code>tree_fit_cv</code>. In diesem Fall scheint CP-Pruning wenig hilfreich zu sein. Tatsächlich liefert ein Baum mit <span class="math inline">\(\alpha=0\)</span> bessere Vorhersagen als <code>tree_fit_cv</code> (überprüfe dies!).</p>
</section><section id="bagging" class="level2" data-number="15.3"><h2 data-number="15.3" class="anchored" data-anchor-id="bagging">
<span class="header-section-number">15.3</span> Bagging</h2>
<p><em>Bagging</em> ist eine Ensemble-Modelle, die durch aus einer Kombination von vielen Entscheidungsbäumen bestehen. Bagging steht für <em>Bootstrap Aggregating</em> und nutzt einen Algorithmus, bei dem Bäume auf <em>zufälligen</em> Stichproben aus dem Trainingsdatensatz angepasst werden: Jeder Baum wird auf einer <em>Bootstrap-Stichprobe</em> (siehe <a href="Simulation.html" class="quarto-xref"><span>Kapitel 5</span></a>) trainiert, die durch zufällige Züge (mit Zurücklegen) erstellt wird. Nach dem Training aggregiert Bagging die Vorhersagen aller Bäume des Ensembles.</p>
<p>Der Vorteil von Bagging gegenüber einem einzelnen Entscheidungsbaum ist, dass die Varianz der Vorhersage deutlich reduziert werden kann: Einzelne Entscheidungsbäume neigen dazu, Muster in den Trainingsdaten zu lernen, die sich zufällig aus der Zusammensetzung der Stichprobe ergeben und nicht repräsentativ für Zusammenhänge zwischen den Regressoren und der Outcome-Variable sind. Diese Überanpassung führt zu hoher Varianz auf von Vorhersagen für ungesehene Daten. Durch das Training vieler Bäume auf unterschiedlichen <em>zufälligen</em> Stichproben aus den Trainingsdaten und das anschließende Aggregieren kann der negative Effekt der Überanpassung auf die Unsicherheit der Vorhersage einzelner Bäume reduziert werden.</p>
<p>Eine Bagging-Spezifikation kann mit <code><a href="https://parsnip.tidymodels.org/reference/bag_tree.html">parsnip::bag_tree()</a></code> festgelegt werden. Mit <code>times = 500</code> wird definiert, dass der Bagging-Algorithmus ein Ensemble mit 500 Bäumen (mit CART) anpassen soll. Das Training und die Vorhersage auf den Testdaten erfolgt analog zur Vorgehensweise in <a href="#sec-simpletrees" class="quarto-xref"><span>Kapitel 15.1</span></a>.</p>
<div class="cell">
<div>
<div id="webr-12">

</div>
<script type="webr-12-contents">
eyJjb2RlIjoiIyBTcGV6aWZpa2F0aW9uIGbDvHIgQmFnZ2luZ1xuYmFnZ2luZ19zcGVjIDwtIGJhZ190cmVlKCkgJT4lXG4gIHNldF9lbmdpbmUoXG4gICAgZW5naW5lID0gXCJycGFydFwiLFxuICAgIHRpbWVzID0gNTAwXG4gICkgJT4lXG4gIHNldF9tb2RlKFwicmVncmVzc2lvblwiKVxuXG5cbiMgVHJhaW5pbmcgZHVyY2hmw7xocmVuXG5zZXQuc2VlZCgxMjM0KVxuXG5iYWdnaW5nX2ZpdCA8LSBiYWdnaW5nX3NwZWMgJT4lXG4gIGZpdChcbiAgICBmb3JtdWxhID0gbWVkdiB+IC4sIFxuICAgIGRhdGEgPSBCb3N0b25fdHJhaW5cbiAgKVxuXG4jIEF1c3dlcnR1bmdcbmJhZ2dpbmdfcHJlZCA8LSBwcmVkaWN0KFxuICBvYmplY3QgPSBiYWdnaW5nX2ZpdCwgXG4gIG5ld19kYXRhID0gQm9zdG9uX3Rlc3RcbiAgKSAlPiVcbiAgYmluZF9jb2xzKEJvc3Rvbl90ZXN0KSAlPiVcbiAgbWV0cmljcyhcbiAgICB0cnV0aCA9IG1lZHYsXG4gICAgZXN0aW1hdGUgPSAucHJlZFxuICApXG5cbmJhZ2dpbmdfcHJlZCIsImF0dHIiOnsiZWRpdCI6dHJ1ZSwiZXZhbCI6dHJ1ZSwiZmlnLXdpZHRoIjoiOCJ9fQ==
</script>
</div>
</div>
<p>Die Auswertung auf den Testdatensatz ergibt eine deutliche Verbesserung der Vorhersageleistung gegenüber einem einfachen Regressionsbaum.</p>
<p>Obwohl die Bäume beim Bagging auf unterschiedlichen Stichproben trainiert werden, kann innerhalb des Ensembles dennoch eine deutliche Korrelation vorliegen: Da jeder Baum auf alle Regressoren für Splits zugreift, können trotz Bootstrapping ähnliche (unverteilhafte) Muster aus dem Datensatz erlernt werden, was sich nachteilig auf die Generalisierungsfähigkeit auswirken kann. Diese Korrelation mindert die Effektivität von Bagging, da stark korrelierte Bäume dazu neigen, ähnliche Fehler zu machen.</p>
</section><section id="sec-brf" class="level2 page-columns page-full" data-number="15.4"><h2 data-number="15.4" class="anchored" data-anchor-id="sec-brf">
<span class="header-section-number">15.4</span> Random Forests</h2>
<p><em>Random Forests</em> erweitern Bagging, indem zusätzlich bei jedem Knoten innerhalb jedes Baumes eine <em>zufällige Teilmenge der Regressoren</em> als potentielle Variable für die Split-Regel ausgewählt wird. Dies führt zu einer Reduktion der Korrelation zwischen den Bäumen, was die Genauigkeit verbessert und das Risiko von Overfitting weiter verringert.</p>
<p>In R erstellen wir die Spezifikation mit <code><a href="https://parsnip.tidymodels.org/reference/rand_forest.html">parsnip::rand_forest()</a></code>. Der Parameter <code>mtry</code> legt fest, wie viele Regressoren <span class="math inline">\(m\)</span> zufällig für jeden Split zur Verfügung stehen. Wir nutzen den im <code>randomForest</code>-Paket implementierten Algorithmus und legen in <code>set_engine()</code> fest, dass die von <code><a href="https://rdrr.io/pkg/randomForest/man/randomForest.html">randomForest::randomForest()</a></code> berechnete Fehler-Metrik im Output-Objekt ausgegeben wird (<code>tree.err = TRUE</code>). Um die Spezifikation für verschiedene Werte von <code>mtry</code> anwenden zu können, implementieren wir die Spezifikation innerhalb einer Wrapper-Funktion <code>rf_spec_mtry()</code>. Mit <code><a href="https://purrr.tidyverse.org/reference/map.html">purrr::map()</a></code> iterieren wir <code>rf_spec_mtry()</code> über drei verschiedene Werte für den Tuning-Parameter <code>mtry</code> (4, 6 und 10 Variablen).<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;Eine Faustregel für die Wahl von <span class="math inline">\(m\)</span> bei <span class="math inline">\(k\)</span> verfügbaren Regressoren ist <span class="math inline">\(m\approx\sqrt{k}\)</span>.</p></div></div><div class="cell">
<div>
<div id="webr-13">

</div>
<script type="webr-13-contents">
eyJjb2RlIjoic2V0LnNlZWQoMTIzNClcblxuIyBXZXJ0ZSBmw7xyIG10cnlcbm10cnlfdmFsdWVzIDwtIGMoNCwgNiwgMTApXG5cbiMgRnVua3Rpb246IFJhbmRvbSBGb3Jlc3QgZsO8ciBtdHJ5ID0gbVxucmZfc3BlY19tdHJ5IDwtIGZ1bmN0aW9uKG0pIHtcbiAgcmFuZF9mb3Jlc3QobXRyeSA9IG0sIHRyZWVzID0gNTAwKSAlPiVcbiAgICBzZXRfZW5naW5lKFxuICAgICAgZW5naW5lID0gXCJyYW5kb21Gb3Jlc3RcIiwgXG4gICAgICB0cmVlLmVyciA9IFRSVUVcbiAgICApICU+JVxuICAgIHNldF9tb2RlKFwicmVncmVzc2lvblwiKVxufVxuXG4jIE1vZGVsbGUgZsO8ciB2ZXJzY2hpZWRlbmUgbXRyeS1XZXJ0ZSB0cmFpbmllcmVuXG5yZl9maXRzIDwtIG1hcChcbiAgLnggPSBtdHJ5X3ZhbHVlcywgXG4gIC5mID0gfiByZl9zcGVjX210cnkoLngpICU+JVxuICAgIGZpdChcbiAgICAgIGZvcm11bGEgPSBtZWR2IH4gLiwgXG4gICAgICBkYXRhID0gQm9zdG9uX3RyYWluXG4gICAgKVxuKVxuXG5yZl9maXRzIDwtIHNldF9uYW1lcyhcbiAgeCA9IHJmX2ZpdHMsXG4gIG5tID0gIHBhc3RlMChcInJmX210cnlcIiwgbXRyeV92YWx1ZXMsIFwiX2ZpdFwiKVxuKVxuXG4jIEF1c2dhYmUgZGVyIEVyZ2Vibmlzc2VcbnJmX2ZpdHMiLCJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWUsImZpZy13aWR0aCI6IjgifX0=
</script>
</div>
</div>
<p>Für eine Beurteilung des Vorhersageleistung dieser drei Modelle können wir den <em>Out-of-Bag</em>-Fehler (OOB) verwenden:</p>
<p>Der OOB-Fehler ist eine Schätzung des Generalisierungsfehlers ohne einen separaten Testdatensatzes. Bei Random Forests (und Bagging) ist dies aufgrund der Berechnung des Ensembles für Bootstrap-Stichproben möglich: Grob ein Drittel der Beobachtungen des Datensatzes sind nicht Teil der Stichprobe, die für das Training jedes Baums im Ensemble genereiert werden.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> Diese nicht gezogenen Datenpunkte sind OOB-Beobachtungen. Der OOB-Fehler des Ensembles ist der durchschnittliche Fehler für die aggregierten Vorhersagen der Bäume des Forests.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;Beachte, dass beim Bootstrap <span class="math inline">\(n\)</span> aus <span class="math inline">\(n\)</span> Beobachtungen mit Zurücklegen gezogen werden. Die Wahrscheinlicht, dass eine Beobachtung <em>nicht</em> gezogen wird (“Out-of-Bag”), ist <span class="math inline">\((1-1/n)^n\approx37\%\)</span>.</p></div></div><p>Der OOB-Fehler kann auch verwendet werden, um die erforderliche Größe des Random Forests zu beurteilen: Eine größere Anzahl von Bäumen reduziert tendenziell die Varianz der Vorhersagen und verbessert die Generalisierungsfähigkeit. Allerdings nimmt dieser Effekt ab, und ab einer bestimmten Baumanzahl sind weitere Verbesserungen marginal. Obwohl das Risiko von Überanpassung durch viele Bäume aufgrund des Bagging minimal ist, kann es bei großen Datensätzen sinnvoll sein, kleinere Wälder zu trainieren, um den Rechenaufwand zu verringern. Wir plotten hierfür den OOB-Fehler für das Modell mit <code>mtry = 10</code> gegen die Anzahl der Bäume.</p>
<div class="cell">
<div>
<div id="webr-14">

</div>
<script type="webr-14-contents">
eyJjb2RlIjoibGlicmFyeShnZ1JhbmRvbUZvcmVzdHMpXG5cbiMgT09CLUZlaGxlciBhbHMgRnVua3Rpb24gZGVyIEJhdW1hbnphaGxcbnJmX2ZpdHMkcmZfbXRyeTEwX2ZpdCRmaXQgJT4lIFxuICBnZ19lcnJvcigpICU+JSBcbiAgXG4gIHBsb3QoKSArIFxuICBsYWJzKFxuICAgIHRpdGxlID0gXCJSYW5kb20gRm9yZXN0OiBFbnNlbWJsZWdyw7bDn2UgdnMuIE9PQi1GZWhsZXIgKG10cnkgPSAxMClcIlxuICApICsgXG4gIHRoZW1lX2Nvd3Bsb3QoKSIsImF0dHIiOnsiZWRpdCI6dHJ1ZSwiZXZhbCI6dHJ1ZSwiZmlnLXdpZHRoIjoiOCJ9fQ==
</script>
</div>
</div>
<p>Die Grafik zeigt, dass die Verbesserung des OOB-Fehlers jenseits von 250 Beobachtungen deutlich nachlässt, sodass ein Training von 500 Bäumen ausreichend scheint.</p>
<p>Zur Beurteiliung der Vorhersagegüte mit dem Testdatensatz gehen wir analog zum Training vor und iterieren mit <code><a href="https://purrr.tidyverse.org/reference/map.html">map()</a></code> über <code>rf_fits</code>, die Liste der angepassten Modelle.</p>
<div class="cell">
<div>
<div id="webr-15">

</div>
<script type="webr-15-contents">
eyJjb2RlIjoiIyBWb3JoZXJzYWdlIHVuZCBCZXJlY2hudW5nIHYuIE1ldHJpa2VuIGbDvHIgamVkZW4gUkZcbnJmX3ByZWRpY3Rpb25zIDwtIG1hcChcbiAgLnggPSByZl9maXRzLCBcbiAgLmYgPSAgfiBwcmVkaWN0KC54LCBCb3N0b25fdGVzdCkgJT4lXG4gICAgYmluZF9jb2xzKEJvc3Rvbl90ZXN0KSAlPiVcbiAgICBtZXRyaWNzKFxuICAgICAgdHJ1dGggPSBtZWR2LCBcbiAgICAgIGVzdGltYXRlID0gLnByZWRcbiAgICApXG4pXG5cbiMgRWludHLDpGdlIGJlbmVubmVuXG5yZl9wcmVkaWN0aW9ucyA8LSBzZXRfbmFtZXMoXG4gIHggPSByZl9wcmVkaWN0aW9ucyxcbiAgbm0gPSAgcGFzdGUwKFwicmZfbXRyeVwiLCBtdHJ5X3ZhbHVlcywgXCJfcHJlZFwiKVxuKVxuXG5yZl9wcmVkaWN0aW9ucyIsImF0dHIiOnsiZWRpdCI6dHJ1ZSwiZXZhbCI6dHJ1ZSwiZmlnLXdpZHRoIjoiOCJ9fQ==
</script>
</div>
</div>
<p>Ähnlich wie für einen einzelnen Baum kann die Relevanz von Variablen anhand der Reduktion der Loss-Funktion durch das Ensemble beurteilt werden. Für einen einfachen Vergleich der Variable Importance für den Random Forests mit <code>mtry = 10</code> in <code>rf_fits$rf_mtry10_fit</code> nutzen wir <code><a href="https://rdrr.io/pkg/ggRandomForests/man/gg_vimp.html">ggRandomForests::gg_vimp()</a></code>.</p>
<div class="cell">
<div>
<div id="webr-16">

</div>
<script type="webr-16-contents">
eyJjb2RlIjoiIyBWYXJpYWJsZSBpbXBvcnRhbmNlIGbDvHIgbXRyeSA9IDEwXG5yZl9maXRzJHJmX210cnkxMF9maXQkZml0ICU+JVxuICBnZ192aW1wKCkgICU+JVxuICBwbG90KCkgK1xuICAgIGxhYnMoXG4gICAgICB0aXRsZSA9IFwiVmFyaWFibGUgSW1wb3J0YW5jZSBmw7xyIFJhbmRvbSBGb3Jlc3QgKG10cnkgPSAxMClcIlxuICAgICkgK1xuICAgIHRoZW1lX2Nvd3Bsb3QoKSIsImF0dHIiOnsiZWRpdCI6dHJ1ZSwiZXZhbCI6dHJ1ZSwiZmlnLXdpZHRoIjoiOCJ9fQ==
</script>
</div>
</div>
<p>Die Grafik bestärkt unsere Schlussfolgerung aus der Analyse des (mit CART trainierten) einzelnen Entscheidungsbaums in <a href="#sec-simpletrees" class="quarto-xref"><span>Kapitel 15.1</span></a>, dass <code>rm</code> und <code>lstat</code> die wichtigsten Regressoren für die Vorhersage von <code>medv</code> sind.</p>
</section><section id="sec-boosting" class="level2 page-columns page-full" data-number="15.5"><h2 data-number="15.5" class="anchored" data-anchor-id="sec-boosting">
<span class="header-section-number">15.5</span> Boosting</h2>
<p>Boosting ist eine leistungsstarke Ensemble-Methode für Vorhersagen, die kleine Modelle (oft Entscheidungsbäume geringer Tiefe) sukzessiv trainiert und zu einem starken Modell kombiniert. Anders als bei Random Forests, bei denen viele Bäume unabhängig voneinander auf zufälligen Stichproben der Daten trainiert werden, geht ein Boosting-Algorithmuss sequentiell vor: Jeder nachfolgende Baum wird darauf optimiert, die Fehler des vorherigen Modells zu reduzieren. Die Idee hierbei ist es, iterativ “schwache” Modelle zu erzeugen, die eine gute Anpassung für Datenpunkte liefern, die in den vorherigen Durchläufen schlecht vorhergesagt wurden.</p>
<p>Für einen Trainingsdatensatz <span class="math inline">\(\{(x_i, y_i)\}_{i=1}^n\)</span>, wobei <span class="math inline">\(x_i\)</span> die Input-Features und <span class="math inline">\(y_i\)</span> Beobachtungen des Outcomes sind, kann Boosting wiefolgt durchgeführt werden.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Boosting für Regression
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><p><strong>Initialisierung</strong>: Initialisiere das Boosting-Modell als <span class="math inline">\(\widehat{F}_0(x)\)</span>. Setze die Residuen <span class="math inline">\(r^0_i=y_i\)</span> für alle <span class="math inline">\(i\)</span></p></li>
<li>
<p><strong>Iteration</strong>: Wiederhole die folgenden Schritte für <span class="math inline">\(b = 1,2,\dots,B\)</span> mit <span class="math inline">\(B\)</span> hinreichend groß:</p>
<p>2.1 <strong>Base Learner</strong>: Trainiere Baum <span class="math inline">\(T_b\)</span> mit <span class="math inline">\(\{(\boldsymbol{x}_i, r^{b-1}_i)\}_{i=1}^n\)</span> für die Vorhersage des <em>Fehlers</em> der vorherigen Iteration <span class="math inline">\(r^{b-1}\)</span>.</p>
<p>2.2 <strong>Aktualisierung</strong>: Aktualisiere das Boosting-Modell,</p>
<p><span class="math display">\[\begin{align*}
   \widehat{F}_{b}(\boldsymbol{x}) = \widehat{F}_{b-1}(\boldsymbol{x}) + \eta \cdot T_{b}(\boldsymbol{x}),
   \end{align*}\]</span></p>
<p>wobei <span class="math inline">\(\eta\)</span> die (oft klein gewählte) <em>Lernrate</em> ist.</p>
<p>2.3 <strong>Fehlerberechnung</strong>: Berechne die Residuen <span class="math inline">\(r^b_i\)</span> als Differenzen zwischen dem tatsächlichen Werten <span class="math inline">\(y_i\)</span> und den Vorhersage des aktuellen Modells <span class="math inline">\(\widehat{F}_m(\boldsymbol{x}_i)\)</span>,</p>
<p><span class="math display">\[\begin{align*}
   r^b_i = y_i - \widehat{F}_b(\boldsymbol{x}_i).
   \end{align*}\]</span></p>
</li>
<li>
<p><strong>Output</strong>: Gib das finale Modell aus:</p>
<p><span class="math display">\[\begin{align*}
   \widehat{F}(\boldsymbol{x}) := \sum_{b=1}^B \eta\cdot \widehat{F}^b(\boldsymbol{x})
\end{align*}\]</span></p>
</li>
</ol>
</div>
</div>
<p>Der Parameter <span class="math inline">\(0\leq\eta\leq0\)</span> steuert, wie stark der Einfluss jedes neuen Baumes auf das Modell ist. Eine kleine Lernrate führt dazu, dass viele Bäume benötigt werden, was Vorhersagen (ähnlich wie bei Bagging) stabiler macht. Beachte die sequentielle Natur des Trainings: Die <span class="math inline">\(r^b_i\)</span> in Schritt 2.3 sind die zu vorhersagenden Outcome-Variable für den nächsten Baum. <span class="math inline">\(T_{b+1}\)</span> wird trainiert wird, um den <em>Fehler des bisherigen Modells</em> <span class="math inline">\(\widehat{F}_b\)</span> zu erklären.</p>
<p>Für die Anwendung auf <code><a href="https://rdrr.io/pkg/MASS/man/Boston.html">MASS::Boston</a></code> in R nutzen wir den im Paket <code>gbm</code> implementierten <em>Gradient-Boosting</em>-Algorithmus. Bei Gradient Boosting wird jeder Baum so trainiert, dass er den negativen Gradienten einer Verlustfunktion approximiert, also die Richtung des größten Fehlers. Das Modell wird schrittweise verbessert, indem es entlang des Gradienten aktualisiert wird, um die Vorhersagegüe zu optimieren; siehe <span class="citation" data-cites="Hastieetal2013">Hastie, Tibshirani, und Friedman (<a href="Literatur.html#ref-Hastieetal2013" role="doc-biblioref">2013</a>)</span> für eine detaillierte Erläuterung.</p>
<p>Mit dem nachfolgenden Code-Chunk trainieren wir ein Boosting-Modell für Regression mit 5000 einfachen Bäumen (<code>n.trees = 5000</code>) mit einer maximalen Tiefe von 2 (<code>interaction.depth = 2</code>), d.h. es folgen maximal 2 Entscheidungs-Regeln nacheinander. Um das Risiko von Overfitting gering zu halten, erlauben wir nur Splits, die zu mindestens zwei Beobachtungen in resultierenden nodes führen (<code>n.minobsinnode = 2</code>). Die Lernrate (Beitrag der Base Learner zum Ensemble) wird typischerweise klein (und in Abhängigkeit von <code>n.trees</code>) gewählt (<code>shrinkage = 0.001</code>).<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;Je kleiner die Lernrate, desto größer sollte <code>n.trees</code> gewählt werden.</p></div></div><div class="cell">
<div>
<div id="webr-17">

</div>
<script type="webr-17-contents">
eyJjb2RlIjoic2V0LnNlZWQoMTIzNClcblxuIyBHcmFkaWVudCBCb29zdGluZyBkdXJjaGbDvGhyZW5cbmdibV9tb2RlbCA8LSBnYm0oXG4gIGZvcm11bGEgPSBtZWR2IH4gLiwgXG4gIGRhdGEgPSBCb3N0b25fdHJhaW4sIFxuICBkaXN0cmlidXRpb24gPSBcImdhdXNzaWFuXCIsICMgZsO8ciBSZWdyZXNzaW9uXG4gIG4udHJlZXMgPSA1MDAwLCAgICAgICAgICAgIyBBbnouIELDpHVtZVxuICBpbnRlcmFjdGlvbi5kZXB0aCA9IDIsICAgICAjIE1heGltYWxlIFRpZWZlIGRlciBiYXNlIGxlYXJuZXJcbiAgc2hyaW5rYWdlID0gMC4wMSwgICAgICAgICAjIExlcm5yYXRlXG4gIG4ubWlub2JzaW5ub2RlID0gMiAgICAgICAgICMgTWluLiBCZW9iYWNodHVuZ2VuIGluIG5vZGVzXG4pXG5cbmdibV9tb2RlbCAiLCJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWUsImZpZy13aWR0aCI6IjgifX0=
</script>
</div>
</div>
<p>Für die Vorhersagen auf dem Test-Datensatz legen wir mit <code>n.trees = gbm_model$n.trees</code> fest, dass das gesamte Ensemble genutzt werden soll.</p>
<div class="cell">
<div>
<div id="webr-18">

</div>
<script type="webr-18-contents">
eyJjb2RlIjoiIyBWb3JoZXJzYWdlbiBUZXN0LURhdGVuc2F0elxuZ2JtX3ByZWRpY3Rpb25zIDwtIHByZWRpY3QoXG4gIG9iamVjdCA9IGdibV9tb2RlbCwgXG4gIG5ld2RhdGEgPSBCb3N0b25fdGVzdCwgXG4gIG4udHJlZXMgPSBnYm1fbW9kZWwkbi50cmVlcyAjIGdlc2FtdGVzIEVuc2VtYmxlIG51dHplblxuKVxuXG4jIEF1c3dlcnR1bmcgVGVzdC1EYXRlbnNhdHpcbnJlc3VsdHMgPC0gQm9zdG9uX3Rlc3QgJT4lXG4gIG11dGF0ZShwcmVkaWN0aW9ucyA9IGdibV9wcmVkaWN0aW9ucykgJT4lXG4gIG1ldHJpY3MoXG4gICAgdHJ1dGggPSBtZWR2LCBcbiAgICBlc3RpbWF0ZSA9IHByZWRpY3Rpb25zXG4gIClcblxucmVzdWx0cyIsImF0dHIiOnsiZWRpdCI6dHJ1ZSwiZXZhbCI6dHJ1ZSwiZmlnLXdpZHRoIjoiOCJ9fQ==
</script>
</div>
</div>
<p>Die Ergebnisse zeigen, dass Gradient Boosting bereits für die naive Parameterwahl im Aufruf von <code><a href="https://rdrr.io/pkg/gbm/man/gbm.html">gbm::gbm()</a></code> zu einer Verbesserung der Vorhersageleistung gegenüber den Random-Forest-Modellen führt.</p>
<p>Anstatt <code>n.trees = 5000</code> können wir <code>n.trees</code> in <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> einen Vektor mit verschiedenen Ensemble-Größen übergeben. Für <code>n.trees = 5000</code> erhalten wir Vorhersagen für jeden Status, den das Boosting-Modell im Training nach seiner Initialisierung bis zu der in <code><a href="https://rdrr.io/pkg/gbm/man/gbm.html">gbm::gbm()</a></code> festgelgten Größe durchläuft. Anhand dieser Vorhersagen können wir die Generalisierungsfähigkeit des Modells in Abhängigkeit der gewählten Lernrate und der Größe beurteilen, in dem wir den RMSE für den gesamten Trainingsprozess berechnen. Für eine leichtere Interpretation erzeugen wir eine Grafik ählich wie bei der OOB-Analyse des Random-Forest-Modells.</p>
<div class="cell">
<div>
<div id="webr-19">

</div>
<script type="webr-19-contents">
eyJjb2RlIjoiIyBWb3JoZXJzYWdlbiBzdWt6ZXNzaXYgdHJlZmZlblxucHJlZGljdChcbiAgICBvYmplY3QgPSBnYm1fbW9kZWwsIFxuICAgIG5ld2RhdGEgPSBCb3N0b25fdGVzdCwgXG4gICAgbi50cmVlcyA9IDE6NTAwMFxuKSAlPiVcbiAgICBcbiAgICMgVGVzdHNldC1STVNFIGJlcmVjaG5lblxuICAgIGFzX3RpYmJsZSgpICU+JVxuICAgIG1hcF9kYmwoXG4gICAgICAuZiA9IH4gc3FydChtZWFuKCgueCAtIEJvc3Rvbl90ZXN0JG1lZHYpXjIpKVxuICAgICkgJT4lXG4gICAgYmluZF9jb2xzKHJtc2UgPSAuLCB0cmVlcyA9IDE6NTAwMCkgJT4lXG4gIFxuICAjIFBsb3R0ZW5cbiAgZ2dwbG90KG1hcHBpbmcgPSBhZXMoeCA9IHRyZWVzLCB5ID0gcm1zZSkpICtcbiAgICBnZW9tX2xpbmUoKSArXG4gICAgbGFicyhcbiAgICAgIHRpdGxlID0gXCJCb29zdGluZzogVGVzdHNldC1STVNFIGFscyBGdW5rdGlvbiB2b24gbi50cmVlc1wiXG4gICAgKSArXG4gICAgdGhlbWVfY293cGxvdCgpIiwiYXR0ciI6eyJlZGl0Ijp0cnVlLCJldmFsIjp0cnVlLCJmaWctd2lkdGgiOiI4In19
</script>
</div>
</div>
<p>Die Grafik zeigt eine schnelle Verbesserung des Out-of-sample-Fehlers mit der Größe des Ensembles. Für die gewählte Lernrate scheinen 5000 Bäume adäquat zu sein.</p>
<p>Analog zu Bagging und Random Forests können wir die Relevanz der Regressoren in <code>Boston</code> für die Vorhersage von <code>medv</code> anhand der mit <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> berechneten (relativen) Variable Importance für die Anpassung auf den Trainingsdatensatz einschätzen.</p>
<div class="cell">
<div>
<div id="webr-20">

</div>
<script type="webr-20-contents">
eyJjb2RlIjoiIyBWYXJpYWJsZSBJbXBvcnRhbmNlIGJlcmVjaG5lblxudmFyX2ltcG9ydGFuY2UgPC0gc3VtbWFyeShcbiAgb2JqZWN0ID0gZ2JtX21vZGVsLCBcbiAgcGxvdGl0ID0gRkFMU0UgIyBrLiBncmFwaGlzY2hlIEF1c2dhYmVcbilcblxuIyAuLi4gdW5kIHBsb3R0ZW5cbnZhcl9pbXBvcnRhbmNlIDwtIHZhcl9pbXBvcnRhbmNlICU+JVxuICBhc190aWJibGUoKSAlPiVcbiAgYXJyYW5nZShcbiAgICBkZXNjKHJlbC5pbmYpXG4gIClcblxuZ2dwbG90KFxuICBkYXRhID0gdmFyX2ltcG9ydGFuY2UsXG4gIG1hcHBpbmcgPSBhZXMoXG4gICAgeCA9IHJlb3JkZXIodmFyLCByZWwuaW5mKSwgXG4gICAgeSA9IHJlbC5pbmZcbiAgKVxuKSArXG4gIGdlb21fYmFyKHN0YXQgPSBcImlkZW50aXR5XCIpICtcbiAgY29vcmRfZmxpcCgpICtcbiAgbGFicyhcbiAgICB0aXRsZSA9IFwiVmFyaWFibGUgSW1wb3J0YW5jZSBmw7xyIEdyYWRpZW50IEJvb3N0aW5nXCIsXG4gICAgeCA9IFwiVmFyaWFibGVcIixcbiAgICB5ID0gXCJSZWxhdGl2ZXIgRWluZmx1c3MgKCUpXCJcbiAgKSArXG4gIHRoZW1lX2Nvd3Bsb3QoKSIsImF0dHIiOnsiZWRpdCI6dHJ1ZSwiZXZhbCI6dHJ1ZSwiZmlnLXdpZHRoIjoiOCJ9fQ==
</script>
</div>
</div>
<p>Obwohl erneut <code>lstat</code> und <code>rm</code> als die wichtigsten Prädiktoren gelistet sind, identifiziert Gradient Boosting im Gegensatz zu Bagging und Random Forests <code>lstat</code> als die Variable mit der größten Vorhersagekraft für <code>medv</code>.</p>
</section><section id="causal-trees-und-causal-forests" class="level2 page-columns page-full" data-number="15.6"><h2 data-number="15.6" class="anchored" data-anchor-id="causal-trees-und-causal-forests">
<span class="header-section-number">15.6</span> Causal Trees und Causal Forests<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>
</h2>
<p>Baum-Algorithmen sind vielversprechende Ansätze zur Schätzung kausaler Effekte, insbesondere in Situationen, in denen die Bestimmung heterogener Effekte gewünscht ist: Der Vorteil von Baum-Methoden liegt darin, dass sie nicht-parametrisch sind: Der Regressorraum wird adaptiv in Partitionen unterteilt, um auf Basis dieser Aufteilung differenzierte Vorhersagen für die Zielvariable zu treffen. Diese Eigenschaft kann für Kausalanalysen hilfreich sein, da wir in vielen empirischen Anwendungen die Effekte einer Behandlung nicht nur im Durchschnitt für die betrachtete Population, sondern <em>differenzierter</em> schätzen möchten: Ein durchschnittlicher Behandlungseffekt (engl. <em>average treatment effect</em>, ATE) kann nicht ausreichend informativ für unsere Forschungsfrage sein, bspw. wenn wir erwarten, dass eine politische Intervention unterschiedliche Auswirkungen auf verschiedene Bevölkerungsgruppen hat. Idealerweise möchten wir <span class="math inline">\(\tau_i\)</span> bestimmen, den individuellen Behandlungseffekt einer Beobachtung <span class="math inline">\(i\)</span>. Das fundamentale Problem der Kausalinferenz ist, dass <span class="math inline">\(\tau_i\)</span> nicht ermittelt werden kann (s. u.), sodass wir unser Ziel abschwächen müssen. Statt <span class="math inline">\(\tau_i\)</span> suchen wir einen Behandlungseffekt in Abhängigkeit von beobachtbaren Charakteristiken <span class="math inline">\(\boldsymbol{X}\)</span> für Untergruppen der Population, einen <em>conditional average treatment effect</em> (CATE). Im Potential-Outcomes-Framework ist der CATE definiert als</p>
<p><span class="math display">\[\begin{align*}
  \tau(\boldsymbol{x}) = \textup{E}\big(Y^{(1)} - Y^{(0)}\big\vert \boldsymbol{X} = \boldsymbol{x}\big),
\end{align*}\]</span></p>
<p>wobei <span class="math inline">\(Y^{(1)}\)</span> und <span class="math inline">\(Y^{(0)}\)</span> die potenziellen Outcomes darstellen, wenn eine Behandlung erfolgt bzw. nicht erfolgt. In der Praxis beobachten wir jedoch nur <span class="math inline">\(Y_i = Y_i^{(B_i)}\)</span>, wobei <span class="math inline">\(B_i\)</span> der Behandlungsindikator für die Beobachtung <span class="math inline">\(i\)</span> ist, sodass <span class="math inline">\(\tau(\boldsymbol{x}_i)\)</span> nicht direkt beobachtet werden kann. Unter der Annahme, dass nach Kontrolle für (beobachtbare) <span class="math inline">\(\boldsymbol{X}\)</span> die Zuordnung zur Behandlung quasi-zufällig ist (<em>unconfoundedness</em>), formal</p>
<p><span class="math display">\[\begin{align*}
Y_i^{(0)},\,Y_i^{(1)} \perp B_i \vert \boldsymbol{X}_i,
\end{align*}\]</span></p>
<p>kann <span class="math inline">\(\tau(\boldsymbol{x})\)</span> geschätzt werden: Wir können Outcome-Differenzen zwischen behandelten und nicht behandelten Beobachtungen als kausal interpretieren, da unbeobachtete Faktoren die Ergebnisse nicht verzerren.</p>
<p>CART und andere traditionelle Entscheidungsbaum-Algorithmen sind für die Schätzung heterogener Behandlungseffekte jedoch ungeeignet. Dafür gibt es zwei wesentliche Ursachen:</p>
<ul>
<li>
<p><strong>Das Splitting-Kriterium</strong></p>
<p>Das Splitting-Kriterium des CART-Algorithmus optimiert die Aufteilungen der Beobachtungen in jedem Knoten, um die Genauigkeit von Vorhersagen für die Outcome-Variable <span class="math inline">\(Y\)</span> durch Minimierung der Heterogienität (Klassifikation) oder des MSE (Regression) zu optimieren. Diese Kriterien zielen also darauf ab, die <em>Homogenität innerhalb der Blätter hinsichtlich</em> <span class="math inline">\(Y\)</span> zu maximieren.</p>
<p>Für die Schätzung heterogener kausaler Effekte ist ein solches Splitting jedoch nicht zielführend. Statt Knoten zu formen, in denen <span class="math inline">\(Y\)</span> möglichst homogen ist, benötigen wir für die Schätzung von Behandlungseffekten grundsätzlich Aufteilungen, bei denen sich <span class="math inline">\(Y\)</span> zwischen den behandelten und unbehandelten Individuen innerhalb der Knoten unterscheidet.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> Das Splitting sollte zu Blättern führen, die hinsichtlich des <em>geschätzten Behandlungseffekts</em> möglichst heterogen sind.</p>
<p>Die Wahl des Splitting-Kriterium für die Schätzung kausaler Effekte mit Bäumen ist nicht trivial: Ein natürliches Kriterium ist der mittlere quadratische Fehler bei der Vorhersage von <span class="math inline">\(\tau\)</span>,</p>
<p><span class="math display">\[\begin{align*}
    \textup{MSE}_\tau = \frac{1}{n} \sum_{i=1}^n (\tau_i - \widehat{\tau}_i(\boldsymbol{X}_i))^2.
  \end{align*}\]</span></p>
<p><span class="math inline">\(\textup{MSE}_\tau\)</span> ist jedoch nicht direkt berechenbar: Aufgrund der nicht-beobachtbaren individuellen Behandlungseffekte <span class="math inline">\(\tau_i\)</span> müsste <span class="math inline">\(\textup{MSE}_\tau\)</span> selbst geschätzt werden!<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
</li>
<li>
<p><strong>Data leakage</strong></p>
<p>Data leakage tritt auf, wenn Informationen aus dem Trainingsprozess in den Modellvalidierungs- oder Schätzprozess einfließen. Bei der Anpassung des Baums berücksichtigt der Algorithmus idealerweise Informationen über <span class="math inline">\(Y\)</span> <em>und</em> <span class="math inline">\(B\)</span> im Splitting-Prozess, um die besten Aufteilungen zu finden. Die hiezu verwendeten Datenpunkte definieren damit <em>den zu schätzten CATE</em> anhand der durch Partionierung gebildeten Blätter. Wenn dieselben Datenpunkte auch für die tatsächliche Schätzung des CATE mit dem trainierten Baum verwendet werden, besteht die Gefahr von Überanpassung und somit verzerrten Schätzungen.</p>
</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn8"><p><sup>8</sup>&nbsp;Wenn die Kontroll- und Behandlungsbeobachtungen in einem Blatt sehr ähnliche Outcomes <span class="math inline">\(Y\)</span> haben, können wir den Effekt nicht schätzen.</p></div><div id="fn9"><p><sup>9</sup>&nbsp;Bei “herkömmlichen” Regressionsbäumen besteht dieses Problem nicht, weil das Splitting-Kriterium Abweichungen von den wahren, <em>beobachteten</em> Werten von <span class="math inline">\(Y\)</span> misst.</p></div></div><section id="causal-trees" class="level3 page-columns page-full" data-number="15.6.1"><h3 data-number="15.6.1" class="anchored" data-anchor-id="causal-trees">
<span class="header-section-number">15.6.1</span> Causal Trees</h3>
<p>Der Causal Tree Algorithmus von <span class="citation" data-cites="AtheyImbens2016">Athey und Imbens (<a href="Literatur.html#ref-AtheyImbens2016" role="doc-biblioref">2016</a>)</span> modifiziert den CART-Algorithmus für die Schätzung heterogener Behandlungseffekte. In diesem Kontext wird die Vorgehensweise als „ehrlich“ (<em>honest</em>) bezeichnet, wenn nicht dieselben Informationen sowohl zur Auswahl des Modells (die Partitionierung des Regressorraums durch Splits) als auch zur Schätzung anhand dieses Modells verwendet werden. <span class="citation" data-cites="AtheyImbens2016">Athey und Imbens (<a href="Literatur.html#ref-AtheyImbens2016" role="doc-biblioref">2016</a>)</span> adressieren das Data-Leakage-Problem durch zufällige Aufteilung des Datensatzes in eine Teilmenge <span class="math inline">\(\mathcal{S}^{tr}\)</span> für das <em>Training des Baums</em> und eine Teilmenge <span class="math inline">\(\mathcal{S}^{est}\)</span> für die <em>Schätzung der Behandlungseffekte</em>.</p>
<p>Für die Erläuterung von <em>honest splitting</em> führen wir folgende Notation aus <span class="citation" data-cites="AtheyImbens2016">Athey und Imbens (<a href="Literatur.html#ref-AtheyImbens2016" role="doc-biblioref">2016</a>)</span> ein:</p>
<ul>
<li><p><span class="math inline">\(\mathcal{S}^{te}\)</span> ist ein hypothetischer <em>Testdatensatz</em></p></li>
<li><p><span class="math inline">\(\Pi\)</span> ist eine <em>Partition</em>, d.h. eine Aufteilung des Regressorraums von <span class="math inline">\(\boldsymbol{X}\)</span><a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p></li>
<li><p>Wir definieren die Schätzung des CATE anhand der Beobachtungen <span class="math inline">\(\mathcal{S}^{est}\)</span>: Der CATE <span class="math inline">\(\widehat{\tau}(\boldsymbol{X}_i,\mathcal{S}^{est},\Pi)\)</span> ist die Differenz der Mittelwerte von <span class="math inline">\(Y_i\)</span> für Behandlungs- und Kontrollbeobachtungen in dem aus <span class="math inline">\(\Pi\)</span> resultierenden Blatt für <span class="math inline">\(\boldsymbol{X}_i\)</span>.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;<span class="math inline">\(\Pi\)</span> sammelt also die Entschidungsregeln eines Baums und ist äquivalent zu <span class="math inline">\(T\)</span> in den füheren Kapiteln.</p></div><div id="fn11"><p><sup>11</sup>&nbsp;Die Notation <span class="math inline">\(\textup{E}_{\mathcal{S}^{est},\,\mathcal{S}^{te}}\)</span> meint, dass die Erwartung über <span class="math inline">\(\mathcal{S}^{est}\)</span>, und <span class="math inline">\(\mathcal{S}^{te}\)</span> gebildet wird.</p></div></div><p>Für die Wahl der Splits (die Partitionierung <span class="math inline">\(\Pi\)</span>) für den Causal Tree schlagen <span class="citation" data-cites="AtheyImbens2016">Athey und Imbens (<a href="Literatur.html#ref-AtheyImbens2016" role="doc-biblioref">2016</a>)</span> statt der Minimierung des MSE der Vorhersagen <span class="math inline">\(\widehat{Y}\)</span> (wie bei Regressionsbäumen) die Minimierung des MSE für den CATE vor. Das Vorgehen hierbei ist <em>honest</em> in dem Sinn, dass der erwartete Schätzfehler für ungesehene Beobachtungen <span class="math inline">\(\mathcal{S}^{te}\)</span> anhand einer Paritionierung <span class="math inline">\(\Pi\)</span> und entsprechenden Schätzungen der Behandlungseffekte <span class="math inline">\(\widehat\tau\)</span> mit unabhängigen Datensätzen <span class="math inline">\(\mathcal{S}^{tr}\)</span> bzw. <span class="math inline">\(\mathcal{S}^{est}\)</span> minimiert wird. Das hierzu verwendete Splitting-Kriterium ist eine Schätzung des <em>Erwartungswerts</em> von <span class="math display">\[\begin{align*}
  \textup{MSE}(\mathcal{S}^{est},\mathcal{S}^{te},\Pi) = \frac{1}{n^{te}} \sum_{i=1}^{n^{te}} \big(\tau_i - \widehat{\tau}(\boldsymbol{X}_i,\mathcal{S}^{est},\Pi)\big)^2,
\end{align*}\]</span> der <em>erwartete</em><a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> mittlere quadratische Fehler der heterogenen Behandlungseffekte,</p>
<p><span class="math display">\[\begin{align*}
  \textup{EMSE}(\Pi) = \textup{E}_{\mathcal{S}^{est},\,\mathcal{S}^{te}}\big[\textup{MSE}(\mathcal{S}^{est},\mathcal{S}^{te},\,\Pi)\big].
\end{align*}\]</span></p>
<p>Eine hilfreiche Umformung für <span class="math inline">\(\textup{EMSE}\)</span> ist</p>
<p><span class="math display">\[\begin{align*}
  \textup{EMSE}(\Pi) = \textup{Var}_{\mathcal{S}^{est},\boldsymbol{X}_i} \big[\widehat\tau(\boldsymbol{X}_i,\mathcal{S}^{est},\Pi)\big] - \textup{E}_{\boldsymbol{X}_i}\big[\tau^2(\boldsymbol{X}_i,\Pi)\big] + \textup{E}[\tau_i^2],
\end{align*}\]</span></p>
<p>denn <span class="citation" data-cites="AtheyImbens2016">Athey und Imbens (<a href="Literatur.html#ref-AtheyImbens2016" role="doc-biblioref">2016</a>)</span> zeigen, wie die ersten beiden Summanden empirisch geschätzt werden können. Der Term <span class="math inline">\(\textup{E}[\tau_i^2]\)</span> ist nicht schätzbar (unbeobachteter individueller Behandlungseffekt <span class="math inline">\(\tau_i\)</span>), kann aber vernachlässigt werden, da er nicht von <span class="math inline">\(\Pi\)</span> oder den Daten abhängt und somit eine <em>Konstante</em> ist, die sich beim Vergleich des geschätzen EMSE für verschiedene <span class="math inline">\(\Pi\)</span> rauskürzt.</p>
<p>Dies sorgt für konsistente Schätzungen. <span class="citation" data-cites="AtheyImbens2016">Athey und Imbens (<a href="Literatur.html#ref-AtheyImbens2016" role="doc-biblioref">2016</a>)</span> zeigen, dass die Minimierung des EMSE sowohl eine ausgewogene Verteilung der behandelten und unbehandelten Individuen als auch eine genaue Schätzung des Behandlungseffekts innerhalb jedes Knotens gewährleistet.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Algorithmus: Causal Tree
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>Passe den Baum an: teile den Regressorraum mit binären Entscheidungsregeln rekursiv in Partitionen <span class="math inline">\(\Pi\)</span>:
<ol type="a">
<li>An jedem Knoten wird die Aufteilung so gewählt, dass die Schätzung des <em>erwarteten mittleren quadratischen Fehlers</em> () über alle möglichen binären Aufteilungen <span class="math inline">\(\Pi\)</span> minimiert wird.</li>
<li>Stelle sicher, dass eine Mindestanzahl von behandelten und Kontroll-Einheiten in jedem Blatt des so angepassten Baums nicht unterschritten wird.</li>
</ol>
</li>
<li>Bestimmte mit Cross-Validation die Tiefe <span class="math inline">\(d^*\)</span> der Partition, die eine Schätzung des MSE der Behandlungseffekte minimiert.</li>
<li>Ehalte die Partition <span class="math inline">\(\Pi^*\)</span> durch das Beschneiden von <span class="math inline">\(\Pi\)</span> auf die Tiefe <span class="math inline">\(d^*\)</span>: Entferne Blätter, die die geringste Verbesserung der Anpassung bieten. Dieser Schritt liefert den finalen Baum.</li>
<li>Schätze die Behandlungseffekte in jedem Blatt von <span class="math inline">\(\Pi^*\)</span> mit den Beobachtungen in <span class="math inline">\(\mathcal{S}^{est}\)</span>.</li>
</ol>
</div>
</div>
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">nl_effects</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/readRDS.html">readRDS</a></span><span class="op">(</span>file <span class="op">=</span> <span class="st">"datasets/nl_effects.Rds"</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><span class="citation" data-cites="grfPackage">Tibshirani u.&nbsp;a. (<a href="Literatur.html#ref-grfPackage" role="doc-biblioref">2024</a>)</span></p>
<div class="cell">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/grf-labs/grf">grf</a></span><span class="op">)</span></span>
<span><span class="co"># Fit a causal tree (one single tree)</span></span>
<span><span class="va">causal_tree</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/grf/man/causal_forest.html">causal_forest</a></span><span class="op">(</span></span>
<span>  X <span class="op">=</span> <span class="va">nl_effects</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">X1</span><span class="op">:</span><span class="va">X3</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  Y <span class="op">=</span> <span class="va">nl_effects</span><span class="op">$</span><span class="va">Y</span>,</span>
<span>  W <span class="op">=</span> <span class="va">nl_effects</span><span class="op">$</span><span class="va">B</span>, </span>
<span>  num.trees <span class="op">=</span> <span class="fl">1</span>, </span>
<span>  honesty <span class="op">=</span> <span class="cn">FALSE</span>, </span>
<span>  min.node.size <span class="op">=</span> <span class="fl">150</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">causal_tree</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>GRF forest object of type causal_forest 
Number of trees: 2 
Number of training samples: 5000 
Variable importance: 
    1     2     3 
0.000 0.000 0.956 </code></pre>
</div>
</div>
<p><a href="#fig-ct" class="quarto-xref">Abbildung&nbsp;<span>15.3</span></a></p>
<div class="cell">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">the_tree</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/grf/man/get_tree.html">get_tree</a></span><span class="op">(</span><span class="va">causal_tree</span>, index <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">the_tree</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="fig-ct" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-ct-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/causal_tree.svg" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-ct-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Abbildung&nbsp;15.3: Mit <code><a href="https://rdrr.io/pkg/grf/man/causal_forest.html">grf::causal_forest</a></code> geschätzter Causal Tree
</figcaption></figure>
</div>
</section><section id="causal-forests" class="level3 page-columns page-full" data-number="15.6.2"><h3 data-number="15.6.2" class="anchored" data-anchor-id="causal-forests">
<span class="header-section-number">15.6.2</span> Causal Forests</h3>
<p><em>Causal Forests</em> sind eine Erweiterung von Random Forests, die speziell entwickelt wurden, um individuelle Behandlungseffekte*(engl. Individual Treatment Effects, ITE) zu schätzen. Der Hauptunterschied zu Random Forests besteht darin, dass Causal Forests nicht nur Vorhersagen treffen, sondern auch den <strong>kausalen Effekt</strong> einer Behandlung <span class="math inline">\(B\)</span> auf das Outcome <span class="math inline">\(Y\)</span> schätzen. Der kausale Effekt wird oft als Differenz der Ergebnisse unter verschiedenen Bedingungen (z. B. mit und ohne Behandlung) modelliert.</p>
<div id="tbl-rfcfcomp" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-tbl figure page-columns page-full"><div aria-describedby="tbl-rfcfcomp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 20%">
<col style="width: 38%">
<col style="width: 41%">
</colgroup>
<thead><tr class="header">
<th>Aspekt</th>
<th>Random Forests</th>
<th>Causal Forests</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Ziel</strong></td>
<td>Vorhersage von Zielvariablen</td>
<td>Schätzung individueller Behandlungseffekte</td>
</tr>
<tr class="even">
<td><strong>Ergebnisse</strong></td>
<td>Globale Vorhersagen</td>
<td>Heterogene, individuelle Effekte</td>
</tr>
<tr class="odd">
<td><strong>Trainingsdaten</strong></td>
<td>Outcome und Prädiktoren</td>
<td>Behandlung, Outcome und Prädiktoren</td>
</tr>
<tr class="even">
<td><strong>Beispiel</strong></td>
<td>Einkommensvorhersage auf Basis von Merkmalen</td>
<td>Effekt einer Werbekampagne auf verschiedene Kunden</td>
</tr>
<tr class="odd">
<td><strong>Schätzung</strong></td>
<td>Bedingte Erwartung <span class="math inline">\(\textup{E}[Y|X]\)</span>
</td>
<td>Bedingter Behandlungseffekt <span class="math inline">\(\textup{E}[Y(1) - Y(0)|X]\)</span>
</td>
</tr>
<tr class="even">
<td><strong>Verwendung</strong></td>
<td>Klassifikation und Regression</td>
<td>Kausalanalyse und individuelle Wirkungsschätzung</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-tbl margin-caption" id="tbl-rfcfcomp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabelle&nbsp;15.1: Vergleich von Random Forests und Causal Forests
</figcaption></figure>
</div>
<p>In diesem Beispiel verwenden wir das Paket <code>grf</code> (Generalized Random Forests) in R, um einen Causal Forest zu trainieren und die individuellen Behandlungseffekte zu schätzen.</p>
<p>Naive Methode mit Random Forest</p>
<div class="cell page-columns page-full">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span><span class="co"># Daten ins Long-Format umwandeln für einfachere Facettierung</span></span>
<span><span class="va">nl_long</span> <span class="op">&lt;-</span> <span class="va">nl_effects</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">X1</span>, <span class="va">X2</span>, <span class="va">X3</span>, <span class="va">tau</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer</a></span><span class="op">(</span>cols <span class="op">=</span> <span class="fu"><a href="https://tidyselect.r-lib.org/reference/starts_with.html">starts_with</a></span><span class="op">(</span><span class="st">"X"</span><span class="op">)</span>, names_to <span class="op">=</span> <span class="st">"Variable"</span>, values_to <span class="op">=</span> <span class="st">"Value"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Erstelle den facettierten Plot</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">nl_long</span>,</span>
<span>  mapping <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Value</span>, y <span class="op">=</span> <span class="va">tau</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.3</span>, size <span class="op">=</span> <span class="fl">.5</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html">facet_wrap</a></span><span class="op">(</span><span class="op">~</span><span class="va">Variable</span>, scales <span class="op">=</span> <span class="st">"free"</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="st">"Wert der Prädiktoren"</span>,</span>
<span>    y <span class="op">=</span> <span class="st">"Wahrer Behandlungseffekt (tau)"</span></span>
<span>  <span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display page-columns page-full">
<div id="fig-xvstau" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-xvstau-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="trees_files/figure-html/fig-xvstau-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-xvstau-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Abbildung&nbsp;15.4: Zusammenhang zwischen X1, X2, X3 und dem wahren Behandlungseffekt tau
</figcaption></figure>
</div>
</div>
</div>
<div class="cell page-columns page-full">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># In langes Format überführen</span></span>
<span><span class="va">df_long</span> <span class="op">&lt;-</span> <span class="va">nl_effects</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">X1</span>, <span class="va">X2</span>, <span class="va">X3</span>, <span class="va">B</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>    <span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer</a></span><span class="op">(</span></span>
<span>      cols <span class="op">=</span> <span class="fu"><a href="https://tidyselect.r-lib.org/reference/starts_with.html">starts_with</a></span><span class="op">(</span><span class="st">"X"</span><span class="op">)</span>,</span>
<span>      names_to <span class="op">=</span> <span class="st">"Variable"</span>,</span>
<span>      values_to <span class="op">=</span> <span class="st">"Value"</span></span>
<span>    <span class="op">)</span></span>
<span></span>
<span><span class="co"># Facetting nach Variable</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">df_long</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Value</span>, fill <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">B</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_density.html">geom_density</a></span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html">facet_wrap</a></span><span class="op">(</span><span class="op">~</span><span class="va">Variable</span>, scales <span class="op">=</span> <span class="st">"free"</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span></span>
<span>      x <span class="op">=</span> <span class="st">"Prädiktoren-Werte"</span>,</span>
<span>      fill <span class="op">=</span> <span class="st">"Behandlung (B)"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.position <span class="op">=</span> <span class="st">"top"</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display page-columns page-full">
<div id="fig-bvsvars" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-bvsvars-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="trees_files/figure-html/fig-bvsvars-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-bvsvars-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Abbildung&nbsp;15.5: Korrelation zwischen Behandlungsindikator (B) und relevanten Kovariablen
</figcaption></figure>
</div>
</div>
</div>
<div class="cell">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/grf-labs/grf">grf</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.stat.berkeley.edu/~breiman/RandomForests/">randomForest</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidymodels.tidymodels.org">tidymodels</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">the_split</span> <span class="op">&lt;-</span> <span class="fu">initial_split</span><span class="op">(</span>data <span class="op">=</span> <span class="va">nl_effects</span>, prop <span class="op">=</span> <span class="fl">.8</span><span class="op">)</span></span>
<span><span class="va">nl_effects_train</span> <span class="op">&lt;-</span> <span class="fu">training</span><span class="op">(</span><span class="va">the_split</span><span class="op">)</span></span>
<span><span class="va">nl_effects_test</span> <span class="op">&lt;-</span> <span class="fu">testing</span><span class="op">(</span><span class="va">the_split</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Variablen in Matrizen / Vektoren überführen</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="va">nl_effects_train</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="fu"><a href="https://tidyselect.r-lib.org/reference/starts_with.html">starts_with</a></span><span class="op">(</span><span class="st">"X"</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">B</span> <span class="op">&lt;-</span> <span class="va">nl_effects_train</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">B</span><span class="op">)</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="va">nl_effects_train</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span></span>
<span><span class="va">tau</span> <span class="op">&lt;-</span> <span class="va">nl_effects_train</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">tau</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Propensity Score Schätzen</span></span>
<span><span class="va">B_hat_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/grf/man/regression_forest.html">regression_forest</a></span><span class="op">(</span></span>
<span>  X <span class="op">=</span> <span class="va">X</span>,</span>
<span>  Y <span class="op">=</span> <span class="va">B</span>,</span>
<span>  tune.parameters <span class="op">=</span> <span class="st">"all"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">B_hat</span> <span class="op">&lt;-</span> <span class="va">B_hat_mod</span><span class="op">$</span><span class="va">predictions</span></span>
<span></span>
<span><span class="co"># Outcome Schätzen</span></span>
<span><span class="va">Y_hat_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/grf/man/regression_forest.html">regression_forest</a></span><span class="op">(</span></span>
<span>  X <span class="op">=</span> <span class="va">X</span>,</span>
<span>  Y <span class="op">=</span> <span class="va">Y</span>,</span>
<span>  tune.parameters <span class="op">=</span> <span class="st">"all"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">Y_hat</span> <span class="op">&lt;-</span> <span class="va">Y_hat_mod</span><span class="op">$</span><span class="va">predictions</span></span>
<span></span>
<span><span class="co"># Causal Forest trainieren</span></span>
<span><span class="va">cf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/grf/man/causal_forest.html">causal_forest</a></span><span class="op">(</span></span>
<span>  X <span class="op">=</span> <span class="va">X</span>, </span>
<span>  Y <span class="op">=</span> <span class="va">Y</span>, </span>
<span>  W <span class="op">=</span> <span class="va">B</span>, </span>
<span>  Y.hat <span class="op">=</span> <span class="va">Y_hat</span>,</span>
<span>  W.hat <span class="op">=</span> <span class="va">B_hat</span></span>
<span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Schritt 4: Vorhersage des durchschnittlichen Behandlungseffekts</span></span>
<span><span class="co"># Causal Forest</span></span>
<span><span class="va">tau.cf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/grf/man/average_treatment_effect.html">average_treatment_effect</a></span><span class="op">(</span><span class="va">cf</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Durchschnittlicher Behandlungseffekt mit Causal Forest:"</span>, <span class="va">tau.cf</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, <span class="st">"\n"</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Durchschnittlicher Behandlungseffekt mit Causal Forest: -0.2468697 </code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Schritt 5: Individuelle Schätzungen der Behandlungseffekte</span></span>
<span><span class="co"># Causal Forest - Individuelle Behandlungseffekte (CATE)</span></span>
<span><span class="va">tau.hat.cf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">cf</span><span class="op">)</span><span class="op">$</span><span class="va">predictions</span></span>
<span></span>
<span><span class="co"># Wahrer Behandlungseffekt tau</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">tau.hat.cf</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -0.0642255 -0.6270295 -1.0991273  3.1437400 -5.1925531  0.5135410</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="va">nl_effects_train</span><span class="op">$</span><span class="va">tau</span>,</span>
<span>    y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">cf</span><span class="op">)</span><span class="op">$</span><span class="va">predictions</span></span>
<span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x  <span class="op">=</span><span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>alpha<span class="op">=</span> <span class="fl">.3</span>, size <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_abline</a></span><span class="op">(</span>intercept <span class="op">=</span> <span class="fl">0</span>, slope <span class="op">=</span> <span class="fl">1</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="trees_files/figure-html/unnamed-chunk-33-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># ATE CF</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">tau.hat.cf</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -0.3033085</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Schritt 6: Evaluierung - Vergleich der Genauigkeit der beiden Modelle</span></span>
<span><span class="co"># MSE für Causal Forest</span></span>
<span><span class="va">mse.cf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">tau.hat.cf</span> <span class="op">-</span> <span class="va">tau</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"MSE des Causal Forest:"</span>, <span class="va">mse.cf</span>, <span class="st">"\n"</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE des Causal Forest: 1.450783 </code></pre>
</div>
</div>
<p>Test set</p>
<div class="cell">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">cf</span>, newdata <span class="op">=</span> <span class="va">nl_effects_test</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">Y</span>, <span class="op">-</span><span class="va">tau</span>, <span class="op">-</span><span class="va">B</span><span class="op">)</span><span class="op">)</span><span class="op">$</span><span class="va">predictions</span> <span class="op">-</span> <span class="va">nl_effects_test</span><span class="op">$</span><span class="va">tau</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.468642</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># ATE CF pred</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">cf</span>, newdata <span class="op">=</span> <span class="va">nl_effects_test</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">Y</span>, <span class="op">-</span><span class="va">tau</span>, <span class="op">-</span><span class="va">B</span><span class="op">)</span><span class="op">)</span><span class="op">$</span><span class="va">predictions</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -0.4234307</code></pre>
</div>
</div>
</section></section><div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;Aus technischen Gründen verzichten wir in diesem Kapitel zur Zeit auf die Einbindung der WebR-Konsole.</p></div></div><section id="zusammenfassung" class="level2" data-number="15.7"><h2 data-number="15.7" class="anchored" data-anchor-id="zusammenfassung">
<span class="header-section-number">15.7</span> Zusammenfassung</h2>
<p>In diesem Kapitel haben wir die Anwendung baum-basierter Methoden in R diskutiert. Darunter Entscheidungsbäume, Bagging, Random Forests und Boosting. Entscheidungsbäume sind Modelle, die die Daten anhand binärer Entscheidungsregeln sukzessiv in kleinere, homogene Gruppen aufgeteilt werden. Baum-Modelle bieten intuitive Interpretierbarkeit, neigen jedoch zur Überanpassung, was durch Beschneiden (Pruning) vermieden werden kann. Die Vorhersage einzelner Bäume ist tendentiell mit hoher Varianz verbunden. Random Forests kombinieren mit Bagging viele Entscheidungsbäume, die auf zufälligen Teilmengen der Daten und Merkmale trainiert werden. Durch die Aggregation der Vorhersagen vieler Bäume reduziert der Random Forest die Varianz und verbessert so die Vorhersagegenauigkeit. Boosting-Methoden mit Entscheidungsbäumen trainieren kleine Bäume sukzessive, wobei jeder weitere Baum zur Korrektur der gegenwärtigen Fehler des Ensembles trainiert wird. Gradient Boosting nutzt den Gradienten der Verlustfunktion, um die Vorhersagequalität des Ensembles optimieren.</p>
<p>Für alle Methoden wurden Implementierungen im <code>parsnip</code>-Framework in R vorgestellt. Zudem wurde gezeigt, wie die Vorhersagegüte durch Testdatensätze beurteilt und die Bedeutung einzelner Variablen mit Variable-Importance-Metriken analysiert werden kann.</p>


<!-- -->

<script type="webr-data">
eyJvcHRpb25zIjp7ImJhc2VVcmwiOiJodHRwczovL3dlYnIuci13YXNtLm9yZy92MC40LjEvIn0sInBhY2thZ2VzIjp7InBrZ3MiOlsiZXZhbHVhdGUiLCJrbml0ciIsImh0bWx0b29scyIsImJhZ3VldHRlIiwiY293cGxvdCIsImdibSIsImRwbHlyIiwiZ2dwbG90MiIsImdnUmFuZG9tRm9yZXN0cyIsIk1BU1MiLCJwdXJyciIsInJhbmRvbUZvcmVzdCIsInJhdHRsZSIsInRpZHltb2RlbHMiLCJ0aWR5ciJdLCJyZXBvcyI6W119LCJyZW5kZXJfZGYiOiJkZWZhdWx0In0=
</script><script type="ojs-module-contents">
{"contents":[{"methodName":"interpretQuiet","cellName":"webr-widget-20","inline":false,"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_20;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"methodName":"interpret","cellName":"webr-20","inline":false,"source":"viewof _webr_editor_20 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-20-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-20-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_20 = webROjs.process(_webr_editor_20, {});\n"},{"methodName":"interpretQuiet","cellName":"webr-widget-19","inline":false,"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_19;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"methodName":"interpret","cellName":"webr-19","inline":false,"source":"viewof _webr_editor_19 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-19-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-19-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_19 = webROjs.process(_webr_editor_19, {});\n"},{"methodName":"interpretQuiet","cellName":"webr-widget-18","inline":false,"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_18;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"methodName":"interpret","cellName":"webr-18","inline":false,"source":"viewof _webr_editor_18 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-18-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-18-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_18 = webROjs.process(_webr_editor_18, {});\n"},{"methodName":"interpretQuiet","cellName":"webr-widget-17","inline":false,"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_17;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"methodName":"interpret","cellName":"webr-17","inline":false,"source":"viewof _webr_editor_17 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-17-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-17-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_17 = webROjs.process(_webr_editor_17, {});\n"},{"methodName":"interpretQuiet","cellName":"webr-widget-16","inline":false,"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_16;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"methodName":"interpret","cellName":"webr-16","inline":false,"source":"viewof _webr_editor_16 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-16-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-16-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_16 = webROjs.process(_webr_editor_16, {});\n"},{"methodName":"interpretQuiet","cellName":"webr-widget-15","inline":false,"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_15;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"methodName":"interpret","cellName":"webr-15","inline":false,"source":"viewof _webr_editor_15 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-15-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-15-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_15 = webROjs.process(_webr_editor_15, {});\n"},{"methodName":"interpretQuiet","cellName":"webr-widget-14","inline":false,"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_14;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"methodName":"interpret","cellName":"webr-14","inline":false,"source":"viewof _webr_editor_14 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-14-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-14-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_14 = webROjs.process(_webr_editor_14, {});\n"},{"methodName":"interpretQuiet","cellName":"webr-widget-13","inline":false,"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_13;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"methodName":"interpret","cellName":"webr-13","inline":false,"source":"viewof _webr_editor_13 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-13-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-13-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_13 = webROjs.process(_webr_editor_13, {});\n"},{"methodName":"interpretQuiet","cellName":"webr-widget-12","inline":false,"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_12;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"methodName":"interpret","cellName":"webr-12","inline":false,"source":"viewof _webr_editor_12 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-12-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-12-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_12 = webROjs.process(_webr_editor_12, {});\n"},{"methodName":"interpretQuiet","cellName":"webr-widget-11","inline":false,"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_11;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"methodName":"interpret","cellName":"webr-11","inline":false,"source":"viewof _webr_editor_11 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-11-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-11-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_11 = webROjs.process(_webr_editor_11, {});\n"},{"methodName":"interpretQuiet","cellName":"webr-widget-10","inline":false,"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_10;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"methodName":"interpret","cellName":"webr-10","inline":false,"source":"viewof _webr_editor_10 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-10-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-10-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_10 = webROjs.process(_webr_editor_10, {});\n"},{"methodName":"interpretQuiet","cellName":"webr-widget-9","inline":false,"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_9;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"methodName":"interpret","cellName":"webr-9","inline":false,"source":"viewof _webr_editor_9 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-9-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-9-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_9 = webROjs.process(_webr_editor_9, {});\n"},{"methodName":"interpretQuiet","cellName":"webr-widget-8","inline":false,"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_8;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"methodName":"interpret","cellName":"webr-8","inline":false,"source":"viewof _webr_editor_8 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-8-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-8-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_8 = webROjs.process(_webr_editor_8, {});\n"},{"methodName":"interpretQuiet","cellName":"webr-widget-7","inline":false,"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_7;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"methodName":"interpret","cellName":"webr-7","inline":false,"source":"viewof _webr_editor_7 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-7-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-7-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_7 = webROjs.process(_webr_editor_7, {});\n"},{"methodName":"interpretQuiet","cellName":"webr-widget-6","inline":false,"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_6;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"methodName":"interpret","cellName":"webr-6","inline":false,"source":"viewof _webr_editor_6 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-6-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-6-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_6 = webROjs.process(_webr_editor_6, {});\n"},{"methodName":"interpretQuiet","cellName":"webr-widget-5","inline":false,"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_5;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"methodName":"interpret","cellName":"webr-5","inline":false,"source":"viewof _webr_editor_5 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-5-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-5-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_5 = webROjs.process(_webr_editor_5, {});\n"},{"methodName":"interpretQuiet","cellName":"webr-widget-4","inline":false,"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_4;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"methodName":"interpret","cellName":"webr-4","inline":false,"source":"viewof _webr_editor_4 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-4-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-4-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_4 = webROjs.process(_webr_editor_4, {});\n"},{"methodName":"interpretQuiet","cellName":"webr-widget-3","inline":false,"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_3;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"methodName":"interpret","cellName":"webr-3","inline":false,"source":"viewof _webr_editor_3 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-3-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-3-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_3 = webROjs.process(_webr_editor_3, {});\n"},{"methodName":"interpretQuiet","cellName":"webr-widget-2","inline":false,"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_2;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"methodName":"interpret","cellName":"webr-2","inline":false,"source":"viewof _webr_editor_2 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-2-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-2-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_2 = webROjs.process(_webr_editor_2, {});\n"},{"methodName":"interpretQuiet","cellName":"webr-widget-1","inline":false,"source":"{\n  // Wait for output to be written to the DOM, then trigger widget rendering\n  await _webr_value_1;\n  if (window.HTMLWidgets) {\n    window.HTMLWidgets.staticRender();\n  }\n  if (window.PagedTableDoc) {\n    window.PagedTableDoc.initAll();\n  }\n}\n"},{"methodName":"interpret","cellName":"webr-1","inline":false,"source":"viewof _webr_editor_1 = {\n  const { WebRExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n  const scriptContent = document.querySelector(`script[type=\\\"webr-1-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `webr-1-contents` }, block.attr);\n  const editor = new WebRExerciseEditor(webROjs.webRPromise, block.code, options);\n\n  return editor.container;\n}\n_webr_value_1 = webROjs.process(_webr_editor_1, {});\n"},{"methodName":"interpretQuiet","cellName":"webr-prelude","inline":false,"source":"webROjs = {\n  const { WebR } = window._exercise_ojs_runtime.WebR;\n  const {\n    WebREvaluator,\n    WebREnvironmentManager,\n    setupR,\n    b64Decode,\n    collapsePath\n  } = window._exercise_ojs_runtime;\n\n  const statusContainer = document.getElementById(\"exercise-loading-status\");\n  const indicatorContainer = document.getElementById(\"exercise-loading-indicator\");\n  indicatorContainer.classList.remove(\"d-none\");\n\n  let statusText = document.createElement(\"div\")\n  statusText.classList = \"exercise-loading-details\";\n  statusText = statusContainer.appendChild(statusText);\n  statusText.textContent = `Initialise`;\n\n  // Hoist indicator out from final slide when running under reveal\n  const revealStatus = document.querySelector(\".reveal .exercise-loading-indicator\");\n  if (revealStatus) {\n    revealStatus.remove();\n    document.querySelector(\".reveal > .slides\").appendChild(revealStatus);\n  }\n\n  // webR supplemental data and options\n  const dataContent = document.querySelector(`script[type=\\\"webr-data\\\"]`).textContent;\n  const data = JSON.parse(b64Decode(dataContent));\n\n  // Grab list of resources to be downloaded\n  const filesContent = document.querySelector(`script[type=\\\"vfs-file\\\"]`).textContent;\n  const files = JSON.parse(b64Decode(filesContent));\n\n  // Initialise webR and setup for R code evaluation\n  let webRPromise = (async (webR) => {\n    statusText.textContent = `Downloading webR`;\n    await webR.init();\n\n    // Install provided list of packages\n    // Ensure webR default repo is included\n    data.packages.repos.push(\"https://repo.r-wasm.org\")\n    await data.packages.pkgs.map((pkg) => () => {\n      statusText.textContent = `Downloading package: ${pkg}`;\n      return webR.evalRVoid(`\n        webr::install(pkg, repos = repos)\n        library(pkg, character.only = TRUE)\n      `, { env: {\n        pkg: pkg,\n        repos: data.packages.repos,\n      }});\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n\n    // Download and install resources\n    await files.map((file) => async () => {\n      const name = file.substring(file.lastIndexOf('/') + 1);\n      statusText.textContent = `Downloading resource: ${name}`;\n      const response = await fetch(file);\n      if (!response.ok) {\n        throw new Error(`Can't download \\`${file}\\`. Error ${response.status}: \"${response.statusText}\".`);\n      }\n      const data = await response.arrayBuffer();\n\n      // Store URLs in the cwd without any subdirectory structure\n      if (file.includes(\"://\")) {\n        file = name;\n      }\n\n      // Collapse higher directory structure\n      file = collapsePath(file);\n\n      // Create directory tree, ignoring \"directory exists\" VFS errors\n      const parts = file.split('/').slice(0, -1);\n      let path = '';\n      while (parts.length > 0) {\n        path += parts.shift() + '/';\n        try {\n          await webR.FS.mkdir(path);\n        } catch (e) {\n          if (!e.message.includes(\"FS error\")) {\n            throw e;\n          }\n        }\n      }\n\n      // Write this file to the VFS\n      return await webR.FS.writeFile(file, new Uint8Array(data));\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n\n    statusText.textContent = `Installing webR shims`;\n    await webR.evalRVoid(`webr::shim_install()`);\n\n    statusText.textContent = `WebR environment setup`;\n    await setupR(webR, data);\n\n    statusText.remove();\n    if (statusContainer.children.length == 0) {\n      statusContainer.parentNode.remove();\n    }\n    return webR;\n  })(new WebR(data.options));\n\n  // Keep track of initial OJS block render\n  const renderedOjs = {};\n\n  const process = async (context, inputs) => {\n    const webR = await webRPromise;\n    const evaluator = new WebREvaluator(webR, context)\n    await evaluator.process(inputs);\n    return evaluator.container;\n  }\n\n  return {\n    process,\n    webRPromise,\n    renderedOjs,\n  };\n}\n"}]}
</script><div id="exercise-loading-indicator" class="exercise-loading-indicator d-none d-flex align-items-center gap-2">
<div id="exercise-loading-status" class="d-flex gap-2">

</div>
<div class="spinner-grow spinner-grow-sm">

</div>
</div>
<script type="vfs-file">
W10=
</script><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-AtheyImbens2016" class="csl-entry" role="listitem">
Athey, Susan, und Guido Imbens. 2016. <span>„Recursive partitioning for heterogeneous causal effects“</span>. <em>Proceedings of the National Academy of Sciences</em> 113 (27): 7353–60. <a href="https://doi.org/10.1073/pnas.1510489113">https://doi.org/10.1073/pnas.1510489113</a>.
</div>
<div id="ref-Breimanetal1984" class="csl-entry" role="listitem">
Breiman, L., J. Friedman, C. J. Stone, und R. A. Olshen. 1984. <em>Classification and Regression Trees</em>. Taylor &amp; Francis.
</div>
<div id="ref-Hastieetal2013" class="csl-entry" role="listitem">
Hastie, T., R. Tibshirani, und J. Friedman. 2013. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Springer Series in Statistics. Springer New York.
</div>
<div id="ref-grfPackage" class="csl-entry" role="listitem">
Tibshirani, Julie, Susan Athey, Erik Sverdrup, und Stefan Wager. 2024. <em>grf: Generalized Random Forests</em>. <a href="https://CRAN.R-project.org/package=grf">https://CRAN.R-project.org/package=grf</a>.
</div>
</div>
</section></main><!-- /main --><script type="ojs-module-contents">
eyJjb250ZW50cyI6W119
</script><script type="module">
if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
window._ojs.paths.runtimeToDoc = "../..";
window._ojs.paths.runtimeToRoot = "../..";
window._ojs.paths.docToRoot = "";
window._ojs.selfContained = false;
window._ojs.runtime.interpretFromScriptTags();
</script><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Kopiert");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Kopiert");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./svm.html" class="pagination-link" aria-label="Support Vector Machines">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Machine Learning.html" class="pagination-link" aria-label="Neuronale Netzwerke">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Neuronale Netzwerke</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Quellcode</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb22" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span><span class="co"> </span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co">  live-html:</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co">    webr: </span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="co">      packages:</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'baguette'</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'cowplot'</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'gbm'</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'dplyr'</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'ggplot2'</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'ggRandomForests'</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'MASS'</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'purrr'</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'randomForest'</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'rattle'</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'tidymodels'</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a><span class="co">        - 'tidyr'</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a><span class="co">      cell-options:</span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a><span class="co">        fig-width: 8</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a><span class="an">engine:</span><span class="co"> knitr</span></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>{{&lt; include ./_extensions/r-wasm/live/_knitr.qmd &gt;}}</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a><span class="fu"># Baum-basierte Methoden {#sec-trees}</span></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>Baum-basierte Methoden bieten eine vielseitige und leistungsstarke Herangehensweise für Vorhersage und Klassifikation in komplexen Datensätzen mit nicht-linearen Zusammenhängen. Ein Vorteil baum-basierter Methoden ist ihre inhärente Fähigkeit, die Bedeutung einzelner Variablen für die Vorhersage zu quantifizieren – eine Eigenschaft, die viele Machine-Learning-Modelle nicht ohne weiteres bieten und insbesondere in hoch-dimensionalen Anwendungen (mit vielen potentiellen Regressoren) nicht trivial ist. Dies ermöglicht es, tiefere Einblicke in den Einfluss einzelner Merkmale auf die Vorhersagen des Modells zu erhalten, was besonders in empirischen Anwendungen für die Entscheidungsstützung mit Machine Learning hilfreich sein kann.</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>*Entscheidungsbäume* stellen die Grundlage dieser Methoden dar. Sie ermöglichen die Aufteilung der Daten in immer kleinere, homogenere Gruppen, basierend auf *binären* Entscheidungsregeln, die aus den Prädiktoren abgleitet werden. Die trainierten Regeln eines solchen Modells lassen sich anhand eines Binärbaums visualisieren, was eine intuitive Interpretierbarkeit der Ergebnisse erlaubt. </span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>*Random Forests* ist ein Ensemble-Ansatz, bei dem viele Entscheidungsbäume kombiniert werden. Jeder Baum wird auf einer zufälligen Teilmenge der Daten trainiert (*Bagging*), und bei jedem Knoten wird zusätzlich eine zufällige Teilmenge der Merkmale berücksichtigt. Die finale Vorhersage des Random Forests basiert auf der Aggregation der Vorhersagen aller Bäume (Mehrheitsvotum für Klassifikation, Durchschnitt für Regression). Dieses Verfahren reduziert das Risiko einer Überanpassung und erhöht oft die Vorhersagegenauigkeit im Vergleich zu einzelnen Entscheidungsbäumen.</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>*Boosting* ist eine weitere Ensemble-Methode zur Anpassung von Modellen mit hoher Vorhersagegüte durch Kombination einfacher Modelle (*Base learner*), wobei Regressions- oder Klassifikationsbäume eingesetzt werden können. Alternativ zu Random Forests trainieren Boosting-Algorithmen sukzessiv einfache (Klassifikations- oder Regressions-)Bäume, wobei jeder nachfolgende Baum das Ziel hat, die Vorhersagefehler der vorherigen Bäume zu korrigieren. </span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>In diesem Kapitel erläutern wir die Anwendung baum-basierter Methoden in R anhand von Beispieldatensätzen. Wir zeigen, wie Regressionsbäume, Random Forests und Boosting-Modelle im <span class="in">`parsnip`</span>-Framework trainiert werden und wie die Vorhersageleistung durch die Wahl geeigneter Hyperparameter mit Cross-Validation und Out-of-Sample-Evaluierungsmethoden optimiert werden kann.</span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a><span class="fu">## Entscheidungsbäume {#sec-simpletrees}</span></span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>Ein Entscheidungsbaum ist ein Modell, das auf der Basis von hierarchischen Bedingungen bzgl. der Regressoren Vorhersagen für die Outcome-Variable trifft. Jeder Baum beginnt mit einem Wurzelknoten (*root node*) und verzweigt sich binär. Jede Verzweigung (*split*) stellt eine Bedingung dar, die auf einem bestimmten Regressor basiert. Der Baum trifft Entscheidungen, indem er diese Bedingungen sukzessive überprüft, bis er zu einem Blattknoten (*leaf node* / *terminal node*) gelangt, der die finale Vorhersage liefert. Hierbei handelt es sich eine Mehrheitsentscheidung für Klassifikation und einen Mittelwert, jeweils gebildet anhand Beobachten des Trainingsdatensatzes im leaf node.</span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a>@fig-exdectree zeigt ein einfaches Beispiel eines Entscheidungsbaums zur Klassifikation der Kreditwürdigkeit einer Person. Die Klassfikation erfolgt, in dem die Beobachtung basierend auf den Merkmalen Alter, Einkommen und Eigentum durch den Baum geleitet wird. Zunächst wird geprüft, die Person 30 Jahre oder jünger ist. Fall ja, entscheidet der Baum anhand des Einkommens: Bei einem Jahreseinkommen von 40.000 oder weniger wird die Person als wenig kreditwürdig klassifiziert, bei höherem Einkommen als mäßig kreditwürdig. Für Personen älter als 30 Jahre überprüft das Modell lediglich, ob die Person eine Immobilie besitzt, um zwischen mäßiger Kreditwürdigkeit und guter Bonität zu unterscheiden.</span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a><span class="in">```{dot}</span></span>
<span id="cb22-47"><a href="#cb22-47" aria-hidden="true" tabindex="-1"></a><span class="in">//| fig-width: 6</span></span>
<span id="cb22-48"><a href="#cb22-48" aria-hidden="true" tabindex="-1"></a><span class="in">//| fig-height: 5</span></span>
<span id="cb22-49"><a href="#cb22-49" aria-hidden="true" tabindex="-1"></a><span class="in">//| fig-cap: "Entscheidungsbaum: Klassifikation von Kreditwürdigkeit"</span></span>
<span id="cb22-50"><a href="#cb22-50" aria-hidden="true" tabindex="-1"></a><span class="in">//| label: fig-exdectree</span></span>
<span id="cb22-51"><a href="#cb22-51" aria-hidden="true" tabindex="-1"></a><span class="in">digraph exdectree {</span></span>
<span id="cb22-52"><a href="#cb22-52" aria-hidden="true" tabindex="-1"></a><span class="in">    node [shape=box];</span></span>
<span id="cb22-53"><a href="#cb22-53" aria-hidden="true" tabindex="-1"></a><span class="in">    splines=false;</span></span>
<span id="cb22-54"><a href="#cb22-54" aria-hidden="true" tabindex="-1"></a><span class="in">    ranksep = 1;</span></span>
<span id="cb22-55"><a href="#cb22-55" aria-hidden="true" tabindex="-1"></a><span class="in">    nodesep = 1.75;</span></span>
<span id="cb22-56"><a href="#cb22-56" aria-hidden="true" tabindex="-1"></a><span class="in">    margin = 0.15;</span></span>
<span id="cb22-57"><a href="#cb22-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-58"><a href="#cb22-58" aria-hidden="true" tabindex="-1"></a><span class="in">    1 [label="Alter &lt;= 30?"];</span></span>
<span id="cb22-59"><a href="#cb22-59" aria-hidden="true" tabindex="-1"></a><span class="in">    2 [label="Einkommen &lt;= 40 Tsd.?"];</span></span>
<span id="cb22-60"><a href="#cb22-60" aria-hidden="true" tabindex="-1"></a><span class="in">    3 [label="Eigentum?"];</span></span>
<span id="cb22-61"><a href="#cb22-61" aria-hidden="true" tabindex="-1"></a><span class="in">    4 [label="Status: Niedrig"];</span></span>
<span id="cb22-62"><a href="#cb22-62" aria-hidden="true" tabindex="-1"></a><span class="in">    5 [label="Status: Mittel"];</span></span>
<span id="cb22-63"><a href="#cb22-63" aria-hidden="true" tabindex="-1"></a><span class="in">    6 [label="Status: Hoch"];</span></span>
<span id="cb22-64"><a href="#cb22-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-65"><a href="#cb22-65" aria-hidden="true" tabindex="-1"></a><span class="in">    1 -&gt; 2 [label="Ja"];</span></span>
<span id="cb22-66"><a href="#cb22-66" aria-hidden="true" tabindex="-1"></a><span class="in">    1 -&gt; 3 [label="Nein"];</span></span>
<span id="cb22-67"><a href="#cb22-67" aria-hidden="true" tabindex="-1"></a><span class="in">    2 -&gt; 4 [label="Ja"];</span></span>
<span id="cb22-68"><a href="#cb22-68" aria-hidden="true" tabindex="-1"></a><span class="in">    2 -&gt; 5 [label="Nein"];</span></span>
<span id="cb22-69"><a href="#cb22-69" aria-hidden="true" tabindex="-1"></a><span class="in">    3 -&gt; 6 [label="Ja"];</span></span>
<span id="cb22-70"><a href="#cb22-70" aria-hidden="true" tabindex="-1"></a><span class="in">    3 -&gt; 5 [label="Nein"];</span></span>
<span id="cb22-71"><a href="#cb22-71" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb22-72"><a href="#cb22-72" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-73"><a href="#cb22-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-74"><a href="#cb22-74" aria-hidden="true" tabindex="-1"></a><span class="fu">## Training von Bäumen</span></span>
<span id="cb22-75"><a href="#cb22-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-76"><a href="#cb22-76" aria-hidden="true" tabindex="-1"></a>Zur Konstruktion von Binär-Bäumen werden etablierte Algorithmen wie *Classification and Regression Trees* (<span class="co">[</span><span class="ot">CART</span><span class="co">]</span>(https://de.wikipedia.org/wiki/CART_(Algorithmus)) von @Breimanetal1984 verwendet. Die wesentliche Vorgehensweise für das Training eines Baums $T$ ist wie folgt:</span>
<span id="cb22-77"><a href="#cb22-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-78"><a href="#cb22-78" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb22-79"><a href="#cb22-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-80"><a href="#cb22-80" aria-hidden="true" tabindex="-1"></a><span class="fu">## CART-Algorithmus</span></span>
<span id="cb22-81"><a href="#cb22-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-82"><a href="#cb22-82" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Splitting**: Beginnend am root node sucht der Algorithmus nach der "besten" Regel, die Daten anhand eines Merkmals in zwei Gruppen zu teilen. Die Qualität des Splits wird in Abhängigkeit der Definition der Outcome-Variable beurteilt:</span>
<span id="cb22-83"><a href="#cb22-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-84"><a href="#cb22-84" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>**Bei Klassifikation**: Die Reinheit (*purtity*) der Klassen in den unmittelbar nachfolgen nodes wird maximiert. Ein gängiges Kriterium hierfür ist der [*Gini-Koeffizient*](https://de.wikipedia.org/wiki/Gini-Koeffizient).^<span class="co">[</span><span class="ot">Der Gini-Koeffizient $0\leq G\leq1$ misst die Homogenität der Outcome-Variable für die Beobachtungen eines Knotens. $G=0$ ergibt sich bei vollständiger "Reinheit" (alle Beobachtungen im Knoten gehören zur gleichen Klasse). $G &gt; 0$ zeigt Heterogenität der Klassen an, die mit $G$ zunimmt</span><span class="co">]</span></span>
<span id="cb22-85"><a href="#cb22-85" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-86"><a href="#cb22-86" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>**Bei Regression**: Der MSE bei Vorhersage des Outcomes durch Mittelwertbildung für Beobachtungen in den unmittelbar nachfolgenden nodes wird minimiert.</span>
<span id="cb22-87"><a href="#cb22-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-88"><a href="#cb22-88" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Rekursion**: Der Prozess wird rekursiv fortgesetzt, bis Abbruchkriterien greifen eine weitere Verzewigung verhindern:</span>
<span id="cb22-89"><a href="#cb22-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-90"><a href="#cb22-90" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Die maximale Baumtiefe (*tree depth*) ist erreicht </span>
<span id="cb22-91"><a href="#cb22-91" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Die leaf nodes sind hinreichend "rein": Alle Beobachtungen in einem leaf node gehören zur gleichen Klasse oder die Verbesserung des Loss durch weitere Splits fällt unter einen festgelegten Schwellenwert</span>
<span id="cb22-92"><a href="#cb22-92" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Weitere Splits führen zu leaf nodes, die eine Mindestanzahl an Beobachtungen (*minimum split*) unterschreiten würden</span>
<span id="cb22-93"><a href="#cb22-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-94"><a href="#cb22-94" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Pruning**: Um Überanpassung an die Trainingsdaten zu vermeiden, kann der Baum beschnitten werden (*pruning*). Der Grundgedanke ist, dass tief verzweigte Bäume die Trainingsdaten zwar gut modellieren können, aber schlecht auf neue, unbekannte Daten generalisieren. </span>
<span id="cb22-95"><a href="#cb22-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-96"><a href="#cb22-96" aria-hidden="true" tabindex="-1"></a>    Bei *cost complexity (CP) pruning* werden, beginnend auf Ebene der leaf nodes sukuzessive Äste entfernt, und eine Balance zwischen Komplexität des Baums und dem Anpassungsfehler zu finden. Ähnlich wie bei regularisierter KQ-Schätzung (@sec-regreg), wird die Verlustfunktion $L$ um einen Strafterm für die Komplexität erweitert. Der Effekt der Strafe wird durch den CP-Parameter $\alpha\in<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ geregelt,</span>
<span id="cb22-97"><a href="#cb22-97" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-98"><a href="#cb22-98" aria-hidden="true" tabindex="-1"></a>    \begin{align*}</span>
<span id="cb22-99"><a href="#cb22-99" aria-hidden="true" tabindex="-1"></a>      L_{\alpha}(T) = L(T) + \alpha \lvert T\rvert,</span>
<span id="cb22-100"><a href="#cb22-100" aria-hidden="true" tabindex="-1"></a>    \end{align*}</span>
<span id="cb22-101"><a href="#cb22-101" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb22-102"><a href="#cb22-102" aria-hidden="true" tabindex="-1"></a>  für einen Baum $T$ mit Komplexitätsmaß $\lvert T\rvert$ (Anzahl der leaf nodes) <span class="co">[</span><span class="ot">@Hastieetal2013</span><span class="co">]</span>.</span>
<span id="cb22-103"><a href="#cb22-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-104"><a href="#cb22-104" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb22-105"><a href="#cb22-105" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb22-106"><a href="#cb22-106" aria-hidden="true" tabindex="-1"></a>Zur Demonstation der Schätzung von Regressionsbäumen mit R betrachten wir nachfolgend den Datensatz <span class="in">`MASS::Bosten`</span>. Ziel hierbei ist es, mittlere Hauswerte <span class="in">`medv`</span> in Stadteilen von Boston, MA vorherzusagen. Wir verwenden hierzu Funktionen aus dem Paket <span class="in">`parsnip`</span>. </span>
<span id="cb22-107"><a href="#cb22-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-108"><a href="#cb22-108" aria-hidden="true" tabindex="-1"></a>Zunächst transformieren wir den Datensatz in ein <span class="in">`tibble`</span>-Objekt und definieren Trainings- und Test-Daten.</span>
<span id="cb22-109"><a href="#cb22-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-112"><a href="#cb22-112" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb22-113"><a href="#cb22-113" aria-hidden="true" tabindex="-1"></a><span class="in">library(parsnip)</span></span>
<span id="cb22-114"><a href="#cb22-114" aria-hidden="true" tabindex="-1"></a><span class="in">library(cowplot)</span></span>
<span id="cb22-115"><a href="#cb22-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-116"><a href="#cb22-116" aria-hidden="true" tabindex="-1"></a><span class="in"># Seed setzen</span></span>
<span id="cb22-117"><a href="#cb22-117" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(1234)</span></span>
<span id="cb22-118"><a href="#cb22-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-119"><a href="#cb22-119" aria-hidden="true" tabindex="-1"></a><span class="in"># Datensatz als tibble</span></span>
<span id="cb22-120"><a href="#cb22-120" aria-hidden="true" tabindex="-1"></a><span class="in">Boston &lt;- as_tibble(MASS::Boston)</span></span>
<span id="cb22-121"><a href="#cb22-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-122"><a href="#cb22-122" aria-hidden="true" tabindex="-1"></a><span class="in"># Splitting in Training- und Test-Daten</span></span>
<span id="cb22-123"><a href="#cb22-123" aria-hidden="true" tabindex="-1"></a><span class="in">Boston_split &lt;- initial_split(</span></span>
<span id="cb22-124"><a href="#cb22-124" aria-hidden="true" tabindex="-1"></a><span class="in">  data = Boston, </span></span>
<span id="cb22-125"><a href="#cb22-125" aria-hidden="true" tabindex="-1"></a><span class="in">  prop = 0.8, </span></span>
<span id="cb22-126"><a href="#cb22-126" aria-hidden="true" tabindex="-1"></a><span class="in">  strata = medv</span></span>
<span id="cb22-127"><a href="#cb22-127" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb22-128"><a href="#cb22-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-129"><a href="#cb22-129" aria-hidden="true" tabindex="-1"></a><span class="in">Boston_train &lt;- training(Boston_split)</span></span>
<span id="cb22-130"><a href="#cb22-130" aria-hidden="true" tabindex="-1"></a><span class="in">Boston_test &lt;- testing(Boston_split)</span></span>
<span id="cb22-131"><a href="#cb22-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-132"><a href="#cb22-132" aria-hidden="true" tabindex="-1"></a><span class="in">slice_head(Boston_train, n = 10)</span></span>
<span id="cb22-133"><a href="#cb22-133" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-134"><a href="#cb22-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-135"><a href="#cb22-135" aria-hidden="true" tabindex="-1"></a><span class="in">`parsnip`</span> bietet eine vereinheitlichetes Framework für das Training von Modellen mit R und eine flexible API für Machine Learning. Wir definieren zunächst mit <span class="in">`parsnip::decision_tree()`</span> eine Spezifikation zum Training von Entschieundgsmodellen und übergeben beispielhaft einen CP-Parameter $\alpha=.1$. Mit <span class="in">`parsnip::set_engine`</span> wählen wir das Paket <span class="in">`raprt`</span>. Der hier implementierte Agorithmus ist CART. Zuletzt legen wir mit <span class="in">` parsnip::set_mode()`</span> fest, dass der Algorithmus für Regression durchgeführt werden soll.</span>
<span id="cb22-136"><a href="#cb22-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-139"><a href="#cb22-139" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb22-140"><a href="#cb22-140" aria-hidden="true" tabindex="-1"></a><span class="in"># Spezifikation festlegen</span></span>
<span id="cb22-141"><a href="#cb22-141" aria-hidden="true" tabindex="-1"></a><span class="in">tree_spec &lt;- decision_tree(</span></span>
<span id="cb22-142"><a href="#cb22-142" aria-hidden="true" tabindex="-1"></a><span class="in">  cost_complexity = 0.1</span></span>
<span id="cb22-143"><a href="#cb22-143" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb22-144"><a href="#cb22-144" aria-hidden="true" tabindex="-1"></a><span class="in">  set_engine("rpart") %&gt;%</span></span>
<span id="cb22-145"><a href="#cb22-145" aria-hidden="true" tabindex="-1"></a><span class="in">  set_mode("regression")</span></span>
<span id="cb22-146"><a href="#cb22-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-147"><a href="#cb22-147" aria-hidden="true" tabindex="-1"></a><span class="in"># Modell trainieren</span></span>
<span id="cb22-148"><a href="#cb22-148" aria-hidden="true" tabindex="-1"></a><span class="in">tree_fit &lt;- tree_spec %&gt;%</span></span>
<span id="cb22-149"><a href="#cb22-149" aria-hidden="true" tabindex="-1"></a><span class="in">  fit(</span></span>
<span id="cb22-150"><a href="#cb22-150" aria-hidden="true" tabindex="-1"></a><span class="in">    formula = medv ~ ., </span></span>
<span id="cb22-151"><a href="#cb22-151" aria-hidden="true" tabindex="-1"></a><span class="in">    data = Boston_train, </span></span>
<span id="cb22-152"><a href="#cb22-152" aria-hidden="true" tabindex="-1"></a><span class="in">    model = TRUE</span></span>
<span id="cb22-153"><a href="#cb22-153" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb22-154"><a href="#cb22-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-155"><a href="#cb22-155" aria-hidden="true" tabindex="-1"></a><span class="in"># Trainierten Baum in Konsole ausgeben</span></span>
<span id="cb22-156"><a href="#cb22-156" aria-hidden="true" tabindex="-1"></a><span class="in">tree_fit$fit</span></span>
<span id="cb22-157"><a href="#cb22-157" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-158"><a href="#cb22-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-159"><a href="#cb22-159" aria-hidden="true" tabindex="-1"></a>Der Output in <span class="in">`tree_fit$fit`</span> zeigt, dass CP-Pruning zu einem kleinen Baum mit 3 Hierarchie-Ebenen geführt hat. Die Struktur zeigt, dass <span class="in">`lstat`</span> und <span class="in">`rm`</span> für Splitting-Regeln (<span class="in">`split`</span>) verwendet werden, wie viele Beobachtungen den  nodes zugeordnet sind (<span class="in">`n`</span>), den Wert der Verlustfunktion (<span class="in">`deviance`</span>) sowie den Durchschnitt von <span class="in">`medv`</span> für jede node (<span class="in">`yval`</span>). Für die drei leaf nodes (gekennzeichnet mit <span class="in">`*`</span>) ist <span class="in">`yval`</span> die Vorhersage der Outcome-Varibale für entsprechend gruppierte Beobachtungen.</span>
<span id="cb22-160"><a href="#cb22-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-161"><a href="#cb22-161" aria-hidden="true" tabindex="-1"></a>Eine leichter interpretierbare Darstellung der Entscheidungsregeln des angepassten Baums in <span class="in">`tree_fit$fit`</span>  erhalten wir mit <span class="in">`rattle::fancyRpartPlot()`</span>.</span>
<span id="cb22-162"><a href="#cb22-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-165"><a href="#cb22-165" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb22-166"><a href="#cb22-166" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-width: 8</span></span>
<span id="cb22-167"><a href="#cb22-167" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-height: 8</span></span>
<span id="cb22-168"><a href="#cb22-168" aria-hidden="true" tabindex="-1"></a><span class="in">library(rattle)</span></span>
<span id="cb22-169"><a href="#cb22-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-170"><a href="#cb22-170" aria-hidden="true" tabindex="-1"></a><span class="in"># Plot the decision tree</span></span>
<span id="cb22-171"><a href="#cb22-171" aria-hidden="true" tabindex="-1"></a><span class="in">fancyRpartPlot(</span></span>
<span id="cb22-172"><a href="#cb22-172" aria-hidden="true" tabindex="-1"></a><span class="in">  tree_fit$fit,</span></span>
<span id="cb22-173"><a href="#cb22-173" aria-hidden="true" tabindex="-1"></a><span class="in">  split.col = "black", </span></span>
<span id="cb22-174"><a href="#cb22-174" aria-hidden="true" tabindex="-1"></a><span class="in">  nn.col = "black", </span></span>
<span id="cb22-175"><a href="#cb22-175" aria-hidden="true" tabindex="-1"></a><span class="in">  caption = "Trainierter Entscheidungsbaum für cp = 0.1",</span></span>
<span id="cb22-176"><a href="#cb22-176" aria-hidden="true" tabindex="-1"></a><span class="in">  palette = "Set1",</span></span>
<span id="cb22-177"><a href="#cb22-177" aria-hidden="true" tabindex="-1"></a><span class="in">  branch.col = "black",</span></span>
<span id="cb22-178"><a href="#cb22-178" aria-hidden="true" tabindex="-1"></a><span class="in">  digits = 3</span></span>
<span id="cb22-179"><a href="#cb22-179" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb22-180"><a href="#cb22-180" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-181"><a href="#cb22-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-182"><a href="#cb22-182" aria-hidden="true" tabindex="-1"></a>@fig-rpartregspace zeigt Beobachtungen von <span class="in">`rm`</span> und <span class="in">`lstat`</span>, die hinsichtlich ihrer in drei Klassen eingeteilten Ausprägung von <span class="in">`medv`</span> eingefärbt sind. Die durch den CART-Algorithmus gelernten Entscheidungsregeln sind als farbige Paritionen des Regressorraums dargestellt. </span>
<span id="cb22-183"><a href="#cb22-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-186"><a href="#cb22-186" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-187"><a href="#cb22-187" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb22-188"><a href="#cb22-188" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Partitionierung des Regressorraums für `lstat` und `rm` durch Regressionsbaum"</span></span>
<span id="cb22-189"><a href="#cb22-189" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rpartregspace</span></span>
<span id="cb22-190"><a href="#cb22-190" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb22-191"><a href="#cb22-191" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb22-192"><a href="#cb22-192" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cowplot)</span>
<span id="cb22-193"><a href="#cb22-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-194"><a href="#cb22-194" aria-hidden="true" tabindex="-1"></a><span class="co"># Seed setzen</span></span>
<span id="cb22-195"><a href="#cb22-195" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb22-196"><a href="#cb22-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-197"><a href="#cb22-197" aria-hidden="true" tabindex="-1"></a><span class="co"># Datensatz als tibble</span></span>
<span id="cb22-198"><a href="#cb22-198" aria-hidden="true" tabindex="-1"></a>Boston <span class="ot">&lt;-</span> tibble<span class="sc">::</span><span class="fu">as_tibble</span>(MASS<span class="sc">::</span>Boston)</span>
<span id="cb22-199"><a href="#cb22-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-200"><a href="#cb22-200" aria-hidden="true" tabindex="-1"></a><span class="co"># Splitting in Training- und Test-Daten</span></span>
<span id="cb22-201"><a href="#cb22-201" aria-hidden="true" tabindex="-1"></a>Boston_split <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">initial_split</span>(</span>
<span id="cb22-202"><a href="#cb22-202" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> Boston, </span>
<span id="cb22-203"><a href="#cb22-203" aria-hidden="true" tabindex="-1"></a>  <span class="at">prop =</span> <span class="fl">0.8</span>, </span>
<span id="cb22-204"><a href="#cb22-204" aria-hidden="true" tabindex="-1"></a>  <span class="at">strata =</span> medv</span>
<span id="cb22-205"><a href="#cb22-205" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb22-206"><a href="#cb22-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-207"><a href="#cb22-207" aria-hidden="true" tabindex="-1"></a>Boston_train <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">training</span>(Boston_split)</span>
<span id="cb22-208"><a href="#cb22-208" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb22-209"><a href="#cb22-209" aria-hidden="true" tabindex="-1"></a>tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(<span class="at">formula =</span> medv <span class="sc">~</span> rm <span class="sc">+</span> lstat, <span class="at">data =</span> Boston_train, <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">cp =</span> .<span class="dv">1</span>))</span>
<span id="cb22-210"><a href="#cb22-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-211"><a href="#cb22-211" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a grid of values for rm and lstat (the two features of interest)</span></span>
<span id="cb22-212"><a href="#cb22-212" aria-hidden="true" tabindex="-1"></a>grid_rm <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(Boston_train<span class="sc">$</span>rm), <span class="fu">max</span>(Boston_train<span class="sc">$</span>rm), <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb22-213"><a href="#cb22-213" aria-hidden="true" tabindex="-1"></a>grid_lstat <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(Boston_train<span class="sc">$</span>lstat), <span class="fu">max</span>(Boston_train<span class="sc">$</span>lstat), <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb22-214"><a href="#cb22-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-215"><a href="#cb22-215" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate all combinations of grid points</span></span>
<span id="cb22-216"><a href="#cb22-216" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">rm =</span> grid_rm, <span class="at">lstat =</span> grid_lstat)</span>
<span id="cb22-217"><a href="#cb22-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-218"><a href="#cb22-218" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict the median value (medv) for each point in the grid</span></span>
<span id="cb22-219"><a href="#cb22-219" aria-hidden="true" tabindex="-1"></a>grid<span class="sc">$</span>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree, <span class="at">newdata =</span> grid)</span>
<span id="cb22-220"><a href="#cb22-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-221"><a href="#cb22-221" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a scatter plot of the original data points colored by the actual medv values</span></span>
<span id="cb22-222"><a href="#cb22-222" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(Boston_train, <span class="fu">aes</span>(<span class="at">x =</span> rm, <span class="at">y =</span> lstat)) <span class="sc">+</span></span>
<span id="cb22-223"><a href="#cb22-223" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Anz. Räume (rm)"</span>, <span class="at">y =</span> <span class="st">"Anteil Bev. niedriger Status  (lstat)"</span>) <span class="sc">+</span></span>
<span id="cb22-224"><a href="#cb22-224" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-225"><a href="#cb22-225" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add the predicted decision boundary</span></span>
<span id="cb22-226"><a href="#cb22-226" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_tile</span>(<span class="at">data =</span> grid, <span class="fu">aes</span>(<span class="at">x =</span> rm, <span class="at">y =</span> lstat, <span class="at">fill =</span> <span class="fu">as.factor</span>(<span class="fu">round</span>(pred,<span class="dv">2</span>))), <span class="at">alpha =</span> <span class="fl">0.3</span>) <span class="sc">+</span></span>
<span id="cb22-227"><a href="#cb22-227" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_fill_viridis_d</span>(<span class="st">"Vorhersage in Partition"</span>) <span class="sc">+</span>  <span class="co"># Fill scale for predicted medv</span></span>
<span id="cb22-228"><a href="#cb22-228" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">color =</span> <span class="fu">cut</span>(medv, <span class="dv">3</span>))) <span class="sc">+</span>  <span class="co"># Original data points</span></span>
<span id="cb22-229"><a href="#cb22-229" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_color_viridis_d</span>(<span class="st">"medv"</span>) <span class="sc">+</span>  <span class="co"># Color scale for the median value of homes</span></span>
<span id="cb22-230"><a href="#cb22-230" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_cowplot</span>() <span class="sc">+</span>  <span class="co"># Use a clean theme'</span></span>
<span id="cb22-231"><a href="#cb22-231" aria-hidden="true" tabindex="-1"></a>    <span class="fu">coord_cartesian</span>(<span class="at">expand =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb22-232"><a href="#cb22-232" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"top"</span>, <span class="at">legend.direction =</span> <span class="st">"vertical"</span>)</span>
<span id="cb22-233"><a href="#cb22-233" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-234"><a href="#cb22-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-235"><a href="#cb22-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-236"><a href="#cb22-236" aria-hidden="true" tabindex="-1"></a>Für eine datengetriebene Wahl des CP-Parameters $\alpha$ kann Cross Validation (CV) verwendet werden. Hierzu erstellen wir zunächst eine <span class="in">`parsnip`</span>-Spezifikation mit <span class="in">`cost_complexity = tune::tune()`</span> in <span class="in">`decision_tree()`</span> und erstellen einen *workflow* mit <span class="in">`parsnip::workflow()`</span></span>
<span id="cb22-237"><a href="#cb22-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-240"><a href="#cb22-240" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb22-241"><a href="#cb22-241" aria-hidden="true" tabindex="-1"></a><span class="in"># Spezifikation für CV von cost_complexity</span></span>
<span id="cb22-242"><a href="#cb22-242" aria-hidden="true" tabindex="-1"></a><span class="in">tree_spec_cv &lt;- decision_tree(</span></span>
<span id="cb22-243"><a href="#cb22-243" aria-hidden="true" tabindex="-1"></a><span class="in">  cost_complexity = tune()</span></span>
<span id="cb22-244"><a href="#cb22-244" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb22-245"><a href="#cb22-245" aria-hidden="true" tabindex="-1"></a><span class="in">  set_engine("rpart") %&gt;%</span></span>
<span id="cb22-246"><a href="#cb22-246" aria-hidden="true" tabindex="-1"></a><span class="in">  set_mode("regression")</span></span>
<span id="cb22-247"><a href="#cb22-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-248"><a href="#cb22-248" aria-hidden="true" tabindex="-1"></a><span class="in"># Workflow definieren</span></span>
<span id="cb22-249"><a href="#cb22-249" aria-hidden="true" tabindex="-1"></a><span class="in">tree_wf_cv &lt;- workflow() %&gt;%</span></span>
<span id="cb22-250"><a href="#cb22-250" aria-hidden="true" tabindex="-1"></a><span class="in">  add_model(tree_spec_cv) %&gt;%</span></span>
<span id="cb22-251"><a href="#cb22-251" aria-hidden="true" tabindex="-1"></a><span class="in">  add_formula(medv ~ .)</span></span>
<span id="cb22-252"><a href="#cb22-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-253"><a href="#cb22-253" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-254"><a href="#cb22-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-255"><a href="#cb22-255" aria-hidden="true" tabindex="-1"></a>Mit <span class="in">`rsample::vfold_cv()`</span> definieren wir den CV-Prozess: 10-fold CV mit 2 Wiederholungen. <span class="in">`tune::tune_grid()`</span> führt CV anhand des in <span class="in">`tree_wf_cv`</span> definierten workflows durch. Hierbei werden in <span class="in">`cp_grid`</span> festgelegte Werte von <span class="in">`cost_complexity`</span> berücksichtigt. Die mit <span class="in">`yardstick::metric_set(rmse)`</span> festgelegte Verlustfunktion ist der mittlere quadratische Fehler (RMSE).^<span class="co">[</span><span class="ot">Die hier verwedete Funktion ist `yardstick::rmse()`.</span><span class="co">]</span></span>
<span id="cb22-256"><a href="#cb22-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-259"><a href="#cb22-259" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb22-260"><a href="#cb22-260" aria-hidden="true" tabindex="-1"></a><span class="in"># CV-Prozess definieren</span></span>
<span id="cb22-261"><a href="#cb22-261" aria-hidden="true" tabindex="-1"></a><span class="in">cv_folds &lt;- vfold_cv(</span></span>
<span id="cb22-262"><a href="#cb22-262" aria-hidden="true" tabindex="-1"></a><span class="in">  data = Boston_train, </span></span>
<span id="cb22-263"><a href="#cb22-263" aria-hidden="true" tabindex="-1"></a><span class="in">  v = 10, </span></span>
<span id="cb22-264"><a href="#cb22-264" aria-hidden="true" tabindex="-1"></a><span class="in">  repeats = 2</span></span>
<span id="cb22-265"><a href="#cb22-265" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb22-266"><a href="#cb22-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-267"><a href="#cb22-267" aria-hidden="true" tabindex="-1"></a><span class="in"># CV durchführen:</span></span>
<span id="cb22-268"><a href="#cb22-268" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(1234)</span></span>
<span id="cb22-269"><a href="#cb22-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-270"><a href="#cb22-270" aria-hidden="true" tabindex="-1"></a><span class="in"># Grid definieren</span></span>
<span id="cb22-271"><a href="#cb22-271" aria-hidden="true" tabindex="-1"></a><span class="in">cp_grid &lt;- tibble(</span></span>
<span id="cb22-272"><a href="#cb22-272" aria-hidden="true" tabindex="-1"></a><span class="in">  cost_complexity = c(</span></span>
<span id="cb22-273"><a href="#cb22-273" aria-hidden="true" tabindex="-1"></a><span class="in">    0.1, .075, 0.05, 0.01, 0.001, 0.0001</span></span>
<span id="cb22-274"><a href="#cb22-274" aria-hidden="true" tabindex="-1"></a><span class="in">    )</span></span>
<span id="cb22-275"><a href="#cb22-275" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb22-276"><a href="#cb22-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-277"><a href="#cb22-277" aria-hidden="true" tabindex="-1"></a><span class="in"># Tuning mit CV</span></span>
<span id="cb22-278"><a href="#cb22-278" aria-hidden="true" tabindex="-1"></a><span class="in">tree_fit_cv &lt;- tree_wf_cv %&gt;%</span></span>
<span id="cb22-279"><a href="#cb22-279" aria-hidden="true" tabindex="-1"></a><span class="in">    tune_grid(</span></span>
<span id="cb22-280"><a href="#cb22-280" aria-hidden="true" tabindex="-1"></a><span class="in">        resamples = cv_folds, </span></span>
<span id="cb22-281"><a href="#cb22-281" aria-hidden="true" tabindex="-1"></a><span class="in">        grid = cp_grid,</span></span>
<span id="cb22-282"><a href="#cb22-282" aria-hidden="true" tabindex="-1"></a><span class="in">        metrics = metric_set(rmse)</span></span>
<span id="cb22-283"><a href="#cb22-283" aria-hidden="true" tabindex="-1"></a><span class="in">    )</span></span>
<span id="cb22-284"><a href="#cb22-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-285"><a href="#cb22-285" aria-hidden="true" tabindex="-1"></a><span class="in"># CV-Ergebnisse</span></span>
<span id="cb22-286"><a href="#cb22-286" aria-hidden="true" tabindex="-1"></a><span class="in">tree_fit_cv</span></span>
<span id="cb22-287"><a href="#cb22-287" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-288"><a href="#cb22-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-289"><a href="#cb22-289" aria-hidden="true" tabindex="-1"></a>Mit <span class="in">`workflowsets::autoplot()`</span> kann der CV-RMSE für als Funktion des CP-Parameter leicht grafisch betrachtet dargestellt werden.</span>
<span id="cb22-290"><a href="#cb22-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-293"><a href="#cb22-293" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb22-294"><a href="#cb22-294" aria-hidden="true" tabindex="-1"></a><span class="in"># CV-Ergebnisse visualisieren</span></span>
<span id="cb22-295"><a href="#cb22-295" aria-hidden="true" tabindex="-1"></a><span class="in">autoplot(tree_fit_cv) +</span></span>
<span id="cb22-296"><a href="#cb22-296" aria-hidden="true" tabindex="-1"></a><span class="in">  labs(</span></span>
<span id="cb22-297"><a href="#cb22-297" aria-hidden="true" tabindex="-1"></a><span class="in">    title = "CV für CP-Parameter: RMSE vs. Komplexität"</span></span>
<span id="cb22-298"><a href="#cb22-298" aria-hidden="true" tabindex="-1"></a><span class="in">  ) +</span></span>
<span id="cb22-299"><a href="#cb22-299" aria-hidden="true" tabindex="-1"></a><span class="in">  theme_cowplot()</span></span>
<span id="cb22-300"><a href="#cb22-300" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-301"><a href="#cb22-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-302"><a href="#cb22-302" aria-hidden="true" tabindex="-1"></a>Für eine tabellierte Übersicht der besten Modelle kann <span class="in">`tune::show_best()`</span> verwendet werden. <span class="in">`tune::select_best()`</span> liest die beste Parameter-Kombination aus. </span>
<span id="cb22-303"><a href="#cb22-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-306"><a href="#cb22-306" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb22-307"><a href="#cb22-307" aria-hidden="true" tabindex="-1"></a><span class="in"># Tabellarische Übersicht</span></span>
<span id="cb22-308"><a href="#cb22-308" aria-hidden="true" tabindex="-1"></a><span class="in">show_best(</span></span>
<span id="cb22-309"><a href="#cb22-309" aria-hidden="true" tabindex="-1"></a><span class="in">  x = tree_fit_cv, </span></span>
<span id="cb22-310"><a href="#cb22-310" aria-hidden="true" tabindex="-1"></a><span class="in">  metric = "rmse"</span></span>
<span id="cb22-311"><a href="#cb22-311" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb22-312"><a href="#cb22-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-313"><a href="#cb22-313" aria-hidden="true" tabindex="-1"></a><span class="in"># Getunter Paremater</span></span>
<span id="cb22-314"><a href="#cb22-314" aria-hidden="true" tabindex="-1"></a><span class="in">best_tree_fit &lt;- select_best(</span></span>
<span id="cb22-315"><a href="#cb22-315" aria-hidden="true" tabindex="-1"></a><span class="in">  x = tree_fit_cv, </span></span>
<span id="cb22-316"><a href="#cb22-316" aria-hidden="true" tabindex="-1"></a><span class="in">  metric = "rmse"</span></span>
<span id="cb22-317"><a href="#cb22-317" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb22-318"><a href="#cb22-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-319"><a href="#cb22-319" aria-hidden="true" tabindex="-1"></a><span class="in">best_tree_fit</span></span>
<span id="cb22-320"><a href="#cb22-320" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-321"><a href="#cb22-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-322"><a href="#cb22-322" aria-hidden="true" tabindex="-1"></a>Anhand <span class="in">`tree_fit_cv`</span> trainieren wir die finale Spezifikation.</span>
<span id="cb22-323"><a href="#cb22-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-326"><a href="#cb22-326" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb22-327"><a href="#cb22-327" aria-hidden="true" tabindex="-1"></a><span class="in"># Finales Modell schätzen</span></span>
<span id="cb22-328"><a href="#cb22-328" aria-hidden="true" tabindex="-1"></a><span class="in">final_tree_spec &lt;- decision_tree(</span></span>
<span id="cb22-329"><a href="#cb22-329" aria-hidden="true" tabindex="-1"></a><span class="in">  cost_complexity = best_tree_fit$cost_complexity</span></span>
<span id="cb22-330"><a href="#cb22-330" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb22-331"><a href="#cb22-331" aria-hidden="true" tabindex="-1"></a><span class="in">  set_engine("rpart") %&gt;%</span></span>
<span id="cb22-332"><a href="#cb22-332" aria-hidden="true" tabindex="-1"></a><span class="in">  set_mode("regression")</span></span>
<span id="cb22-333"><a href="#cb22-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-334"><a href="#cb22-334" aria-hidden="true" tabindex="-1"></a><span class="in">final_tree_fit &lt;- final_tree_spec %&gt;%</span></span>
<span id="cb22-335"><a href="#cb22-335" aria-hidden="true" tabindex="-1"></a><span class="in">  fit(</span></span>
<span id="cb22-336"><a href="#cb22-336" aria-hidden="true" tabindex="-1"></a><span class="in">    formula = medv ~ ., </span></span>
<span id="cb22-337"><a href="#cb22-337" aria-hidden="true" tabindex="-1"></a><span class="in">    data = Boston_train</span></span>
<span id="cb22-338"><a href="#cb22-338" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb22-339"><a href="#cb22-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-340"><a href="#cb22-340" aria-hidden="true" tabindex="-1"></a><span class="in"># final_tree_fit</span></span>
<span id="cb22-341"><a href="#cb22-341" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-342"><a href="#cb22-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-343"><a href="#cb22-343" aria-hidden="true" tabindex="-1"></a>Der geringe CP-Parameter führt zu einem großen Entscheidungsbaum.^<span class="co">[</span><span class="ot">Die Dimension der Grafik wurde hier zwecks Darstellung des gesamten Baums gewählt. `print(final_tree_fit$fit)` druckt die Entscheidungsregeln in die R-Konsole (hierzu die letzte Zeile ausführen).</span><span class="co">]</span> </span>
<span id="cb22-344"><a href="#cb22-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-347"><a href="#cb22-347" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb22-348"><a href="#cb22-348" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-width: 8</span></span>
<span id="cb22-349"><a href="#cb22-349" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-height: 8</span></span>
<span id="cb22-350"><a href="#cb22-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-351"><a href="#cb22-351" aria-hidden="true" tabindex="-1"></a><span class="in"># CV-Fit plotten</span></span>
<span id="cb22-352"><a href="#cb22-352" aria-hidden="true" tabindex="-1"></a><span class="in">fancyRpartPlot(</span></span>
<span id="cb22-353"><a href="#cb22-353" aria-hidden="true" tabindex="-1"></a><span class="in">  final_tree_fit$fit,</span></span>
<span id="cb22-354"><a href="#cb22-354" aria-hidden="true" tabindex="-1"></a><span class="in">  split.col = "black", </span></span>
<span id="cb22-355"><a href="#cb22-355" aria-hidden="true" tabindex="-1"></a><span class="in">  nn.col = "black", </span></span>
<span id="cb22-356"><a href="#cb22-356" aria-hidden="true" tabindex="-1"></a><span class="in">  caption = "Mit CV ermittelter Entscheidungsbaum",</span></span>
<span id="cb22-357"><a href="#cb22-357" aria-hidden="true" tabindex="-1"></a><span class="in">  palette = "Set1",</span></span>
<span id="cb22-358"><a href="#cb22-358" aria-hidden="true" tabindex="-1"></a><span class="in">  branch.col = "black"</span></span>
<span id="cb22-359"><a href="#cb22-359" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb22-360"><a href="#cb22-360" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-361"><a href="#cb22-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-362"><a href="#cb22-362" aria-hidden="true" tabindex="-1"></a>Zur Beurteilung der Relevanz von Variablen für die Reduktion des Anpassungsfehlers (*variable importance*) kann der Eintrag <span class="in">`variable.importance`</span> des <span class="in">`rpart`</span>-Objekts herangezogen werden. Variable importance misst hier die Gesamtreduktion der Fehlerquadratsumem über alle Knoten, an denen die jeweilige Variable für Splits verwendet wird. </span>
<span id="cb22-363"><a href="#cb22-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-366"><a href="#cb22-366" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb22-367"><a href="#cb22-367" aria-hidden="true" tabindex="-1"></a><span class="in"># Variable-Importance auslesen</span></span>
<span id="cb22-368"><a href="#cb22-368" aria-hidden="true" tabindex="-1"></a><span class="in">final_tree_fit$fit$variable.importance</span></span>
<span id="cb22-369"><a href="#cb22-369" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-370"><a href="#cb22-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-371"><a href="#cb22-371" aria-hidden="true" tabindex="-1"></a>Die Werte von Variable Importance zeigen, dass der mit CV ermittelte Baum *alle* Regressoren in <span class="in">`boston_train`</span> für Splits nutzt, wobei <span class="in">`lstat`</span> und <span class="in">`rm`</span> die relevantesten Variablen sind.</span>
<span id="cb22-372"><a href="#cb22-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-373"><a href="#cb22-373" aria-hidden="true" tabindex="-1"></a>Anhand von Vorhersagen für <span class="in">`medv`</span> mit dem Test-Datensatz <span class="in">`boston_test`</span> können wir das naive Baum-Modell <span class="in">`tree_fit`</span> mit dem durch CV ermittelten Modell <span class="in">`tree_fit_cv`</span> hinsichtlich des Vorhersagefehlers für ungesehene Beobachtungen vergleich. <span class="in">`yardstick::metric()`</span> berechnet hierzu automatisch gängige Statistiken für Regressionsprobleme. </span>
<span id="cb22-374"><a href="#cb22-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-377"><a href="#cb22-377" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb22-378"><a href="#cb22-378" aria-hidden="true" tabindex="-1"></a><span class="in"># Vorhersagegüte naives Modell</span></span>
<span id="cb22-379"><a href="#cb22-379" aria-hidden="true" tabindex="-1"></a><span class="in">tree_pred &lt;- predict(</span></span>
<span id="cb22-380"><a href="#cb22-380" aria-hidden="true" tabindex="-1"></a><span class="in">  object = tree_fit, </span></span>
<span id="cb22-381"><a href="#cb22-381" aria-hidden="true" tabindex="-1"></a><span class="in">  new_data = Boston_test</span></span>
<span id="cb22-382"><a href="#cb22-382" aria-hidden="true" tabindex="-1"></a><span class="in">) %&gt;%</span></span>
<span id="cb22-383"><a href="#cb22-383" aria-hidden="true" tabindex="-1"></a><span class="in">  bind_cols(Boston_test) %&gt;%</span></span>
<span id="cb22-384"><a href="#cb22-384" aria-hidden="true" tabindex="-1"></a><span class="in">  metrics(truth = medv, estimate = .pred)</span></span>
<span id="cb22-385"><a href="#cb22-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-386"><a href="#cb22-386" aria-hidden="true" tabindex="-1"></a><span class="in"># Vorhersagegüte bei CV</span></span>
<span id="cb22-387"><a href="#cb22-387" aria-hidden="true" tabindex="-1"></a><span class="in">tree_pred_cv &lt;- predict(</span></span>
<span id="cb22-388"><a href="#cb22-388" aria-hidden="true" tabindex="-1"></a><span class="in">  object = final_tree_fit, </span></span>
<span id="cb22-389"><a href="#cb22-389" aria-hidden="true" tabindex="-1"></a><span class="in">  new_data = Boston_test</span></span>
<span id="cb22-390"><a href="#cb22-390" aria-hidden="true" tabindex="-1"></a><span class="in">) %&gt;%</span></span>
<span id="cb22-391"><a href="#cb22-391" aria-hidden="true" tabindex="-1"></a><span class="in">  bind_cols(Boston_test) %&gt;%</span></span>
<span id="cb22-392"><a href="#cb22-392" aria-hidden="true" tabindex="-1"></a><span class="in">  metrics(truth = medv, estimate = .pred)</span></span>
<span id="cb22-393"><a href="#cb22-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-394"><a href="#cb22-394" aria-hidden="true" tabindex="-1"></a><span class="in">tree_pred</span></span>
<span id="cb22-395"><a href="#cb22-395" aria-hidden="true" tabindex="-1"></a><span class="in">tree_pred_cv</span></span>
<span id="cb22-396"><a href="#cb22-396" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-397"><a href="#cb22-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-398"><a href="#cb22-398" aria-hidden="true" tabindex="-1"></a>Der Vergleich zeigt eine bessere Vorsageleistung des großen Baums in <span class="in">`tree_fit_cv`</span>. In diesem Fall scheint CP-Pruning wenig hilfreich zu sein. Tatsächlich liefert ein Baum mit $\alpha=0$ bessere Vorhersagen als <span class="in">`tree_fit_cv`</span> (überprüfe dies!). </span>
<span id="cb22-399"><a href="#cb22-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-400"><a href="#cb22-400" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bagging</span></span>
<span id="cb22-401"><a href="#cb22-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-402"><a href="#cb22-402" aria-hidden="true" tabindex="-1"></a>*Bagging* ist eine Ensemble-Modelle, die durch aus einer Kombination von vielen Entscheidungsbäumen bestehen. Bagging steht für *Bootstrap Aggregating* und nutzt einen Algorithmus, bei dem Bäume auf *zufälligen* Stichproben aus dem Trainingsdatensatz angepasst werden: Jeder Baum wird auf einer *Bootstrap-Stichprobe* (siehe @sec-sim) trainiert, die durch zufällige Züge (mit Zurücklegen) erstellt wird. Nach dem Training aggregiert Bagging die Vorhersagen aller Bäume des Ensembles.</span>
<span id="cb22-403"><a href="#cb22-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-404"><a href="#cb22-404" aria-hidden="true" tabindex="-1"></a>Der Vorteil von Bagging gegenüber einem einzelnen Entscheidungsbaum ist, dass die Varianz der Vorhersage deutlich reduziert werden kann: Einzelne Entscheidungsbäume neigen dazu, Muster in den Trainingsdaten zu lernen, die sich zufällig aus der Zusammensetzung der Stichprobe ergeben und nicht repräsentativ für Zusammenhänge zwischen den Regressoren und der Outcome-Variable sind. Diese Überanpassung führt zu hoher Varianz auf von Vorhersagen für ungesehene Daten. Durch das Training vieler Bäume auf unterschiedlichen *zufälligen* Stichproben aus den Trainingsdaten und das anschließende Aggregieren kann der negative Effekt der Überanpassung auf die Unsicherheit der Vorhersage einzelner Bäume reduziert werden.</span>
<span id="cb22-405"><a href="#cb22-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-406"><a href="#cb22-406" aria-hidden="true" tabindex="-1"></a>Eine Bagging-Spezifikation kann mit <span class="in">`parsnip::bag_tree()`</span> festgelegt werden. Mit <span class="in">`times = 500`</span> wird definiert, dass der Bagging-Algorithmus ein Ensemble mit 500 Bäumen (mit CART) anpassen soll. Das Training und die Vorhersage auf den Testdaten erfolgt analog zur Vorgehensweise in @sec-simpletrees.</span>
<span id="cb22-407"><a href="#cb22-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-410"><a href="#cb22-410" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb22-411"><a href="#cb22-411" aria-hidden="true" tabindex="-1"></a><span class="in"># Spezifikation für Bagging</span></span>
<span id="cb22-412"><a href="#cb22-412" aria-hidden="true" tabindex="-1"></a><span class="in">bagging_spec &lt;- bag_tree() %&gt;%</span></span>
<span id="cb22-413"><a href="#cb22-413" aria-hidden="true" tabindex="-1"></a><span class="in">  set_engine(</span></span>
<span id="cb22-414"><a href="#cb22-414" aria-hidden="true" tabindex="-1"></a><span class="in">    engine = "rpart",</span></span>
<span id="cb22-415"><a href="#cb22-415" aria-hidden="true" tabindex="-1"></a><span class="in">    times = 500</span></span>
<span id="cb22-416"><a href="#cb22-416" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb22-417"><a href="#cb22-417" aria-hidden="true" tabindex="-1"></a><span class="in">  set_mode("regression")</span></span>
<span id="cb22-418"><a href="#cb22-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-419"><a href="#cb22-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-420"><a href="#cb22-420" aria-hidden="true" tabindex="-1"></a><span class="in"># Training durchführen</span></span>
<span id="cb22-421"><a href="#cb22-421" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(1234)</span></span>
<span id="cb22-422"><a href="#cb22-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-423"><a href="#cb22-423" aria-hidden="true" tabindex="-1"></a><span class="in">bagging_fit &lt;- bagging_spec %&gt;%</span></span>
<span id="cb22-424"><a href="#cb22-424" aria-hidden="true" tabindex="-1"></a><span class="in">  fit(</span></span>
<span id="cb22-425"><a href="#cb22-425" aria-hidden="true" tabindex="-1"></a><span class="in">    formula = medv ~ ., </span></span>
<span id="cb22-426"><a href="#cb22-426" aria-hidden="true" tabindex="-1"></a><span class="in">    data = Boston_train</span></span>
<span id="cb22-427"><a href="#cb22-427" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb22-428"><a href="#cb22-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-429"><a href="#cb22-429" aria-hidden="true" tabindex="-1"></a><span class="in"># Auswertung</span></span>
<span id="cb22-430"><a href="#cb22-430" aria-hidden="true" tabindex="-1"></a><span class="in">bagging_pred &lt;- predict(</span></span>
<span id="cb22-431"><a href="#cb22-431" aria-hidden="true" tabindex="-1"></a><span class="in">  object = bagging_fit, </span></span>
<span id="cb22-432"><a href="#cb22-432" aria-hidden="true" tabindex="-1"></a><span class="in">  new_data = Boston_test</span></span>
<span id="cb22-433"><a href="#cb22-433" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb22-434"><a href="#cb22-434" aria-hidden="true" tabindex="-1"></a><span class="in">  bind_cols(Boston_test) %&gt;%</span></span>
<span id="cb22-435"><a href="#cb22-435" aria-hidden="true" tabindex="-1"></a><span class="in">  metrics(</span></span>
<span id="cb22-436"><a href="#cb22-436" aria-hidden="true" tabindex="-1"></a><span class="in">    truth = medv,</span></span>
<span id="cb22-437"><a href="#cb22-437" aria-hidden="true" tabindex="-1"></a><span class="in">    estimate = .pred</span></span>
<span id="cb22-438"><a href="#cb22-438" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb22-439"><a href="#cb22-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-440"><a href="#cb22-440" aria-hidden="true" tabindex="-1"></a><span class="in">bagging_pred</span></span>
<span id="cb22-441"><a href="#cb22-441" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-442"><a href="#cb22-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-443"><a href="#cb22-443" aria-hidden="true" tabindex="-1"></a>Die Auswertung auf den Testdatensatz ergibt eine deutliche Verbesserung der Vorhersageleistung gegenüber einem einfachen Regressionsbaum.</span>
<span id="cb22-444"><a href="#cb22-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-445"><a href="#cb22-445" aria-hidden="true" tabindex="-1"></a>Obwohl die Bäume beim Bagging auf unterschiedlichen Stichproben trainiert werden, kann innerhalb des Ensembles dennoch eine deutliche Korrelation vorliegen: Da jeder Baum auf alle Regressoren für Splits zugreift, können trotz Bootstrapping ähnliche (unverteilhafte) Muster aus dem Datensatz erlernt werden, was sich nachteilig auf die Generalisierungsfähigkeit auswirken kann. Diese Korrelation mindert die Effektivität von Bagging, da stark korrelierte Bäume dazu neigen, ähnliche Fehler zu machen.</span>
<span id="cb22-446"><a href="#cb22-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-447"><a href="#cb22-447" aria-hidden="true" tabindex="-1"></a><span class="fu">## Random Forests {#sec-brf}</span></span>
<span id="cb22-448"><a href="#cb22-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-449"><a href="#cb22-449" aria-hidden="true" tabindex="-1"></a>*Random Forests* erweitern Bagging, indem zusätzlich bei jedem Knoten innerhalb jedes Baumes eine *zufällige Teilmenge der Regressoren* als potentielle Variable für die Split-Regel ausgewählt wird. Dies führt zu einer Reduktion der Korrelation zwischen den Bäumen, was die Genauigkeit verbessert und das Risiko von Overfitting weiter verringert.</span>
<span id="cb22-450"><a href="#cb22-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-451"><a href="#cb22-451" aria-hidden="true" tabindex="-1"></a>In R erstellen wir die Spezifikation mit <span class="in">`parsnip::rand_forest()`</span>. Der Parameter <span class="in">`mtry`</span> legt fest, wie viele Regressoren $m$ zufällig für jeden Split zur Verfügung stehen. Wir nutzen den im <span class="in">`randomForest`</span>-Paket implementierten Algorithmus und legen in <span class="in">`set_engine()`</span> fest, dass die von <span class="in">`randomForest::randomForest()`</span> berechnete Fehler-Metrik im Output-Objekt ausgegeben wird (<span class="in">`tree.err = TRUE`</span>). Um die Spezifikation für verschiedene Werte von <span class="in">`mtry`</span> anwenden zu können, implementieren wir die Spezifikation innerhalb einer Wrapper-Funktion <span class="in">`rf_spec_mtry()`</span>. Mit <span class="in">`purrr::map()`</span> iterieren wir <span class="in">`rf_spec_mtry()`</span> über drei verschiedene Werte für den Tuning-Parameter <span class="in">`mtry`</span> (4, 6 und 10 Variablen).^<span class="co">[</span><span class="ot">Eine Faustregel für die Wahl von $m$ bei $k$ verfügbaren Regressoren ist $m\approx\sqrt{k}$.</span><span class="co">]</span></span>
<span id="cb22-452"><a href="#cb22-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-455"><a href="#cb22-455" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb22-456"><a href="#cb22-456" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(1234)</span></span>
<span id="cb22-457"><a href="#cb22-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-458"><a href="#cb22-458" aria-hidden="true" tabindex="-1"></a><span class="in"># Werte für mtry</span></span>
<span id="cb22-459"><a href="#cb22-459" aria-hidden="true" tabindex="-1"></a><span class="in">mtry_values &lt;- c(4, 6, 10)</span></span>
<span id="cb22-460"><a href="#cb22-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-461"><a href="#cb22-461" aria-hidden="true" tabindex="-1"></a><span class="in"># Funktion: Random Forest für mtry = m</span></span>
<span id="cb22-462"><a href="#cb22-462" aria-hidden="true" tabindex="-1"></a><span class="in">rf_spec_mtry &lt;- function(m) {</span></span>
<span id="cb22-463"><a href="#cb22-463" aria-hidden="true" tabindex="-1"></a><span class="in">  rand_forest(mtry = m, trees = 500) %&gt;%</span></span>
<span id="cb22-464"><a href="#cb22-464" aria-hidden="true" tabindex="-1"></a><span class="in">    set_engine(</span></span>
<span id="cb22-465"><a href="#cb22-465" aria-hidden="true" tabindex="-1"></a><span class="in">      engine = "randomForest", </span></span>
<span id="cb22-466"><a href="#cb22-466" aria-hidden="true" tabindex="-1"></a><span class="in">      tree.err = TRUE</span></span>
<span id="cb22-467"><a href="#cb22-467" aria-hidden="true" tabindex="-1"></a><span class="in">    ) %&gt;%</span></span>
<span id="cb22-468"><a href="#cb22-468" aria-hidden="true" tabindex="-1"></a><span class="in">    set_mode("regression")</span></span>
<span id="cb22-469"><a href="#cb22-469" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb22-470"><a href="#cb22-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-471"><a href="#cb22-471" aria-hidden="true" tabindex="-1"></a><span class="in"># Modelle für verschiedene mtry-Werte trainieren</span></span>
<span id="cb22-472"><a href="#cb22-472" aria-hidden="true" tabindex="-1"></a><span class="in">rf_fits &lt;- map(</span></span>
<span id="cb22-473"><a href="#cb22-473" aria-hidden="true" tabindex="-1"></a><span class="in">  .x = mtry_values, </span></span>
<span id="cb22-474"><a href="#cb22-474" aria-hidden="true" tabindex="-1"></a><span class="in">  .f = ~ rf_spec_mtry(.x) %&gt;%</span></span>
<span id="cb22-475"><a href="#cb22-475" aria-hidden="true" tabindex="-1"></a><span class="in">    fit(</span></span>
<span id="cb22-476"><a href="#cb22-476" aria-hidden="true" tabindex="-1"></a><span class="in">      formula = medv ~ ., </span></span>
<span id="cb22-477"><a href="#cb22-477" aria-hidden="true" tabindex="-1"></a><span class="in">      data = Boston_train</span></span>
<span id="cb22-478"><a href="#cb22-478" aria-hidden="true" tabindex="-1"></a><span class="in">    )</span></span>
<span id="cb22-479"><a href="#cb22-479" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb22-480"><a href="#cb22-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-481"><a href="#cb22-481" aria-hidden="true" tabindex="-1"></a><span class="in">rf_fits &lt;- set_names(</span></span>
<span id="cb22-482"><a href="#cb22-482" aria-hidden="true" tabindex="-1"></a><span class="in">  x = rf_fits,</span></span>
<span id="cb22-483"><a href="#cb22-483" aria-hidden="true" tabindex="-1"></a><span class="in">  nm =  paste0("rf_mtry", mtry_values, "_fit")</span></span>
<span id="cb22-484"><a href="#cb22-484" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb22-485"><a href="#cb22-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-486"><a href="#cb22-486" aria-hidden="true" tabindex="-1"></a><span class="in"># Ausgabe der Ergebnisse</span></span>
<span id="cb22-487"><a href="#cb22-487" aria-hidden="true" tabindex="-1"></a><span class="in">rf_fits</span></span>
<span id="cb22-488"><a href="#cb22-488" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-489"><a href="#cb22-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-490"><a href="#cb22-490" aria-hidden="true" tabindex="-1"></a>Für eine Beurteilung des Vorhersageleistung dieser drei Modelle können wir den *Out-of-Bag*-Fehler (OOB) verwenden: </span>
<span id="cb22-491"><a href="#cb22-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-492"><a href="#cb22-492" aria-hidden="true" tabindex="-1"></a>Der OOB-Fehler ist eine Schätzung des Generalisierungsfehlers ohne einen separaten Testdatensatzes. Bei Random Forests (und Bagging) ist dies aufgrund der Berechnung des Ensembles für Bootstrap-Stichproben möglich: Grob ein Drittel der Beobachtungen des Datensatzes sind nicht Teil der Stichprobe, die für das Training jedes Baums im Ensemble genereiert werden.^<span class="co">[</span><span class="ot">Beachte, dass beim Bootstrap $n$ aus $n$  Beobachtungen mit Zurücklegen gezogen werden. Die Wahrscheinlicht, dass eine Beobachtung *nicht* gezogen wird ("Out-of-Bag"), ist $(1-1/n)^n\approx37\%$.</span><span class="co">]</span> Diese nicht gezogenen Datenpunkte sind OOB-Beobachtungen. Der OOB-Fehler des Ensembles ist der durchschnittliche Fehler für die aggregierten Vorhersagen der Bäume des Forests.</span>
<span id="cb22-493"><a href="#cb22-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-494"><a href="#cb22-494" aria-hidden="true" tabindex="-1"></a>Der OOB-Fehler kann auch verwendet werden, um die erforderliche Größe des Random Forests zu beurteilen: Eine größere Anzahl von Bäumen reduziert tendenziell die Varianz der Vorhersagen und verbessert die Generalisierungsfähigkeit. Allerdings nimmt dieser Effekt ab, und ab einer bestimmten Baumanzahl sind weitere Verbesserungen marginal. Obwohl das Risiko von Überanpassung durch viele Bäume aufgrund des Bagging minimal ist, kann es bei großen Datensätzen sinnvoll sein, kleinere Wälder zu trainieren, um den Rechenaufwand zu verringern. Wir plotten hierfür den OOB-Fehler für das Modell mit <span class="in">`mtry = 10`</span> gegen die Anzahl der Bäume.</span>
<span id="cb22-495"><a href="#cb22-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-498"><a href="#cb22-498" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb22-499"><a href="#cb22-499" aria-hidden="true" tabindex="-1"></a><span class="in">library(ggRandomForests)</span></span>
<span id="cb22-500"><a href="#cb22-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-501"><a href="#cb22-501" aria-hidden="true" tabindex="-1"></a><span class="in"># OOB-Fehler als Funktion der Baumanzahl</span></span>
<span id="cb22-502"><a href="#cb22-502" aria-hidden="true" tabindex="-1"></a><span class="in">rf_fits$rf_mtry10_fit$fit %&gt;% </span></span>
<span id="cb22-503"><a href="#cb22-503" aria-hidden="true" tabindex="-1"></a><span class="in">  gg_error() %&gt;% </span></span>
<span id="cb22-504"><a href="#cb22-504" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb22-505"><a href="#cb22-505" aria-hidden="true" tabindex="-1"></a><span class="in">  plot() + </span></span>
<span id="cb22-506"><a href="#cb22-506" aria-hidden="true" tabindex="-1"></a><span class="in">  labs(</span></span>
<span id="cb22-507"><a href="#cb22-507" aria-hidden="true" tabindex="-1"></a><span class="in">    title = "Random Forest: Ensemblegröße vs. OOB-Fehler (mtry = 10)"</span></span>
<span id="cb22-508"><a href="#cb22-508" aria-hidden="true" tabindex="-1"></a><span class="in">  ) + </span></span>
<span id="cb22-509"><a href="#cb22-509" aria-hidden="true" tabindex="-1"></a><span class="in">  theme_cowplot()</span></span>
<span id="cb22-510"><a href="#cb22-510" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-511"><a href="#cb22-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-512"><a href="#cb22-512" aria-hidden="true" tabindex="-1"></a>Die Grafik zeigt, dass die Verbesserung des OOB-Fehlers jenseits von 250 Beobachtungen deutlich nachlässt, sodass ein Training von 500 Bäumen ausreichend scheint.</span>
<span id="cb22-513"><a href="#cb22-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-514"><a href="#cb22-514" aria-hidden="true" tabindex="-1"></a>Zur Beurteiliung der Vorhersagegüte mit dem Testdatensatz gehen wir analog zum Training vor und iterieren mit <span class="in">`map()`</span> über <span class="in">`rf_fits`</span>, die Liste der angepassten Modelle.</span>
<span id="cb22-515"><a href="#cb22-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-518"><a href="#cb22-518" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb22-519"><a href="#cb22-519" aria-hidden="true" tabindex="-1"></a><span class="in"># Vorhersage und Berechnung v. Metriken für jeden RF</span></span>
<span id="cb22-520"><a href="#cb22-520" aria-hidden="true" tabindex="-1"></a><span class="in">rf_predictions &lt;- map(</span></span>
<span id="cb22-521"><a href="#cb22-521" aria-hidden="true" tabindex="-1"></a><span class="in">  .x = rf_fits, </span></span>
<span id="cb22-522"><a href="#cb22-522" aria-hidden="true" tabindex="-1"></a><span class="in">  .f =  ~ predict(.x, Boston_test) %&gt;%</span></span>
<span id="cb22-523"><a href="#cb22-523" aria-hidden="true" tabindex="-1"></a><span class="in">    bind_cols(Boston_test) %&gt;%</span></span>
<span id="cb22-524"><a href="#cb22-524" aria-hidden="true" tabindex="-1"></a><span class="in">    metrics(</span></span>
<span id="cb22-525"><a href="#cb22-525" aria-hidden="true" tabindex="-1"></a><span class="in">      truth = medv, </span></span>
<span id="cb22-526"><a href="#cb22-526" aria-hidden="true" tabindex="-1"></a><span class="in">      estimate = .pred</span></span>
<span id="cb22-527"><a href="#cb22-527" aria-hidden="true" tabindex="-1"></a><span class="in">    )</span></span>
<span id="cb22-528"><a href="#cb22-528" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb22-529"><a href="#cb22-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-530"><a href="#cb22-530" aria-hidden="true" tabindex="-1"></a><span class="in"># Einträge benennen</span></span>
<span id="cb22-531"><a href="#cb22-531" aria-hidden="true" tabindex="-1"></a><span class="in">rf_predictions &lt;- set_names(</span></span>
<span id="cb22-532"><a href="#cb22-532" aria-hidden="true" tabindex="-1"></a><span class="in">  x = rf_predictions,</span></span>
<span id="cb22-533"><a href="#cb22-533" aria-hidden="true" tabindex="-1"></a><span class="in">  nm =  paste0("rf_mtry", mtry_values, "_pred")</span></span>
<span id="cb22-534"><a href="#cb22-534" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb22-535"><a href="#cb22-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-536"><a href="#cb22-536" aria-hidden="true" tabindex="-1"></a><span class="in">rf_predictions</span></span>
<span id="cb22-537"><a href="#cb22-537" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-538"><a href="#cb22-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-539"><a href="#cb22-539" aria-hidden="true" tabindex="-1"></a>Ähnlich wie für einen einzelnen Baum kann die Relevanz von Variablen anhand der Reduktion der Loss-Funktion durch das Ensemble beurteilt werden. Für einen einfachen Vergleich der Variable Importance für den Random Forests mit <span class="in">`mtry = 10`</span> in <span class="in">`rf_fits$rf_mtry10_fit`</span> nutzen wir <span class="in">`ggRandomForests::gg_vimp()`</span>.</span>
<span id="cb22-540"><a href="#cb22-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-543"><a href="#cb22-543" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb22-544"><a href="#cb22-544" aria-hidden="true" tabindex="-1"></a><span class="in"># Variable importance für mtry = 10</span></span>
<span id="cb22-545"><a href="#cb22-545" aria-hidden="true" tabindex="-1"></a><span class="in">rf_fits$rf_mtry10_fit$fit %&gt;%</span></span>
<span id="cb22-546"><a href="#cb22-546" aria-hidden="true" tabindex="-1"></a><span class="in">  gg_vimp()  %&gt;%</span></span>
<span id="cb22-547"><a href="#cb22-547" aria-hidden="true" tabindex="-1"></a><span class="in">  plot() +</span></span>
<span id="cb22-548"><a href="#cb22-548" aria-hidden="true" tabindex="-1"></a><span class="in">    labs(</span></span>
<span id="cb22-549"><a href="#cb22-549" aria-hidden="true" tabindex="-1"></a><span class="in">      title = "Variable Importance für Random Forest (mtry = 10)"</span></span>
<span id="cb22-550"><a href="#cb22-550" aria-hidden="true" tabindex="-1"></a><span class="in">    ) +</span></span>
<span id="cb22-551"><a href="#cb22-551" aria-hidden="true" tabindex="-1"></a><span class="in">    theme_cowplot()</span></span>
<span id="cb22-552"><a href="#cb22-552" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-553"><a href="#cb22-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-554"><a href="#cb22-554" aria-hidden="true" tabindex="-1"></a>Die Grafik bestärkt unsere Schlussfolgerung aus der Analyse des (mit CART trainierten) einzelnen Entscheidungsbaums in @sec-simpletrees, dass <span class="in">`rm`</span> und <span class="in">`lstat`</span> die wichtigsten Regressoren für die Vorhersage von <span class="in">`medv`</span> sind.</span>
<span id="cb22-555"><a href="#cb22-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-556"><a href="#cb22-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-557"><a href="#cb22-557" aria-hidden="true" tabindex="-1"></a><span class="fu">## Boosting {#sec-boosting}</span></span>
<span id="cb22-558"><a href="#cb22-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-559"><a href="#cb22-559" aria-hidden="true" tabindex="-1"></a>Boosting ist eine leistungsstarke Ensemble-Methode für Vorhersagen, die kleine Modelle (oft Entscheidungsbäume geringer Tiefe) sukzessiv trainiert und zu einem starken Modell kombiniert. Anders als bei Random Forests, bei denen viele Bäume unabhängig voneinander auf zufälligen Stichproben der Daten trainiert werden, geht ein Boosting-Algorithmuss sequentiell vor: Jeder nachfolgende Baum wird darauf optimiert, die Fehler des vorherigen Modells zu reduzieren. Die Idee hierbei ist es, iterativ "schwache" Modelle zu erzeugen, die eine gute Anpassung für Datenpunkte liefern, die in den vorherigen Durchläufen schlecht vorhergesagt wurden.</span>
<span id="cb22-560"><a href="#cb22-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-561"><a href="#cb22-561" aria-hidden="true" tabindex="-1"></a>Für einen Trainingsdatensatz $<span class="sc">\{</span>(x_i, y_i)<span class="sc">\}</span>_{i=1}^n$, wobei $x_i$ die Input-Features und $y_i$ Beobachtungen des Outcomes sind, kann Boosting wiefolgt durchgeführt werden.</span>
<span id="cb22-562"><a href="#cb22-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-563"><a href="#cb22-563" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb22-564"><a href="#cb22-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-565"><a href="#cb22-565" aria-hidden="true" tabindex="-1"></a><span class="fu">## Boosting für Regression</span></span>
<span id="cb22-566"><a href="#cb22-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-567"><a href="#cb22-567" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Initialisierung**: Initialisiere das Boosting-Modell als $\widehat{F}_0(x)$. Setze die Residuen $r^0_i=y_i$ für alle $i$</span>
<span id="cb22-568"><a href="#cb22-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-569"><a href="#cb22-569" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Iteration**: Wiederhole die folgenden Schritte für $b = 1,2,\dots,B$ mit $B$ hinreichend groß:</span>
<span id="cb22-570"><a href="#cb22-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-571"><a href="#cb22-571" aria-hidden="true" tabindex="-1"></a>    2.1 **Base Learner**: Trainiere Baum $T_b$ mit $\{(\boldsymbol{x}_i, r^{b-1}_i)\}_{i=1}^n$ für die Vorhersage des *Fehlers* der vorherigen Iteration $r^{b-1}$.</span>
<span id="cb22-572"><a href="#cb22-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-573"><a href="#cb22-573" aria-hidden="true" tabindex="-1"></a>    2.2 **Aktualisierung**: Aktualisiere das Boosting-Modell,</span>
<span id="cb22-574"><a href="#cb22-574" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-575"><a href="#cb22-575" aria-hidden="true" tabindex="-1"></a>      \begin{align*}</span>
<span id="cb22-576"><a href="#cb22-576" aria-hidden="true" tabindex="-1"></a>      \widehat{F}_{b}(\boldsymbol{x}) = \widehat{F}_{b-1}(\boldsymbol{x}) + \eta \cdot T_{b}(\boldsymbol{x}),</span>
<span id="cb22-577"><a href="#cb22-577" aria-hidden="true" tabindex="-1"></a>      \end{align*}</span>
<span id="cb22-578"><a href="#cb22-578" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb22-579"><a href="#cb22-579" aria-hidden="true" tabindex="-1"></a>      wobei $\eta$ die (oft klein gewählte) *Lernrate* ist.</span>
<span id="cb22-580"><a href="#cb22-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-581"><a href="#cb22-581" aria-hidden="true" tabindex="-1"></a>    2.3 **Fehlerberechnung**: Berechne die Residuen $r^b_i$ als Differenzen zwischen dem tatsächlichen Werten $y_i$ und den Vorhersage des aktuellen Modells $\widehat{F}_m(\boldsymbol{x}_i)$,</span>
<span id="cb22-582"><a href="#cb22-582" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-583"><a href="#cb22-583" aria-hidden="true" tabindex="-1"></a>      \begin{align*}</span>
<span id="cb22-584"><a href="#cb22-584" aria-hidden="true" tabindex="-1"></a>      r^b_i = y_i - \widehat{F}_b(\boldsymbol{x}_i).</span>
<span id="cb22-585"><a href="#cb22-585" aria-hidden="true" tabindex="-1"></a>      \end{align*}</span>
<span id="cb22-586"><a href="#cb22-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-587"><a href="#cb22-587" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Output**: Gib das finale Modell aus:</span>
<span id="cb22-588"><a href="#cb22-588" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb22-589"><a href="#cb22-589" aria-hidden="true" tabindex="-1"></a>    \begin{align*}</span>
<span id="cb22-590"><a href="#cb22-590" aria-hidden="true" tabindex="-1"></a>      \widehat{F}(\boldsymbol{x}) := \sum_{b=1}^B \eta\cdot \widehat{F}^b(\boldsymbol{x})</span>
<span id="cb22-591"><a href="#cb22-591" aria-hidden="true" tabindex="-1"></a>    \end{align*}</span>
<span id="cb22-592"><a href="#cb22-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-593"><a href="#cb22-593" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb22-594"><a href="#cb22-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-595"><a href="#cb22-595" aria-hidden="true" tabindex="-1"></a>Der Parameter $0\leq\eta\leq0$ steuert, wie stark der Einfluss jedes neuen Baumes auf das Modell ist. Eine kleine Lernrate führt dazu, dass viele Bäume benötigt werden, was Vorhersagen (ähnlich wie bei Bagging) stabiler macht. Beachte die sequentielle Natur des Trainings: Die $r^b_i$ in Schritt 2.3 sind die zu vorhersagenden Outcome-Variable für den nächsten Baum. $T_{b+1}$ wird trainiert wird, um den *Fehler des bisherigen Modells* $\widehat{F}_b$ zu erklären.</span>
<span id="cb22-596"><a href="#cb22-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-597"><a href="#cb22-597" aria-hidden="true" tabindex="-1"></a>Für die Anwendung auf <span class="in">`MASS::Boston`</span> in R nutzen wir den im Paket <span class="in">`gbm`</span> implementierten *Gradient-Boosting*-Algorithmus. Bei Gradient Boosting wird jeder Baum so trainiert, dass er den negativen Gradienten einer Verlustfunktion approximiert, also die Richtung des größten Fehlers. Das Modell wird schrittweise verbessert, indem es entlang des Gradienten aktualisiert wird, um die Vorhersagegüe zu optimieren; siehe @Hastieetal2013 für eine detaillierte Erläuterung.</span>
<span id="cb22-598"><a href="#cb22-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-599"><a href="#cb22-599" aria-hidden="true" tabindex="-1"></a>Mit dem nachfolgenden Code-Chunk trainieren wir ein Boosting-Modell für Regression mit 5000 einfachen Bäumen (<span class="in">`n.trees = 5000`</span>) mit einer maximalen Tiefe von 2 (<span class="in">`interaction.depth = 2`</span>), d.h. es folgen maximal 2 Entscheidungs-Regeln nacheinander. Um das Risiko von Overfitting gering zu halten, erlauben wir nur Splits, die zu mindestens zwei Beobachtungen in resultierenden nodes führen (<span class="in">`n.minobsinnode = 2`</span>). Die Lernrate (Beitrag der Base Learner zum Ensemble) wird typischerweise klein (und in Abhängigkeit von <span class="in">`n.trees`</span>) gewählt (<span class="in">`shrinkage = 0.001`</span>).^<span class="co">[</span><span class="ot">Je kleiner die Lernrate, desto größer sollte `n.trees` gewählt werden.</span><span class="co">]</span></span>
<span id="cb22-600"><a href="#cb22-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-603"><a href="#cb22-603" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb22-604"><a href="#cb22-604" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(1234)</span></span>
<span id="cb22-605"><a href="#cb22-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-606"><a href="#cb22-606" aria-hidden="true" tabindex="-1"></a><span class="in"># Gradient Boosting durchführen</span></span>
<span id="cb22-607"><a href="#cb22-607" aria-hidden="true" tabindex="-1"></a><span class="in">gbm_model &lt;- gbm(</span></span>
<span id="cb22-608"><a href="#cb22-608" aria-hidden="true" tabindex="-1"></a><span class="in">  formula = medv ~ ., </span></span>
<span id="cb22-609"><a href="#cb22-609" aria-hidden="true" tabindex="-1"></a><span class="in">  data = Boston_train, </span></span>
<span id="cb22-610"><a href="#cb22-610" aria-hidden="true" tabindex="-1"></a><span class="in">  distribution = "gaussian", # für Regression</span></span>
<span id="cb22-611"><a href="#cb22-611" aria-hidden="true" tabindex="-1"></a><span class="in">  n.trees = 5000,           # Anz. Bäume</span></span>
<span id="cb22-612"><a href="#cb22-612" aria-hidden="true" tabindex="-1"></a><span class="in">  interaction.depth = 2,     # Maximale Tiefe der base learner</span></span>
<span id="cb22-613"><a href="#cb22-613" aria-hidden="true" tabindex="-1"></a><span class="in">  shrinkage = 0.01,         # Lernrate</span></span>
<span id="cb22-614"><a href="#cb22-614" aria-hidden="true" tabindex="-1"></a><span class="in">  n.minobsinnode = 2         # Min. Beobachtungen in nodes</span></span>
<span id="cb22-615"><a href="#cb22-615" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb22-616"><a href="#cb22-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-617"><a href="#cb22-617" aria-hidden="true" tabindex="-1"></a><span class="in">gbm_model </span></span>
<span id="cb22-618"><a href="#cb22-618" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-619"><a href="#cb22-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-620"><a href="#cb22-620" aria-hidden="true" tabindex="-1"></a>Für die Vorhersagen auf dem Test-Datensatz legen wir mit <span class="in">`n.trees = gbm_model$n.trees`</span> fest, dass das gesamte Ensemble genutzt werden soll.</span>
<span id="cb22-621"><a href="#cb22-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-624"><a href="#cb22-624" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb22-625"><a href="#cb22-625" aria-hidden="true" tabindex="-1"></a><span class="in"># Vorhersagen Test-Datensatz</span></span>
<span id="cb22-626"><a href="#cb22-626" aria-hidden="true" tabindex="-1"></a><span class="in">gbm_predictions &lt;- predict(</span></span>
<span id="cb22-627"><a href="#cb22-627" aria-hidden="true" tabindex="-1"></a><span class="in">  object = gbm_model, </span></span>
<span id="cb22-628"><a href="#cb22-628" aria-hidden="true" tabindex="-1"></a><span class="in">  newdata = Boston_test, </span></span>
<span id="cb22-629"><a href="#cb22-629" aria-hidden="true" tabindex="-1"></a><span class="in">  n.trees = gbm_model$n.trees # gesamtes Ensemble nutzen</span></span>
<span id="cb22-630"><a href="#cb22-630" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb22-631"><a href="#cb22-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-632"><a href="#cb22-632" aria-hidden="true" tabindex="-1"></a><span class="in"># Auswertung Test-Datensatz</span></span>
<span id="cb22-633"><a href="#cb22-633" aria-hidden="true" tabindex="-1"></a><span class="in">results &lt;- Boston_test %&gt;%</span></span>
<span id="cb22-634"><a href="#cb22-634" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(predictions = gbm_predictions) %&gt;%</span></span>
<span id="cb22-635"><a href="#cb22-635" aria-hidden="true" tabindex="-1"></a><span class="in">  metrics(</span></span>
<span id="cb22-636"><a href="#cb22-636" aria-hidden="true" tabindex="-1"></a><span class="in">    truth = medv, </span></span>
<span id="cb22-637"><a href="#cb22-637" aria-hidden="true" tabindex="-1"></a><span class="in">    estimate = predictions</span></span>
<span id="cb22-638"><a href="#cb22-638" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb22-639"><a href="#cb22-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-640"><a href="#cb22-640" aria-hidden="true" tabindex="-1"></a><span class="in">results</span></span>
<span id="cb22-641"><a href="#cb22-641" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-642"><a href="#cb22-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-643"><a href="#cb22-643" aria-hidden="true" tabindex="-1"></a>Die Ergebnisse zeigen, dass Gradient Boosting bereits für die naive Parameterwahl im Aufruf von <span class="in">`gbm::gbm()`</span> zu einer Verbesserung der Vorhersageleistung gegenüber den Random-Forest-Modellen führt.</span>
<span id="cb22-644"><a href="#cb22-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-645"><a href="#cb22-645" aria-hidden="true" tabindex="-1"></a>Anstatt <span class="in">`n.trees = 5000`</span> können wir <span class="in">`n.trees`</span> in <span class="in">`predict()`</span> einen Vektor mit verschiedenen Ensemble-Größen übergeben. Für <span class="in">`n.trees = 5000`</span> erhalten wir Vorhersagen für jeden Status, den das Boosting-Modell im Training nach seiner Initialisierung bis zu der in <span class="in">`gbm::gbm()`</span> festgelgten Größe durchläuft. Anhand dieser Vorhersagen können wir die Generalisierungsfähigkeit des Modells in Abhängigkeit der gewählten Lernrate und der Größe beurteilen, in dem wir den RMSE für den gesamten Trainingsprozess berechnen. Für eine leichtere Interpretation erzeugen wir eine Grafik ählich wie bei der OOB-Analyse des Random-Forest-Modells.</span>
<span id="cb22-646"><a href="#cb22-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-649"><a href="#cb22-649" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb22-650"><a href="#cb22-650" aria-hidden="true" tabindex="-1"></a><span class="in"># Vorhersagen sukzessiv treffen</span></span>
<span id="cb22-651"><a href="#cb22-651" aria-hidden="true" tabindex="-1"></a><span class="in">predict(</span></span>
<span id="cb22-652"><a href="#cb22-652" aria-hidden="true" tabindex="-1"></a><span class="in">    object = gbm_model, </span></span>
<span id="cb22-653"><a href="#cb22-653" aria-hidden="true" tabindex="-1"></a><span class="in">    newdata = Boston_test, </span></span>
<span id="cb22-654"><a href="#cb22-654" aria-hidden="true" tabindex="-1"></a><span class="in">    n.trees = 1:5000</span></span>
<span id="cb22-655"><a href="#cb22-655" aria-hidden="true" tabindex="-1"></a><span class="in">) %&gt;%</span></span>
<span id="cb22-656"><a href="#cb22-656" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb22-657"><a href="#cb22-657" aria-hidden="true" tabindex="-1"></a><span class="in">   # Testset-RMSE berechnen</span></span>
<span id="cb22-658"><a href="#cb22-658" aria-hidden="true" tabindex="-1"></a><span class="in">    as_tibble() %&gt;%</span></span>
<span id="cb22-659"><a href="#cb22-659" aria-hidden="true" tabindex="-1"></a><span class="in">    map_dbl(</span></span>
<span id="cb22-660"><a href="#cb22-660" aria-hidden="true" tabindex="-1"></a><span class="in">      .f = ~ sqrt(mean((.x - Boston_test$medv)^2))</span></span>
<span id="cb22-661"><a href="#cb22-661" aria-hidden="true" tabindex="-1"></a><span class="in">    ) %&gt;%</span></span>
<span id="cb22-662"><a href="#cb22-662" aria-hidden="true" tabindex="-1"></a><span class="in">    bind_cols(rmse = ., trees = 1:5000) %&gt;%</span></span>
<span id="cb22-663"><a href="#cb22-663" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb22-664"><a href="#cb22-664" aria-hidden="true" tabindex="-1"></a><span class="in">  # Plotten</span></span>
<span id="cb22-665"><a href="#cb22-665" aria-hidden="true" tabindex="-1"></a><span class="in">  ggplot(mapping = aes(x = trees, y = rmse)) +</span></span>
<span id="cb22-666"><a href="#cb22-666" aria-hidden="true" tabindex="-1"></a><span class="in">    geom_line() +</span></span>
<span id="cb22-667"><a href="#cb22-667" aria-hidden="true" tabindex="-1"></a><span class="in">    labs(</span></span>
<span id="cb22-668"><a href="#cb22-668" aria-hidden="true" tabindex="-1"></a><span class="in">      title = "Boosting: Testset-RMSE als Funktion von n.trees"</span></span>
<span id="cb22-669"><a href="#cb22-669" aria-hidden="true" tabindex="-1"></a><span class="in">    ) +</span></span>
<span id="cb22-670"><a href="#cb22-670" aria-hidden="true" tabindex="-1"></a><span class="in">    theme_cowplot()</span></span>
<span id="cb22-671"><a href="#cb22-671" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-672"><a href="#cb22-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-673"><a href="#cb22-673" aria-hidden="true" tabindex="-1"></a>Die Grafik zeigt eine schnelle Verbesserung des Out-of-sample-Fehlers mit der Größe des Ensembles. Für die gewählte Lernrate scheinen 5000 Bäume adäquat zu sein.</span>
<span id="cb22-674"><a href="#cb22-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-675"><a href="#cb22-675" aria-hidden="true" tabindex="-1"></a>Analog zu Bagging und Random Forests können wir die Relevanz der Regressoren in <span class="in">`Boston`</span> für die Vorhersage von <span class="in">`medv`</span> anhand der mit <span class="in">`summary()`</span> berechneten (relativen) Variable Importance für die Anpassung auf den Trainingsdatensatz einschätzen. </span>
<span id="cb22-676"><a href="#cb22-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-679"><a href="#cb22-679" aria-hidden="true" tabindex="-1"></a><span class="in">```{webr}</span></span>
<span id="cb22-680"><a href="#cb22-680" aria-hidden="true" tabindex="-1"></a><span class="in"># Variable Importance berechnen</span></span>
<span id="cb22-681"><a href="#cb22-681" aria-hidden="true" tabindex="-1"></a><span class="in">var_importance &lt;- summary(</span></span>
<span id="cb22-682"><a href="#cb22-682" aria-hidden="true" tabindex="-1"></a><span class="in">  object = gbm_model, </span></span>
<span id="cb22-683"><a href="#cb22-683" aria-hidden="true" tabindex="-1"></a><span class="in">  plotit = FALSE # k. graphische Ausgabe</span></span>
<span id="cb22-684"><a href="#cb22-684" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb22-685"><a href="#cb22-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-686"><a href="#cb22-686" aria-hidden="true" tabindex="-1"></a><span class="in"># ... und plotten</span></span>
<span id="cb22-687"><a href="#cb22-687" aria-hidden="true" tabindex="-1"></a><span class="in">var_importance &lt;- var_importance %&gt;%</span></span>
<span id="cb22-688"><a href="#cb22-688" aria-hidden="true" tabindex="-1"></a><span class="in">  as_tibble() %&gt;%</span></span>
<span id="cb22-689"><a href="#cb22-689" aria-hidden="true" tabindex="-1"></a><span class="in">  arrange(</span></span>
<span id="cb22-690"><a href="#cb22-690" aria-hidden="true" tabindex="-1"></a><span class="in">    desc(rel.inf)</span></span>
<span id="cb22-691"><a href="#cb22-691" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb22-692"><a href="#cb22-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-693"><a href="#cb22-693" aria-hidden="true" tabindex="-1"></a><span class="in">ggplot(</span></span>
<span id="cb22-694"><a href="#cb22-694" aria-hidden="true" tabindex="-1"></a><span class="in">  data = var_importance,</span></span>
<span id="cb22-695"><a href="#cb22-695" aria-hidden="true" tabindex="-1"></a><span class="in">  mapping = aes(</span></span>
<span id="cb22-696"><a href="#cb22-696" aria-hidden="true" tabindex="-1"></a><span class="in">    x = reorder(var, rel.inf), </span></span>
<span id="cb22-697"><a href="#cb22-697" aria-hidden="true" tabindex="-1"></a><span class="in">    y = rel.inf</span></span>
<span id="cb22-698"><a href="#cb22-698" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb22-699"><a href="#cb22-699" aria-hidden="true" tabindex="-1"></a><span class="in">) +</span></span>
<span id="cb22-700"><a href="#cb22-700" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_bar(stat = "identity") +</span></span>
<span id="cb22-701"><a href="#cb22-701" aria-hidden="true" tabindex="-1"></a><span class="in">  coord_flip() +</span></span>
<span id="cb22-702"><a href="#cb22-702" aria-hidden="true" tabindex="-1"></a><span class="in">  labs(</span></span>
<span id="cb22-703"><a href="#cb22-703" aria-hidden="true" tabindex="-1"></a><span class="in">    title = "Variable Importance für Gradient Boosting",</span></span>
<span id="cb22-704"><a href="#cb22-704" aria-hidden="true" tabindex="-1"></a><span class="in">    x = "Variable",</span></span>
<span id="cb22-705"><a href="#cb22-705" aria-hidden="true" tabindex="-1"></a><span class="in">    y = "Relativer Einfluss (%)"</span></span>
<span id="cb22-706"><a href="#cb22-706" aria-hidden="true" tabindex="-1"></a><span class="in">  ) +</span></span>
<span id="cb22-707"><a href="#cb22-707" aria-hidden="true" tabindex="-1"></a><span class="in">  theme_cowplot()</span></span>
<span id="cb22-708"><a href="#cb22-708" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-709"><a href="#cb22-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-710"><a href="#cb22-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-711"><a href="#cb22-711" aria-hidden="true" tabindex="-1"></a>Obwohl erneut <span class="in">`lstat`</span> und <span class="in">`rm`</span> als die wichtigsten Prädiktoren gelistet sind, identifiziert Gradient Boosting im Gegensatz zu Bagging und Random Forests <span class="in">`lstat`</span> als die Variable mit der größten Vorhersagekraft für <span class="in">`medv`</span>. </span>
<span id="cb22-712"><a href="#cb22-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-713"><a href="#cb22-713" aria-hidden="true" tabindex="-1"></a><span class="fu">## Causal Trees und Causal Forests^[Aus technischen Gründen verzichten wir in diesem Kapitel zur Zeit auf die Einbindung der WebR-Konsole.]</span></span>
<span id="cb22-714"><a href="#cb22-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-715"><a href="#cb22-715" aria-hidden="true" tabindex="-1"></a>Baum-Algorithmen sind vielversprechende Ansätze zur Schätzung kausaler Effekte, insbesondere in Situationen, in denen die Bestimmung heterogener Effekte gewünscht ist: Der Vorteil von Baum-Methoden liegt darin, dass sie nicht-parametrisch sind: Der Regressorraum wird adaptiv in Partitionen unterteilt, um auf Basis dieser Aufteilung differenzierte Vorhersagen für die Zielvariable zu treffen. Diese Eigenschaft kann für Kausalanalysen hilfreich sein, da wir in vielen empirischen Anwendungen die Effekte einer Behandlung nicht nur im Durchschnitt für die betrachtete Population, sondern *differenzierter* schätzen möchten: Ein durchschnittlicher Behandlungseffekt (engl. *average treatment effect*, ATE) kann nicht ausreichend informativ für unsere Forschungsfrage sein, bspw. wenn wir erwarten, dass eine politische Intervention unterschiedliche Auswirkungen auf verschiedene Bevölkerungsgruppen hat. Idealerweise möchten wir $\tau_i$ bestimmen, den individuellen Behandlungseffekt einer Beobachtung $i$. Das fundamentale Problem der Kausalinferenz ist, dass $\tau_i$ nicht ermittelt werden kann (s. u.), sodass wir unser Ziel abschwächen müssen. Statt $\tau_i$ suchen wir einen Behandlungseffekt in Abhängigkeit von beobachtbaren Charakteristiken $\boldsymbol{X}$ für Untergruppen der Population, einen *conditional average treatment effect* (CATE). Im Potential-Outcomes-Framework ist der CATE definiert als</span>
<span id="cb22-716"><a href="#cb22-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-717"><a href="#cb22-717" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb22-718"><a href="#cb22-718" aria-hidden="true" tabindex="-1"></a>  \tau(\boldsymbol{x}) = \textup{E}\big(Y^{(1)} - Y^{(0)}\big\vert \boldsymbol{X} = \boldsymbol{x}\big),</span>
<span id="cb22-719"><a href="#cb22-719" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb22-720"><a href="#cb22-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-721"><a href="#cb22-721" aria-hidden="true" tabindex="-1"></a>wobei $Y^{(1)}$ und $Y^{(0)}$ die potenziellen Outcomes darstellen, wenn eine Behandlung erfolgt bzw. nicht erfolgt. In der Praxis beobachten wir jedoch nur $Y_i = Y_i^{(B_i)}$, wobei $B_i$ der Behandlungsindikator für die Beobachtung $i$ ist, sodass $\tau(\boldsymbol{x}_i)$ nicht direkt beobachtet werden kann. Unter der Annahme, dass nach Kontrolle für (beobachtbare) $\boldsymbol{X}$ die Zuordnung zur Behandlung quasi-zufällig ist (*unconfoundedness*), formal</span>
<span id="cb22-722"><a href="#cb22-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-723"><a href="#cb22-723" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb22-724"><a href="#cb22-724" aria-hidden="true" tabindex="-1"></a>Y_i^{(0)},\,Y_i^{(1)} \perp B_i \vert \boldsymbol{X}_i,</span>
<span id="cb22-725"><a href="#cb22-725" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb22-726"><a href="#cb22-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-727"><a href="#cb22-727" aria-hidden="true" tabindex="-1"></a>kann $\tau(\boldsymbol{x})$ geschätzt werden: Wir können Outcome-Differenzen zwischen behandelten und nicht behandelten Beobachtungen als kausal interpretieren, da unbeobachtete Faktoren die Ergebnisse nicht verzerren.</span>
<span id="cb22-728"><a href="#cb22-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-729"><a href="#cb22-729" aria-hidden="true" tabindex="-1"></a>CART und andere traditionelle Entscheidungsbaum-Algorithmen sind für die Schätzung heterogener Behandlungseffekte jedoch ungeeignet. Dafür gibt es zwei wesentliche Ursachen:</span>
<span id="cb22-730"><a href="#cb22-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-731"><a href="#cb22-731" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Das Splitting-Kriterium**</span>
<span id="cb22-732"><a href="#cb22-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-733"><a href="#cb22-733" aria-hidden="true" tabindex="-1"></a>    Das Splitting-Kriterium des CART-Algorithmus optimiert die Aufteilungen der Beobachtungen in jedem Knoten, um die Genauigkeit von Vorhersagen für die Outcome-Variable $Y$ durch Minimierung der Heterogienität (Klassifikation) oder des MSE (Regression) zu optimieren. Diese Kriterien zielen also darauf ab, die *Homogenität innerhalb der Blätter hinsichtlich* $Y$ zu maximieren.</span>
<span id="cb22-734"><a href="#cb22-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-735"><a href="#cb22-735" aria-hidden="true" tabindex="-1"></a>    Für die Schätzung heterogener kausaler Effekte ist ein solches Splitting jedoch nicht zielführend. Statt Knoten zu formen, in denen $Y$ möglichst homogen ist, benötigen wir für die Schätzung von Behandlungseffekten grundsätzlich Aufteilungen, bei denen sich $Y$ zwischen den behandelten und unbehandelten Individuen innerhalb der Knoten unterscheidet.^<span class="co">[</span><span class="ot">Wenn die Kontroll- und Behandlungsbeobachtungen in einem Blatt sehr ähnliche Outcomes $Y$ haben, können wir den Effekt nicht schätzen.</span><span class="co">]</span> Das Splitting sollte zu Blättern führen, die hinsichtlich des *geschätzten Behandlungseffekts* möglichst heterogen sind.</span>
<span id="cb22-736"><a href="#cb22-736" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-737"><a href="#cb22-737" aria-hidden="true" tabindex="-1"></a>    Die Wahl des Splitting-Kriterium für die Schätzung kausaler Effekte mit Bäumen ist nicht trivial: Ein natürliches Kriterium ist der mittlere quadratische Fehler bei der Vorhersage von $\tau$,</span>
<span id="cb22-738"><a href="#cb22-738" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-739"><a href="#cb22-739" aria-hidden="true" tabindex="-1"></a>    \begin{align*}</span>
<span id="cb22-740"><a href="#cb22-740" aria-hidden="true" tabindex="-1"></a>      \textup{MSE}_\tau = \frac{1}{n} \sum_{i=1}^n (\tau_i - \widehat{\tau}_i(\boldsymbol{X}_i))^2.</span>
<span id="cb22-741"><a href="#cb22-741" aria-hidden="true" tabindex="-1"></a>    \end{align*}</span>
<span id="cb22-742"><a href="#cb22-742" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-743"><a href="#cb22-743" aria-hidden="true" tabindex="-1"></a>    $\textup{MSE}_\tau$ ist jedoch nicht direkt berechenbar: Aufgrund der nicht-beobachtbaren individuellen Behandlungseffekte $\tau_i$ müsste $\textup{MSE}_\tau$ selbst geschätzt werden!^<span class="co">[</span><span class="ot">Bei "herkömmlichen" Regressionsbäumen besteht dieses Problem nicht, weil das Splitting-Kriterium Abweichungen von den wahren, *beobachteten* Werten von $Y$ misst.</span><span class="co">]</span></span>
<span id="cb22-744"><a href="#cb22-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-745"><a href="#cb22-745" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Data leakage**</span>
<span id="cb22-746"><a href="#cb22-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-747"><a href="#cb22-747" aria-hidden="true" tabindex="-1"></a>    Data leakage tritt auf, wenn Informationen aus dem Trainingsprozess in den Modellvalidierungs- oder Schätzprozess einfließen. Bei der Anpassung des Baums berücksichtigt der Algorithmus idealerweise Informationen über $Y$ *und* $B$ im Splitting-Prozess, um die besten Aufteilungen zu finden. Die hiezu verwendeten Datenpunkte definieren damit *den zu schätzten CATE* anhand der durch Partionierung gebildeten Blätter. Wenn dieselben Datenpunkte auch für die tatsächliche Schätzung des CATE mit dem trainierten Baum verwendet werden, besteht die Gefahr von Überanpassung und somit verzerrten Schätzungen.</span>
<span id="cb22-748"><a href="#cb22-748" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-749"><a href="#cb22-749" aria-hidden="true" tabindex="-1"></a><span class="fu">### Causal Trees</span></span>
<span id="cb22-750"><a href="#cb22-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-751"><a href="#cb22-751" aria-hidden="true" tabindex="-1"></a>Der Causal Tree Algorithmus von @AtheyImbens2016 modifiziert den CART-Algorithmus für die Schätzung heterogener Behandlungseffekte. In diesem Kontext wird die Vorgehensweise als „ehrlich“ (*honest*) bezeichnet, wenn nicht dieselben Informationen sowohl zur Auswahl des Modells (die Partitionierung des Regressorraums durch Splits) als auch zur Schätzung anhand dieses Modells verwendet werden. @AtheyImbens2016 adressieren das Data-Leakage-Problem durch zufällige Aufteilung des Datensatzes in eine Teilmenge $\mathcal{S}^{tr}$ für das *Training des Baums* und eine Teilmenge $\mathcal{S}^{est}$ für die *Schätzung der Behandlungseffekte*. </span>
<span id="cb22-752"><a href="#cb22-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-753"><a href="#cb22-753" aria-hidden="true" tabindex="-1"></a>Für die Erläuterung von *honest splitting* führen wir folgende Notation aus @AtheyImbens2016 ein:</span>
<span id="cb22-754"><a href="#cb22-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-755"><a href="#cb22-755" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathcal{S}^{te}$ ist ein hypothetischer *Testdatensatz*</span>
<span id="cb22-756"><a href="#cb22-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-757"><a href="#cb22-757" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\Pi$ ist eine *Partition*, d.h. eine Aufteilung des Regressorraums von $\boldsymbol{X}$^<span class="co">[</span><span class="ot">$\Pi$ sammelt also die Entschidungsregeln eines Baums und ist äquivalent zu $T$ in den füheren Kapiteln.</span><span class="co">]</span></span>
<span id="cb22-758"><a href="#cb22-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-759"><a href="#cb22-759" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Wir definieren die Schätzung des CATE anhand der Beobachtungen $\mathcal{S}^{est}$: Der CATE  $\widehat{\tau}(\boldsymbol{X}_i,\mathcal{S}^{est},\Pi)$ ist die Differenz der Mittelwerte von $Y_i$ für Behandlungs- und Kontrollbeobachtungen in dem aus $\Pi$ resultierenden Blatt für $\boldsymbol{X}_i$.</span>
<span id="cb22-760"><a href="#cb22-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-761"><a href="#cb22-761" aria-hidden="true" tabindex="-1"></a>Für die Wahl der Splits (die Partitionierung $\Pi$) für den Causal Tree schlagen @AtheyImbens2016 statt der Minimierung des MSE der Vorhersagen $\widehat{Y}$ (wie bei Regressionsbäumen) die Minimierung des MSE für den CATE vor. Das Vorgehen hierbei ist *honest* in dem Sinn, dass der erwartete Schätzfehler für ungesehene Beobachtungen $\mathcal{S}^{te}$ anhand einer Paritionierung $\Pi$ und entsprechenden Schätzungen der Behandlungseffekte $\widehat\tau$ mit unabhängigen Datensätzen $\mathcal{S}^{tr}$ bzw. $\mathcal{S}^{est}$ minimiert wird. Das hierzu verwendete Splitting-Kriterium ist eine Schätzung des *Erwartungswerts* von </span>
<span id="cb22-762"><a href="#cb22-762" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb22-763"><a href="#cb22-763" aria-hidden="true" tabindex="-1"></a>  \textup{MSE}(\mathcal{S}^{est},\mathcal{S}^{te},\Pi) = \frac{1}{n^{te}} \sum_{i=1}^{n^{te}} \big(\tau_i - \widehat{\tau}(\boldsymbol{X}_i,\mathcal{S}^{est},\Pi)\big)^2,</span>
<span id="cb22-764"><a href="#cb22-764" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb22-765"><a href="#cb22-765" aria-hidden="true" tabindex="-1"></a>der *erwartete*^<span class="co">[</span><span class="ot">Die Notation $\textup{E}_{\mathcal{S}^{est},\,\mathcal{S}^{te}}$ meint, dass die Erwartung über $\mathcal{S}^{est}$, und $\mathcal{S}^{te}$ gebildet wird.</span><span class="co">]</span> mittlere quadratische Fehler der heterogenen Behandlungseffekte,</span>
<span id="cb22-766"><a href="#cb22-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-767"><a href="#cb22-767" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb22-768"><a href="#cb22-768" aria-hidden="true" tabindex="-1"></a>  \textup{EMSE}(\Pi) = \textup{E}_{\mathcal{S}^{est},\,\mathcal{S}^{te}}\big<span class="co">[</span><span class="ot">\textup{MSE}(\mathcal{S}^{est},\mathcal{S}^{te},\,\Pi)\big</span><span class="co">]</span>.</span>
<span id="cb22-769"><a href="#cb22-769" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb22-770"><a href="#cb22-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-771"><a href="#cb22-771" aria-hidden="true" tabindex="-1"></a>Eine hilfreiche Umformung für $\textup{EMSE}$ ist</span>
<span id="cb22-772"><a href="#cb22-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-773"><a href="#cb22-773" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb22-774"><a href="#cb22-774" aria-hidden="true" tabindex="-1"></a>  \textup{EMSE}(\Pi) = \textup{Var}_{\mathcal{S}^{est},\boldsymbol{X}_i} \big[\widehat\tau(\boldsymbol{X}_i,\mathcal{S}^{est},\Pi)\big] - \textup{E}_{\boldsymbol{X}_i}\big<span class="co">[</span><span class="ot">\tau^2(\boldsymbol{X}_i,\Pi)\big</span><span class="co">]</span> + \textup{E}<span class="co">[</span><span class="ot">\tau_i^2</span><span class="co">]</span>,</span>
<span id="cb22-775"><a href="#cb22-775" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb22-776"><a href="#cb22-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-777"><a href="#cb22-777" aria-hidden="true" tabindex="-1"></a>denn @AtheyImbens2016 zeigen, wie die ersten beiden Summanden empirisch geschätzt werden können. Der Term $\textup{E}<span class="co">[</span><span class="ot">\tau_i^2</span><span class="co">]</span>$ ist nicht schätzbar (unbeobachteter individueller Behandlungseffekt $\tau_i$), kann aber vernachlässigt werden, da er nicht von $\Pi$ oder den Daten abhängt und somit eine *Konstante* ist, die sich beim Vergleich des geschätzen EMSE für verschiedene $\Pi$ rauskürzt.</span>
<span id="cb22-778"><a href="#cb22-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-779"><a href="#cb22-779" aria-hidden="true" tabindex="-1"></a>Dies sorgt für konsistente Schätzungen. @AtheyImbens2016 zeigen, dass die Minimierung des EMSE sowohl eine ausgewogene Verteilung der behandelten und unbehandelten Individuen als auch eine genaue Schätzung des Behandlungseffekts innerhalb jedes Knotens gewährleistet.</span>
<span id="cb22-780"><a href="#cb22-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-781"><a href="#cb22-781" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb22-782"><a href="#cb22-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-783"><a href="#cb22-783" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithmus: Causal Tree</span></span>
<span id="cb22-784"><a href="#cb22-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-785"><a href="#cb22-785" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Passe den Baum an: teile den Regressorraum mit binären Entscheidungsregeln rekursiv in Partitionen $\Pi$:</span>
<span id="cb22-786"><a href="#cb22-786" aria-hidden="true" tabindex="-1"></a>    a. An jedem Knoten wird die Aufteilung so gewählt, dass die Schätzung des *erwarteten mittleren quadratischen Fehlers* \textup{EMSE}(\Pi) über alle möglichen binären Aufteilungen $\Pi$ minimiert wird.</span>
<span id="cb22-787"><a href="#cb22-787" aria-hidden="true" tabindex="-1"></a>    b. Stelle sicher, dass eine Mindestanzahl von behandelten und Kontroll-Einheiten in jedem Blatt des so angepassten Baums nicht unterschritten wird.</span>
<span id="cb22-788"><a href="#cb22-788" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Bestimmte mit Cross-Validation die Tiefe $d^*$ der Partition, die eine Schätzung des MSE der Behandlungseffekte minimiert.</span>
<span id="cb22-789"><a href="#cb22-789" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Ehalte die Partition $\Pi^*$ durch das Beschneiden von $\Pi$ auf die Tiefe $d^*$: Entferne Blätter, die die geringste Verbesserung der Anpassung bieten. Dieser Schritt liefert den finalen Baum.</span>
<span id="cb22-790"><a href="#cb22-790" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Schätze die Behandlungseffekte in jedem Blatt von $\Pi^*$ mit den Beobachtungen in $\mathcal{S}^{est}$.</span>
<span id="cb22-791"><a href="#cb22-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-792"><a href="#cb22-792" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb22-793"><a href="#cb22-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-796"><a href="#cb22-796" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-797"><a href="#cb22-797" aria-hidden="true" tabindex="-1"></a>nl_effects <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="at">file =</span> <span class="st">"datasets/nl_effects.Rds"</span>)</span>
<span id="cb22-798"><a href="#cb22-798" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-799"><a href="#cb22-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-800"><a href="#cb22-800" aria-hidden="true" tabindex="-1"></a>@grfPackage</span>
<span id="cb22-801"><a href="#cb22-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-804"><a href="#cb22-804" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-805"><a href="#cb22-805" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb22-806"><a href="#cb22-806" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(grf)</span>
<span id="cb22-807"><a href="#cb22-807" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a causal tree (one single tree)</span></span>
<span id="cb22-808"><a href="#cb22-808" aria-hidden="true" tabindex="-1"></a>causal_tree <span class="ot">&lt;-</span> <span class="fu">causal_forest</span>(</span>
<span id="cb22-809"><a href="#cb22-809" aria-hidden="true" tabindex="-1"></a>  <span class="at">X =</span> nl_effects <span class="sc">%&gt;%</span> <span class="fu">select</span>(X1<span class="sc">:</span>X3) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>(),</span>
<span id="cb22-810"><a href="#cb22-810" aria-hidden="true" tabindex="-1"></a>  <span class="at">Y =</span> nl_effects<span class="sc">$</span>Y,</span>
<span id="cb22-811"><a href="#cb22-811" aria-hidden="true" tabindex="-1"></a>  <span class="at">W =</span> nl_effects<span class="sc">$</span>B, </span>
<span id="cb22-812"><a href="#cb22-812" aria-hidden="true" tabindex="-1"></a>  <span class="at">num.trees =</span> <span class="dv">1</span>, </span>
<span id="cb22-813"><a href="#cb22-813" aria-hidden="true" tabindex="-1"></a>  <span class="at">honesty =</span> <span class="cn">FALSE</span>, </span>
<span id="cb22-814"><a href="#cb22-814" aria-hidden="true" tabindex="-1"></a>  <span class="at">min.node.size =</span> <span class="dv">150</span></span>
<span id="cb22-815"><a href="#cb22-815" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb22-816"><a href="#cb22-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-817"><a href="#cb22-817" aria-hidden="true" tabindex="-1"></a>causal_tree</span>
<span id="cb22-818"><a href="#cb22-818" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-819"><a href="#cb22-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-820"><a href="#cb22-820" aria-hidden="true" tabindex="-1"></a>@fig-ct</span>
<span id="cb22-821"><a href="#cb22-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-824"><a href="#cb22-824" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-825"><a href="#cb22-825" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb22-826"><a href="#cb22-826" aria-hidden="true" tabindex="-1"></a>the_tree <span class="ot">&lt;-</span> <span class="fu">get_tree</span>(causal_tree, <span class="at">index =</span> <span class="dv">1</span>)</span>
<span id="cb22-827"><a href="#cb22-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-828"><a href="#cb22-828" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(the_tree)</span>
<span id="cb22-829"><a href="#cb22-829" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-830"><a href="#cb22-830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-831"><a href="#cb22-831" aria-hidden="true" tabindex="-1"></a><span class="al">![Mit `grf::causal_forest` geschätzter Causal Tree](img/causal_tree.svg)</span>{#fig-ct width=70%}</span>
<span id="cb22-832"><a href="#cb22-832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-833"><a href="#cb22-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-834"><a href="#cb22-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-835"><a href="#cb22-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-836"><a href="#cb22-836" aria-hidden="true" tabindex="-1"></a><span class="fu">### Causal Forests</span></span>
<span id="cb22-837"><a href="#cb22-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-838"><a href="#cb22-838" aria-hidden="true" tabindex="-1"></a>*Causal Forests* sind eine Erweiterung von Random Forests, die speziell entwickelt wurden, um individuelle Behandlungseffekte*(engl. Individual Treatment Effects, ITE) zu schätzen. Der Hauptunterschied zu Random Forests besteht darin, dass Causal Forests nicht nur Vorhersagen treffen, sondern auch den **kausalen Effekt** einer Behandlung $B$ auf das Outcome $Y$ schätzen. Der kausale Effekt wird oft als Differenz der Ergebnisse unter verschiedenen Bedingungen (z. B. mit und ohne Behandlung) modelliert.</span>
<span id="cb22-839"><a href="#cb22-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-840"><a href="#cb22-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-841"><a href="#cb22-841" aria-hidden="true" tabindex="-1"></a>| Aspekt                 | Random Forests                              | Causal Forests                                 |</span>
<span id="cb22-842"><a href="#cb22-842" aria-hidden="true" tabindex="-1"></a>|------------------------|---------------------------------------------|------------------------------------------------|</span>
<span id="cb22-843"><a href="#cb22-843" aria-hidden="true" tabindex="-1"></a>| **Ziel**                | Vorhersage von Zielvariablen                | Schätzung individueller Behandlungseffekte     |</span>
<span id="cb22-844"><a href="#cb22-844" aria-hidden="true" tabindex="-1"></a>| **Ergebnisse**          | Globale Vorhersagen                         | Heterogene, individuelle Effekte               |</span>
<span id="cb22-845"><a href="#cb22-845" aria-hidden="true" tabindex="-1"></a>| **Trainingsdaten**      | Outcome und Prädiktoren               | Behandlung, Outcome und Prädiktoren           |</span>
<span id="cb22-846"><a href="#cb22-846" aria-hidden="true" tabindex="-1"></a>| **Beispiel**            | Einkommensvorhersage auf Basis von Merkmalen| Effekt einer Werbekampagne auf verschiedene Kunden |</span>
<span id="cb22-847"><a href="#cb22-847" aria-hidden="true" tabindex="-1"></a>| **Schätzung**           | Bedingte Erwartung $\textup{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$               | Bedingter Behandlungseffekt $\textup{E}<span class="co">[</span><span class="ot">Y(1) - Y(0)|X</span><span class="co">]</span>$ |</span>
<span id="cb22-848"><a href="#cb22-848" aria-hidden="true" tabindex="-1"></a>| **Verwendung**          | Klassifikation und Regression               | Kausalanalyse und individuelle Wirkungsschätzung|</span>
<span id="cb22-849"><a href="#cb22-849" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-850"><a href="#cb22-850" aria-hidden="true" tabindex="-1"></a>Table: Vergleich von Random Forests und Causal Forests {#tbl-rfcfcomp}</span>
<span id="cb22-851"><a href="#cb22-851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-852"><a href="#cb22-852" aria-hidden="true" tabindex="-1"></a>In diesem Beispiel verwenden wir das Paket <span class="in">`grf`</span> (Generalized Random Forests) in R, um einen Causal Forest zu trainieren und die individuellen Behandlungseffekte zu schätzen.</span>
<span id="cb22-853"><a href="#cb22-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-854"><a href="#cb22-854" aria-hidden="true" tabindex="-1"></a>Naive Methode mit Random Forest</span>
<span id="cb22-855"><a href="#cb22-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-856"><a href="#cb22-856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-859"><a href="#cb22-859" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-860"><a href="#cb22-860" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Zusammenhang zwischen X1, X2, X3 und dem wahren Behandlungseffekt tau"</span></span>
<span id="cb22-861"><a href="#cb22-861" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-xvstau</span></span>
<span id="cb22-862"><a href="#cb22-862" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb22-863"><a href="#cb22-863" aria-hidden="true" tabindex="-1"></a><span class="co"># Daten ins Long-Format umwandeln für einfachere Facettierung</span></span>
<span id="cb22-864"><a href="#cb22-864" aria-hidden="true" tabindex="-1"></a>nl_long <span class="ot">&lt;-</span> nl_effects <span class="sc">%&gt;%</span> </span>
<span id="cb22-865"><a href="#cb22-865" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(X1, X2, X3, tau) <span class="sc">%&gt;%</span> </span>
<span id="cb22-866"><a href="#cb22-866" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">starts_with</span>(<span class="st">"X"</span>), <span class="at">names_to =</span> <span class="st">"Variable"</span>, <span class="at">values_to =</span> <span class="st">"Value"</span>)</span>
<span id="cb22-867"><a href="#cb22-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-868"><a href="#cb22-868" aria-hidden="true" tabindex="-1"></a><span class="co"># Erstelle den facettierten Plot</span></span>
<span id="cb22-869"><a href="#cb22-869" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(</span>
<span id="cb22-870"><a href="#cb22-870" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> nl_long,</span>
<span id="cb22-871"><a href="#cb22-871" aria-hidden="true" tabindex="-1"></a>  <span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">x =</span> Value, <span class="at">y =</span> tau)) <span class="sc">+</span> </span>
<span id="cb22-872"><a href="#cb22-872" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.3</span>, <span class="at">size =</span> .<span class="dv">5</span>) <span class="sc">+</span> </span>
<span id="cb22-873"><a href="#cb22-873" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>Variable, <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span> </span>
<span id="cb22-874"><a href="#cb22-874" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span> </span>
<span id="cb22-875"><a href="#cb22-875" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb22-876"><a href="#cb22-876" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Wert der Prädiktoren"</span>,</span>
<span id="cb22-877"><a href="#cb22-877" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Wahrer Behandlungseffekt (tau)"</span></span>
<span id="cb22-878"><a href="#cb22-878" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb22-879"><a href="#cb22-879" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-880"><a href="#cb22-880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-883"><a href="#cb22-883" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-884"><a href="#cb22-884" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Korrelation zwischen Behandlungsindikator (B) und relevanten Kovariablen"</span></span>
<span id="cb22-885"><a href="#cb22-885" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-bvsvars</span></span>
<span id="cb22-886"><a href="#cb22-886" aria-hidden="true" tabindex="-1"></a><span class="co"># In langes Format überführen</span></span>
<span id="cb22-887"><a href="#cb22-887" aria-hidden="true" tabindex="-1"></a>df_long <span class="ot">&lt;-</span> nl_effects <span class="sc">%&gt;%</span> </span>
<span id="cb22-888"><a href="#cb22-888" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(X1, X2, X3, B) <span class="sc">%&gt;%</span> </span>
<span id="cb22-889"><a href="#cb22-889" aria-hidden="true" tabindex="-1"></a>    <span class="fu">pivot_longer</span>(</span>
<span id="cb22-890"><a href="#cb22-890" aria-hidden="true" tabindex="-1"></a>      <span class="at">cols =</span> <span class="fu">starts_with</span>(<span class="st">"X"</span>),</span>
<span id="cb22-891"><a href="#cb22-891" aria-hidden="true" tabindex="-1"></a>      <span class="at">names_to =</span> <span class="st">"Variable"</span>,</span>
<span id="cb22-892"><a href="#cb22-892" aria-hidden="true" tabindex="-1"></a>      <span class="at">values_to =</span> <span class="st">"Value"</span></span>
<span id="cb22-893"><a href="#cb22-893" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb22-894"><a href="#cb22-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-895"><a href="#cb22-895" aria-hidden="true" tabindex="-1"></a><span class="co"># Facetting nach Variable</span></span>
<span id="cb22-896"><a href="#cb22-896" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df_long, <span class="fu">aes</span>(<span class="at">x =</span> Value, <span class="at">fill =</span> <span class="fu">factor</span>(B))) <span class="sc">+</span> </span>
<span id="cb22-897"><a href="#cb22-897" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_density</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span> </span>
<span id="cb22-898"><a href="#cb22-898" aria-hidden="true" tabindex="-1"></a>    <span class="fu">facet_wrap</span>(<span class="sc">~</span>Variable, <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span> </span>
<span id="cb22-899"><a href="#cb22-899" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(</span>
<span id="cb22-900"><a href="#cb22-900" aria-hidden="true" tabindex="-1"></a>      <span class="at">x =</span> <span class="st">"Prädiktoren-Werte"</span>,</span>
<span id="cb22-901"><a href="#cb22-901" aria-hidden="true" tabindex="-1"></a>      <span class="at">fill =</span> <span class="st">"Behandlung (B)"</span></span>
<span id="cb22-902"><a href="#cb22-902" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">+</span></span>
<span id="cb22-903"><a href="#cb22-903" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_minimal</span>() <span class="sc">+</span> </span>
<span id="cb22-904"><a href="#cb22-904" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"top"</span>)</span>
<span id="cb22-905"><a href="#cb22-905" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-906"><a href="#cb22-906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-907"><a href="#cb22-907" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-910"><a href="#cb22-910" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-911"><a href="#cb22-911" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(grf)</span>
<span id="cb22-912"><a href="#cb22-912" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb22-913"><a href="#cb22-913" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb22-914"><a href="#cb22-914" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-915"><a href="#cb22-915" aria-hidden="true" tabindex="-1"></a>the_split <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(<span class="at">data =</span> nl_effects, <span class="at">prop =</span> .<span class="dv">8</span>)</span>
<span id="cb22-916"><a href="#cb22-916" aria-hidden="true" tabindex="-1"></a>nl_effects_train <span class="ot">&lt;-</span> <span class="fu">training</span>(the_split)</span>
<span id="cb22-917"><a href="#cb22-917" aria-hidden="true" tabindex="-1"></a>nl_effects_test <span class="ot">&lt;-</span> <span class="fu">testing</span>(the_split)</span>
<span id="cb22-918"><a href="#cb22-918" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-919"><a href="#cb22-919" aria-hidden="true" tabindex="-1"></a><span class="co"># Variablen in Matrizen / Vektoren überführen</span></span>
<span id="cb22-920"><a href="#cb22-920" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> nl_effects_train <span class="sc">%&gt;%</span> </span>
<span id="cb22-921"><a href="#cb22-921" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">starts_with</span>(<span class="st">"X"</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb22-922"><a href="#cb22-922" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb22-923"><a href="#cb22-923" aria-hidden="true" tabindex="-1"></a>B <span class="ot">&lt;-</span> nl_effects_train <span class="sc">%&gt;%</span> <span class="fu">pull</span>(B)</span>
<span id="cb22-924"><a href="#cb22-924" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> nl_effects_train <span class="sc">%&gt;%</span> <span class="fu">pull</span>(Y)</span>
<span id="cb22-925"><a href="#cb22-925" aria-hidden="true" tabindex="-1"></a>tau <span class="ot">&lt;-</span> nl_effects_train <span class="sc">%&gt;%</span> <span class="fu">pull</span>(tau)</span>
<span id="cb22-926"><a href="#cb22-926" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-927"><a href="#cb22-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-928"><a href="#cb22-928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-929"><a href="#cb22-929" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, cache=T}</span></span>
<span id="cb22-930"><a href="#cb22-930" aria-hidden="true" tabindex="-1"></a><span class="in"># Propensity Score Schätzen</span></span>
<span id="cb22-931"><a href="#cb22-931" aria-hidden="true" tabindex="-1"></a><span class="in">B_hat_mod &lt;- regression_forest(</span></span>
<span id="cb22-932"><a href="#cb22-932" aria-hidden="true" tabindex="-1"></a><span class="in">  X = X,</span></span>
<span id="cb22-933"><a href="#cb22-933" aria-hidden="true" tabindex="-1"></a><span class="in">  Y = B,</span></span>
<span id="cb22-934"><a href="#cb22-934" aria-hidden="true" tabindex="-1"></a><span class="in">  tune.parameters = "all"</span></span>
<span id="cb22-935"><a href="#cb22-935" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb22-936"><a href="#cb22-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-937"><a href="#cb22-937" aria-hidden="true" tabindex="-1"></a><span class="in">B_hat &lt;- B_hat_mod$predictions</span></span>
<span id="cb22-938"><a href="#cb22-938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-939"><a href="#cb22-939" aria-hidden="true" tabindex="-1"></a><span class="in"># Outcome Schätzen</span></span>
<span id="cb22-940"><a href="#cb22-940" aria-hidden="true" tabindex="-1"></a><span class="in">Y_hat_mod &lt;- regression_forest(</span></span>
<span id="cb22-941"><a href="#cb22-941" aria-hidden="true" tabindex="-1"></a><span class="in">  X = X,</span></span>
<span id="cb22-942"><a href="#cb22-942" aria-hidden="true" tabindex="-1"></a><span class="in">  Y = Y,</span></span>
<span id="cb22-943"><a href="#cb22-943" aria-hidden="true" tabindex="-1"></a><span class="in">  tune.parameters = "all"</span></span>
<span id="cb22-944"><a href="#cb22-944" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb22-945"><a href="#cb22-945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-946"><a href="#cb22-946" aria-hidden="true" tabindex="-1"></a><span class="in">Y_hat &lt;- Y_hat_mod$predictions</span></span>
<span id="cb22-947"><a href="#cb22-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-948"><a href="#cb22-948" aria-hidden="true" tabindex="-1"></a><span class="in"># Causal Forest trainieren</span></span>
<span id="cb22-949"><a href="#cb22-949" aria-hidden="true" tabindex="-1"></a><span class="in">cf &lt;- causal_forest(</span></span>
<span id="cb22-950"><a href="#cb22-950" aria-hidden="true" tabindex="-1"></a><span class="in">  X = X, </span></span>
<span id="cb22-951"><a href="#cb22-951" aria-hidden="true" tabindex="-1"></a><span class="in">  Y = Y, </span></span>
<span id="cb22-952"><a href="#cb22-952" aria-hidden="true" tabindex="-1"></a><span class="in">  W = B, </span></span>
<span id="cb22-953"><a href="#cb22-953" aria-hidden="true" tabindex="-1"></a><span class="in">  Y.hat = Y_hat,</span></span>
<span id="cb22-954"><a href="#cb22-954" aria-hidden="true" tabindex="-1"></a><span class="in">  W.hat = B_hat</span></span>
<span id="cb22-955"><a href="#cb22-955" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb22-956"><a href="#cb22-956" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-957"><a href="#cb22-957" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-960"><a href="#cb22-960" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-961"><a href="#cb22-961" aria-hidden="true" tabindex="-1"></a><span class="co"># Schritt 4: Vorhersage des durchschnittlichen Behandlungseffekts</span></span>
<span id="cb22-962"><a href="#cb22-962" aria-hidden="true" tabindex="-1"></a><span class="co"># Causal Forest</span></span>
<span id="cb22-963"><a href="#cb22-963" aria-hidden="true" tabindex="-1"></a>tau.cf <span class="ot">&lt;-</span> <span class="fu">average_treatment_effect</span>(cf)</span>
<span id="cb22-964"><a href="#cb22-964" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Durchschnittlicher Behandlungseffekt mit Causal Forest:"</span>, tau.cf[<span class="dv">1</span>], <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb22-965"><a href="#cb22-965" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-966"><a href="#cb22-966" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-969"><a href="#cb22-969" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-970"><a href="#cb22-970" aria-hidden="true" tabindex="-1"></a><span class="co"># Schritt 5: Individuelle Schätzungen der Behandlungseffekte</span></span>
<span id="cb22-971"><a href="#cb22-971" aria-hidden="true" tabindex="-1"></a><span class="co"># Causal Forest - Individuelle Behandlungseffekte (CATE)</span></span>
<span id="cb22-972"><a href="#cb22-972" aria-hidden="true" tabindex="-1"></a>tau.hat.cf <span class="ot">&lt;-</span> <span class="fu">predict</span>(cf)<span class="sc">$</span>predictions</span>
<span id="cb22-973"><a href="#cb22-973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-974"><a href="#cb22-974" aria-hidden="true" tabindex="-1"></a><span class="co"># Wahrer Behandlungseffekt tau</span></span>
<span id="cb22-975"><a href="#cb22-975" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(tau.hat.cf)</span>
<span id="cb22-976"><a href="#cb22-976" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-977"><a href="#cb22-977" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-980"><a href="#cb22-980" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-981"><a href="#cb22-981" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb22-982"><a href="#cb22-982" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> nl_effects_train<span class="sc">$</span>tau,</span>
<span id="cb22-983"><a href="#cb22-983" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="fu">predict</span>(cf)<span class="sc">$</span>predictions</span>
<span id="cb22-984"><a href="#cb22-984" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb22-985"><a href="#cb22-985" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x  =</span>x, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb22-986"><a href="#cb22-986" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">alpha=</span> .<span class="dv">3</span>, <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb22-987"><a href="#cb22-987" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb22-988"><a href="#cb22-988" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_minimal</span>()</span>
<span id="cb22-989"><a href="#cb22-989" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-990"><a href="#cb22-990" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-991"><a href="#cb22-991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-994"><a href="#cb22-994" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-995"><a href="#cb22-995" aria-hidden="true" tabindex="-1"></a><span class="co"># ATE CF</span></span>
<span id="cb22-996"><a href="#cb22-996" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(tau.hat.cf)</span>
<span id="cb22-997"><a href="#cb22-997" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-998"><a href="#cb22-998" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1001"><a href="#cb22-1001" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-1002"><a href="#cb22-1002" aria-hidden="true" tabindex="-1"></a><span class="co"># Schritt 6: Evaluierung - Vergleich der Genauigkeit der beiden Modelle</span></span>
<span id="cb22-1003"><a href="#cb22-1003" aria-hidden="true" tabindex="-1"></a><span class="co"># MSE für Causal Forest</span></span>
<span id="cb22-1004"><a href="#cb22-1004" aria-hidden="true" tabindex="-1"></a>mse.cf <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((tau.hat.cf <span class="sc">-</span> tau)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb22-1005"><a href="#cb22-1005" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"MSE des Causal Forest:"</span>, mse.cf, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb22-1006"><a href="#cb22-1006" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-1007"><a href="#cb22-1007" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1008"><a href="#cb22-1008" aria-hidden="true" tabindex="-1"></a>Test set</span>
<span id="cb22-1009"><a href="#cb22-1009" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1012"><a href="#cb22-1012" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-1013"><a href="#cb22-1013" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">mean</span>((<span class="fu">predict</span>(<span class="at">object =</span> cf, <span class="at">newdata =</span> nl_effects_test <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>Y, <span class="sc">-</span>tau, <span class="sc">-</span>B))<span class="sc">$</span>predictions <span class="sc">-</span> nl_effects_test<span class="sc">$</span>tau)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb22-1014"><a href="#cb22-1014" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-1015"><a href="#cb22-1015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1018"><a href="#cb22-1018" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-1019"><a href="#cb22-1019" aria-hidden="true" tabindex="-1"></a><span class="co"># ATE CF pred</span></span>
<span id="cb22-1020"><a href="#cb22-1020" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">predict</span>(<span class="at">object =</span> cf, <span class="at">newdata =</span> nl_effects_test <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>Y, <span class="sc">-</span>tau, <span class="sc">-</span>B))<span class="sc">$</span>predictions)</span>
<span id="cb22-1021"><a href="#cb22-1021" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-1022"><a href="#cb22-1022" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1023"><a href="#cb22-1023" aria-hidden="true" tabindex="-1"></a><span class="fu">## Zusammenfassung</span></span>
<span id="cb22-1024"><a href="#cb22-1024" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1025"><a href="#cb22-1025" aria-hidden="true" tabindex="-1"></a>In diesem Kapitel haben wir die Anwendung baum-basierter Methoden in R diskutiert. Darunter Entscheidungsbäume, Bagging, Random Forests und Boosting. Entscheidungsbäume sind Modelle, die die Daten anhand binärer Entscheidungsregeln sukzessiv in kleinere, homogene Gruppen aufgeteilt werden. Baum-Modelle bieten intuitive Interpretierbarkeit, neigen jedoch zur Überanpassung, was durch Beschneiden (Pruning) vermieden werden kann. Die Vorhersage einzelner Bäume ist tendentiell mit hoher Varianz verbunden. Random Forests kombinieren mit Bagging viele Entscheidungsbäume, die auf zufälligen Teilmengen der Daten und Merkmale trainiert werden. Durch die Aggregation der Vorhersagen vieler Bäume reduziert der Random Forest die Varianz und verbessert so die Vorhersagegenauigkeit. Boosting-Methoden mit Entscheidungsbäumen trainieren kleine Bäume sukzessive, wobei jeder weitere Baum zur Korrektur der gegenwärtigen Fehler des Ensembles trainiert wird. Gradient Boosting nutzt den Gradienten der Verlustfunktion, um die Vorhersagequalität des Ensembles optimieren. </span>
<span id="cb22-1026"><a href="#cb22-1026" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1027"><a href="#cb22-1027" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1028"><a href="#cb22-1028" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1029"><a href="#cb22-1029" aria-hidden="true" tabindex="-1"></a>Für alle Methoden wurden Implementierungen im <span class="in">`parsnip`</span>-Framework in R vorgestellt. Zudem wurde gezeigt, wie die Vorhersagegüte durch Testdatensätze beurteilt und die Bedeutung einzelner Variablen mit Variable-Importance-Metriken analysiert werden kann.</span>
</code><button title="In die Zwischenablage kopieren" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>