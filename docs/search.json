[
  {
    "objectID": "Matching.html",
    "href": "Matching.html",
    "title": "\n3  Matching\n",
    "section": "",
    "text": "4 Literatur\nAbadie, Alberto, and Guido W. Imbens. (2008). On the Failure of the Bootstrap for Matching Estimators.Econometrica 76 (6): 1537–57.\nBodory, H., Camponovo, L., Huber, M., & Lechner, M. (2020). The Finite Sample Performance of Inference Methods for Propensity Score Matching and Weighting Estimators. Journal of Business & Economic Statistics, 38(1), 183–200.\nWooldridge, J. M. (2010). Econometric analysis of cross section and panel data. MIT press."
  },
  {
    "objectID": "Matching.html#sec-balance",
    "href": "Matching.html#sec-balance",
    "title": "\n3  Matching\n",
    "section": "\n3.1 Balance: Vergleichbarkeit von Behandlungs- und Kontrollgruppe",
    "text": "3.1 Balance: Vergleichbarkeit von Behandlungs- und Kontrollgruppe\nDer Lehrstuhl für Ökonometrie an der Universität Duisburg-Essen betreibt einen Ökonometrie-Blog und interessiert sich für den kausalen Effekt der Einführung eines darkmode auf die Verweildauer der User auf der Webseite. Die Webseite ist zwar nicht-kommerziell, hat sich allerdings insb. für die Aquise internationaler Studierender für den Studiengang MSc. Econometrics als wichtiges Marketing-Instrument erwiesen. Ein anprechendes Design wird daher als hoch-relevant erachtet.\nIdealerweise sollte der Effekt des Design-Relaunches auf die Nutzungsintensität in einem kontrollierten randomisierten Experiment untersucht werden. Hierbei würden wir Nutzern zufällig das neue oder das alte Design zuweisen und den Effekt als Differnz des durchschnittlichen Verweildauer für die Gruppen bestimmen. Eine solche Studie ist jedoch aus technischen und finanziellen Gründen nicht realisierbar, sodass die Auswirkungen des darkmode mit vorliegenden nicht-experimenellen Nutzungsstatistiken für die Webseite geschätzt werden sollen.\nDie Nutzungsstatistiken sind im Datensatz darkmode.csv enthalten und sollen der Analyse des Effekts des darkmode (dark_mode) auf die Verweildauer der Leser auf der Webseite (read_time) dienen.\nTabelle 3.1 zeigt die Definitionen der Variablen in darkmode.csv.\n\n\n\n\n\n\n\n\n  \nVariable\n      Beschreibung\n    \n\n\nread_time\nLesezeit (Minuten/Woche)\n\n\ndark_mode\nIndikator: Beobachtung nach Einführung darkmode\n\n\nmale\nIndikator: Individuum männlich\n\n\nage\nAlter (in Jahren)\n\n\nhours\nBisherige Verweildauer auf der Seite\n\n\n\nTabelle 3.1:  Variablen im Datensatz darkmode \n\n\n\nFür die Analyse lesen wir zunächst den Datensatz darkmode.csv mit readr::read_csv() ein und verschaffen uns einen Überblick über die verfügbaren Variablen.\n\n# Paket `tidyverse` laden\nlibrary(tidyverse)\n\n# Datensatz 'darkmode' einlesen\ndarkmode &lt;- read_csv(\n  file = \"datasets/darkmode.csv\"\n)\n\ndark_mode hat den Typ logical. Mit dplyr::mutate_all() können wir komfortabel alle Spalten in den Typ numeric transformieren.\n\n# Alle Variablen zu typ 'numeric' formatieren...\ndarkmode &lt;- darkmode %&gt;% \n  mutate_all(.funs = as.numeric)\n\n# ... und überprüfen\nglimpse(darkmode)\n\nRows: 300\nColumns: 5\n$ read_time &lt;dbl&gt; 14.4, 15.4, 20.9, 20.0, 21.5, 19.5, 22.0, 17.4, 23.6, 15.7, …\n$ dark_mode &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, …\n$ male      &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, …\n$ age       &lt;dbl&gt; 43, 55, 23, 41, 29, 64, 18, 53, 59, 53, 43, 38, 42, 23, 39, …\n$ hours     &lt;dbl&gt; 65.6, 125.4, 642.6, 129.1, 190.2, 185.3, 333.5, 279.3, 1302.…\n\n\nEine naive Schätzung des durchschnittlichen Behandlungseffekts (ATE) \\(\\widehat{\\tau}^{\\text{naiv}}\\) erhalten wir als Mittelwertdifferenz von read_time für die Behandlungsgruppe (dark_mode == 1) und die Kontrollgruppe (dark_mode == 0) \\[\\begin{align}\n  \\widehat{\\tau}^{\\text{naiv}} = \\overline{\\text{read\\_time}}_{\\text{Behandlung}} - \\overline{\\text{read\\_time}}_{\\text{Kontrolle}}.\\label{eq:naivATEdarkmode}\n\\end{align}\\]\nDiese Berechnung ist schnell mit R durchgeführt.\n\n# Naiver Schätzer für ATE: \n# Differenz der Gruppen-Durchschnitte\n\n# Outcome in Behandlungsgruppe\nread_time_mTG &lt;- darkmode %&gt;% \n  filter(dark_mode == 1) %&gt;% \n  pull(\"read_time\")\n\n# Outcome in Kontrollgruppe\nread_time_mKG &lt;- darkmode %&gt;% \n  filter(dark_mode == 0) %&gt;% \n  pull(\"read_time\")\n\n# Mittelwert-Differenz\nmean(read_time_mTG) - mean(read_time_mKG)\n\n[1] -0.4446331\n\n\nDie Schätzung ergibt einen negativen Behandlungseffekt, mit der Interpreation, dass das neue Design zu einer Reduktion der Lesezeit um etwa 0.44 Minuten pro Woche führt. Dieses Ergebnis ist allerdings zweifelhaft, weil eine Isolierung des Behandlungseffekts aufgrund von Backdoor-Pfaden im DGP vermutlich nicht gewähleistet ist. Ein Indikator hierfür sind systematische Unterschiede hinsichtlich von (möglicherweise unbeobachtbaren) Charakteristika von Kontrollgruppe und Behandlungsgruppe.\nDa die User sich beim Aufrufen der Seite aktiv für oder gegen den das neue Design entscheiden müssen (und somit selektieren, ob Sie in der Behandlungs- oder Kontrollgruppe landen), liegt wahrscheinlich Confounding vor: Unsere Hypothese ist zunächst, dass männliche User eine durchschnittlich längere Lesezeit aufweisen und mit größerer Wahrscheinlichkeit auf das neue Design wechseln als nicht-männliche Leser. Dann ist male eine Backdoor-Variable. Diese Situation ist unter der Annahme, dass nur diese Faktoren den DGP bestimmen, in Abbildung 3.1 dargestellt.\n\n\n\n\n\nread_time\n\nread_time\ndark_mode\n\ndark_mode\ndark_mode-&gt;read_time\n\n\nmale\n\nmale\nmale-&gt;read_time\n\n\nmale-&gt;dark_mode\n\n\n\nAbbildung 3.1: Backdoor durch ‘male’ im Website-Design-Bespiel\n\n\n\nDer DGP in Abbildung 3.1 führt zu einer verzerrten Schätzung des kausalen Effekts von dark_mode auf read_time mit \\(\\eqref{eq:naivATEdarkmode}\\), wenn das Verhältnis von männlichen und nicht-männlichen Usern in Bahandlungs- und Kontrollgruppe nicht ausgeglichen ist. Wir überprüfen dies mit R.\n\n# Anteile männlicher und nicht-männlicher User\n(\n  anteile &lt;- darkmode %&gt;% \n  group_by(dark_mode) %&gt;% \n  summarise(\n    gesamt = n(),\n    ant_m = mean(male),\n    ant_nm = 1 - ant_m,\n    anz_m = sum(male),\n    anz_nm = gesamt - anz_m\n    )\n)\n\n# A tibble: 2 × 6\n  dark_mode gesamt ant_m ant_nm anz_m anz_nm\n      &lt;dbl&gt;  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1         0    151 0.338  0.662    51    100\n2         1    149 0.658  0.342    98     51\n\n\nDie Zusammenfassung anteile_m zeigt, dass der Anteil männlicher User in der Behandlungsgruppe deutlich höher ist als in der Kontrollgruppe.\n\n3.1.1 Matching durch Gewichtung\nMatching eliminiert die Variation von male zwischen den Gruppen. Eine Möglichkeit hierfür ist die Gewichtung der Beobachtungen in der Kontrollgruppe entsprechend der Anteile von Männern und Nicht-Männern in der Behandlungsgruppe, sodass die Vergleichbarkeit mit der Behandlungsgruppe hinsichtlich des Geschlechts gewährleistet ist. Dies wird in der Literatur als Balance bezeichnet. Der Behandlungseffekt wird dann analog zu \\(\\eqref{eq:naivATEdarkmode}\\) geschätzt.\nDie Gewichte für Beobachtungen in der Kontrollgruppe \\(w_i\\) werden berechnet als \\[\\begin{align}\n  w_i =\n  \\begin{cases}\n    \\text{ant\\_m}_B/\\text{anz\\_m}_{K}, & \\text{falls } \\text{male}_i = 1\\\\\n        \\text{ant\\_nm}_B/\\text{anz\\_nm}_{K}, & \\text{sonst.}\\\\\n  \\end{cases}\\label{eq:darkmodeweights}\n\\end{align}\\] Anhand der Formel für einen gewichteten Durchschnitt, \\[\\begin{align}\n  \\overline{X}_w = \\frac{\\sum_i w_i \\cdot X_i}{\\sum_i w_i},\n\\end{align}\\] berechnen wir die gewichteten Mittelwerte für male und read_time in der Kontrollgruppe.\n\n# Anteile und Anzahlen aus `anteile` auslesen\nanz_m_K &lt;- anteile %&gt;% \n  filter(dark_mode == 0) %&gt;% pull(anz_m)\n\nanz_nm_K &lt;- anteile %&gt;% \n  filter(dark_mode == 0) %&gt;% pull(anz_nm)\n\nant_m_B &lt;- anteile %&gt;% \n  filter(dark_mode == 1) %&gt;% pull(ant_m)\n\nant_nm_B &lt;- anteile %&gt;% \n  filter(dark_mode == 1) %&gt;% pull(ant_nm)\n\n\n# Gewichtete Mittel für Kontrollgruppe berechnen\n(\ngew_K &lt;- darkmode %&gt;% \n  filter(dark_mode == 0) %&gt;% \n  select(read_time, male) %&gt;%\n  mutate(w = ifelse(\n    male == 1, \n    ant_m_B/anz_m_K, \n    ant_nm_B/anz_nm_K)\n    ) %&gt;%\n  summarise(\n    male_k = sum(male * w) / sum(w),\n    mean_read_time_wK = sum(read_time * w) / sum(w)\n  )\n)\n\n# A tibble: 1 × 2\n  male_k mean_read_time_wK\n   &lt;dbl&gt;             &lt;dbl&gt;\n1  0.658              18.1\n\n\nEin Vergleich des gewichteten Mittelwertes von male in der Kontrollgruppe mit dem Mittelwert in der Behandlungsgruppe (male_k) zeigt, dass die Gewichte die Variation in male zwischen beiden Gruppen eliminieren, sodass die Backdoor durch male geschlossen ist. Mit wmean_read_time_K haben wir einen entsprechend gewichteten Mittelwert der Verweildauer für die Kontrollgruppe berechnet. Wir schätzen den Behandlungseffekt nun als \\[\\begin{align}\n  \\widehat{\\tau}^{\\text{w}} = \\overline{\\text{read\\_time}}_{B} - \\overline{\\text{read\\_time}}_{w,K}.\\label{eq:weightedATEdarkmode}\n\\end{align}\\]\n\nmean(read_time_mTG)  - gew_K$mean_read_time_wK\n\n[1] 0.6383579\n\n\nEntgegen der naiven Schätzung andhand von \\(\\eqref{eq:naivATEdarkmode}\\) erhalten wir nach Matching für male eine positive Schätzung des Behandlungseffekts von etwa \\(0.64\\).\nDie Schätzung des Behandlungseffekts anhand von \\(\\eqref{eq:weightedATEdarkmode}\\) entspricht dem geschätzten Koeffizienten \\(\\widehat{\\beta}_1\\) aus einer gewichteten KQ-Regression im Modell \\[\\begin{align*}\n  \\text{read\\_time} = \\beta_0 + \\beta_1 \\text{dark\\_mode} + u,\n\\end{align*}\\] wobei die Beobachtungen der Kontrollgruppe wie in \\(\\eqref{eq:darkmodeweights}\\) gewichtet werden und \\(w_i=1\\) für Beobachtungen der Behandlungsgruppe ist. Wir überprüfen dies mit R.\n\ndarkmode_w &lt;- darkmode %&gt;% \n  mutate(\n    w = case_when(\n      male == 1 & dark_mode == 0 ~ ant_m_B/anz_m_K,\n      male == 0 & dark_mode == 0 ~ ant_nm_B/anz_nm_K,\n      T ~ 1\n    )\n  ) \n\nlm(read_time ~ dark_mode, weights = w, data = darkmode_w) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = read_time ~ dark_mode, data = darkmode_w, weights = w)\n\nWeighted Residuals:\n     Min       1Q   Median       3Q      Max \n-11.4302  -0.6929   0.0814   0.7230  12.8698 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  18.0918     3.4796   5.199 3.72e-07 ***\ndark_mode     0.6384     3.4912   0.183    0.855    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.48 on 298 degrees of freedom\nMultiple R-squared:  0.0001122, Adjusted R-squared:  -0.003243 \nF-statistic: 0.03343 on 1 and 298 DF,  p-value: 0.855\n\n\nDer geschätzte Koeffizient von dark_mode entspricht \\(\\widehat{\\tau}^w\\).\nDa male eine binäre Variable ist, reduziert sich eine Beurteilung der Vergleichbarkeit der Verteilungen von male in Behandlungs- und Kontrollgruppe auf einen simplen Vergleich des Männeranteils beider Gruppen. In der Praxis gibt es meist eine Vielzahl potentieller Backdoor-Variablen, die zudem kontinuierlich verteilt sind. Es scheint plausibel, dass das Alter der Nutzer sowohl die Akzeptanz des Design-Updates als auch die Lesezeit beeinflusst. Die bisherige Verweildauer ist mindestens eine plausible Determinante der Lesezeit.\nDer erweiterte DGP ist in Abbildung Abbildung 3.2 dargestellt, wobei der zusätzliche Backdoor-Pfad durch age ebenfalls mit roten Pfeilen gekennzeichnet sind.\n\n\n\n\n\nread_time\n\nread_time\ndark_mode\n\ndark_mode\ndark_mode-&gt;read_time\n\n\nmale\n\nmale\nmale-&gt;read_time\n\n\nmale-&gt;dark_mode\n\n\nage\n\nage\nage-&gt;read_time\n\n\nage-&gt;dark_mode\n\n\nhours\n\nhours\nhours-&gt;read_time\n\n\n\nAbbildung 3.2: Erweiterter DGP im Website-Design-Beispiel\n\n\n\nDie Beurteilung der Balance von Kontrollgruppe und Behandlungsgruppe kann durch eine grafische Gegenüberstellung der empirischen Verteilungen der Kovariablen beider Gruppen erfolgen. Wir visualisieren die empirischen Verteilungen mit ggplot2. Hierzu standardisieren wir age und hours zunächst mit scale().\n\n# Datensatz für graphische Darstellung formatieren\ndarkmode_p &lt;- darkmode %&gt;% \n  # Standardisierung mit 'scale()'\n  mutate(\n    dark_mode = as_factor(dark_mode),\n    age = scale(age), \n    hours = scale(hours)\n  )\n\nhead(darkmode_p)\n\n# A tibble: 6 × 5\n  read_time dark_mode  male age[,1] hours[,1]\n      &lt;dbl&gt; &lt;fct&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1      14.4 0             0  0.0377    -0.591\n2      15.4 0             1  1.11      -0.459\n3      20.9 1             0 -1.74       0.684\n4      20   0             0 -0.141     -0.451\n5      21.5 1             0 -1.21      -0.316\n6      19.5 0             0  1.91      -0.327\n\n\nFür age und hours eignen sich die geschätzten Dichtefunktionen für einen Vergleich der Verteilungen in Behandlungs- und Kontrollgruppe.\n\n# Vergleich mit Dichteschätzungen\ndarkmode_p %&gt;%\n  select(dark_mode, hours, age) %&gt;%\n  # in langes Format überführen\n  pivot_longer(cols = c(-dark_mode)) %&gt;%\n  \n  ggplot(\n    aes(x = value, fill = dark_mode)\n    ) +\n  geom_density(alpha = .5) + \n  facet_wrap(\n    facets = ~ name, \n    scales = \"free\", \n    nrow = 2\n    )\n\n\n\n\nDie graphische Analyse zeigt deutliche Unterschiede in den Verteilungen von age zwischen Kontroll- und Behandlungsgruppe. Für einen Beurteilung mit deskriptiven Statistiken wird häufig eine sogenannte Balance Table herangezogen. Wir berechnen diese für age, hours und male mit cobalt::bal.tab()\n\nlibrary(cobalt)\n\n# Balance table mit 'cobalt::bal.tab()'\nbal.tab(\n  x = darkmode %&gt;% \n    select(age, hours, male), \n  treat = darkmode$dark_mode, \n   # berechne SMD für KG und TG:\n  disp = \"m\", \n  s.d.denom = \"pooled\"\n)\n\nBalance Measures\n         Type   M.0.Un   M.1.Un Diff.Un\nage   Contin.  46.0132  39.0940 -0.6469\nhours Contin. 337.7775 328.5738 -0.0203\nmale   Binary   0.3377   0.6577  0.3200\n\nSample sizes\n    Control Treated\nAll     151     149\n\n\nDie Einträge M.0.Un und M.1.Un zeigen die jeweiligen Stichprobenmittelwerte der Variablen für Kontroll- und Behandlungsgruppe. Diff.Un gibt eine standardisierte Mittelwertdifferenz \\(SMD\\) an, wobei \\[\\begin{align*}\n  SMD_j := \\left(\\overline{X}_{j,B} - \\overline{X}_{j,K}\\right) \\bigg/ \\sqrt{\\frac{1}{2}\\left(\\widehat{\\text{Var}}(X_{j,B}) + \\widehat{\\text{Var}}(X_{j,K})\\right)},\n\\end{align*}\\] mit Stichprobenmitteln \\(\\overline{X}_{j,B}\\) und \\(\\overline{X}_{j,K}\\) und Stichprobenvarianzen \\(\\widehat{\\text{Var}}(X_{j,B})\\) und \\(\\widehat{\\text{Var}}(X_{j,K})\\) für eine kontinuierliche Kovariable \\(j\\).1 Obwohl es keinen einheitlichen Schwellenwert für die standardisierte Differenz gibt, der ein erhebliches Ungleichgewicht anzeigt, gilt für kontinuierliche Variablen eine standardisierte (absolute) Differenz von weniger als \\(0.1\\) als Hinweis auf einen vernachlässigbaren Unterschied zwischen den Gruppen.1 Siehe P. Austin (2011) für einen Überblick zu Balance-Statistiken.\nDie Balance Table weist also auf einen vernachlässigbaren Unterschied für hours hin und bestätigt den aus den Grafiken abgeleiteten Eindruck einer relevanten Differenzen für age.\n\n3.1.2 Entropy Balancing\nEntropy Balancing (Hainmueller 2012) ist eine weitere Methode zur Gewährleistung der Vergleichbarkeit von Behandlungs- und Kontrollgruppe anhand von Gewichten. Das Verfahren nutzt Konzepte aus der Informationstheorie um die Gewichte für Subjekte in der Kontrollgruppe so anzupassen, dass die Verteilung der Matchingvariablen in der Kontrollgruppe die Verteilung in der Behandlungsgruppe möglichst gut approximiert. Dies geschieht unter der Restriktion, dass bestimmte empirische Momente (meist Mittelwerte und Varianzen) der Matchingvariablen exakt übereinstimmen. Mathematisch werden die Gewichte für Kontrollgruppenbeobachtungen durch Minimierung der Kullback-Leibler-Divergenz zwischen den Verteilungen gefunden, wobei die Divergenz ein Maß für den Unterschied von Wahrscheinlichkeitsverteilungen ist.\nEntropy Balancing ist im R-Paket WeightIt implementiert. Wir zeigen, wie die benötigten Gewichte für eine Schätzungen des ATT im Website-Beispiel mit WeightIt::wightit() bestimmt werden können. Über das Argument moments legen wir fest, dass die Gewichte unter der Restriktion übereinstimmender Mittelwerte aller Matching-Variablen zwischen Behandlungs- und Kontrollgruppe erfolgen soll.\n\nlibrary(WeightIt)\n\n# Gewichte für Entropy Balancing\n(\n  W1 &lt;- weightit(\n  dark_mode ~ age + male + hours,\n  data = darkmode,\n  method = \"ebal\", \n  estimand = \"ATT\",\n  moments = 1\n  )\n )\n\nA weightit object\n - method: \"ebal\" (entropy balancing)\n - number of obs.: 300\n - sampling weights: none\n - treatment: 2-category\n - estimand: ATT (focal: 1)\n - covariates: age, male, hours\n\n\nWir schätzen den Behandlungseffekt nach Entropy Balancing mit gewichteter Regression.\n\n# Mittelwert-Vergleich mit lm()\nfit &lt;- lm(\n  formula = read_time ~ dark_mode, \n  data = darkmode, \n  weights = W1$weights\n)\nsummary(fit)\n\n\nCall:\nlm(formula = read_time ~ dark_mode, data = darkmode, weights = W1$weights)\n\nWeighted Residuals:\n     Min       1Q   Median       3Q      Max \n-16.6662  -2.3552   0.8786   2.9583  27.1840 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  17.7598     0.4093  43.394   &lt;2e-16 ***\ndark_mode     0.9704     0.5807   1.671   0.0958 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.029 on 298 degrees of freedom\nMultiple R-squared:  0.009283,  Adjusted R-squared:  0.005958 \nF-statistic: 2.792 on 1 and 298 DF,  p-value: 0.09578\n\n\nBeachte, dass die von summary() berechneten Standardfehler bei Entropy Balancing ungültig sind. In Abschnitt Kapitel 3.4 erläutern wir die Berechnung von Standardfehlern für Matching-Schätzer mit dem Bootstrap.\n\n3.1.3 Mehrere Matching-Variablen und der Propensity Score\nBei mehreren Backdoor-Variablen kann eine Gewichtung anhand der Behandlungswahrscheinlichkeit (Treatment Propensity) erfolgen. Die Idee hierbei ist, dass der DGP wie in Abbildung 3.3 dargestellt werden kann.\n\n\n\n\n\nread_time\n\nread_time\ndark_mode\n\ndark_mode\ndark_mode-&gt;read_time\n\n\nTreatmentPropensity\n\nTreatmentPropensity\nTreatmentPropensity-&gt;dark_mode\n\n\nmale\n\nmale\nmale-&gt;read_time\n\n\nmale-&gt;TreatmentPropensity\n\n\nage\n\nage\nage-&gt;read_time\n\n\nage-&gt;TreatmentPropensity\n\n\nhours\n\nhours\nhours-&gt;read_time\n\n\n\nAbbildung 3.3: Propensity im Website-Design-Beispiel\n\n\n\nHierbei beeinflussen die Backdoor-Variablen age und male die Behandlungsvariable dark_mode lediglich durch die Behandlungswahrscheinlichkeit Treatment Propensity. Diese Darstellung zeigt, das die mehrdimensionale Information bzgl. der Ähnlichkeit von Subjekten hinsichtlich der beobachteten Kovariablen in einer einzigen Variable zusammengefasst werden kann. Die Backdoor-Pfade können daher geschlossen werden, indem wir Subjekte anhand von Treatment Propensity derart gewichten, dass beide Gruppen hinsichtlich der Verteilung der Behandlungswahrscheinlichkeit vergleichbar sind. Betrachte erneut \\(\\eqref{eq:cia}\\) und beachte, dass \\[\\begin{align}\n  Y_i = Y_i^{(1)} D_i + Y_i^{(0)} (1-D_i).\n\\end{align}\\] Rosenbaum und Rubin (1983) zeigen, dass es hinsichtlich \\(\\eqref{eq:cia}\\) äquivalent ist für die Treatment Propensity \\(P_i(X_i):=P(B_i=1\\vert X_i = x)\\) zu kontrollieren, d.h. \\[\\begin{align}\n  \\left\\{Y_i^{(1)},Y_i^{(0)}\\right\\} \\perp B_i\\vert X_i \\quad\\Leftrightarrow\\quad \\left\\{Y_i^{(1)},Y_i^{(0)}\\right\\} \\perp B_i\\vert P_i(X_i).\n\\end{align}\\]\nDer Behandlungseffekt kann so als Differenz von gewichteten Gruppenmittelwerten berechnet werden, mit inversem Wahrscheinlichkeitsgewicht (IPW) \\(w_{i,B} = 1/P_i(X_i)\\) für Beobachtungen in der Behandlungsgruppe und \\(w_{i,K} = 1/(1-P_i(X_i))\\) für Beobachtungen in der Kontrollgruppe, \\[\\begin{align}\n  \\tau^{\\text{IPW}} = \\frac{1}{n}\\sum_{i=1}^n \\left[\\frac{B_i Y_i}{P_i(X_i)} - \\frac{(1-B_i)Y_i}{1-P_i(X_i)} \\right].\\label{eq:tauipw}\n\\end{align}\\]\nGrundsätzlich ist TreatmentPropensity eine nicht beobachtbare Variable und muss daher aus den Daten geschätzt werden. Eine geschätzte Behandlungswahrscheinlichkeiten \\(\\widehat{P}_i(X_i)\\) wird als Propensity Score bezeichnet. In der Praxis erfolgt die Schätzung von Propensity Scores meist mit logistischer Regression. Ein erwartungstreuer Schätzer des ATE ist \\[\\begin{align}\n  \\widehat{\\tau}^{\\text{IPW}} = \\frac{1}{n}\\sum_{i=1}^n \\left[\\frac{B_i Y_i}{\\widehat{P}_i(X_i)} - \\frac{(1-B_i)Y_i}{1-\\widehat{P}_i(X_i)} \\right].\\label{eq:hattauipw}\n\\end{align}\\] Hirano, Imbens, und Ridder (2003) diskutieren Alternativen zu \\(\\eqref{eq:hattauipw}\\) für die Schätzung anderer Typen von Behandlungseffekten.\nWir schätzen nachfolgend die Propensity Scores für unser Anwendungsbeispiel, erläutern die Berechnung der Gewichte sowie die Schätzung von Behandlungseffekten mit gewichteter Regression. Hierbei betrachten wir eine Variante von \\(\\eqref{eq:hattauipw}\\) mit normalisierten Gewichten \\(\\tilde{w}_{i,B} = w_{i,B}/\\sum_i w_{i,B}\\) und \\(\\tilde{w}_{i,K} = w_{i,K}/\\sum_i w_{i,K}\\) die sich jeweils zu 1 summieren.2 Dies ergibt den Hájek-Schätzer3 \\[\\begin{align}\n    \\widehat{\\tau}_N^{\\text{IPW}} = \\frac{\\sum_i\\tilde{w}_{i,B}Y_i}{\\sum_i\\tilde{w}_{i,B}} -  \\frac{\\sum_i\\tilde{w}_{i,K}Y_i}{\\sum_i\\tilde{w}_{i,K}}.\\label{eq:hattauhajek}\n\\end{align}\\]2 Eine Normalisierung der Gewichte reduziert die Varianz des Schätzers, vgl. Hirano, Imbens, und Ridder (2003)3 Siehe Hájek (1971).\nZunächst Schätzen wir ein logistisches Regressionsmodell mit age, male und hours als erklärende Variablen für dark_mode.\n\n# Logit-Modell mit 'glm()' schätzen\n(\n  darkmode_ps_logit &lt;- glm(\n    formula = dark_mode ~ age + male + hours,\n    data = darkmode,\n    family = binomial\n  )\n)\n\n\nCall:  glm(formula = dark_mode ~ age + male + hours, family = binomial, \n    data = darkmode)\n\nCoefficients:\n(Intercept)          age         male        hours  \n  2.330e+00   -7.330e-02    1.623e+00   -9.293e-05  \n\nDegrees of Freedom: 299 Total (i.e. Null);  296 Residual\nNull Deviance:      415.9 \nResidual Deviance: 346.4    AIC: 354.4\n\n\nDie Propensity Scores erhalten wir als angepasste Werte aus der Regression darkmode_ps_logit mit fitted(). Wir erweitern den Datensatz mit den Ergebnissen.\n\n# Datensatz um Propensity Scores erweitern\n(\n  darkmode_probs &lt;- \n    darkmode %&gt;%\n    mutate(\n      PS = fitted(darkmode_ps_logit)\n    )\n)\n\n# A tibble: 300 × 6\n   read_time dark_mode  male   age  hours     PS\n       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1      14.4         0     0    43   65.6 0.304 \n 2      15.4         0     1    55  125.  0.478 \n 3      20.9         1     0    23  643.  0.642 \n 4      20           0     0    41  129.  0.335 \n 5      21.5         1     0    29  190.  0.547 \n 6      19.5         0     0    64  185.  0.0849\n 7      22           1     0    18  334.  0.727 \n 8      17.4         0     0    53  279.  0.171 \n 9      23.6         0     0    59 1303.  0.108 \n10      15.7         0     0    53   16.1 0.174 \n# ℹ 290 more rows\n\n\nZur Beurteilung der Überlappung (vgl. Annahme \\(\\eqref{eq:overlap}\\) können wir die Verteilung der Propensity Scores nach Behandlungs-Indikator mit Histogrammen visualisieren.\n\n# Überlappung prüfen:\n# Histogramme der PS nach Treatment-Indikator\ndarkmode_probs %&gt;%\n  ggplot(\n    mapping = aes(\n      x = PS, \n      fill = factor(dark_mode)\n    )\n  ) + \n  geom_histogram(\n    alpha = .5, \n    bins = 25, \n    position = \"identity\"\n  )\n\n\n\n\nEin Vergleich der Histogramme zeigt, dass die Überlappung der Propensity Scores in der linken Flanken der Verteilungen der Kontrollgruppe und in der rechten Flanke der Behandlungsgruppe schlechter wird. Wir entfernen zunächst Beobachtungen aus der Stichprobe deren Propensity Scores wenig bzw. keine Überlappung aufweisen.\n\n# Datensatz nach PS trimmen\ndarkmode_probs &lt;- darkmode_probs %&gt;% \n  filter(\n    between(\n      x = PS,\n      left = .25,\n      right = .75\n    )\n  )\n\n\n# Überlappung nach trimming prüfen:\n# Dichteschätzung der PS nach Treatment-Indikator\ndarkmode_probs %&gt;%\nggplot(\n  mapping = aes(\n    x = PS, \n    fill = factor(dark_mode))\n  ) + \n  geom_histogram(\n    alpha = .5, \n    bins = 25, \n    position = \"identity\"\n  )\n\n\n\n\nIPWs anhand der Propensity Scores können schnell mit der Vorschrift \\[\\begin{align}\n  \\text{IPW} = \\frac{\\text{dark\\_mode}}{\\text{PS}} + \\frac{1 - \\text{dark\\_mode}}{1 - \\text{PS}},\n\\end{align}\\] berechnet werden.\n\n# Datensatz um IPWs erweitern\ndarkmode_IPW &lt;- darkmode_probs %&gt;%\n  mutate(\n    IPW = dark_mode / PS + (1 - dark_mode) / (1 - PS)\n  )\n\ndarkmode_IPW %&gt;% \n  select(IPW)\n\n# A tibble: 194 × 1\n     IPW\n   &lt;dbl&gt;\n 1  1.44\n 2  1.91\n 3  1.56\n 4  1.50\n 5  1.83\n 6  1.38\n 7  1.43\n 8  1.47\n 9  2.71\n10  1.42\n# ℹ 184 more rows\n\n\nEine Schätzung des durchschnittlichen Behandlungseffekts gemäß \\(\\eqref{eq:hattauhajek}\\) implementieren wir mit dplyr.\n\ndarkmode_IPW %&gt;%\n  group_by(dark_mode) %&gt;%\n  mutate(w = IPW / sum(IPW)) %&gt;%\n  summarise(weighted_mean = sum(read_time * w)) %&gt;%\n  summarise(diff = diff(weighted_mean))\n\n# A tibble: 1 × 1\n   diff\n  &lt;dbl&gt;\n1  1.90\n\n\nDiese Schätzung des Behandlungseffekts ist äquivalent zur gewichteten KQ-Schätzung anhand eines einfachen linearen Regressionsmodells.\n\n# Mit IPWs gewichteter KQ-Schaetzer berechnet den ATE\nmodel_ipw &lt;- lm(\n  formula = read_time ~ dark_mode, \n  data = darkmode_IPW,\n  weights = IPW\n)\n\nsummary(model_ipw)\n\n\nCall:\nlm(formula = read_time ~ dark_mode, data = darkmode_IPW, weights = IPW)\n\nWeighted Residuals:\n     Min       1Q   Median       3Q      Max \n-18.5086  -4.4579   0.6096   4.1345  20.4566 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  17.9709     0.4942  36.362  &lt; 2e-16 ***\ndark_mode     1.9011     0.6952   2.735  0.00683 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.913 on 192 degrees of freedom\nMultiple R-squared:  0.03749,   Adjusted R-squared:  0.03248 \nF-statistic: 7.478 on 1 and 192 DF,  p-value: 0.006829\n\n\nUnsere Schätzung des ATE ist der geschätzte Koeffizient von dark_mode. Die ausgegebenen Standardfehler und Inferenzstatistiken sind jedoch ungültig aufgrund der Gewichtung mit IPWs, den inversen geschätzten Wahrscheinlichkeiten für eine Behandlung. Der Grund hierfür ist, dass die Berechnung der Standardfehler in summary() die zusätzliche Unsicherheit durch die geschätzen Propensity Scores nicht berücksichtigt! Später im Kapitel erläutern wir die Berechnung gültiger Standardfehler für IPW-Schätzer basierend auf Propensity Scores mit dem Bootstrap."
  },
  {
    "objectID": "Matching.html#matching-verfahren",
    "href": "Matching.html#matching-verfahren",
    "title": "\n3  Matching\n",
    "section": "\n3.2 Matching-Verfahren",
    "text": "3.2 Matching-Verfahren\nDas grundsätzliche Konzept von Matching wird in der nachstehenden interaktiven Grafik veranschaulicht. Hier betrachten wir beobachtete Ausprägungen von zwei (unabhängig und identisch verteilten) Matching-Variablen für Subjekte in der Behandlungsgruppe (blau) sowie Kontrollgruppe (rot). Per Klick auf eine Beobachtung werden Matches aus der anderen Gruppe in grün kenntlich gemacht. Als Matches zählen sämtliche Beobachtungen der anderen Gruppe, deren Euklidische Distanz zu dem ausgewählten Punkt das über den Slider eingestellte Maximum Caliper nicht überschreitet.4 Diese Region wird durch den gestrichelten Kreis gekennzeichnet. Die Grafik illustriert inbesondere, dass Beobachtungen mehrfach (s.g. Matching mit zurücklegen) oder gar nicht gematcht werden können.4 Es handelt sich hierbei um einen Spezialfall von Matching anhand der Mahalanobis-Distanz.\n\n\nMatchIt::matchit() nutzt standardmäßig Eins-zu-Eins-Matching (ohne Zurücklegen) von Beobachtungen der Treatment-Gruppe mit Beobachtungen der Kontrollgruppe. Die für das Matching zu verwendenden Variablen werden über das Argument formula als Funktion des Behandlungsindikators definiert. matchit() bereitet das Objekt für eine Schätzung des ATT mit einer geeigneten Funktionen, s. ?matchit und hier insb. die Erläuterungen der Argumente replace = F, ratio = 1 und estimand = \"ATT\" für Details. Mit cobalt::balt.tab() erhalten wir eine balance table für den gematchten Datensatz.\nWir zeigen als nächstes, wie MatchIt::matchit() für Matching anhand der Regressoren age, hours, und male in unserem Website-Beispiel für unterschiedliche Varianten durchgeführt werden kann.\nExaktes Matching\nExaktes Matching ordnet einem Subjekt aus der Behandlungsgruppe ein oder mehrere Subjekte aus der Kontrollgruppe zu, wenn die boebachteten Ausprägung der Matching-Variablen exakt übereinstimmen. Hierbei muss die ‘Distanz’ zwischen den Ausprägung der Matching-Variablen folglich \\(0\\) sein. Dieses Verfahren findet meist bei ausschließlich diskret verteilten Merkmalen Anwendung. Bei kontinuierlich verteilten Merkmalen (vgl. die obige interaktive Grafik) sind exakte Matches zwar theoretisch unmöglich, ergeben sich jedoch in der Praxis aus der Datenerfassung, bspw. durch Rundungsfehler. In matchit() erhalten wir exaktes Ein-zu-eins-Matching mit method = \"exact\".\n\nlibrary(MatchIt)\n\n# Exaktes Eins-zu-Eins-Matching durchführen\nres_em &lt;- matchit(\n  formula = dark_mode ~ age + male + hours, \n  data = darkmode,\n  estimand = \"ATT\",\n  method = \"exact\"\n)\n\nError in `matchit()`:\n! No exact matches were found.\n\nres_em\n\nError in eval(expr, envir, enclos): object 'res_em' not found\n\n\nAufgrund der kontinulierliche Verteilten Variable hours gibt es in unserem Website-Beispiel keine exakten Matches. Dieses Verfahren ist hier folglich ungeeignet.\nCoarsened Exact Matching\nBei dieser Methode werden kontinuierliche Matching-Variablen grob (Engl. coarse) klassiert, ähnlich wie bei einem Histogram. Diese Diskretisierung ermöglicht es exakte Übereinstimmungen zwischen Behandlungs- und Kontrollgruppenbeobachtungen hinsichtlich ihrer klassierten Ausprägungen zu finden. Sowohl Behandlungs- als auch Kontrollbeobachtungen die mindestents einen exakten Match haben, werden Teil des gematchten Datensatzes. In matchit() wird Coarsened Exact Matching mit method = \"cem\" durchgeführt. Über das Argument cutpoints geben wir an, dass hours in 6 Klassen und age in 4 Klassen eingeteilt werden soll.5 Mit k1k = TRUE erfolgt Eins-zu-eins-Matching: Bei mehreren exakten Matches wird die Beobachtung mit der geringsten Mahalanobis-Distanz (für die unklassierten Matching-Variablen) gewählt.5 Diese Werte wurden ad-hoc gewählt da sie zu einem guten Ergebnis führen.\n\n# Coarsened Exact Matching\nres_CEM &lt;- matchit(\n  formula = dark_mode ~ age + male + hours, \n  data = darkmode, \n  estimand = \"ATT\",\n  method = \"cem\", \n  k2k = TRUE,\n  cutpoints = list(\n    \"hours\" = 6, \n    \"age\" = 4\n  ) \n)\nres_CEM\n\nA matchit object\n - method: Coarsened exact matching\n - number of obs.: 300 (original), 164 (matched)\n - target estimand: ATT\n - covariates: age, male, hours\n\n\n\n# Balance-Table Coarsened Exact Matching\nbal.tab(res_CEM)\n\nBalance Measures\n         Type Diff.Adj\nage   Contin.   0.0106\nmale   Binary   0.0000\nhours Contin.  -0.0135\n\nSample sizes\n          Control Treated\nAll           151     149\nMatched        82      82\nUnmatched      69      67\n\n\nMit Coarsened Exact Matching erhalten wir einen Datensatz mit 82 Beobachtungen und guter Balance.\nMatching anhand der Mahalanobis-Distanz\nDie Euklidische Distanz misst den direkten Abstand zwischen zwei Punkten und ist nicht invariant gegenüber Transformationen, insbesondere bei unterschiedlichen Skalierungen und bei Korrelation der Matching-Variablen. Die Mahalanobis-Distanz hingegen ist ein standardisiertes Distanzmaß, das unter Berücksichtigung der Varianz-Kovarianz-Struktur der Daten angibt, wie viele Standardabweichungen zwei Datenpunkte voneinander entfernt sind. Die Mahalanobis-Distanz ist invariant gegenüber linearen Transformationen (Skalierung, Translation und Rotation) der Daten und bietet ein genaueres Maß für die Unähnlichkeit zweier Beobachtungen hinsichtlich ihrer Ausprägungen der Matching-Variablen.\nBetrachte die Datenpunkte \\(P_1=(X_1,Y_1)'\\) und \\(P_2=(X_2,Y_2)'\\) für die Matching-Variablen \\(X\\) und \\(Y\\). Die Mahalanobis-Distanz zwischen \\(P_1\\) und \\(P_2\\) ist definiert als \\[\\begin{align*}\n  d_M(P_1,\\,P_2) = \\sqrt{(P_1 - P_2)'\\boldsymbol{S}^{-1} (P_1 - P_2)},\n\\end{align*}\\] wobei \\(\\boldsymbol{S}\\) die Varianz-Kovarianz-Matrix von \\(X\\) und \\(Y\\) ist. Die Mahalanobis-Distanz \\(d_M(\\cdot,\\cdot)\\) ist also die Euklidische Distanz zwischen den standardisierten Datenpunkten.\nFür beobachtete Daten ersetzen wir die Komponenten der Varianz-Kovarianz-Matrix durch Stichprobenmaße. Dies ergibt die Formel\n\\[\\begin{align*}\n  \\widehat{d}_M(P_1,\\,P_2) = \\sqrt{\n  \\begin{pmatrix}\n    X_1 - X_2\\\\\n    Y_1 - Y_2\n  \\end{pmatrix}'\n  \\begin{pmatrix}\n    \\widehat{\\text{Var}}(X^2) & \\widehat{\\text{Cov}}(X, Y) \\\\\n     \\widehat{\\text{Cov}}(X, Y) & \\widehat{\\text{Var}}(X^2)\n  \\end{pmatrix}^{-1}\n    \\begin{pmatrix}\n    X_1 - X_2\\\\\n    Y_1 - Y_2\n  \\end{pmatrix}\n}.\n\\end{align*}\\]\nDie nachstehende interaktive Grafik zeigt Beobachtungen zweier Matching-Variablen, die aus einer bivariaten Normalverteilung mit positiver Korrelation generiert wurden. Diese bivariate Verteilung ist identisch für Beobachtungen aus der Kontrollgruppe (rot) und Beobachtungen aus der Behandlungsgruppe (blau). Für die ausgewählte Beobachtung aus der Behandlungsgruppe (schwarzer Rand) werden potentielle Matches in der Kontrollgruppe innerhalb der vorgegebenen Mahalanobis-Distanz in Cyan kenntlich gemacht. Beachte, dass die Mahalanobis-Distanz Varianzen und Kovarianzen der Daten berücksichtigt, sodass die gematchten Beobachtungen in einem elliptischen Bereich um die betrachtete behandelte Beobachtung liegen. Eine Euklidische Distanz hingegen (gestrichelte Linie) ignoriert die Skalierung der Daten.\n\n\nFür Eins-zu-Eins-Matching im Website-Beispiel anhand der Mahalanobis-Distanz mit matchit() setzen wir distance = \"mahalanobis\" und wählen method = \"nearest\". Mit diesen Parametern wird jeder Behandlung aus der Behandlungsgruppe die gemäß \\(d_M\\) am ehesten vergleichbarste Beobachtung aus der Kontrollgruppe zugewiesen, wobei keine mehrfachen Matches zulässig sind.\n\n# 1:1 Mahalanobis-Distanz-Matching\nres_maha &lt;- matchit(\n  formula = dark_mode ~ age + male + hours, \n  data = darkmode, \n  estimand = \"ATT\",\n  distance = \"mahalanobis\", \n  method = \"nearest\"\n)\nres_maha\n\nA matchit object\n - method: 1:1 nearest neighbor matching without replacement\n - distance: Mahalanobis\n - number of obs.: 300 (original), 298 (matched)\n - target estimand: ATT\n - covariates: age, male, hours\n\n\n\n# Balance-Table für 1:1 Mahalanobis-Matching\nbal.tab(res_maha)\n\nBalance Measures\n         Type Diff.Adj\nage   Contin.  -0.5826\nmale   Binary   0.3154\nhours Contin.   0.0106\n\nSample sizes\n          Control Treated\nAll           151     149\nMatched       149     149\nUnmatched       2       0\n\n\nDie Ergebnisse zeigen, dass für sämtliche \\(149\\) Beobachtungen aus der Behandlungsgruppe ein individueller Match in der Kontrollgruppe gefunden werden konnte. Es werden lediglich \\(2\\) Beobachtungen der \\(151\\) Beobachtungen in der Kontrollgruppe nicht gematcht.\nEntsprechend zeigt die Balance-Table eine ähnliche Diskrepanz beider Gruppen hinsichtlich der Matching-Variablen an.\nMahalanobis-Distanz mit Caliper .25 für Propensity Scores basierend auf logistischer Regression\nFür eine strengeres Matching-Kriterium kann ein Caliper, d.h. eine maximal zulässige Distanz, herangezogen werden. Die Mahalanobis-Distanz hat jedoch keine einheitliche Skala: Ob eine Distanz als groß oder klein betrachten werden kann, hängt von der Anzahl der Matching-Variablen und dem Überlappungsgrad zwischen den Gruppen ab. Daher wird die Beschränkung durch einen Caliper nicht auf \\(\\widehat{d}_M\\) sondern auf Propensity Scores angewendet.\nIm nächsten Code-Beispiel spezifizieren wir mit distance = \"glm\", dass Propensity Scores gemäß der Vorschrift in formula geschätzt werden. Mit mahvars = ~ age + male + hours legen wir die Matching-Variablen für die Berechnung von \\(\\widehat{d}_M\\) fest. caliper = .25 legt fest, dass lediglich Beobachtungen der Kontrollgruppe bei einer absoluten Differenz der Propensity Scores von höchstens \\(0.25\\) Standardabweichungen als Match für eine Beobachtung in der Behandlungsgruppe qualifiziert sind.\n\n# Mahalanobis-Matchig mit PS-Caliper\nres_mahaC &lt;- matchit(\n  formula = dark_mode ~ age + male + hours, \n  data = darkmode, \n  distance = \"glm\",\n  estimand = \"ATT\",\n  method = \"nearest\",\n  mahvars = ~ age + male + hours,\n  caliper = .25\n)\nres_mahaC\n\nA matchit object\n - method: 1:1 nearest neighbor matching without replacement\n - distance: Mahalanobis [matching]\n             Propensity score [caliper]\n             - estimated with logistic regression\n - caliper: &lt;distance&gt; (0.058)\n - number of obs.: 300 (original), 208 (matched)\n - target estimand: ATT\n - covariates: age, male, hours\n\n\n\n# Balance Table\nbal.tab(res_mahaC)\n\nBalance Measures\n             Type Diff.Adj\ndistance Distance   0.1812\nage       Contin.  -0.1176\nmale       Binary   0.0481\nhours     Contin.  -0.0001\n\nSample sizes\n          Control Treated\nAll           151     149\nMatched       104     104\nUnmatched      47      45\n\n\nDie Balance-Table zeigt einen deutlichen Effekt der Beschränkung qualifizierter Beobachtungen durch caliper = .25: Aufgrund der oberen Grenze für die Propensity-Score-Differenz von \\(0.058\\) wird für lediglich \\(104\\) Beobachtungen aus der Behandlungsgruppe ein individueller Match in der Kontrollgruppe gefunden.6 Weiterhin finden wir eine verbesserte Balance für den gematchten Datensatz.6 Die durch caliper implizierte Obergrenze ergibt sich als .25 * sd(fitted(darkmode_ps_logit))).\nMatching mit Propensity Scores und Caliper\nEine gängige Variante ist Matching ausschließlich anhand von Propensity Scores innerhalb eines Calipers.\n\n# 1:1 Matching mit PS und Caliper\nres_PSC &lt;- matchit(\n  formula = dark_mode ~ age + male + hours, \n  data = darkmode, \n  estimand = \"ATT\",\n  distance = \"glm\", \n  method = \"nearest\", \n  caliper = .25\n)\nres_PSC\n\nA matchit object\n - method: 1:1 nearest neighbor matching without replacement\n - distance: Propensity score [caliper]\n             - estimated with logistic regression\n - caliper: &lt;distance&gt; (0.058)\n - number of obs.: 300 (original), 208 (matched)\n - target estimand: ATT\n - covariates: age, male, hours\n\n\n\n# Balance Table\nbal.tab(res_PSC)\n\nBalance Measures\n             Type Diff.Adj\ndistance Distance   0.1640\nage       Contin.  -0.0976\nmale       Binary   0.0481\nhours     Contin.   0.0134\n\nSample sizes\n          Control Treated\nAll           151     149\nMatched       104     104\nUnmatched      47      45\n\n\nLaut Balance-Table führt Eins-zu-Eins-Matching basierend auf Propensity Scores zu einem Datensatz mit \\(104\\) gematchten Beobachtungen in der Behandlungsgruppe. Hinsichtlich der standardisierten Mittelwertdifferenz (Diff.Adj) erzielt diese Methode die beste Balance unter den betrachteten Ansätzen.\nVergleich der Balance verschiedener Verfahren mit Love-Plot\nStandardisierte Mittelwertdifferenzen für verschiedene Matching-Verfahren können grafisch mit einem Love-Plot (Love 2004) veranschaulicht werden. Hierzu nutzen wir cobalt::love.plot() und übergeben die mit matchit() generierten Objekte im Argument weights.\n\n# Love-Plot für\nlove.plot(\n  x = dark_mode ~ age + male + hours, \n  weights = list(\n    CEM = res_CEM,\n    Mahalanobis = res_maha,\n    Mahalanobis_Cal = res_mahaC,\n    PSC = res_PSC\n  ),\n  data = darkmode, \n  line = T,\n  # absolute Mittelwertdifferenz plotten\n  abs = T\n)\n\n\n\n\nDie Grafik zeigt, dass Coarsened Exact Matching (CEM) unter allen betrachteten Verfahren die Stichprobe mit der besten Balance ergibt. Diesen gematchten Datensatz erhalten wir mit MatchIt::match.data().\n\n# gematchten Datensatz zuweisen\ndarkmode_matched_CEM &lt;- match.data(res_CEM)\nhead(darkmode_matched_CEM)\n\n# A tibble: 6 × 7\n  read_time dark_mode  male   age hours weights subclass\n      &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;   \n1      15.4         0     1    55  125.       1 26      \n2      20.9         1     0    23  643.       1 70      \n3      21.5         1     0    29  190.       1 79      \n4      22           1     0    18  334.       1 80      \n5      17.4         0     0    53  279.       1 11      \n6      20.4         0     0    43  138.       1 9       \n\n\ndarkmode_matched enthält Gewichte (weights) für die jeweilige Gruppe zu denen gemachte Beobachtungen gehören (subclass). Dies ist relevant, falls Beobachtungen mehrfach gematcht werden. Wegen Eins-zu-eins-Matching ohne Zurücklegen gibt es in unserem Beispiel 82 Beobachtungspaare und sämtliche Gewichte sind 1. Die Berücksichtigung der Gewicht in den nachfolgenden Aufrufen von Schätzfunktionen (bspw.lm()) ist daher nicht nötig und erfolgt lediglich zur Illustration der grundsätzlichen Vorgehensweise.\nEine Wiederholung der grafischen Analyse in Kapitel 3.1 zeigt eine deutlich verbesserte Vergleichbarkeit hinsichtlich der Verteilung der Matching-Variablen in darkmode_matched.\n\ndarkmode_matched_CEM %&gt;%\n  group_by(dark_mode) %&gt;%\n  select(age, hours) %&gt;%\n  mutate_all(scale) %&gt;%\n  pivot_longer(cols = c(-dark_mode)) %&gt;%\n  \n  ggplot(\n    aes(x = value, fill = as.factor(dark_mode))\n  ) +\n  geom_density( alpha = .5) + \n  facet_wrap(\n    facets = ~ name, \n    scales = \"free\", \n    nrow = 3\n  )\n\n\n\ndarkmode_matched_CEM %&gt;% \n  group_by(dark_mode) %&gt;%\n  mutate(\n    male = as.factor(male), \n    dark_mode = as.factor(dark_mode)\n  ) %&gt;%\n  \n  ggplot(\n    aes(x = dark_mode, fill = male)\n  ) +\n  geom_bar(position = \"fill\") +\n  ylab(\"Anteil\")\n\n\n\n\nWir beobachten eine bessere Balance bei age und hours. Inbesondere ist male für Kontroll- und Behandlungsgruppe ausgeglichen."
  },
  {
    "objectID": "Matching.html#schätzung-und-inferenz-für-den-behandlungseffekts-nach-matching",
    "href": "Matching.html#schätzung-und-inferenz-für-den-behandlungseffekts-nach-matching",
    "title": "\n3  Matching\n",
    "section": "\n3.3 Schätzung und Inferenz für den Behandlungseffekts nach Matching",
    "text": "3.3 Schätzung und Inferenz für den Behandlungseffekts nach Matching\nWir schätzen nun den Behandlungseffekt von dark_mode auf read_time für die mit CEM und Propensity Score Matching ermittelten Datensätzen und vergleichen die Ergebniss anschließend mit einer Regressionsschätzung ohne Matching.\nWir kombinieren die Matching-Verfahren mit linearer Regression, d.h. wir Schätzen den Behandlungseffekt anhand es gematchten Datensatzes als Mittelwertdifferenz nach zusätzlicher Kontrolle für die Matching-Variablen. Diese Kombination von Matching mit Regression wird in der Literatur als Regression Adjustment bezeichnet und ist insbesondere hilfreich, wenn Backdoors mit Matching geschlossen werden sollen, der kausale Effekt jedoch nur unter Verwendung einer nicht-trivialen Regressionsfunktion ermittelt werden kann. Zum Beispiel kann bei einer kontinuierlichen Behandlungsvariable und einem nicht-linearen Zusammenhang mit \\(Y\\) der kausale Effekt nicht durch einen bloßen Mittelwertvergleich erfasst werden, sondern erfordert eine adäquate Berücksichtigung in der Regressionsfunktion. Die zusätzliche Kontrolle für Matching-Variablen kann die Varianz der Schätzung verringern und das Risiko einer verzerrten Schätzung abmildern, falls nach Matching noch Unterschiede in der Balance von Behandlungs- und Kontrollgruppe vorliegen.\n\n# ATT mit linearem Modell schätzen: CEM Datensatz\nATT_mod_CEM &lt;- lm(\n  formula = read_time ~ age + male + hours + dark_mode,\n  data = darkmode_matched_CEM, \n  weights = weights \n)\n\nsummary(ATT_mod_CEM)\n\n\nCall:\nlm(formula = read_time ~ age + male + hours + dark_mode, data = darkmode_matched_CEM, \n    weights = weights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.9310 -2.3856 -0.0194  2.5407 14.0020 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 16.6088953  1.5255529  10.887  &lt; 2e-16 ***\nage          0.0380982  0.0344699   1.105  0.27072    \nmale        -4.0915725  0.6858131  -5.966 1.53e-08 ***\nhours        0.0050129  0.0006977   7.185 2.45e-11 ***\ndark_mode    1.6532234  0.6149439   2.688  0.00794 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.937 on 159 degrees of freedom\nMultiple R-squared:  0.414, Adjusted R-squared:  0.3992 \nF-statistic: 28.08 on 4 and 159 DF,  p-value: &lt; 2.2e-16\n\n\n\n# Datensatz für Propensity Score Matching zuweisen\ndarkmode_matched_PSC &lt;- match.data(res_PSC)\n\n# ATT mit linearem Modell schätzen: PSM Datensatz\nATT_mod_PSC &lt;- lm(\n  formula = read_time ~ age + male + hours + dark_mode,\n  data = darkmode_matched_PSC, \n  weights = weights \n)\n\nsummary(ATT_mod_PSC)\n\n\nCall:\nlm(formula = read_time ~ age + male + hours + dark_mode, data = darkmode_matched_PSC, \n    weights = weights)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.972 -2.605 -0.044  2.587 14.951 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.3654519  1.2762648  13.606  &lt; 2e-16 ***\nage          0.0283941  0.0301484   0.942  0.34741    \nmale        -3.9858702  0.6480072  -6.151 4.03e-09 ***\nhours        0.0046119  0.0006685   6.899 6.53e-11 ***\ndark_mode    1.6346636  0.5769812   2.833  0.00507 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.141 on 203 degrees of freedom\nMultiple R-squared:  0.3171,    Adjusted R-squared:  0.3037 \nF-statistic: 23.57 on 4 and 203 DF,  p-value: 5.098e-16\n\n\n\n# ATT mit linearem Modell ohne Matching\nATT_mod_org &lt;- lm(\n  formula = read_time ~ age + male + hours + dark_mode,\n  data = darkmode\n)\n\nsummary(ATT_mod_org)\n\n\nCall:\nlm(formula = read_time ~ age + male + hours + dark_mode, data = darkmode)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.2697  -2.6710   0.0164   2.5909  14.5739 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 16.859075   1.082303  15.577  &lt; 2e-16 ***\nage          0.051332   0.022215   2.311  0.02154 *  \nmale        -4.485545   0.498957  -8.990  &lt; 2e-16 ***\nhours        0.004348   0.000516   8.427 1.58e-15 ***\ndark_mode    1.385810   0.523793   2.646  0.00859 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.03 on 295 degrees of freedom\nMultiple R-squared:  0.3434,    Adjusted R-squared:  0.3345 \nF-statistic: 38.57 on 4 and 295 DF,  p-value: &lt; 2.2e-16\n\n\nBeachte, dass für die gematchten Datensätze jeweils ein durchschnittlicher Behandlungseffekt für die Beobachtungen mit erfolgter Behandlung ermittelt wird: In sämtlichen oben gezeigten Matchibg-Verfahren werden mit estimand = \"ATT\" vergleichbarere Kontrollbeobachtungen für die behandelten Beobachtungen ermittelt. Wir schätzen den Effekt der Behandlung, indem wir die Ergebnisse von behandelten Personen mit denen von gematchten Personen vergleichen, die keine Behandlung erhalten haben. Diese Vergleichsgruppe dient als Ersatz für den hypothetischen Zustand der Behandlungsgruppe, wenn keine Behandlung erfolgt wäre. Dies entspricht der Definition eines ATT — ein average treatment effect on the treated.\n\n3.3.1 Cluster-robuste Standardfehler\nFür Matching-Verfahren sind die mit summary() berechneten Standardfehler (und damit auch Konfidenzintervalle, t-Statistiken und p-Werte) für den Behandlungseffekt grundsätzlich ungültig. Je nach Matching-Verfahren liegen unterschiedliche Quellen von Schätzunsicherheit vor, die bei der Berechnung von Standardfehlern zusätzlich zu der “üblichen” Stichproben-Variabilität berücksichtig werden müssen. Gründe hierfür sind der Matching-Prozess ansich oder weitere Unsicherheit durch die Schätzung zusätzlicher Parameter, etwa bei der Berechnung von Propensity Scores mit logistischer Regression. Eine weiterere Ursache zusätzlicher Variation durch den Matching-Prozess, die wir bisher nicht näher betrachtet haben ensteht durch Zurücklegen, d.h. wenn Beobachtungen mehrfach gematcht werden können. Auch dieser Faktor wird in der von summary() verwendeten Formel für den Standardfehler des Effekt-Schätzers nicht berücksichtigt.\nStandardfehlerberechnung für Matching-Schätzer von Behandlungseffekten ist Gegenstand aktueller Forschung. P. C. Austin und Small (2014) und Abadie und Spiess (2022) belegen die Gültigkeit von cluster-robusten Standardfehlern mit Clustering auf Ebene der Beobachtungsgruppen (subclass im output von match.data()) bei Matching ohne Zurücklegen. Für Matching anhand von Propensity Scores (auch mit Zurücklegen) zeigen Imbens (2016), dass ignorieren der zusätzlichen Unsicherheit durch die Schätzung der Propensity Scores zu konservativer Inferenz für den ATE anhand eines cluster-robusten Standardfehlerschätzers führt, jedoch ungültige Inferenz für die Schätzung des ATT bedeuten kann. Ähnlich zu P. C. Austin und Small (2014) deuten die Ergebnisse der Simulationsstudie von Bodory u. a. (2020) auf grundsätzlich bessere Eigenschaften der Schätzung hin, wenn die Standardfehler nicht für die Schätzung der Propensity Scores adjustiert werden.\nWeiterhin ist die Kontrolle für Kovariablen mit Erklärungskraft für die Outcome-Variable und für die Matching-Variablen (wie oben erfolgt) mit Regression Adjustment für die Schätzung des Behandlungseffekts nach Matching eine etablierte Strategie, vgl. Hill und Reiter (2006) und Abadie und Spiess (2022). So können die Varianz der Schätzung und das Risiko einer Verzerrung der Standardfehler aufgrund verbleibender Imbalance von Behandlungs- und Kontrollgruppe nach Matching verringert werden.\nZur Demonstration von (cluster)-robuster Inferenz und für eine tabellarische Zusammenfassung der Ergebnisse nutzen wir die Pakete marginaleffects und modelsummary. Mit marginaleffects::avg_comparisons() können p-Werte und Kofindenzintervalle unter Berücksichtigung von robuster Standardfehlern und der Gewichte aus dem Matching-Verfahren berechnet werden.\n\nlibrary(marginaleffects)\n\n# Inferenz: Multiple Regression, ungematchter Datensatz\n(\n  sum_orig &lt;- avg_comparisons(\n    model = ATT_mod_org,\n    variables = \"dark_mode\",\n    # Heteroskedastie-robuste SE:\n    vcov = \"HC3\", \n    # Identifizierung der Kontrollgruppe:\n    newdata = subset(darkmode, dark_mode == 1) \n  )\n) \n\n\n      Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n dark_mode    1 - 0     1.39      0.537 2.58  0.00988 6.7 0.333   2.44\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\n\n# Inferenz: Multiple Regression bei CEM\n(\n  sum_CEM &lt;- avg_comparisons(\n  model = ATT_mod_CEM ,\n  variables = \"dark_mode\",\n  # Cluster-robuste SE\n  vcov = ~ subclass, \n  newdata = subset(darkmode_matched_CEM, dark_mode == 1),\n  wts = \"weights\"\n  )\n)\n\n\n      Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n dark_mode    1 - 0     1.65      0.549 3.01  0.00262 8.6 0.576   2.73\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\n\n# Inferenz: Multiple Regression bei PSM\n(\n  sum_PSC &lt;- avg_comparisons(\n    model = ATT_mod_PSC ,\n    variables = \"dark_mode\",\n    vcov = ~ subclass, \n    newdata = subset(darkmode_matched_PSC, dark_mode == 1),\n    wts = \"weights\"\n  )\n)\n\n\n      Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n dark_mode    1 - 0     1.63      0.565 2.89  0.00381 8.0 0.527   2.74\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nWir fassen die Ergebnisse mit modelsummary::modelsummary() tabellarisch zusammen.\n\nlibrary(modelsummary)\n\n# Tabellarische Zusammenfassung erzeugen\nmodelsummary(\n  models = list(\n   \"Kein Matching\" = sum_orig, \n   \"Coarsened Exact\" = sum_CEM, \n   \"Propensity Scores\" = sum_PSC\n  ),\n  stars = T, \n  gof_map = \"nobs\", \n  output = \"gt\"\n) %&gt;%\n  tabopts()\n\n\n\n\n\n\n \n      Kein Matching\n      Coarsened Exact\n      Propensity Scores\n    \n\n\ndark_mode\n1.386**\n1.653**\n1.635**\n\n\n\n(0.537)\n(0.549)\n(0.565)\n\n\nNum.Obs.\n300\n164\n208\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "Matching.html#inferenz-für-attate-propensity-score-matching-mit-bootstrap",
    "href": "Matching.html#inferenz-für-attate-propensity-score-matching-mit-bootstrap",
    "title": "\n3  Matching\n",
    "section": "\n3.4 Inferenz für ATT/ATE: Propensity-Score-Matching mit Bootstrap",
    "text": "3.4 Inferenz für ATT/ATE: Propensity-Score-Matching mit Bootstrap\nBei Matching mit Zurücklegen besteht zusätzliche Unsicherheit durch Zurücklegen, d.h. Beobachtungen aus der Kontroll-Gruppe können mehrfach als Match für Beobachtungen aus der Treatment-Gruppe genutzt werden. Mit summary() berechnete Standardfehler berücksichtigen dies nicht!\nEin Bootstrap-Verfahren generiert mit Resampling (wiederholtes Ziehen mit Zurücklegen) aus dem Original-Datensatz (viele) künstliche Datensätze, für die der Schätzer (d.h. das gesamte Verfahren inkl. Matching!) jeweils berechnet wird. Die Verteilung der so gewonnenen Bootstrap-Schätzwerte approximiert die wahre, unbekannte Stichprobenverteilung des Schätzers des Behandlungseffekts. Mit dieser simulierten Verteilung können wir Inferenz betreiben: Wir können einen Bootstrap-Punktschätzer des Behandlungseffekts (Stichprobenmittel der Bootstrap-Schätzungen) sowie Standardfehler (Standardabweichung der der Bootstrap-Schätzungen) und p-Werte berechnen.\nWir Implementieren nun einen Bootstrap-Schätzer des ATT als R-Funktion boot_fun().\n\nboot_fun &lt;- function(data, i) {\n  \n  boot_data &lt;- data[i, ]\n  \n  # 1:1 PS Matching _mit_ Zurücklegen\n  match_res &lt;- matchit(\n    dark_mode ~ age + hours + male,\n    estimand = \"ATT\",\n    distance = \"glm\", \n    method = \"nearest\", \n    caliper = .3,\n    data = boot_data,\n    # mit Zurücklegen:\n    replace = TRUE\n  ) \n  \n  # Gematchten Datensatz zuweisen\n  darkmode_matched &lt;- match.data(match_res, data = boot_data)\n  \n  # Outcome-Modell schätzen\n  ATT_mod &lt;- lm(\n    formula = read_time ~ age + male + hours + dark_mode,\n    data = darkmode_matched, \n    weights = weights # hier teilweise &gt; 1 wg. Matching mit Zurücklegen!\n  )\n  \n  #  ATT-Schätzer auslesen\n  return(\n    ATT_mod$coefficients[\"dark_mode\"]  \n  )\n}\n\nAbadie & Imbens (2008) zeigen analytisch, dass ein Standard-Bootstrap bei Matching grundsätzlich ungültig ist: Die unbekannte Varianz der Stichprobenverteilung des Matching-Schätzers (und damit der Standardfehler des Schätzers) kann durch den Bootstrap nicht repliziert werden. Problematisch hierbei sind grundsätzlich zu liberale (d.h. zu große) mit dem Bootstrap berechnete Standardfehler. Es gibt jedoch Simulationsnachweise die zeigen, dass Bootstrap-Standardfehler bei Matching mit Zurücklegen konservativ sind (Bodory et al., 2020), also tendentiell zu kleine Standardfehler produzieren und damit das gewünschte nominale Signifikanzniveau eines Bootstrap-Hypothesentests nicht überschritten wird.\nWir berechnen nun eine Bootstrap-Schätzung des ATT von dark_mode auf readingtime sowie den zugehörigen Standardfehler und ein 95%-KI mit der zuvor definierten Funktion boot_fun.\n\nlibrary(\"boot\")\nset.seed(4321)\nboot_out &lt;- boot(darkmode, boot_fun, R = 999)\n\nboot_out\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = darkmode, statistic = boot_fun, R = 999)\n\n\nBootstrap Statistics :\n    original    bias    std. error\nt1* 1.307298 -0.163572   0.8561828\n\n\n\n# Bootstrap-Schätzer für den Treatment-Effekt\nmean(boot_out$t) \n\n[1] 1.143725\n\n# = mean(t0) + bias = mean(Bootstrap_samples)\n# vgl. 't0 = boot_fun(darkmode, i = 1:1e3)'\n\n# Bootstrap-Standardfehler\nsd(boot_out$t)\n\n[1] 0.8561828\n\n# 95% Bootstrap-KI für den Treatment-Effekt\nboot.ci(boot_out, type = \"perc\")\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 999 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_out, type = \"perc\")\n\nIntervals : \nLevel     Percentile     \n95%   (-0.660,  2.721 )  \nCalculations and Intervals on Original Scale"
  },
  {
    "objectID": "Matching.html#doubly-robust-schätzer-für-attate",
    "href": "Matching.html#doubly-robust-schätzer-für-attate",
    "title": "\n3  Matching\n",
    "section": "\n3.5 Doubly-Robust-Schätzer für ATT/ATE",
    "text": "3.5 Doubly-Robust-Schätzer für ATT/ATE\nImplementieren und berechnen Sie einen Doubly-Robust-Schätzer des ATT, vgl. Wooldridge (2010).\n\n# IPW estimation with regression adjustment\nipwra &lt;- function(br, index = 1:nrow(br)) {\n    # slice bootstrapped observations\n    br &lt;- br %&gt;% slice(index)\n    \n    # estimate and predict propensity score\n    m &lt;- glm(formula = dark_mode ~ age + hours + male,\n             data = br, \n             family = binomial(link = 'logit'))\n    \n    br &lt;- br %&gt;%\n        mutate(ps = predict(m, type = 'response'))\n    \n    # trim control observations outside of treated PS range\n    # minps &lt;- br %&gt;%\n    #     filter(dark_mode == 1) %&gt;%\n    #     pull(ps) %&gt;%\n    #     min(na.rm = TRUE)\n    # \n    # maxps &lt;- br %&gt;%\n    #     filter(dark_mode == 1) %&gt;%\n    #     pull(ps) %&gt;%\n    #     max(na.rm = TRUE)\n    # \n    # # do the trimming\n    # br &lt;- br %&gt;%\n    #     filter(ps &gt;= minps & ps &lt;= maxps)\n   \n    br &lt;- br %&gt;%\n      filter(\n        between(\n          x = ps,\n          left = .2,\n          right = .7\n          )\n    )\n    \n    # compute IPWs\n    br &lt;- br %&gt;%\n      mutate(\n        ipw = case_when(\n          dark_mode == 1 ~ 1 / ps,\n          dark_mode == 0 ~ 1 / (1 - ps))\n      )\n    \n    # Simple _ATT_ estimate:\n    # w_means &lt;- br %&gt;%\n    #     group_by(dark_mode) %&gt;%\n    #     summarize(m = weighted.mean(read_time, w = ipw)) %&gt;% \n    #     arrange(dark_mode)\n    # \n    # # simple diff-in-means _ATT_ estimate\n    #  return(w_means$m[2] - w_means$m[1]) \n    \n    # Do regression adjustment for _ATE_ estimate\n    # TE prediction for whole sample based on TG model\n    mtreat &lt;- br %&gt;%\n      filter(dark_mode == 1) %&gt;%\n      lm(read_time ~ 1 + age + hours + male, data = ., weights = .$ipw) %&gt;%\n      predict(newdata = br) %&gt;%\n      mean()\n    \n    # TE prediction for whole sample based on CG model\n    mcont &lt;- br %&gt;%\n      filter(dark_mode == 0) %&gt;%\n      lm(read_time ~ 1 + age + hours + male, data = ., weights = .$ipw) %&gt;%\n      predict(newdata = br) %&gt;%\n      mean()\n\n    return(mtreat - mcont) # Regression adjusted _ATE_ estimate\n}\n\n\nb &lt;- boot(data = darkmode, ipwra, R = 999)\n# Bootstrap estimate and standard error\nmean(b$t)\n\n[1] 1.945025\n\nsd(b$t)\n\n[1] 0.6190766\n\n# 95% Bootstrap-KI für den Treatment-Effekt\nboot.ci(b, type = \"perc\")\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 999 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = b, type = \"perc\")\n\nIntervals : \nLevel     Percentile     \n95%   ( 0.728,  3.085 )  \nCalculations and Intervals on Original Scale\n\n\n\n\n\n\n\n\nAbadie, Alberto, und Guido W. Imbens. 2008. „On the Failure of the Bootstrap for Matching Estimators.“ Econometrica. Journal of the Econometric Society 76 (6): 1537–57. https://doi.org/10.3982/ECTA6474.\n\n\nAbadie, Alberto, und Jann Spiess. 2022. „Robust Post-Matching Inference.“ Journal of the American Statistical Association 117 (538): 983–95. https://doi.org/10.1080/01621459.2020.1840383.\n\n\nAustin, P. 2011. „An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies“. Multivariate Behavioral Research 46 (3): 399–424. https://doi.org/10.1080/00273171.2011.568786.\n\n\nAustin, Peter C., und Dylan S. Small. 2014. „The use of bootstrapping when using propensity-score matching without replacement: A simulation study.“ Statistics in Medicine 33 (24): 4306–19. https://doi.org/10.1002/sim.6276.\n\n\nAustin, Peter C., und Elizabeth A. Stuart. 2017. „Estimating the Effect of Treatment on Binary Outcomes Using Full Matching on the Propensity Score.“ Statistical Methods in Medical Research 26 (6): 2505–25. https://doi.org/10.1177/0962280215601134.\n\n\nBodory, Hugo, Lorenzo Camponovo, Martin Huber, und Michael Lechner. 2020. „The Finite Sample Performance of Inference Methods for Propensity Score Matching and Weighting Estimators.“ Journal of Business & Economic Statistics. https://doi.org/10.2139/ssrn.2731969.\n\n\nHainmueller, Jens. 2012. „Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies“. Political Analysis 20 (1): 25–46. https://doi.org/10.1093/pan/mpr025.\n\n\nHájek, J. 1971. „Comment on ‚An essay on the logical foundations of survey sampling‘ by Basu, D“. Foundations of Statistical Inference 236.\n\n\nHill, Jennifer, und Jerome P. Reiter. 2006. „Interval estimation for treatment effects using propensity score matching. Statistics in Medicine“. Statistics in Medicine 25 (13): 2230–56. https://doi.org/10.1002/sim.2277.\n\n\nHirano, Keisuke, Guido Imbens, und Geert Ridder. 2003. „Efficient Estimation of Average Treatment Effects Using the Estimated Propensity Score.“ Econometrica 71 (4): 1161–89. https://doi.org/10.1111/1468-0262.00442.\n\n\nImbens. 2016. „Matching on the Estimated Propensity Score.“ Econometrica 84 (2): 781–807. https://doi.org/10.3982/ecta11293.\n\n\nLove, Thomas. 2004. „Graphical display of covariate balance“. Presentation.\n\n\nRosenbaum, Paul R., und Donald R. Rubin. 1983. „The central role of the propensity score in observational studies for causal effects“. Biometrika 70 (1): 170–84. https://doi.org/10.1017/cbo9780511810725.016.\n\n\nWooldridge, Jeffrey. 2010. Econometric Analysis of Cross Section and Panel Data. Second edition. Cambridge, Massachusetts: MIT."
  },
  {
    "objectID": "R_Einfuehrung.html#lange-weite-und-tidy-datenformate",
    "href": "R_Einfuehrung.html#lange-weite-und-tidy-datenformate",
    "title": "\n2  Statistische Programmierung mit R\n",
    "section": "\n2.1 Lange, weite und “tidy” Datenformate",
    "text": "2.1 Lange, weite und “tidy” Datenformate\nWir betrachten den in Tabelle 2.1 dargestellten Datensatz Klausurergebnisse.\n\n\n\n\n\n\n\n\n  \nName\n      Mikro\n      Makro\n      Mathe\n    \n\n\nTim\nNA\n1.3\n3\n\n\nLena\n1\n3\nNA\n\n\nRicarda\n2\n1.7\n1.3\n\n\nSimon\n2.3\n3.3\nNA\n\n\n\nTabelle 2.1:  Datensatz Klausurergebnisse \n\n\n\nDer Datensatz ist noch nicht in der R-Arbeitsumgebung verfügbar. Mit der Funktion tribble() können wir Tabelle 2.1 händisch als R-Objekt der Klasse tibble definieren\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nklausurergebnisse enhält die Klausurnoten der vier Studierenden (Boebachtungen) spaltenweise pro Modul, d.h. die Spaltennamen Mikro, Makro und Mathe sind Ausprägungen der Variable Modul. Der Datensatz liegt also nicht im s.g. Tidy-Format vor.\n\n\n\n\n\n\nTidy-Format\n\n\n\nTidy-Format: Jede Spalte ist eine Variable, jede Reihe ist eine Beobachtung und jede Zelle enthält einen einen Wert. Datensätze im Tidy-Format sind häufig lang: Die Zeilendimension ist größer als die Spaltendimension.\n\n\nDas Tidy-Format ist hilfreich für statistische Analysen mit tidyverse-Funktionen wie bspw. ggplot(). Wir nutzen die Funktion tidyr::pivot_longer(), um klausurergebnisse ein (langes) Tidy-Format zu transformieren.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nBeachte, dass die Spalte Name die Zugehörigkeit der Ausprägungen (Note) jeder Variable (Modul) zu einer Beobachtung identifiziert. Mit dieser Information können wir den langen Datensatz wieder in das ursprüngliche (weite) Format zurückführen. Wir nutzen hierzu tidyr::pivot_wider().\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nWenn die Zuweisung von Zwischenergebnissen in Variablen nicht benötigt wird, kann eine Verkettung von Funktionsaufrufen die Verständlichkeit des Codes verbessern. Hierzu wird der Pipe-Operator %&gt;% genutzt. Wir wiederholen die Transformationen mit den tidyr::pivot_*-Funktion bei Verwendung von %&gt;%.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nEin Beispiel für den Nachteil des weiten Formats im Umgang mit tidyverse-Paketen ist die Funktion tidyr::drop_na(). Diese entfernt sämtliche Zeilen eines Datensatzes, die NA-Einträge (d.h. fehlende Werte) aufweisen. Beachte, dass diese Operation im ursprünglichen weiten Format zum Entfernen ganzer Beobachtungen aus wide führt.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nIm Tidy-Format long hingegen bleiben die übrigen Informationen betroffener Beobachtungen erhalten.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte…."
  },
  {
    "objectID": "R_Einfuehrung.html#pinguine-und-pipes",
    "href": "R_Einfuehrung.html#pinguine-und-pipes",
    "title": "\n2  Statistische Programmierung mit R\n",
    "section": "\n2.2 Pinguine und Pipes",
    "text": "2.2 Pinguine und Pipes\nIn diesem Abschnitt zeigen wir die Verwendung häufig verwendeter dplyr-Funktionen (s.g. Verben) für die Transformation von Datensätzen: mutate(), select(), filter(),summarise() und arrange().\nFür die Illustration verwenden wir den Datensatz penguins aus dem R-Paket palmerpenguins. Dieser Datensatz wurde im Zeitraum 2007 bis 2009 von Dr. Kristen Gorman im Rahmen des Palmer Station Long Term Ecological Research Program zusammengetragen und enthält Größenmessungen für drei Pinguinarten, die auf den Inseln des Palmer-Archipels in der Antarktis beobachtet wurden.\n\n# Paket 'palmerpenguins' installieren\n# install.packages(\"palmerpenguins\")\n\n# Paket 'palmerpenguins' laden\nlibrary(palmerpenguins)\n\nMit data() wird der Datensatz in der Arbeitsumgebung verfügbar gemacht. Wir nutzen glimpse(), um einen Überblick zu erhalten.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n2.2.1 dplyr::mutate()\n\nMit mutate() können bestehende Variablen überschrieben oder neue Variablen als Funktion bestehender Variablen definiert werden. mutate() operiert in der Spaltendimension des Datensatz.\nWir definieren eine neue Variable body_mass_kg als Transformation body_mass_g/1000.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nMit across() kann die dieselbe Operation auf mehrere Variablen angewendet werden.\nIm nachstehenden Beispiel ändern wir den typ (type) der Variablen species, island, sex und year zu character.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\ntransmute() ist eine Variante von mutate(), die lediglich die transformierten Variablen beibehält.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n2.2.2 dplyr::select()\n\nMit select() werden Variablen aus dem Datensatz ausgewählt. Dies geschieht entweder über den Variablennamen…\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n… oder über eine Indexmenge.33 Hilfreich: dplyr::pull() selektiert eine Variable und wandelt diese in einen Vektor um.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nVariablen können anhand eines Muster im Namen selektiert werden. Die Selektion von ends_with(\"mm\") bezieht nur Variablen mit Endung mm im Namen ein:\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nMit where() können wir Variablen aufgrund bestimmter Eigenschaften ihrer Ausprägungen selektieren.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n2.2.3 dplyr::filter()\n\nDas Verb filter() filtert den Datensatz in der Zeilendimension. So können Beobachtungen ausgewält werden, deren Merkmalsausprägungen bestimmte Kriterien erfüllen. Hierzu muss filter() ein logischer (logical) Ausdruck übergeben werden. Häufig erfolgt dies über Vergleichsoperatoren.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nOft ist es praktisch, mehrere Kriterien zu kombinieren.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nAnalog: komma-getrennte Kriterien werden intern über den Und-Operator (&) verknüpft.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nÄhnlich wie bei select() verwenden wir häufig nützliche Funktionen, welche die Interpretation des Codes erleichtern. dplyr::between() erlaubt filtern innerhalb eines Intervals.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nMit diesen Verben sind wir bereits in der Lage, den Datensatz gemäß folgender Vorschrift zu bereinigen:\n\nEntfernen der Maßeinheiten aus den Variablennamen\nEntfernen von Pinguinen mit fehlenden Werten (NA)\nEntfernen von Pinguinen mit einem Gewicht oberhalb des 95%-Stichprobenquantils\n\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nDurch die Verkettung mit %&gt;% können wir sämtliche Schritte für die Bereinigung ohne das Abspeichern von Zwischenergebnissen durchführen.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n2.2.4 dplyr::summarise()\n\nDas Verb summarise() fasst Variablen über Beobachtungen hinweg zusammen. Der nachstehende Code-Chunk erzeugt eine Tabelle mit Stichprobenmittelwert und -standardabweichung von flipper_length_mm.4 Um zu vermeiden, dass die Auswertung aufgrund fehlender Werte (NA) in flipper_length_mm scheitert, lassen wir NAs mit na.rm = TRUE bei der Berechnung unberücksichtigt (wir verwenden weiterhin den unbereinigten Datensatz penguins).4 dplyr::summarise() darf nicht mit base::summary() verwechselt werden!\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nVarianten von summarise() können über mehrere Variablen angewendet werden. Wir verwenden across() und where(), um lediglich numerische Variablen mit den in der liste definierten Funktionen zusammenzufassen. Beachte, dass \\(x) mean(x) eine anonyme Funktion definiert.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n2.2.5 dplyr::arrange()\n\nMit arrange() können Datensätze in Abhängigkeit der beobachteten Ausprägungen von Variablen sortiert werden.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nDie Funktion dplyr::desc() kehrt die Reihenfolge zu einer absteigenden Sortierung um.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nKomplexe Sortier-Muster werden durch Übergabe von Variablennamen in der gewünschten Reihenfolge erreicht.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n2.2.6 Operationen mit gruppierten Datensätzen\nFür manche Transformationen ist eine Gruppierung der Daten hilfreich. Wir illustrieren nachfolgend die unterschiedlichen Verhaltensweisen ausgewählter Verben durch Vergleiche von gruppierten und nicht-gruppierten Anwendungen.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nspecies hat drei Ausprägungen. Entsprechend ist penguins_grouped nun in drei Gruppen eingeteilt.\nBei gruppierten Datensätzen fasst summarise() die Variablen pro Guppe zusammen.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nmutate() definiert bzw. transformiert für jede Gruppe separat. Im dies zu veranschaulichen, ziehen wir eine Zufallsstichprobe von 10 Pinguinen aus der Datensatz.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nFür den ungruppierten Datensatz berechnet mutate() das Stichprobenmittel von bill_length_mm über alle zehn Datenpunkte und weißt diesen Wert jeweils in der Variable mean zu.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nBei gruppierten Daten berechnet mutate() das Stichprobenmittel pro Gruppe und weist die Mittelwerte entsprechend zu."
  },
  {
    "objectID": "R_Einfuehrung.html#eine-explorative-analyse-mit-ggplot2",
    "href": "R_Einfuehrung.html#eine-explorative-analyse-mit-ggplot2",
    "title": "\n2  Statistische Programmierung mit R\n",
    "section": "\n2.3 Eine explorative Analyse mit ggplot2\n",
    "text": "2.3 Eine explorative Analyse mit ggplot2\n\nDer bereinigte Datensatz penguins_cleaned eignet sich gut für eine graphische Auswertung mit dem R-Paket ggplot2, welches Bestandteil des tidyverse ist. Nachfolgend untersuchen wir Zusammenhänge zwischen den Körpermaßen der Pinguine.\nWir erstellen zunächst einen einfachen Punkteplot des Gewichts (body_mass) und der Schnabeltiefe (bill_depth).\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nDie Grafik zeigt einen positiven Zusammenhang zwischen dem Gewicht und der Schnabeltiefe. Als nächstes passen wir den Code so an, dass die Datenpunkte entsprechend der Art (species) eingefärbt sind.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nOffenbar gibt es deutliche Unterschiede in der (gemeinsamen) Verteilung von Gewicht und Schnabeltiefe zwischen den verschiedenen Arten.\nUm den Zusammenhang zwischen Gewicht und Schnabeltiefe zu untersuchen, schätzen wir lineare Regressionen \\[body\\_mass = \\beta_0 + \\beta_1 bill\\_depth + u\\] separat für jede der drei Pinguinarten mit der KQ-Methode. Anschließend zeichnen wir die geschätzten Regressionsgeraden ein.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nDie Schätzungen bekräftigen die Vermutung, dass der lineare Zusammenhang zwischen Gewicht und Schnabeltiefe sich nicht zwischen den verschiedenen Pinguinarten unterscheidet: Pinguine der Art Gentoo sind im Mittel schwerer als Pinguine der übrigen Arten, haben jedoch eine geringere Schnabeltiefe.\nDer nachfolgende Code fügt der Grafik eine Regressionsline über alle Arten hinzu. Wir setzen hierbei das Argment inherit_aes = FALSE und legen damit fest, dass die Regression für body_mass und bill_depth ohne Differenzierung per species durchgeführt wird.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nOffenbar ist die vorherige Analyse per Spezies sinnvoller: Die Regression über alle Arten suggeriert einen negativen Zusammenhang zwischen Gewicht und Schnabeltiefe.\nFacetting mit facet_wrap() erlaubt eine Untersuchung des Zusammenhangs je Insel (island), auf der die Messung erfolgt ist.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nWir sehen, dass es hinsichtlich des Zusammenhangs von Gewicht und Schnabeltiefe keine wesentlichen Diskrepanzen zwischen den drei Inseln gibt. Darüber hinaus lässt sich anhand der Facetten leicht erkennen, wie die drei Arten über die Inseln verteilt sind."
  },
  {
    "objectID": "RDD.html#sharp-regression-discontinuity-design",
    "href": "RDD.html#sharp-regression-discontinuity-design",
    "title": "\n4  Regression Discontiniuty Designs\n",
    "section": "\n4.1 Sharp Regression Discontinuity Design",
    "text": "4.1 Sharp Regression Discontinuity Design\nModell und funktionale Form\nDie korrekte Spezifikation der funktionalen Form für ein RDD ist wichtig, um eine verzerrte Schätzung des Effekts zu vermeiden. Die einfachste Form eines SRDD kann anhand der linearen Regression \\[\\begin{align}\nY_i = \\beta_0 + \\beta_1 B_i + \\beta_2 X_i + u_i\\label{eq-simpleSRDD}\n\\end{align}\\] geschätzt werden, wobei \\(B_i\\) eine Dummy-Variable für das Überschreiten des Schwellenwertes \\(c\\) ist, d.h. \\[\\begin{align*}\n  B_i=\\begin{cases}\n    0 & X_i &lt; c\\\\\n    1 & X_i \\geq c.\n  \\end{cases}\n\\end{align*}\\] Damit ist \\(B_i\\) eine deterministische Funktion der Laufvariable \\(X_i\\) und zeigt die Zugehörigkeit zur Behandlungs- oder Treatmentgruppe an. Der Koeffizient \\(\\beta_1\\) misst den Behandlungseffekt.\nDas Modell \\(\\eqref{eq-simpleSRDD}\\) unterstellt, dass \\(X\\) links- und rechtsseitig von \\(c\\) denselben Effekt auf \\(Y\\) hat. Diese Annahme ist restriktiv. Eine Alternative ist ein lineares Interaktionsmodell \\[\\begin{align}\nY_i = \\beta_0 + \\beta_1 B_i + \\beta_2 (X_i - c) + \\beta_3(X_i - c)\\times B_i + u_i.\\label{eq:linearSRDD}\n\\end{align}\\] Das Modell \\(\\eqref{eq:linearSRDD}\\) kann unterschiedliche lineare Effekte von \\(X\\) auf \\(Y\\) unterhalb (\\(\\beta_2\\)) und oberhalb (\\(\\beta_2 + \\beta_3\\)) von \\(c\\) abbilden. Beachte, dass \\((X_i - c)\\) die um den Schwellenwert zentrierte Laufvariable ist, sodass \\(\\beta_1\\) wie in \\(\\eqref{eq-simpleSRDD}\\) den Unterschied des Effekts von \\(X\\) auf \\(Y\\) für Beoabachtungen am Schwellenwert erfasst.\nUm unterschiedliche nicht-lineare Zusammenhänge von \\(X\\) und \\(Y\\) unterhalb und oberhalb von \\(c\\) abzubilden, können (interargierte) Polynom-Terme in \\(X\\) verwendet werden. Häufig wird eine quadratische Regressionsfunktion genutzt, \\[\\begin{align}\n  Y_i =&\\, \\beta_0 + \\beta_1 B_i + \\beta_2 (X_i - c) + \\beta_3 (X_i - c)^2\\\\\n       &+\\, \\beta_4(X_i - c)\\times B_i + \\beta_5(X_i - c)\\times B_i + u_i.\\label{eq:quadSRDD}\n\\end{align}\\] Gelman und Imbens (2019) zeigen, dass Polynome höherer Ordnung zu verzerrten Schätzern und hoher Varianz führen können.2 Die Authoren empfehlen stattdessen die Schätzung mit lokaler Regression.2 Ursachen sind Überanpassung an die Daten sowie instabiles Verhalten der Schätzung nahe des Schwellenwertes.\nNicht-parametrische Schätzung und Bandweite\nAktuelle Studien nutzen nicht-parametrische Schätzer, die den Behandlungseffekt als Differenz der geschätzten Regressionsfunktionen am Schwellenwert \\(c\\) berechnen. Um auch nicht-lineare Regressionsfunktionen abzubilden zu können, wird häufig lokale Regression verwendet. Dieses Verfahren liefert eine “lokale” Schätzung der Regressionsfunktionen am Schwellenwert, bei der nur Beobachtungen nahe \\(X = c\\) für die Schätzung berücksichtigt werden. Hinreichende Nähe wird hierbei durch eine sogenannte Bandweite \\(h\\) festgelegt, wobei \\[\\begin{align}\n  \\lvert(X_i-c)\\rvert\\leq h \\label{eq:bwc}\n\\end{align}\\] das Kriterium für eine Berücksichtigung von Beobachtung \\(i\\) bei der Schätzung ist.\nUnter Verwendung einer Bandweite \\(h\\) wird der Regressionsansatz \\(\\eqref{eq:linearSRDD}\\) als lokale lineare Regression mit Uniform-Kernelfunktion bezeichnet. Der Uniform-Kernel gibt allen Beobachtungen, innerhalb der Bandweite \\(h\\) dasselbe Gewicht. Ist \\(h\\) so groß, dass der gesamte Datensatz in die Schätzung einbezogen wird, entspricht der lokale lineare Regressions-Schätzer mit Uniform-Kernel dem (globalen) KQ-Schätzer in einem linearen Interaktionsmodell anhand aller Beobachtungen. Neben dem Uniform-Kernel ist der Triangular-Kernel eine in der Praxis häufig genutzte lineare Kernelfunktion. Der nachstehende Code plottet die Uniform- (grün) sowie die Triangular-Kernelfunktion (blau), siehe Abbildung 4.2.\n\nCodelibrary(ggplot2)\nlibrary(cowplot)\n\n# Kernelfunktionen zeichnen\nggplot() + \n    geom_function(\n      fun = ~ ifelse(\n        test = abs(.) &lt;= 1,\n        yes =  1/2, \n        no = 0\n      ), \n      col = \"green\", \n      n = 1000\n      ) + \n    geom_function(\n      fun = ~ ifelse(\n        test = abs(.) &lt;= 1, \n        yes = 1 - abs(.), \n        no = 0\n      ), \n      col = \"blue\", \n      n = 100\n      ) + \n    scale_x_continuous(\n      name = \"x\", \n      limits = c(-1.5, 1.5), \n      breaks = c(-1, 0, 1)\n    ) +\n    scale_y_continuous(\n      name = \"K(x)\", \n      breaks = c(0, 1), \n      limits = c(0, 1.25)\n    ) +\n    theme_cowplot()\n\n\n\nAbbildung 4.2: Kernelfunktionen auf [-1, 1]\n\n\n\nIn empirischen Studien wird als Basis-Spezifikation oft eine lokale lineare Regression anhand von \\(\\eqref{eq:linearSRDD}\\) mit einer linearen Kernelfunktionen und geringer bandweite \\(h\\) genutzt. Anschließend wird die Robustheit der Ergebnisse anhand flexiblerer Spezifikationen, die Nicht-Linearitäten in der Regressionsfunktion besser abbilden können, geprüft.\nDie nachstehende Visualisierung zeigt die Schätzung des kausalen Effektes der Behandlung \\(B_i\\) anhand lokaler linearer Regression mit einem Uniform-Kernel für wiefolgt simulierte Daten: \\[\\begin{align*}\n  Y_i =&\\, \\beta_1 X_i + \\beta_2 B + \\beta_3 X_i^2 \\times B_i + u_i,\\\\\n  \\\\\n  u_i \\sim&\\, N(0, 0.5), \\quad X_i \\sim U(0, 10), \\quad B = \\mathbb{I}(X_i \\geq c = 5)\\\\\n  \\beta_1 =&\\, .5, \\quad \\beta_2 = 1.5, \\quad \\beta_3 = -0.15\n\\end{align*}\\]\nDiese Vorschrift ist schnell mit R umgesetzt:\n\nset.seed(1234)\n# Anz.Beobachtungen\nn &lt;- 750\n\n# Parameter definieren\nc &lt;- 5\nbeta_1 &lt;- .5\nbeta_2 &lt;- 1.5\nbeta_3 &lt;- -.15\n\n# Regressionsfunktion definieren\nf &lt;- function(X) {\n  beta_1 * (X - c) + beta_2 * B + beta_3 * B * (X - c)^2\n}\n\n# Daten erzeugen\nX &lt;- runif(n, 0, 11)\nB &lt;- ifelse(X - c &gt;= 0, 1, 0)\nY &lt;- f(X) + rnorm(n, sd = .5)\n\n# Beoabchtungen sammeln\ndat &lt;- data.frame(\n  Y = Y, X = X - c, B = B\n)\n\n\nhtml`\n&lt;style&gt;\n.regression {\n  fill: none;\n  stroke: #000;\n  stroke-width: 1.5px;\n}\n.axis line {\n  stroke: #ddd;\n}\n.axis .baseline line {\n  stroke: #555;\n}\n.axis .domain {\n  display: none;\n} \n&lt;/style&gt;\n&lt;link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css\"&gt;\n`\n\n\n\n\n\n\n\nd3 = require(\"d3-array@3\", \"d3-axis@3\", \"d3-regression@1\", \"d3-scale@4\", \"d3-shape@3\", \"d3-selection@3\", \"d3-format\")\n\nmargin = ({left: 55, right: 8, top: 13, bottom: 24});\nbase = Math.min(width, 500);\ninnerWidth = base - margin.left - margin.right;\ninnerHeight = base-100 - margin.top - margin.bottom;\n\n\nviewof bw_daten_LLRU = Inputs.range([.1, 6], {\n  label: \"Bandweite (h)\",\n  step: .1,\n  value: 1.3\n});\n\nxScaleLLRU = d3.scaleLinear()\n   .domain([-6, 6])\n   .range([0, innerWidth]);\n   \nyScaleLLRU = d3.scaleLinear()\n  .domain([-4, 6])\n  .range([innerHeight, 0]);\n\nlineLLRU = d3.line()\n  .x(d =&gt; xScaleLLRU(d[0]))\n  .y(d =&gt; yScaleLLRU(d[1]));\n  \nxAxisLLRU = d3.axisBottom(xScaleLLRU)\n  .tickSize(innerHeight + 10)\n  .tickValues([-6, -4, -2, 0, 2, 4, 6])\n  .tickFormat(d =&gt; d);\n\nyAxisLLRU = d3.axisLeft(yScaleLLRU)\n  .tickSize(innerWidth + 10)\n  .tickFormat(d =&gt; d);\n\nLLRURegression = d3.regressionLinear()\n  .x(d =&gt; d.X)\n  .y(d =&gt; d.Y);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  const svg = d3.select(DOM.svg(innerWidth + margin.left + margin.right + 20, innerHeight + margin.top + margin.bottom + 20))\n  \n  const g = svg.append(\"g\")\n      .attr(\"transform\", `translate(${margin.left}, ${margin.top})`);\n\n  g.append(\"g\")\n      .attr(\"class\", \"axis\")\n      .call(xAxisLLRU);\n\n  g.append(\"g\")\n    .attr(\"class\", \"axis\")\n    .attr(\"transform\", `translate(${innerWidth})`)\n    .call(yAxisLLRU);\n\n  // Add X axis label:\n  g.append(\"text\")\n    .attr(\"text-anchor\", \"end\")\n    .attr(\"font-size\", 13)\n    .attr(\"x\", innerWidth)\n    .attr(\"y\", innerHeight + margin.top + 25)\n    .text(\"X - c\");\n\n  // Y axis label:\n  g.append(\"text\")\n    .attr(\"text-anchor\", \"end\")\n    .attr(\"transform\", \"rotate(-90)\")\n    .attr(\"font-size\", 13)\n    .attr(\"y\", -margin.left+10)\n    .attr(\"x\", -margin.top+10)\n    .text(\"Y\");\n\n  g.selectAll(\"circle\")\n    .data(transpose(SRDD))\n    .enter().append(\"circle\")\n    .attr(\"r\", 2)\n    .attr(\"cx\", d =&gt; xScaleLLRU(d.X))\n    .attr(\"cy\", d =&gt; yScaleLLRU(d.Y));\n\ng.selectAll(\"circle\")\n .filter( function(d){ return Math.abs(d.X) &lt;= bw_daten_LLRU } )\n .attr(\"fill\", \"orange\")\n .attr(\"stroke\", \"none\");\n \n  &lt;!-- trf --&gt;\nvar  line = d3.line()\n           .x(function(d) { return xScaleLLRU(d.X); }) \n           .y(function(d) { return yScaleLLRU(d.Y_true); }) \n           .curve(d3.curveLinear); \n\n  g.append(\"path\")\n    .datum(transpose(SRDD))\n    .attr(\"d\", line)\n    .attr(\"fill\", \"none\")\n    .attr(\"stroke\", \"red\")\n    .attr(\"stroke-width\", 2);\n\n\nfunction b(d) { return LLRURegression(\n        transpose(SRDD).filter(function(d){ return d.X &lt;= 0 & d.X &gt;= -bw_daten_LLRU })\n    ); }\n\nfunction a(d) { return LLRURegression(\n      transpose(SRDD).filter(function(d){ return d.X &gt; 0 & d.X &lt;= bw_daten_LLRU })\n    ); }\n\n  g.append(\"path\")\n      .attr(\"class\", \"regression\")\n      .datum(b)\n      .attr(\"d\", lineLLRU)\n      .attr(\"stroke-width\", 2)\n      .style(\"stroke\", \"#39FF14\");\n\n  g.append(\"path\")\n      .attr(\"class\", \"regression\")\n      .datum(a)\n      .attr(\"d\", lineLLRU)\n      .attr(\"stroke-width\", 2)\n      .style(\"stroke\", \"#39FF14\");\n  \n  g.append(\"line\")\n  .attr(\"x1\", xScaleLLRU(0))\n  .attr(\"y1\", yScaleLLRU((b().slice(-1))[0][1]))\n  .attr(\"x2\", xScaleLLRU(0))\n  .attr(\"y2\", yScaleLLRU((a().slice(0))[0][1]))\n  .attr(\"stroke\", \"#39FF14\")\n  .attr(\"stroke-width\", 2);\n  \n   g.append(\"text\")\n    .attr(\"x\", d =&gt; xScaleLLRU(-2.75))\n    .attr(\"y\", d =&gt; yScaleLLRU(4.5))\n    .attr(\"dy\", \".35em\")\n    .attr(\"fill\", \"#39FF14\")\n    .text(\n    d3.format(\",.2f\")( (a().slice(0))[0][1] - (b().slice(-1))[0][1] ) \n    );\n  \n  /* dashed line data bw upper */\n  g.append(\"line\")\n  .attr(\"x1\", xScaleLLRU(bw_daten_LLRU))\n  .attr(\"y1\", 0)\n  .attr(\"x2\", xScaleLLRU(bw_daten_LLRU))\n  .attr(\"y2\", innerHeight)\n  .style(\"stroke\", \"blue\")\n  .style(\"stroke-dasharray\", \"4\")\n  .style(\"stroke-width\", \"1\");\n  \n  /* dashed line data bw lower */\n  g.append(\"line\")\n  .attr(\"x1\", xScaleLLRU(-bw_daten_LLRU))\n  .attr(\"y1\", 0)\n  .attr(\"x2\", xScaleLLRU(-bw_daten_LLRU))\n  .attr(\"y2\", innerHeight)\n  .style(\"stroke\", \"blue\")\n  .style(\"stroke-dasharray\", \"4\")\n  .style(\"stroke-width\", \"1\");\n\n  return svg.node();\n}\n\n\n\n\nAbbildung 4.3: Nicht-parametrische Regression auf beiden Seiten des Cut-offs\n\n\nDer interssierende Effekt am Schwellenwert \\(c=5\\) beträgt \\(\\beta_2 = 1.5\\). Beachte, dass aufgrund des Terms \\(\\beta_3 X_i^2 \\times B_i\\) ein quadratischer Zusammenhang von \\(Y\\) und \\(X\\) oberhalb von \\(X_i = c\\) vorliegt. Es können folgende Eigenschaften der Schätzung in Abhängigkeit von der Bandweite \\(h\\) beobachtet werden:\n\nFür die voreingestellte Bandweite \\(h = 1.3\\) liefert die lokale lineare Regression eine gute Approximation des Regressionszusammenhangs auf beiden Seiten des Schwellenwertes und die Schätzung des Behandlungseffekts liegt nahe beim wahren Wert \\(\\beta_2 = 1.5\\).\nFür kleinere Bandweiten verringert sich die Datenbasis der Schätzung. Die Varianz der Schätzung nimmt zu und die Approximation der Regressionsfunktion verschlechtert sich. Wir beobachten eine mit \\(h\\to0\\) zunehmende Verzerrung bei der Schätzung des Behandlungseffekts.\nGrößere Bandweiten \\(h\\) erhöhen die Datenbasis der Schätzung, führen aber zu einer Annäherung der lokalen Schätzung an die globale KQ-Schätzung. Linksseitig des Schwellenwertes erzielen wir damit eine Schätzung mit hoher Güte. Rechsseitig von \\(X_i = c\\) verschlechtert sich die lokale Anpassung am Schwellenwert deutlich, weil die lineare Schätzung den tatsächlichen (nicht-linearen) Zusammenhang nicht adäquat abbilden kann. Die Schätzung des Behandlungseffekts ist hier deutlich verzerrt.\n\nDie Wahl der Bandweite ist also eine wichtige Komponenten der RDD-Schätzung: Kleine Bandweiten erlauben eine Schätzung der Regressionsfunktion nahe des Schwellenwertes mit wenig Verzerrung. Allerdings kann diese Schätzung unpräzise sein, wenn nur wenige Beobachtungen \\(\\eqref{eq:bwc}\\) erfüllen. In der Praxis wird \\(h\\) daher mit einem analytischen Schätzer (vgl. G. Imbens und Kalyanaraman 2012) oder anhand von Cross Validation (bspw. G. W. Imbens und Lemieux 2008) bestimmt. Die später in diesem Kapitel betrachteten R-Pakete halten diese Methoden bereit."
  },
  {
    "objectID": "RDD.html#manipulation-am-schwellenwert",
    "href": "RDD.html#manipulation-am-schwellenwert",
    "title": "\n4  Regression Discontiniuty Designs\n",
    "section": "\n4.2 Manipulation am Schwellenwert",
    "text": "4.2 Manipulation am Schwellenwert\nEine wichtige Annahmen für die Gültigkeit einer RDD-Schätzung ist, dass keine Manipulation der Gruppenzugehörigkeit am Schwellenwert vorliegt. Wenn sich Subjekte nahe des Schwellenwertes \\(c\\) — d.h. in Abhängigkeit der Laufvariable \\(X\\) — systematisch in den Confoundern \\(Z\\) unterscheiden, können wir den Backdoor-Pfad Oberhalb C → Behandlung B → Y nicht isolieren. Wir erhalten dann eine verzerrte Schätzung des Behandlungseffekts.\nIn empirischen Studien mit Individuen kann Selbstselektion auftreten: Menschen mit \\(X&lt;c\\) aber nahe \\(c\\) (hier Kontrollgruppe) könnten aufgrund unbeobachtbarer Eigenschaften \\(Z\\) die Ausprägung ihrer Laufvariable zu \\(X&gt;c\\) (hier Behandlungsgruppe) manipulieren. Wenn \\(Z\\) die Outcome-Variable beeinflusst, bleibt der Backdoor-Pfad Oberhalb C → Behandlung B → Y so bestehen.\nManipulation resultiert in Häufung von Beobachtungen am Schwellenwert. Dei Verteilung der Laufvariable kann auf diese Unregelmäßigkeit hin untersucht werden. McCrary (2008) schlägt hierfür einen Verfahren vor, das die Kontinuität der Dichtefunktion von \\(X\\) am Schwellenwert testet.\nDer Test von McCrary (2008) ist in rdd::DCdensity() implementiert. Wir zeigen die Anwendung des Tests anhand der oben simulierten Daten. Beachte, dass \\(X_i\\sim U(0, 10)\\), d.h. die Laufvariable ist bei \\(X_i = c\\) kontinuierlich verteilt. Die Nullhypothese (keine Manipulation) gilt für die simulierten Daten\n\n# McCrary-Test durchführen\np_mccrary &lt;- rdd::DCdensity(\n  runvar = X, \n  cutpoint = c, \n  plot = F\n)\n\n# p-Wert\np_mccrary\n\n[1] 0.5013939\n\n\nDer p-Wert 0.5 ist größer als jedes übliche Signifikanzniveau. Damit liegt starke Evidenz für die Nullhypothese (keine Diskontinuität) und gegen Manipulation am Schwellenwert vor.\nCattaneo, Jansson, und Ma (2020) (CMJ) schlagen eine Weiterentwicklung des McCrary-Tests vor, die höhere statistische Macht gegenüber Diskontinuitäten hat am Schwellenwert hat. Der CJM-Test ist im Paket rddensity implementiert.\n\nlibrary(rddensity)\n\n# CJM Schätzer berechnen\nCJM &lt;- rddensity(X, c = 5)\n\nMit der Funktion rddensity::rdplotdensity() erzeugen wir Abbildung 4.4.\n\n# Plot für Dichtefunktion erstellen\nplot &lt;- rdplotdensity(\n  rdd = CJM, \n  X = X, \n  # für Punkte- und Linienplots:\n  type = \"both\" \n)\n\n\n\nAbbildung 4.4: CJM-Test – geschätzte Dichtefunktionen der Laufvariable auf beiden Seiten des Schwellenwerts c = 5\n\n\n\nAbbildung 4.4 zeigt die geschätzten Dichtefunktionen. Erwartungsgemäß finden wir eine große Überlappung der zugehörigen Konfidenzbänder (schattierte Flächen) am Schwellenwert \\(c=5\\).\nMit summary() erhalten wir eine detaillierte Zusammenfassung des Tests.\n\n# Statistische Zusammenfassung des CJM-Tests\nsummary(CJM)\n\n\nManipulation testing using local polynomial density estimation.\n\nNumber of obs =       750\nModel =               unrestricted\nKernel =              triangular\nBW method =           estimated\nVCE method =          jackknife\n\nc = 5                 Left of c           Right of c          \nNumber of obs         329                 421                 \nEff. Number of obs    133                 154                 \nOrder est. (p)        2                   2                   \nOrder bias (q)        3                   3                   \nBW est. (h)           1.918               2.124               \n\nMethod                T                   P &gt; |T|             \nRobust                -0.3338             0.7385              \n\n\nP-values of binomial tests (H0: p=0.5).\n\nWindow Length              &lt;c     &gt;=c    P&gt;|T|\n0.346     + 0.346          20      21    1.0000\n0.521     + 0.544          34      37    0.8126\n0.696     + 0.742          44      57    0.2323\n0.870     + 0.939          54      64    0.4075\n1.045     + 1.137          62      77    0.2349\n1.220     + 1.334          73      98    0.0661\n1.394     + 1.532          86     106    0.1701\n1.569     + 1.729          96     124    0.0685\n1.743     + 1.927         119     140    0.2139\n1.918     + 2.124         133     154    0.2377\n\n\nGemäß des p-Werts (P &gt; |T|) von 0.74 spricht der CJM-Test noch deutlicher gegen eine Diskontinuität als der McCrary-Test.\n\n4.2.1 Case Study: Amtsinhaber-Vorteil (Lee 2008)\n\nLee (2008) untersucht den Einfluss des Amtsinhaber-Vorteils auf die Wahl von Mitgliedern des US-Repräsentantenhaus. In den meisten Wahlkreisen entfallen große Anteile der Stimmen (oder gar ausschließlich) auf demokratische und republikanische Kanditat*innen, sodass sich die Studie auf diese Parteien beschränkt. Entfällt die Mehrheit der Stimmen auf eine*n Kandiat*in, gewinnt diese*r den Sitz für den Wahlkreis. Durch die Analyse der 6558 Wahlen im Zeitraum 1946-1998 mit einem SRDD kommt die Studie zu dem Ergebnis, dass Amtsinhabende im Durchschnitt einen Vorteil von etwa 8% bis 10% bei der Wahl haben. Dieses Ergebnis kann verschiedene Ursachen haben, bspw. dass die amtierende Partei höhere finanzielle Ressourcen besitzt und von einer besseren Organisation und durch Instrumenalisierung staatlicher Strukturen für die eigenen Zwecke profitiert.\nAnhand der Datensätze house und house_binned illustrieren wir nachfolgend die Schätzung von SRDD-Modellen für den Wahlerfolg der demokratischen Partei, wenn diese Amtsinhaber ist. Wir lesen hierfür zunächst die Datensätze house und house_binned ein und verschaffen uns einen Überblick.\n\nlibrary(tidyverse)\nlibrary(modelsummary)\n\n# Daten einlesen\nhouse &lt;- read_csv(\"datasets/house.csv\")\n# Gruppierter Datensatz\nhouse_binned &lt;- read_csv(\"datasets/house_binned.csv\")\n\n# Überblick verschaffen\nglimpse(house)\n\nRows: 6,558\nColumns: 2\n$ StimmenTm1 &lt;dbl&gt; 0.1049, 0.1393, -0.0736, 0.0868, 0.3994, 0.1681, 0.2516, 0.…\n$ StimmenT   &lt;dbl&gt; 0.5810, 0.4611, 0.5434, 0.5846, 0.5803, 0.6244, 0.4873, 0.5…\n\nglimpse(house_binned)\n\nRows: 100\nColumns: 2\n$ StimmenT   &lt;dbl&gt; 0.5995600, 0.5657000, 0.4272554, 0.5637456, 0.6868627, 0.60…\n$ StimmenTm1 &lt;dbl&gt; 0.104764444, 0.135005263, -0.075690769, 0.084570886, 0.3951…\n\n\nDer Datensatz house enthält die Stimmenanteile demokratischer Kandidat*innen bei der Wahl zum Zeitpunkt \\(T\\) (\\(StimmenT\\)) sowie die Differenz zwischen demokratischen und republikanischen Stimmenanteilen bei der vorherigen Wahl, d.h. zum Zeitpunkt \\(T-1\\) (\\(StimmenTm1\\)). Der Schwellenwert für einen Wahlsieg liegt bei Stimmengleichheit, d.h. \\(StimmenTm1 = 0\\).\nhouse_binned ist eine aggregierte Version von house mit Mittelwerten von jeweils 50 gleichgroßen Intervallen oberhalb und unterhalb der Schwelle von \\(StimmenTm1 = 0\\). Dieser Datensatz eignet sich, um einen ersten Eindruck des funktionalen Zusammenhangs auf beiden Seiten zu erhalten. Wir stellen zunächst diese klassierten Daten mit ggplot2 graphisch dar.\n\n# Klassierte Daten plotten\nhouse_binned %&gt;%\n  ggplot(\n    aes(x = StimmenTm1, y = StimmenT)\n    ) +\n  geom_point() +\n  geom_vline(xintercept = 0, lty = 2)\n\n\n\nAbbildung 4.5: Klassierte Daten aus Lee (2008)\n\n\n\nDie Grafik zeigt eindeutig einen Sprung von \\(StimmenT\\) bei \\(StimmenTm1 = 0\\). Weiterhin erkennen wir, dass der Zusammenhang nahe \\(0\\) vermutlich jeweils gut durch eine lineare Funktion approximiert werden kann. Eine Modell-Spezifikation mit gleicher Steigung auf beiden Seiten des Schwellenwertes scheint hingegen weniger gut geeignet. Wir vergleichen diese Spezifikationen nachfolgend.\nZunächst fügen wir dem Datensatz eine Dummyvariable B hinzu. Diese dient als Indikator für den Wahlgewinn in der letzten Wahl und zeigt die Amtsinhaberschaft (Behandlung) an.\n\n# Behandlungsindikator B hinzufügen\nhouse &lt;- house %&gt;% \n  mutate(B = StimmenTm1 &gt; 0)\n\nglimpse(house)\n\nRows: 6,558\nColumns: 3\n$ StimmenTm1 &lt;dbl&gt; 0.1049, 0.1393, -0.0736, 0.0868, 0.3994, 0.1681, 0.2516, 0.…\n$ StimmenT   &lt;dbl&gt; 0.5810, 0.4611, 0.5434, 0.5846, 0.5803, 0.6244, 0.4873, 0.5…\n$ B          &lt;lgl&gt; TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n\n\nWir überprüfen die Laufvariable mit dem CJM-Test auf Manipulation am Schwellenwert \\(c=0\\).\n\n# CJM-Test durchführen\nCJM_Lee &lt;- rddensity(X = house$StimmenTm1)\n\n# Zusammenfassung anzeigen\nsummary(CJM_Lee)\n\n\nManipulation testing using local polynomial density estimation.\n\nNumber of obs =       6558\nModel =               unrestricted\nKernel =              triangular\nBW method =           estimated\nVCE method =          jackknife\n\nc = 0                 Left of c           Right of c          \nNumber of obs         2740                3818                \nEff. Number of obs    1297                1360                \nOrder est. (p)        2                   2                   \nOrder bias (q)        3                   3                   \nBW est. (h)           0.236               0.243               \n\nMethod                T                   P &gt; |T|             \nRobust                1.4346              0.1514              \n\n\nP-values of binomial tests (H0: p=0.5).\n\nWindow Length / 2          &lt;c     &gt;=c    P&gt;|T|\n0.004                      21      24    0.7660\n0.007                      38      46    0.4452\n0.011                      50      60    0.3909\n0.014                      73      77    0.8066\n0.018                      91     104    0.3902\n0.022                     124     132    0.6618\n0.025                     149     149    1.0000\n0.029                     163     174    0.5860\n0.032                     176     202    0.1984\n0.036                     197     223    0.2225\n\n\n\n# CJM-Plot\nplot &lt;- rdplotdensity(\n  rdd = CJM_Lee,\n  X = house$StimmenTm1, \n  type = \"both\", \n)\n\n\n\nAbbildung 4.6: CJM-Test – geschätzte Dichtefunktionen der Laufvariable\n\n\n\nAbbildung 4.6 und der p-Wert von \\(0.15\\) sind Evidenz gegen eine Manipulation am Schwellenwert.\nUm den Behandlungseffekt anhand eines SRDDs zu ermitteln, schätzen wir das Interaktionsmodell \\[\\begin{align*}\n  \\text{StimmenT}_i =&\\, \\beta_0 + \\beta_1 B_i + \\beta_2 (\\text{StimmenTm1}_i - 50)\\\\\n  +&\\, \\beta_3(\\text{StimmenTm1}_i - 50)\\times B_i + u_i\n\\end{align*}\\] zunächst für eine Bandweite von \\(h = 0.5\\). Aufgrund der Skalierung der Daten (Wahlergebnisse in %) bedeutet dies die Verwendung des gesamten Datensatzes für die Schätzung.\n\n# Interaktionsmodell schätzen\nhouse_llr1 &lt;- lm(\n  formula = StimmenT ~ B * StimmenTm1, \n  data = house\n)\n\n# Zusammenfassung anzeigen  \nmodelsummary(\n  models = house_llr1, \n  vcov = \"HC1\", # robuste Standardfehler\n  stars = T, \n  gof_map = \"nobs\", \n  output = \"gt\"\n) %&gt;% \n  tabopts\n\n\n\n\n\n\n \n      (1)\n    \n\n\n(Intercept)\n0.433***\n\n\n\n(0.004)\n\n\nBTRUE\n0.118***\n\n\n\n(0.006)\n\n\nStimmenTm1\n0.297***\n\n\n\n(0.016)\n\n\nBTRUE × StimmenTm1\n0.046*\n\n\n\n(0.018)\n\n\nNum.Obs.\n6558\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n    \n\n\n\n\nDer geschätzte Koeffizient von \\(B\\) (BTRUE) beträgt etwa \\(0.12\\) und ist hochsignifikant. Übereinstimmend mit Abbildung 4.5 erhalten wir also eine positive Schätzung des Behandlungseffekts. Die Interpretation ist, dass die amtierenden Demokraten bei der Wahl von einem Amtsinhabervorteil profitieren. Dieser Effekt schlägt sich als Stimmenbonus von geschätzten 12% nieder. Diese Schätzung des Behandlungseffekts könnte jedoch verzerrt sein:\n\nDie (implizite) Wahl von \\(h=0.5\\) in unserer Schätzung macht die Isolation des relevanten Frontdoor-Paths (\\(c=0\\) → Treatment → StimmenT) wenig plausibel. \\(h\\) sollte mit einer datengetriebenen Methode gewählt werden.\nWeiterhin könnte die lineare funktionale Form der Regression inadäquat sein: Die lineare Approximation der wahren Regressionsfunktion nahe des Schwellenwerts \\(0\\) könnte unzureichend sein und in einer verzerrten Schätzung des Effekts resultieren. Zur Überprüfung der Robustheit der Ergebnisse sollte mit Schätzungen anhand nicht-linearer Spezifikationen verglichen werden.\n\nUm diesen Gefahren für die Validität der Studie zu begegnen, schätzen wir nun weitere Spezifikationen. Im Folgenden verwenden wir eine Bandweitenschätzung gemäß G. Imbens und Kalyanaraman (2012).\n\n# Bandweite mit Schätzer von IK (2012) berechnen\n(\nIK_BW &lt;- \n  rdd::IKbandwidth(\n    X = house$StimmenTm1, \n    Y = house$StimmenT\n  )\n)\n\n[1] 0.2685123\n\n\nWir schätzen zunächst erneut das lineare Interaktionsmodell, diesmal jedoch mit der Bandweite IK_BW.\n\n# Lineares Interaktionsmodelle mit IK-Bandweite\nhouse_llin_IK &lt;- lm(\n  formula = StimmenT ~ B * StimmenTm1, \n  data = house %&gt;% \n    filter(\n      abs(StimmenTm1) &lt;= IK_BW\n    )\n)\n\nFür den Vergleich mit einer nicht-linearen Spezifikation schätzen wir auch ein quadratisches Interaktionsmodell.\n\n# Quadratisches Interaktionsmodell mit IK-Bandweite\nhouse_poly_IK &lt;- update(\n  object = house_llin_IK,\n  formula = StimmenT ~ B * poly(StimmenTm1, degree = 2, raw = T)\n)\n\nFür eine Gegenüberstellung der Ergebnisse verwenden wir modelsummary().\n\n# Tabellarischer Modellvergleich\nmodelsummary(\n  models = list(\n    \"Linear int.\" = house_llin_IK, \n    \"Quadratisch int.\" = house_poly_IK\n  ),  \n  vcov = \"HC1\", \n  stars = T,\n  gof_map = \"nobs\", \n  output = \"gt\"\n) %&gt;% \n  tabopts\n\n\n\n\n\n\n\n  \n \n      Linear int.\n      Quadratisch int.\n    \n\n\n(Intercept)\n0.450***\n0.460***\n\n\n\n(0.005)\n(0.008)\n\n\nBTRUE\n0.085***\n0.068***\n\n\n\n(0.008)\n(0.012)\n\n\nStimmenTm1\n0.360***\n\n\n\n\n(0.036)\n\n\n\nBTRUE × StimmenTm1\n0.055\n\n\n\n\n(0.059)\n\n\n\npoly(StimmenTm1, degree = 2, raw = T)1\n\n0.573***\n\n\n\n\n(0.138)\n\n\npoly(StimmenTm1, degree = 2, raw = T)2\n\n0.798\n\n\n\n\n(0.493)\n\n\nBTRUE × poly(StimmenTm1, degree = 2, raw = T)1\n\n0.036\n\n\n\n\n(0.219)\n\n\nBTRUE × poly(StimmenTm1, degree = 2, raw = T)2\n\n-1.529+\n\n\n\n\n(0.834)\n\n\nNum.Obs.\n2956\n2956\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n    \n\nTabelle 4.1:  Vergleich von SRDD-Interaktionsmodellen für Lee (2008) \n\n\n\nDie Spalte (1) in Tabelle 4.1 zeigt die lokale Schätzung mit einem linearen Interaktionsmodell. Wir erhalten damit einen Behandlungseffekt von etwa \\(8.5\\%\\). Der Schätzwert fällt also etwas geringer aus als für die globale KQ-Schätzung des linearen Interaktionsmodells. Für das Modell (2) mit quadratischer Spezifikation liegt der Schätzwert mit \\(6.8\\%\\) in der selben Größenordnung. Beide Schätzungen ergeben einen signifikant von \\(0\\) verschieden Effekt. Weiterhin fällt auf, dass in beiden Modellen keine Evidenz für unterschiedliche Formen der Regressionsfunktionen auf beiden Seiten des Schwellenwerts vorliegen: sämtliche Koeffizientenschätzwerte der Interaktionsterme haben hohe Standardfehler und sind nicht signifikant. Im quadratischen Modell hat auch der Term \\(StimmenTm1^2\\) keinen signifikanten Effekt. Diese Ergebnisse deuten darauf hin, dass eine lineare Spezifikation ausreichend ist.\nSRDD-Schätzung mit LOESS\nWir illustrieren nachfolgend die Schätzung des Behandlungseffekts mit einer flexiblen und in der Praxis häufig verwendeten Methode für lokale Regression. Die nachfolgende interaktive Grafik zeigt die klassierten Daten aus Lee (2008) auf dem Intervall \\([-0.5,0.5]\\) gemeinsam mit einer nicht-parametrischen Schätzung des Zusammenhangs von StimmenT und StimmenTm1 mittels LOESS.3 Diese Implementierung von lokaler Regression nutzt einen tricube kernel. Über den Input kann eine Bandweite \\(l\\in(0,1]\\) für den LOESS-Schätzer auf beiden Seiten des Schwellenwerts \\(0\\) gewählt werden. Die Bandweite ist hier der Anteil der Beobachtungen an der gesamten Anzahl an Beobachtungen, die in die Schätzung einbezogen werden sollen.3 LOESS ist eine Variante von lokaler Polynom-Regression.\nFür die Schätzung am Schwellenwert berücksichtigte Daten sind in orange kenntlich gemacht. Die rote linie zeigt die geschätzte Regressionsfunktion über gleichmäßig verteilte Werte von StimmenTm1 auf \\([-0.5,0.5]\\). Die Grafik verdeutlicht, dass die LOESS-Methode flexibel genug ist, um lineare und nicht-lineare Zusammenhänge abbilden zu können. Wie zuvor ist eine adäquate Wahl der Bandweite wichtig:\n\nDer mit LOESS geschätzte Zusammenhang auf beiden Seiten des Schwellenwerts ist etwa linear für den voreingestellten Parameter (\\(l = 0.28\\)).\nFür größere Werte von \\(l\\) nähert sich die Schätzung weiter einem linearen Verlauf an. Die Schätzung des Effekts bleibt vergleichbar mit den Ergebnissen des linearen Interaktionsmodell (s. oben).\nFür kleinere \\(l\\) erhalten wir eine stärkere Anpassung der Schätzung an die Daten. Zu kleine Werte führen zu einer Überanpassung (overfitting). Insbesondere tendiert die geschätzte Funktion zu extremer Steigung nahe des Schwellenwerts → stark verzerrte Schätzung des Effekts!\n\n\nhtml`\n&lt;style&gt;\n.regression {\n  fill: none;\n  stroke: #000;\n  stroke-width: 1.5px;\n}\n.axis line {\n  stroke: #ddd;\n}\n.axis .baseline line {\n  stroke: #555;\n}\n.axis .domain {\n  display: none;\n} \n&lt;/style&gt;\n&lt;link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css\"&gt;\n`\n\n\n\n\n\n\n\nviewof bandwidth = Inputs.range([.01, 1], {\n  label: \"Bandweite LOESS (l)\",\n  step: .01,\n  value: .28\n});\n\nxScaleLoess = d3.scaleLinear()\n   .domain([-.55, .55])\n   .range([0, innerWidth]);\n   \nyScaleLoess = d3.scaleLinear()\n  .domain([.2, .8])\n  .range([innerHeight, 0]);\n\nlineLoess = d3.line()\n  .x(d =&gt; xScaleLoess(d[0]))\n  .y(d =&gt; yScaleLoess(d[1]));\n  \nxAxisLoess = d3.axisBottom(xScaleLoess)\n  .tickSize(innerHeight + 10)\n  .tickValues([-.5, -.25, 0, .25, .5])\n  .tickFormat(d =&gt; d);\n\nyAxisLoess = d3.axisLeft(yScaleLoess)\n  .tickSize(innerWidth + 10)\n  .tickValues([.2, .35, .5, .65, .8])\n  .tickFormat(d =&gt; d);\n\nloessRegression = d3.regressionLoess()\n  .x(d =&gt; d.StimmenTm1)\n  .y(d =&gt; d.StimmenT)\n  .bandwidth(bandwidth);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  const svg = d3.select(DOM.svg(innerWidth + margin.left + margin.right + 20, innerHeight + margin.top + margin.bottom + 20))\n  \n  const g = svg.append(\"g\")\n      .attr(\"transform\", `translate(${margin.left}, ${margin.top})`);\n\n  g.append(\"g\")\n      .attr(\"class\", \"axis\")\n      .call(xAxisLoess);\n\n  g.append(\"g\")\n    .attr(\"class\", \"axis\")\n    .attr(\"transform\", `translate(${innerWidth})`)\n    .call(yAxisLoess);\n\n  // Add X axis label:\n  g.append(\"text\")\n    .attr(\"text-anchor\", \"end\")\n    .attr(\"font-size\", 13)\n    .attr(\"x\", innerWidth)\n    .attr(\"y\", innerHeight + margin.top + 25)\n    .text(\"Differenz Stimmenanteil Demokraten letzte Wahl zur 50%-Schwelle\");\n\n  // Y axis label:\n  g.append(\"text\")\n    .attr(\"text-anchor\", \"end\")\n    .attr(\"transform\", \"rotate(-90)\")\n    .attr(\"font-size\", 13)\n    .attr(\"y\", -margin.left+10)\n    .attr(\"x\", -margin.top+10)\n    .text(\"Stimmenanteil Demokraten\");\n\n  g.selectAll(\"circle\")\n    .data(transpose(house_binned))\n    .enter().append(\"circle\")\n    .attr(\"r\", 2)\n    .attr(\"cx\", d =&gt; xScaleLoess(d.StimmenTm1))\n    .attr(\"cy\", d =&gt; yScaleLoess(d.StimmenT));\n\n  g.selectAll(\"circle\")\n   .filter( function(d){ return Math.abs(d.StimmenTm1) &lt;= bandwidth/2 } )\n   .attr(\"fill\", \"orange\")\n   .attr(\"stroke\", \"none\");\n\nfunction b(d) { return loessRegression(\n        transpose(house).filter(function(d){ return d.StimmenTm1 &lt;= 0 & d.StimmenTm1 &gt;= -.5 })\n    ); }\n\nfunction a(d) { return loessRegression(\n      transpose(house).filter(function(d){ return d.StimmenTm1 &gt; 0 & d.StimmenTm1 &lt;= .5  })\n    ); }\n\n  g.append(\"path\")\n      .attr(\"class\", \"regression\")\n      .datum(b)\n      .attr(\"d\", lineLoess)\n      .style(\"stroke\", \"red\");\n\n  g.append(\"path\")\n      .attr(\"class\", \"regression\")\n      .datum(a)\n      .attr(\"d\", lineLoess)\n      .style(\"stroke\", \"red\");\n  \n  g.append(\"text\")\n    .attr(\"x\", d =&gt; xScaleLoess(-.24))\n    .attr(\"y\", d =&gt; yScaleLoess(.55))\n    .attr(\"dy\", \".35em\")\n    .attr(\"fill\", \"#39FF14\")\n    .text(d3.format(\",.2f\")((a().slice(0))[0][1] - (b().slice(-1))[0][1]));\n  \n  g.append(\"line\")\n  .attr(\"x1\", xScaleLoess(0))\n  .attr(\"y1\", yScaleLoess((b().slice(-1))[0][1]))\n  .attr(\"x2\", xScaleLoess(0))\n  .attr(\"y2\", yScaleLoess((a().slice(0))[0][1]))\n  .attr(\"stroke\", \"#39FF14\")\n  .attr(\"stroke-width\", 2);\n  \n  /* dashed line at cutoff */\n  g.append(\"line\")\n  .attr(\"x1\", xScaleLoess(0))\n  .attr(\"y1\", 0)\n  .attr(\"x2\", xScaleLoess(0))\n  .attr(\"y2\", innerHeight)\n  .style(\"stroke\", \"black\")\n  .style(\"stroke-dasharray\", \"1\")\n  .style(\"stroke-width\", \"1\");\n  \n  /* dashed line data bw upper */\n  g.append(\"line\")\n  .attr(\"x1\", xScaleLoess(bandwidth/2))\n  .attr(\"y1\", 0)\n  .attr(\"x2\", xScaleLoess(bandwidth/2))\n  .attr(\"y2\", innerHeight)\n  .style(\"stroke\", \"blue\")\n  .style(\"stroke-dasharray\", \"4\")\n  .style(\"stroke-width\", \"1\");\n  \n  /* dashed line data bw lower */\n  g.append(\"line\")\n  .attr(\"x1\", xScaleLoess(-bandwidth/2))\n  .attr(\"y1\", 0)\n  .attr(\"x2\", xScaleLoess(-bandwidth/2))\n  .attr(\"y2\", innerHeight)\n  .style(\"stroke\", \"blue\")\n  .style(\"stroke-dasharray\", \"4\")\n  .style(\"stroke-width\", \"1\");\n\n  return svg.node();\n}\n\n\n\n\nAbbildung 4.7: Nicht-parametrische Regression auf beiden Seiten des Cut-offs."
  },
  {
    "objectID": "RDD.html#fuzzy-regression-discontinuity-design",
    "href": "RDD.html#fuzzy-regression-discontinuity-design",
    "title": "\n4  Regression Discontiniuty Designs\n",
    "section": "\n4.3 Fuzzy Regression Discontinuity Design",
    "text": "4.3 Fuzzy Regression Discontinuity Design\n\n\n\n\n\nX\n\nX\nY\n\nY\nX-&gt;Y\n\n\noberhalb c\n\noberhalb c\nX-&gt;oberhalb c\n\n\nBehandlung\n\nBehandlung\nX-&gt;Behandlung\n\n\noberhalb c-&gt;Behandlung\n\n\nZ\n\nZ\nZ-&gt;X\n\n\nZ-&gt;Y\n\n\nZ-&gt;Behandlung\n\n\nBehandlung-&gt;Y\n\n\n\nAbbildung 4.8: Kausales Diagram für FRDD\n\n\n\nEin FRDD liegt vor, wenn die Zuweisung der Behandlung \\(B\\) durch die Laufvariable \\(X\\) (und möglicherweise weitere Variablen \\(Z\\)) beeinflusst wird. Im Vergleich zum SRDD ist die Behandlung dann also nicht ausschließlich durch Überschreiten des Schwellenwerts \\(X = c\\) bestimmt.\nAbbildung 4.8 zeigt den grundsätzlichen Zusammenhang. Hier genügt es weiterhin für \\(X\\) (und ggf. \\(Z\\)) zu kontrollieren, um den Pfad oberhalb \\(C\\) → Behandlung \\(B\\) → \\(Y\\) zu isolieren. Der so für Behandlung \\(B\\) ermittelte Effekt auf \\(Y\\) entspricht jedoch nicht dem “vollständigen” Behandlungseffekt, da bei \\(c\\) die Zuweisung der Behandlung nicht von \\(0\\) auf \\(100\\%\\) springt. Die Schätzung des FRDD berücksichtigt dies und skaliert den geschätzten Effekt entsprechend.\nWir betrachten zunächst den Zusammenhang \\[\\begin{align}\n  Y_i = \\beta_0 + \\beta_1 B_i + \\beta_2 (X_i - c) + u_i.\\label{eq-simpleFRDD}\n\\end{align}\\] In einem FRDD springt die Behandlungswahrscheinlichkeit am Schwellenwert \\(c\\) um \\(\\Delta p&lt;1\\). Wir können \\(B\\) also nicht als deterministische Funktion von \\(X\\), welche die Zuweisung zu Behandlungs- bzw. Kontrollgruppe am Schwellenwert \\(c\\) anzeigt (wie im SRDD), definieren. Stattdessen betrachten wir \\[\\begin{align}\n  P(B_i=1\\vert X_i) =\n  \\begin{cases}\n    g_{X_i&lt;c}(X_i), & X_i &lt; c \\\\\n    g_{X_i\\geq c}(X_i) & X_i \\geq c\n  \\end{cases}\\,. \\label{eq-BFRDD}\n\\end{align}\\] Die Funktionen \\(g_{X_i&lt;c}\\) und \\(g_{X_i\\geq c}\\) können verschieden sein. Es muss jedoch \\[g_{X_i&lt;c}(X_i = c) \\neq g_{X_i\\geq c}(X_i = c)\\] gelten. Die Behandlungsvariable \\(B_i\\) ist im FRDD also eine (binäre) Zufallsvariable, deren bedingte Wahrscheinlichkeitsfunktion \\(P(B_i=1\\vert X_i)\\) am Schwellenwert \\(c\\) eine Diskontinuität aufweist. Abbildung 4.9 zeigt heispielhafte Verläufe nicht-linearer bedingter Wahrscheinlichkeitsfunktion für die Behandlung mit einer Diskontinuität bei \\(X_i = c\\).\n\nCodelibrary(ggplot2)\nlibrary(cowplot)\n\n# Bedingte Behandlungswahrscheinlichkeit im FRDD illustrieren\nggplot() + \n  geom_function(\n    fun = ~ ifelse(\n      . &lt; 0, \n      -.1 * .^2 + .25, \n      -.1 * (.-1.5)^2 + 1\n    ), \n    n = 1000\n  ) + \n    geom_function(\n    fun = ~ ifelse(\n      . &lt; 0, \n     .35, \n     .65\n    ),\n    n = 1000, \n    lty = 2, \n    col = \"red\"\n  ) + \n  scale_x_continuous(\n    name = \"Laufvariable X\", \n    limits = c(-1.5, 1.5),\n    labels = NULL,\n    breaks = NULL\n  ) +\n  scale_y_continuous(\n    name = \"P(D=1|X)\", \n    breaks = c(0, 1), \n    limits = c(0, 1)\n  ) +\n  theme_cowplot()\n\n\n\nAbbildung 4.9: Bedingte Behandlungswahrscheinlichkeiten im FRDD\n\n\n\nDefinition \\(\\eqref{eq-BFRDD}\\) bedeutet, dass eine KQ-Schätzung von \\(\\beta_1\\) anhand \\(\\eqref{eq-simpleFRDD}\\) eine verzerrte Schätzung des Behandlungseffekts ist: Der in \\(\\widehat{\\beta}_1\\) erfasste Effekt auf \\(Y\\) ist auf einen Sprung der Behandlungswahrscheinlichkeit bei \\(X_i = c\\) um weniger als \\(100\\%\\) zurückzuführen. Der wahre Behandlungseffekt wird also unterschätzt. Daher muss \\(\\widehat{\\beta}_1\\) skaliert werden, sodass die Schätzung als Effekt einer Änderung der Behandlungswahrscheinlichkei um \\(100\\%\\) interpretiert werden kann — der erwartete Effekt, wenn ausschließlich Subjekte mit \\(X_i\\geq c\\) behandelt würden. Diese skalierte Schätzung erhalten wir mit IV-Regression (vgl. Kapitel XYZ). Hierfür nutzen wir für \\(B_i\\) die Instrumentvariable \\[\\begin{align*}\n  D_i = \\begin{cases}\n    0, & X_i &lt; c \\\\\n    1, & X_i \\geq c.\n  \\end{cases}\n\\end{align*}\\]\nAngenommen \\(g_{X_i\\geq c}(X_i) = \\alpha_0\\) und \\(g_{X_i&lt;c}(X_i) = \\alpha_0 + \\alpha_1\\) mit \\(\\alpha_0 + \\alpha_1 &lt; 1\\) (vgl. rote Funktion in Abbildung 4.8). Der FRDD-Schätzer des Behandlungseffekts ist dann \\(\\widehat{\\gamma}_\\textup{FRDD}\\) im 2SLS-Verfahren mit den Regressionen \\[\\begin{align}\n  \\begin{split}\n  (\\mathrm{I})\\qquad B_i =&\\, \\alpha_0 + \\alpha_1 D_i + \\alpha_2 (X_i - c) + e_i,\\\\\n  (\\mathrm{II})\\qquad Y_i =&\\, \\gamma_0 + \\gamma_1 \\widehat{B}_i + \\gamma_2 (X_i - c) + \\epsilon_i,\n  \\end{split}\\label{eq:FRDD_simpleIV}\n\\end{align}\\] wobei \\(\\widehat{B}_i\\) die angepassten Werte aus Stufe \\((\\mathrm I)\\) und \\(e_i\\) sowie \\(\\epsilon_i\\) Fehlterterme sind.\nAnalog zum SRDD müssen in empirischen Anwendungen geeignete Spezifikationen für die Regressionsfunktionen \\(\\eqref{eq-simpleFRDD}\\) und \\(\\eqref{eq-BFRDD}\\) gewählt und der 2SLS-Schätzer \\(\\eqref{eq:FRDD_simpleIV}\\) entsprechend angepasst werden. Ein einfaches Interaktionsmodell wäre \\[\\begin{align}\n  \\begin{split}\n  (\\mathrm{I})\\qquad B_i =&\\, \\alpha_0 + \\alpha_1 D_i + \\alpha_2 (X_i - c)\\\\\n  +&\\, \\alpha_3 (X_i - c) \\times D_i + e_i,\\\\\n  \\\\\n  (\\mathrm{II})\\qquad Y_i =&\\, \\gamma_0 + \\gamma_1 \\widehat{B}_i\\\\\n  +&\\, \\gamma_2 (X_i - c) + \\gamma_3 (X_i-c)\\times\\widehat{B}_i, \\epsilon_i\n  \\end{split},\\label{eq:FRDD_lintIV}\n\\end{align}\\] d.h. wir instrumentieren \\(B_i\\) mit \\(D_i\\) und dem Interaktionsterm \\((X_i-c)\\times D_i\\).\nWie im SRDD werden die IV-Ansätze für das FRDD \\(\\eqref{eq:FRDD_simpleIV}\\) und \\(\\eqref{eq:FRDD_lintIV}\\) in empirischen Studien unter Berücksichtigung einer Bandweite (i.d.R. dieselbe Bandweite für beide Stufen) angewendet."
  },
  {
    "objectID": "RDD.html#case-study-protestantische-arbeitsethik",
    "href": "RDD.html#case-study-protestantische-arbeitsethik",
    "title": "\n4  Regression Discontiniuty Designs\n",
    "section": "\n4.4 Case Study: Protestantische Arbeitsethik",
    "text": "4.4 Case Study: Protestantische Arbeitsethik\n\n\n\nDie Studie Beyond Work Ethic: Religion, Individual, and Political Preferences (Basten und Betz 2013) untersucht den Zusammenhang zwischen Religion, individuellen Merkmalen und politischen Präferenzen. Das Hauptaugenmerk ist die Rolle von Religiosität als Einflussfaktor auf politische Einstellungen. Die Hypothese der Autoren ist, dass Religiosität eines Individuums über den traditionellen Rahmen von Moralvorstellungen und sozialen Normen hinaus auch die politischen Präferenzen beeinflusst. Eine entsprechende Theorie wurde zu Beginn des 20. Jahrhunderts entwickelt und prominent von Max Weber (vgl. Weber 2004) vertreten. Weber argumentiert, dass die protestantische Arbeitsethik einen entscheidenden Einfluss auf die Entwicklung des Kapitalismus hatte. Laut Weber führte der protestantische Glaube an harte Arbeit, ein sparsames Leben und ethisches Verhalten zur einer in den damaligen Gesellschaften weit verbreiteten Geisteshaltung, die wirtschaftliches Wachstum förderte und den Aufstieg des Kapitalismus begünstigte.\nBasten und Betz (2013) nutzen Wahlergebnisse sowie geo- und soziodemographische Datensätze für schweizer Gemeinden, um den Zusammenhang zwischen Religiosität und politischen Präferenzen wie links-rechts-Ausrichtung, Einstellungen zur Umverteilung und Einwanderung zu untersuchen. Hierfür verwenden die Autoren ein FRDD, dass eine historisch bedingte Diskontinuität der geographischen Verteilung von evanglischer bzw. katholischer Religionszugehörigkeit zwischen den Kantonen Freiburg (überwiegend dunkelrote Region, frz. Fribourg) und Waadt (kleinere hellrote Region, frz. Vaud) ausnutzt. Die historische Verteilung der Konfessionen in der betrachteten Region im 16. Jahrhundert durch Abspaltung des Kantons Freiburg ist in Abbildung 4.10 dargestellt.\nAufgrund von Bevölkerungsbewegungen ist die Verteilung der Konfessionen zwar nicht mehr eindeutig durch die Kantonsgrenze bestimmt, jedoch sind die Gemeinden der betrachteten Kantone auch heute noch mehrheitlich protestantisch bzw. katholisch. Es ist plausibel, dass eine Prägung gemäß Webers Theorie vorliegt, sich die Gemeinden nahe der Grenz aber hinsichtlich anderer Charakteristika (insb. der Bevölkerungsstruktur) nicht systematisch unterscheiden. Somit liegt ein quai\n\n\n\n\nAbbildung 4.10: Historische Verteilung von Religionszugehörigkeit in Schweizer Gemeinden im 16. Jahrhundert. Quelle: Basten und Betz (2013).\n\n\n\nDie Ergebnisse der Studie zeigen einen signifikanten Einfluss von Protestantismus auf politische Präferenzen, die über traditionelle Moralvorstellungen hinausgehen: Die Autoren finden Hinweise, dass Einwohner evangelisch geprägter Gemeinden eher konservative soziale und politische Ansichten vertreten. Eine mögliche Erklärung für diesen Effekt ist, dass religiöse Institutionen auch eine soziale und politische Agenda verfolgen, die von den Gläubigen internalisiert wird.\n\n4.4.1 Aufbereitung der Daten\nIn diesem Kapitel zeigen wir, wie die Kernergebnisse der Studie mit R reproduziert werden können. Hierfür werden folgende Pakete benötigt.\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(vtable)\nlibrary(rdrobust)\n\nDas Papier sowie der Datensatz BastenBetz.dta sind auf der Übersichtsseite der AEA verfügbar und liegt im STATA-Format .dta vor.44 Siehe alternativ das working paper, falls kein Abbonement für AEA-Journals vorliegt.\n\n# Datensatz einlesen\nBastenBetz &lt;- read_dta('BastenBetz.dta')\n\nDer Datensatz BastenBetz enthält Beobachtungen zu 509 schweizer Gemeinden. Eine Vielzahl an Variablen ist lediglich für Robustheits-Checks relevant. Für die Reproduktion der Kernergebnisse erstellen wir zunächst einen reduzierten Datensatz und transformieren einige Variablen.\n\n# Reduzierten Datensatz erstellen\nBastenBetz &lt;- BastenBetz %&gt;%\n  transmute(\n    gini = Ecoplan_gini,\n    prot = prot1980s,\n    bord = borderdis, \n    vaud,\n    pfl, \n    pfr, \n    pfi\n  )\n\nDie Definitionen der Variablen sind in Tabelle 4.2 gegeben. Die Präferenzen pfl, pfr und pfi basieren auf Wahlergebnissen auf Gemeindeebene zu Volksentscheiden.\n\n\n\n\n\n\n\n\nVariable\nDefinition\n\n\n\nprot\nAnteil Prothestanten im Jahr 1980 (%)\n\n\ngini\nGini-Koeffizient\n\n\nbord\nLaufdistanz zur Kantonsgrenze (Km)\n\n\nvaud\nDummyvariable: Gemeine im Kanton Waadt\n\n\npfl\nPräferenz für Freizeit (%)\n\n\npfr\nPräferenz für Umverteilung (%)\n\n\npfi\nPräferenz für wirtschaftliche Intervention des Staats (%)\n\n\n\nTabelle 4.2: BastenBetz – Variablen und Definitionen\nFür die Berechnung der optimalen Bandweite des FRDD verwenden wir einen MSE-optimalen Schätzer, der in der Funktion rdrobust::rdbwselect() implementiert ist.55 Basten und Betz (2013) setzen BW = 5.01, den Durchschnitt von IK-Schätzungen über Modelle sämtlicher betrachteter Outcome-Variablen. Diese Bandweite liegt nahe des Ergebnisses von rdbwselect. Wir verwenden nachfolgend die Schätzung OB.\n\n# Bandweite schätzen (Bsp. für Freizeitpräferenz)\nbw_selection &lt;- rdbwselect(\n  y = BastenBetz$pfl,\n  x = BastenBetz$bord,\n  fuzzy = BastenBetz$prot, \n  bwselect = \"mserd\", \n  kernel = \"uniform\"\n) \n\n# Bandweite auslesen und zuweisen\n(OB &lt;- bw_selection$bws[1])\n\n[1] 5.078001\n\n\n\n4.4.2 Deskriptive Statistiken\nZur Reproduktion von Tabelle 1 aus Basten und Betz (2013) erzeugen wir eine nach Kantonen gruppierte Zusammenfassung der Daten und berechnen deskriptive Statistiken. Wie im Paper berücksichtigen wir hierbei nur Gemeinden innerhalb der geschätzten optimalen Bandweite OB.\n\n# Datensatz für Reproduktion von Table 1 formatieren\nT1 &lt;- BastenBetz %&gt;%\n  filter(abs(bord) &lt; OB) %&gt;%\n  mutate(\n    vaud = ifelse(\n      test = vaud == 1, \n      yes = \"Waadt\", \n      no = \"Freiburg\"\n    ),\n    prot = prot * 100\n  ) %&gt;%\n  group_by(vaud) %&gt;%\n  summarise(\n    across(\n      everything(), \n      list(\n        Mean = mean, \n        SD = sd, \n        N = length\n      )\n    )\n  ) %&gt;%\n  pivot_longer(\n    cols = -vaud,\n    names_to = c(\"variable\", \"statistic\"), \n    names_sep = \"_\"\n  )\n\nFür die tabellarische Darstellung transformieren wir in ein weites Format, sodass die Tabelle die deskriptive Statistiken spaltenweise für die Kantone zeigt.\n\n# Daten in weites Format überführen\nT1_wider &lt;- T1 %&gt;% \n  pivot_wider(\n    names_from = c(\"vaud\", \"statistic\")\n  )\n\nDie Tabelle erzeugen wir mit gt::gt().\n\n# Tabelle mit gt() erzeugen\nT1_wider %&gt;%\n  gt(rowname_col = \"Variable\") %&gt;% \n  tab_spanner_delim(\n    delim = \"_\",\n  ) %&gt;%\n tabopts\n\n\n\n\n\n\n\n  \n\nvariable\n      \n        Freiburg\n      \n      \n        Waadt\n      \n    \n\nMean\n      SD\n      N\n      Mean\n      SD\n      N\n    \n\n\n\ngini\n0.302\n0.029\n49\n0.367\n0.052\n84\n\n\nprot\n9.428\n5.695\n49\n83.245\n11.411\n84\n\n\nbord\n−2.327\n1.274\n49\n2.493\n1.201\n84\n\n\npfl\n48.239\n4.774\n49\n39.508\n5.723\n84\n\n\npfr\n43.049\n2.634\n49\n39.19\n5.025\n84\n\n\npfi\n52.642\n2.94\n49\n47.086\n3.368\n84\n\n\n\nTabelle 4.3:  Datensatz BastenBetz – Zusammenfassende Statistiken \n\n\n\nDie Statistiken in Tabelle 4.3 scheinen konsistent mit der (historischen) Verteilung der Religionszugehörigkeit und politischen Einstellung gemäß der Hypothese: Im überwiegend katholischen Freiburg finden wir eine größere Einkommensungleichkeit und höhere aus Wahlergebnissen abgeleitete Präferenzen für Freizeit, Umverteilung sowie staatliche Interventionen.\n\n4.4.3 Modellspezifikation und First-Stage-Ergebnisse\nDie Kantone Waadt und Freiburg haben bis heute mehrheitlich protestantische bzw. katholische Gemeinden. Die Verteilung von Protestantismus ist also, u.a. aufgrund von Bevölkerungsbewegungen, nicht mehr deterministisch. An der Kantonsgrenze besteht jedoch eine deutliche Diskontinuität im Anteil protestantischer Einwohner, die auf die historische Verteilung der Religionszugehörigkeit zurückzuführen ist. Damit kann ein FRDD implementiert werden, bei dem die Distanz zur Grenze (bord) die zentrierte Laufvariable ist und die Zugehörigkeit zum Kanton Waadt (vaud) ein Instrument für die Behandlungsvariable (prot) ist.\nWir nutzen die Funktion rdrobust::rdplot um diesen Zusammenhang für verschiedene Bandweiten anhand des linearen Interaktionsmodells \\[\\begin{align}\n  \\begin{split}\n  prot_i =&\\, \\alpha_0 + \\alpha_1 vaud_i + \\alpha_2 bord_i \\\\\n  +&\\, \\alpha_3 bord_i \\times vaud_i + u_i\n  \\end{split}\\label{eq:BBFSR}\n\\end{align}\\] grafisch darzustellen. Dies ist die First-Stage-Regression für die 2SLS-Schätzung der Behandlungseffekte.\n\n# Reproduktion von Abbildung 3 in Basten und Betz (2013)\nplots_BB &lt;- list(\n  # gesch. optimale Bandweite\n  p_OB = rdplot(\n    y = BastenBetz$prot, \n    x = BastenBetz$bord, \n    h = c(OB, OB), \n    x.label = \"Distanz zur Grenze (bord)\",\n    y.label = \"Anteil Protestanten (prot)\", \n    title = \"Gesch. Bandweite\",\n    p = 1, \n    nbins = c(6, 14), \n    masspoints = \"off\"\n  ),\n  \n  # Bandweite 10\n  p_BW10 = rdplot(\n    y = BastenBetz$prot, \n    x = BastenBetz$bord, \n    h = c(10, 10), \n    x.label = \"Distanz zur Grenze (bord)\",\n    y.label = \"Anteil Protestanten  (prot)\", \n    title = \"Bandweite = 10\",\n    p = 1, \n    nbins = c(6, 14),\n    masspoints = \"off\"\n  ),\n  \n  # Bandweite 20\n  p_BW20 = rdplot(\n    y = BastenBetz$prot, \n    x = BastenBetz$bord, \n    h = c(20, 20), \n    x.label = \"Distanz zur Grenze (bord)\",\n    y.label = \"Anteil Protestanten  (prot)\", \n    title = \"Bandweite = 20\",\n    p = 1, \n    nbins = c(6, 14),\n    masspoints = \"off\"\n  ),\n  \n  # Gesamter Datensatz\n  p_G = rdplot(\n    y = BastenBetz$prot, \n    x = BastenBetz$bord,\n    x.label = \"Distanz zur Grenze (bord)\",\n    y.label = \"Anteil Protestanten\", \n    title = \"Ges. Datensatz\",\n    p = 1, \n    nbins = c(6, 14),\n    masspoints = \"off\"\n  )\n)\n\nWir sammeln die Ergebnisse in einem Plot-Gitter mit cowplot::plot_grid().\n\n# Reproduktion von Abbildung 3 in Basten und Betz (2013)\nplot_grid(\n  plotlist = map(plots_BB, ~ .$rdplot), ncol = 2\n)\n\n\n\nAbbildung 4.11: First-Stage-Regressionen\n\n\n\nDie Grafiken in Abbildung 4.11 zeigen deutliche Hinweise auf die Diskontinuität in prot nahe der Kantonsgrenze. Die Größe des geschätzten Sprungs scheint nur wenig sensitiv gegenüber der gewählten Bandweite zu sein. Die Signifikanz des Effekts können wir anhand der jeweiligen KQ-Regressionen beurteilen.66 Wir nutzen update() um die Regression mit weniger Code für verschiedene Bandweiten zu schätzen.\n\n# Reproduktion der First-Stage-Regressionen\n# s. Tabelle 2 in Basten und Betz (2013)\n\n# (1) BW = OB\nFS1 &lt;- lm(\n  formula = prot ~ vaud + bord + vaud * bord, \n  data = BastenBetz %&gt;% \n    filter(\n      abs(bord) &lt;= OB\n    )\n)\n\n# (2) BW = 10\nFS2 &lt;- update(\n  FS1,\n  data = BastenBetz %&gt;% \n    filter(\n      abs(bord) &lt;= 10\n    )\n)\n\n# (3) BW = 20\nFS3 &lt;- update(\n  FS1,\n  data = BastenBetz %&gt;%\n    filter(\n      abs(bord) &lt;= 20\n    )\n)\n\n# (4) Ges. Datensatz\nFS4 &lt;- update(\n  object = FS1,\n  data = BastenBetz\n)\n\n\n# Tabellarische Darstellung\nmodelsummary(\n  list(\n    \"BW = OB\"= FS1, \n    \"BW = 10\" = FS2, \n    \"BW=20\" = FS3, \n    \"Ges. Datensatz\" = FS4\n  ), \n  vcov = \"HC1\", \n  stars = T, \n  gof_map = \"nobs\", \n  output = \"gt\"\n) %&gt;%\n  tabopts()\n\n\n\n\n\n\n\n  \n \n      BW = OB\n      BW = 10\n      BW=20\n      Ges. Datensatz\n    \n\n\n(Intercept)\n0.134***\n0.100***\n0.103***\n0.109***\n\n\n\n(0.017)\n(0.013)\n(0.010)\n(0.009)\n\n\nvaud\n0.671***\n0.726***\n0.756***\n0.710***\n\n\n\n(0.034)\n(0.022)\n(0.018)\n(0.014)\n\n\nbord\n0.017**\n0.001\n0.001\n0.002*\n\n\n\n(0.006)\n(0.003)\n(0.001)\n(0.001)\n\n\nvaud × bord\n-0.006\n-0.001\n-0.009***\n-0.004***\n\n\n\n(0.012)\n(0.005)\n(0.003)\n(0.001)\n\n\nNum.Obs.\n133\n207\n312\n509\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n    \n\nTabelle 4.4:  First-Stage Regressionen \n\n\n\nFür die geschätze Bandweite schätzen wir einen hochsignifikanten Sprung in prot von etwa 67% an der Kantonsgrenze. Auch für größere Bandweiten von 10km und 20km sowie für den gesamten Datensatz finden wir vergleichbare signifikante Effekte, was eine bei zunehmender Distanz zur Grenze persistente Diskrepanz der Religionszugehörigkeit bestätigt.\n\n4.4.4 Second-Stage-Ergebnisse\nWir schätzen nun den LATE von Protestantismus für die Outcome-Variablen gini, pfl, pfi und pfr, vgl. Tabelle 4.2. Die Spezifikation für die Second-Stage-Regression der FRDD-Schätzung ist \\[\\begin{align}\n  \\begin{split}\n    Y_i = \\gamma_0 + \\gamma_1 \\widehat{prot}_i +  \\gamma_2 bord_i + \\gamma_3 bord_i  \\times vaud_i + e_i\n  \\end{split},\n\\end{align}\\] wobei \\(\\widehat{prot}_i\\) angepasste Werte aus der KQ-Schätzung von \\(\\eqref{eq:BBFSR}\\) mit Bandweite OB sind. Dazu erzeugen wir zunächst eine angepasste Version des Objekts BastenBetz, welche nur Gemeinden innerhalb der Bandweite enthält.\n\n# Gemeinden innerhalb der Bandweite filtern\nBastenBetz_OB &lt;- BastenBetz %&gt;% \n  filter(\n    abs(bord) &lt;= OB\n  )\n\nZur Illustration schätzen wir nun die Second-Stage-Regression für \\(Y = pfl\\).\n\n# Second-Stage-Regression für `pfl`\nBastenBetz_OB %&gt;% \n  mutate(\n    prot_fitted = fitted(FS1)\n    ) %&gt;%\n\nlm(\n  pfl ~ prot_fitted + bord + vaud:bord, \n  data = .\n) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = pfl ~ prot_fitted + bord + vaud:bord, data = .)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.8870  -3.8621  -0.0423   3.4993  12.1636 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  50.5275     1.9721  25.621  &lt; 2e-16 ***\nprot_fitted -13.4600     3.1749  -4.240 4.24e-05 ***\nbord          0.4380     0.6528   0.671    0.503    \nbord:vaud    -0.3636     0.7939  -0.458    0.648    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.433 on 129 degrees of freedom\nMultiple R-squared:  0.383, Adjusted R-squared:  0.3686 \nF-statistic: 26.69 on 3 and 129 DF,  p-value: 1.704e-13\n\n\nDer Koeffizient prot_fitted ist der gesuchte Behandlungseffekt. Beachte, dass die von summary() berechneten Standardfehler ungültig sind, weil diese die zusätzliche Unsicherheit durch die Berechnung von \\(\\widehat{prot}\\) über die First-Stage-Regression nicht berücksichtigen. Nachfolgend nutzen wir AER::ivreg(), um komfortabel gültige (heteroskedastie-robuste) Inferenz betreiben zu können.77 Die Autoren geben an, robuste SEs zu nutzen. Das scheint nicht der Fall zu sein, denn vcov = \"HC0\" liefert die Ergebinsse im Paper. Die von Stata berechneten HC1-SEs weichen ab. Dies ändert allerdings nichts an der Signifikanz der Koeffizienten. Wir nutzen vcov = \"HC1\".\n\n# Schätzung mit 2SlS\n# s. Tabelle 4 in Basten und Betz (2013)\n#\n# Wir instrumentieren Treatment (`prot1980s`) mit dem Schwellenindikator (`vaud`)\n# ivreg: exogene Variablen instrumentieren sich selbst, daher\n# ' | vaud * borderdis '\nlibrary(AER)\n# (1) Präferenz für Freizeit\nSS_pfl &lt;- ivreg(\n  formula = pfl ~ prot + bord:vaud + bord | vaud * bord,\n  data = BastenBetz_OB\n)\n\n# (2) Präferenz für Umverteilung\nSS_pfr &lt;- update(\n  object = SS_pfl,\n  formula = pfr ~ prot + bord:vaud + bord | vaud * bord,\n)\n\n# (3) Präferenz für Intervention\nSS_pfi &lt;- update(\n  object = SS_pfl,\n  formula = pfi ~ prot + bord:vaud + bord | vaud * bord,\n)\n\n# (4) Einkommensungleichheit\nSS_gini &lt;- update(\n  object = SS_pfl,\n  formula = pfi ~ prot + bord:vaud + bord | vaud * bord,\n)\n\n\n# Tabellarische Darstellung\nmodelsummary(\n  list(\n    \"(1) Freizeit\"= SS_pfl, \n    \"(2) Umverteilung\" = SS_pfr, \n    \"(3) Intervention\" = SS_pfi, \n    \"(4) Ungleichheit\" = SS_gini\n  ), \n  vcov = \"HC1\", \n  stars = T, \n  gof_map = \"nobs\", \n  output = \"gt\"\n) %&gt;%\n  tabopts()\n\n\n\n\n\n\n\n  \n \n      (1) Freizeit\n      (2) Umverteilung\n      (3) Intervention\n      (4) Ungleichheit\n    \n\n\n(Intercept)\n50.528***\n44.560***\n52.871***\n52.871***\n\n\n\n(1.918)\n(0.950)\n(1.063)\n(1.063)\n\n\nprot\n-13.460***\n-5.061*\n-6.487***\n-6.487***\n\n\n\n(3.161)\n(2.161)\n(1.738)\n(1.738)\n\n\nbord\n0.438\n0.444\n-0.165\n-0.165\n\n\n\n(0.639)\n(0.357)\n(0.332)\n(0.332)\n\n\nbord × vaud\n-0.364\n-0.909\n0.011\n0.011\n\n\n\n(0.811)\n(0.561)\n(0.432)\n(0.432)\n\n\nNum.Obs.\n133\n133\n133\n133\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n    \n\nTabelle 4.5:  Ergebnisse der Second-Stage-Regressionen \n\n\n\nDie Koeffizienten von prot in Tabelle 4.5 sind die mit 2SLS ermittelten erwarteten Behandlungseffekte einer 100%-Reformation (d.h. von 100% katholisch zu 100% protestantisch) für eine durchschnittliche Gemeine nahe der Kantonsgrnze. Es handelt sich jeweils um einen lokalen durchschnittlichen Behandlungseffekt (LATE). Gem. der Definition der abhängigen Variablen, interpretieren wir die Koeffizienten von prot in de Regressionen (1), (2) und (3) als erwartete Prozentänderung durch Reformation. Der Koeffizient in Regression (4) gibt die erwartete Änderung des Gini-Index an. Sämtliche geschätzte Effekte sind signifikant und haben ein mit der Hypothese der Autoren konsistentes negatives Vorzeichen.\nDie Ergebnisse sind Evidenz, dass Protestantismus zu verringerter Präferenz für Freizeit, Umverteilung sowie wirtschaftspolitische Intervention seitens des Staats führt. Auch die ökonomische Ungleichheit ist signifikant geringer, als in einer durchschnittlichen vollständig katholischen Gemeinde.\n\n4.4.5 Addendum: FRDD-Schätzung mit rdrobust()\n\nDie Funktion rdrobust::rdrobust() erlaubt die Schätzung von SRDD und FRDD mit einer Vielzahl von Optionen, s. ?rdrobust. Dies erleichtert die Schätzung mehrerer Modellspezifikationenen und Bandweiten. Mit dem nachstehenden Befehl schätzen wir den LATE von Reformation auf die Präferenz für Umverteilung anhand lokaler quadratischer Regression. Der Output gibt einen Überblick der Bandweitenschätzung sowie der 2 Stufen des 2SLS-Schätzers, inkl. robuster Inferenzstatistiken.\n\npfr_rdr &lt;- rdrobust(\n  y = BastenBetz$pfr,\n  x = BastenBetz$bord,\n  fuzzy = BastenBetz$prot, \n  p = 2,\n  kernel = \"uniform\",\n  vce = \"HC1\"\n) \n\npfr_rdr %&gt;% \n  summary()\n\nFuzzy RD estimates using local polynomial regression.\n\nNumber of Obs.                  509\nBW type                       mserd\nKernel                      Uniform\nVCE method                      HC1\n\nNumber of Obs.                  127          382\nEff. Number of Obs.              85          131\nOrder est. (p)                    2            2\nOrder bias  (q)                   3            3\nBW est. (h)                  10.796       10.796\nBW bias (b)                  22.271       22.271\nrho (h/b)                     0.485        0.485\nUnique Obs.                      97          261\n\nFirst-stage estimates.\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     0.701     0.039    17.782     0.000     [0.624 , 0.778]     \n        Robust         -         -    15.837     0.000     [0.599 , 0.768]     \n=============================================================================\n\nTreatment effect estimates.\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional    -5.047     2.254    -2.239     0.025    [-9.464 , -0.629]    \n        Robust         -         -    -2.210     0.027   [-10.114 , -0.607]    \n=============================================================================\n\n\nAuch für die quadratische Spezifikation erhalten wir mit -5.047 ein vergleichbares signifikantes Ergebnis für den LATE von Protestantismus auf Umverteilung, vgl. Spalte (2) in Tabelle 4.5.\nMit der Option bwselect = \"msetwo\" kann die Bandweite jeweils für die lokale Regression links- und rechtssetig des Schwellenwerts geschätzt werden.\n\npfr_rdr %&gt;% \n  update(bwselect = \"msetwo\") %&gt;%\n  summary()\n\nFuzzy RD estimates using local polynomial regression.\n\nNumber of Obs.                  509\nBW type                      msetwo\nKernel                      Uniform\nVCE method                      HC1\n\nNumber of Obs.                  127          382\nEff. Number of Obs.              51          134\nOrder est. (p)                    2            2\nOrder bias  (q)                   3            3\nBW est. (h)                   5.340       11.387\nBW bias (b)                  13.917       22.330\nrho (h/b)                     0.384        0.510\nUnique Obs.                      97          261\n\nFirst-stage estimates.\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     0.649     0.046    14.216     0.000     [0.560 , 0.739]     \n        Robust         -         -    11.970     0.000     [0.534 , 0.743]     \n=============================================================================\n\nTreatment effect estimates.\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional    -7.487     3.378    -2.216     0.027   [-14.109 , -0.866]    \n        Robust         -         -    -2.156     0.031   [-14.750 , -0.704]    \n=============================================================================\n\n\nTrotz Diskrepanz der geschätzten Bandweiten erhalten wir eine größere aber vergleichbare Schätzung für einen negativen Effekt.\n\n\n\n\n\n\nBasten, Christoph, und Frank Betz. 2013. „Beyond work ethic: Religion, individual, and political preferences“. American Economic Journal: Economic Policy 5 (3): 67–91.\n\n\nCattaneo, Matias D, Michael Jansson, und Xinwei Ma. 2020. „Simple local polynomial density estimators“. Journal of the American Statistical Association 115 (531): 1449–55.\n\n\nGelman, Andrew, und Guido Imbens. 2019. „Why high-order polynomials should not be used in regression discontinuity designs“. Journal of Business & Economic Statistics 37 (3): 447–56.\n\n\nImbens, G. W., und Thomas Lemieux. 2008. „Regression discontinuity designs: A guide to practice“. Journal of econometrics 142 (2): 615–35.\n\n\nImbens, Guido, und Karthik Kalyanaraman. 2012. „Optimal bandwidth choice for the regression discontinuity estimator“. The Review of economic studies 79 (3): 933–59.\n\n\nLee, David S. 2008. „Randomized experiments from non-random selection in US House elections“. Journal of Econometrics 142 (2): 675–97.\n\n\nMcCrary, Justin. 2008. „Manipulation of the running variable in the regression discontinuity design: A density test“. Journal of Econometrics 142 (2): 698–714.\n\n\nWeber, Max. 2004. Die protestantische Ethik und der Geist des Kapitalismus. Bd. 1614. CH Beck."
  },
  {
    "objectID": "RegReg.html#ridge-regression",
    "href": "RegReg.html#ridge-regression",
    "title": "\n5  Regularisierte Regression\n",
    "section": "\n5.1 Ridge Regression",
    "text": "5.1 Ridge Regression\nRidge Regression wurde von Hoerl und Kennard (1970) als Alternative zur KQ-Schätzung bei hoch-korrelierten Regressoren eingeführt. Die Verlustfunktion lautet \\[\\begin{align}\n  \\mathrm{RSS}(\\boldsymbol{\\beta},p=2,\\lambda) = \\mathrm{RSS}(\\boldsymbol{\\beta}) + \\lambda \\lVert\\boldsymbol{\\beta}\\rVert_2,\\label{eq:ridgeloss}\n\\end{align}\\] d.h. der Parameter \\(\\lambda\\) reguliert den Einfluss eines \\(\\ell_2\\)-Strafterms \\[\\begin{align*}\n  \\lVert\\boldsymbol{\\beta}\\rVert_2 = \\sqrt{\\sum_{j=1}^k\\beta_j^2}\n\\end{align*}\\] auf die Verlustfunktion \\(\\mathrm{RSS}(\\boldsymbol{\\beta},p=2,\\lambda)\\). Der Ridge-Schätzer ergibt sich als \\[\\begin{align}\n  \\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_\\lambda := \\arg\\min_{\\boldsymbol{\\beta}}\\mathrm{RSS}(\\boldsymbol{\\beta}) + \\lambda \\lVert\\boldsymbol{\\beta}\\rVert_2.\\label{eq:ridgereg}\n\\end{align}\\]\nFür Das Optimierungsproblem \\(\\eqref{eq:ridgereg}\\) kann wir aus den Bedingungen 1. Ordnung \\[\\begin{align}\n  -2\\boldsymbol{X}'(\\boldsymbol{Y} - \\boldsymbol{X}\\boldsymbol{\\beta}) + 2\\lambda\\boldsymbol{\\beta} = \\boldsymbol{0}\n\\end{align}\\] die analytische Lösung \\[\\begin{align}\n  \\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_\\lambda = (\\boldsymbol{X}'\\boldsymbol{X} + \\lambda\\boldsymbol{I}_p)^{-1}\\boldsymbol{X}'\\boldsymbol{Y},\\label{eq:ridgecf}\n\\end{align}\\] bestimmt werden, wobei \\(\\boldsymbol{I}_k\\) die \\(k\\times k\\) Einheitsmatrix ist. Aus Gleichung \\(\\eqref{eq:ridgecf}\\) kann die Wirkungsweise des Strafterms \\(\\lambda \\lVert\\boldsymbol{\\beta}\\rVert_2\\) abgeleitet werden: Ridge Regression modifiziert die Diagonale der zu invertierenden Matrix \\(\\boldsymbol{X}'\\boldsymbol{X}\\) durch Addition von \\(\\lambda&gt;0\\). Dies ist hilfreich, wenn\n\n\\(k\\geq n\\) und damit \\(\\boldsymbol{X}'\\boldsymbol{X}\\) nicht invertiertbar (singulär) ist. Dann kann der KQ-Schätzer nicht berechnet werden.3 Die Inverse \\((\\boldsymbol{X}'\\boldsymbol{X} + \\lambda\\boldsymbol{I}_p)^{-1}\\) hingegen existiert unter milden Bedingungen.\nhohe Kollinearität vorliegt, sodass \\((\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\) zwar existiert, aber zu einer instablilen KQ-Schätzung mit hoher Varianz führt.\n\n3 Beispiel: X &lt;- matrix(rnorm(100), ncol = 10). Vergleiche solve(t(X) %*% X) und solve(t(X) %*% X + diag(.01, nrow = 10))Für eine grafische Betrachtung des Optimierungskalküls \\(\\eqref{eq:ridgereg}\\) betrachten wir die äquivalente Darstellung als Lagrange-Problem \\[\\begin{align}\n  \\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_\\lambda := \\arg\\min_{\\lVert\\boldsymbol{\\beta}\\rVert&lt;t}\\mathrm{RSS}(\\boldsymbol{\\beta}).\\label{eq:ridgeLg}\n\\end{align}\\] In der folgenden interaktiven Grafik illustrieren wir das Optimierungsproblem \\(\\eqref{eq:ridgeLg}\\) sowie den resultierenden Schätzer der Koeffizienten \\((\\beta_1, \\beta_2)\\) in einem multiplen Regressionsmodell mit den Regressoren \\(X_1\\) und \\(X_2\\).\n\nDie blaue Ellipse ist die Menge aller Schätzwerte \\(\\left(\\widehat\\beta_{1},\\, \\widehat\\beta_{2}\\right)\\) für den angegebenen Wert von \\(\\mathrm{RSS}\\). Im Zentrum der Ellipse liegt der KQ-Schätzer, welcher \\(\\mathrm{RSS}\\) minimiert.\nDer blaue Kreis ist die Menge aller Koeffizienten-Paare \\((\\beta_1, \\beta_2)\\), welche die Restriktion \\(\\beta_1^2 + \\beta_2^2\\leq t\\) erfüllen. Beachte, dass die Größe des Kreises nur durch den Parameter \\(t\\) bestimmt wird, welcher für einen vorgegebenen Wertebereich variiert werden kann.\nDer blaue Punkt ist der Ridge-Schätzer \\((\\widehat\\beta^R_{1,t},\\, \\widehat\\beta^R_{2,t})\\). Dieser ergibt sich als Schnittpunkt zwischen der blauen \\(\\mathrm{RSS}\\)-Ellipse und der Restriktionsregion und variiert mit \\(t\\). Die gestrichelte rote Kurve zeigt den Ridge-Lösungspfad.\nFür kleine Werte \\(t\\) drückt die Shrinkage die geschätzten Koeffizienten Richtung 0, wobei der Lösungspfad i.d.R. nicht-linear verläuft, d.h. die Shrinkage auf den Koeffizienten ist grundsätzlich unterschiedlich. Die Lösung \\((\\widehat\\beta^R_{1,t},\\, \\widehat\\beta^R_{2,t}) = (0,0)\\) existiert nur als Grenzwert für \\(t\\to0\\).\nBeachte, dass der Effekt von \\(t\\) auf die Schätzung umgekehrt für \\(\\lambda\\) verläuft: Größere \\(\\lambda\\) führen zu stärkerer Regularisierung.\n\n\n\n\n5.1.1 Eigenschaften des Schätzers\nDer Ridge-Schätzer \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_\\lambda\\) ist nicht invariant gegenüber der Skalierung der Regressoren. Für empirische Daten sollte daher vorab eine Standardisierung der erklärenden Variablen durchgeführt werden.4 Um die Eigenschaften des Ridge-Schätzers besser zu verstehen, betrachten wir hier den Fall orthonormaler Regressoren \\(\\boldsymbol{X}_j\\).5 Dann ist \\[\\begin{align}\n  \\widehat{\\beta}^{\\mathrm{R}}_{\\lambda,\\,j} = (1+\\lambda)^{-1} \\cdot\\widehat{\\beta}_j,\\quad j = 1,\\dots,k,\\label{eq:ridgeortho}\n\\end{align}\\] d.h. der Ridge-Schätzer skaliert die KQ-Lösung mit einem von \\(\\lambda\\) abhängigen Faktor.64 Bspw. mit der Funktion scale().5 Orthonormalität heißt \\(\\boldsymbol{X}_i'\\boldsymbol{X}_j = 1\\) für \\(i=j\\) und \\(0\\) sonst. Dann ist \\(\\boldsymbol{X}\\)’\\(\\boldsymbol{X} = \\boldsymbol{I}_k\\).6 \\((1+\\lambda)^{-1}\\) wird auch als Shrinkage-Faktor bezeichnet.\nWir illustrieren dies, indem wir den Zusammenhang zwischen KQ- und Ridge-Schätzer im orthonormalen Fall als R-Funktion ridge_ortho() implementieren und für die Parameterwerte \\(\\lambda\\in\\{0,0.5,2\\}\\) plotten.\n\nlibrary(tidyverse)\n\n# Funktion für Rige Regression bei orthonormalen Regressoren\nridge_ortho &lt;- function(KQ, lambda) {\n  1/(1 + lambda) * KQ\n}\n\n\n# KQ-Schätzer gegen Ridge-Schätzer plotten\ndat &lt;- tibble(KQ = seq(-1, 1, .01))\n\nggplot(dat) +\n  geom_function(fun = ridge_ortho, \n                args = list(lambda =  0), \n                lty = 2) + \n  geom_function(fun = ridge_ortho, \n                args = list(lambda = .5), \n                col = \"red\") + \n  geom_function(fun = ridge_ortho, \n                args = list(lambda = 2), \n                col = \"blue\") + \n  xlim(-.4, .4) +\n  xlab(\"KQ-Schätzer von beta_1\") +\n  ylab(\"Ridge-Schätzer von beta_1\")\n\n\n\nAbbildung 5.1: Shrinkage des OLS-Schätzers bei Ridge Regression\n\n\n\nAbbildung 5.1 zeigt, dass der Ridge-Schätzer eine lineare Transformation des KQ-Schätzers (gestrichelte Linie) ist. Größere Werte des Regularisierungsparameters \\(\\lambda\\) führen zu stärkerer Shrinkage des Koeffizientenschätzers in Richtung 0. Die \\(\\ell_2\\)-Norm führt zu proportional zum Absolutwert des KQ-Schätzers verlaufender Shrinkage: Größere Koeffizienten werden stärker bestraft als kleine Koeffizienten.\nDie Eigenschaft \\[\\mathrm{E}\\left(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_{\\lambda,\\,j}\\right) = (1+\\lambda)^{-1} \\cdot \\beta_j\\] zeigt, dass \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_{\\lambda,\\,j}\\) (für fixes \\(\\lambda&gt;0\\)) nicht erwartungstreu für \\(\\beta_j\\) ist. Weiterhin ist \\[\\begin{align*}\n  \\mathrm{Var}\\left(\\widehat{\\beta}^{\\mathrm{R}}_{\\lambda,\\,j}\\right) =&\\,\n  \\mathrm{Var}\\left(\\widehat{\\beta}_j\\right) \\cdot \\left(\\frac{\\lambda}{1+\\lambda^2}\\right)\\\\\n    =&\\, \\sigma^2\\cdot \\left(\\frac{\\lambda}{1+\\lambda^2}\\right),\n\\end{align*}\\] wobei \\(\\sigma^2\\) die Varianz des Regressionsfehlers \\(u\\) ist. Wegen \\(\\lambda&lt;(1+\\lambda)^2\\) für \\(\\lambda&gt;0\\) gilt \\[\\mathrm{Var}\\left(\\widehat{\\beta}^{\\mathrm{R}}_{\\lambda,\\,j}\\right)&lt;\\mathrm{Var}\\left(\\widehat{\\beta}_j\\right).\\] Der Ridge-Schätzer hat also eine kleinere Varianz als der KQ-Schätzer. Diese Eigenschaften können auch für korrelierte Regressoren gezeigt werden.\n\n5.1.2 Ridge Regression mit glmnet\n\nWir zeigen nun anhand simulierter Daten, wie der Ridge-Lösungspfad mit dem R-Paket glmnet berechnet werden kann. Wir erzeugen zunächst Daten gemäß der Vorschrift \\[\\begin{align}\n  \\begin{split}\n  Y_i =&\\, \\boldsymbol{X}_i' \\boldsymbol{\\beta} + u_i,\\\\\n  \\\\\n  \\beta_j =&\\,  \\frac{5}{j^2}, \\qquad\\qquad\\ j=1,\\dots,5,\\\\\n  \\beta_j =&\\, -\\frac{5}{(j-5)^2}, \\quad j=6,\\dots,10,\\\\\n  \\\\\n  \\boldsymbol{X}_i \\sim&\\, N(\\boldsymbol{0}, \\boldsymbol{\\Sigma}), \\quad u_i \\overset{u.i.v.}{\\sim} N(0, 1), \\quad i = 1,\\dots,25.\n  \\end{split} \\label{eq:ridgedgp1}\n\\end{align}\\] Hierbei wird \\(\\boldsymbol{\\Sigma}\\) so definiert, dass jeder Regressor \\(N(0,1)\\)-verteilt ist und eine Korrelation von \\(0.8\\) mit allen anderen Regressoren aufweist. Mit der Vorschrift für die \\(\\beta_j\\) stellen wir sicher, dass es wenige Variablen gibt, die \\(Y\\) stark beeinflussen, da der Absolutbetrag der Koeffizienten in \\(j\\) abnimmt.77 Für bessere Interpretierbarkeit der Grafischen Auswertung, wählen wir positive und negative Koeffizienten mit gleichem Bertag.\n\nlibrary(gendata)\nset.seed(1234)\n\n# Parameter definieren\nN &lt;- 80\nk &lt;- 10\n\ncoefs &lt;- 5/(1:(k/2))^2\nbeta &lt;- c(coefs, -coefs)\n\n# Beobachtungen simulieren\nX &lt;- as.matrix(\n  genmvnorm(\n    k = k, \n    cor = rep(.8, (k^2-k)/2), \n    n = N)\n  )\nY &lt;- X %*% beta + rnorm(N)\n\nWir schätzen nun ein Modell mit allen 10 Regressoren mit glmnet. Beachte, dass für den Ridge-Strafterm alpha = 0 gesetzt werden muss.88 alpha ist ein Mischparameter im Algorithmus für elastic net, siehe ?glmnet.\n\nlibrary(glmnet)\n\n# Ridge-Regression anpassen\nridge_fit &lt;- glmnet(\n  x = X, \n  y = Y, \n  alpha = 0 # für Ridge-Strafterm\n)\n\nDer Lösungspfad der Ridge-Schätzung kann nach Transformation der geschätzen Koeffizienten und der zugehörigen \\(\\lambda\\)-Werte in ein langes Format überführt und komfortabel mit ggplot2 dargestellt werden.\n\n# Lambda-Sequenz auslesen\nlambdas &lt;- ridge_fit$lambda\n\n# Ridge-Schätzung für Lambdas im langen Format \nas.matrix(ridge_fit$beta) %&gt;% \n  as_tibble() %&gt;% \n  rownames_to_column(\"Variable\") %&gt;%\n  pivot_longer(-Variable) %&gt;% \n  group_by(Variable) %&gt;% \n  mutate(lambda = lambdas) %&gt;%\n  \n  # Grafik mit ggplot erzeugen\n  ggplot(\n    mapping = aes(\n      x = lambda, \n      y = value, \n      col = Variable\n    )\n  ) + \n  geom_line() +\n  ylab(\"gesch. Koeffizienten\") +\n  scale_x_log10(\"log_10(lambda)\")\n\n\n\nAbbildung 5.2: Lösungspfad für Ridge-Schätzung\n\n\n\nAbbildung 5.2 zeigt den nicht-linearen Verlauf der Shrinkage auf den geschätzten Modellkoeffizienten. Die Koeffizienten werden mit zunehmendem \\(\\lambda\\) von der KQ-Lösung ausgehend (linkes Ende der Skala) in Richtung 0 gezwungen.\nÜber die Funktion cv.glmnet() kann ein optimales \\(\\lambda\\) mit Cross Validation (CV) ermittelt werden. Ähnlich wie bei glmnet() wird für die Validierung automatisch eine \\(\\lambda\\)-Sequenz erzeugt. Wir nutzen autoplot() aus dem R-Paket ggfortify für die Visualisierung der Ergebnisse mit ggplot2.\n\nlibrary(ggfortify)\n\n# Cross-validierte Bestimmung von lambda\nridge_cvfit &lt;- cv.glmnet(\n  y = Y, \n  x = X, \n  intercept = F,\n  alpha = 0\n) \n\n# Ergebnisse plotten\nridge_cvfit %&gt;% \n  autoplot(label.n = 0)\n\n\n\nAbbildung 5.3: Lösungspfad für Ridge-Schätzung\n\n\n\nAbbildung 5.3 zeigt ridge_cvfit$lambda.min, das optimale \\(\\lambda\\) mit dem geringsten CV Mean-Squarred-Error (linke gestrichelte Linie) und ridge_cvfit$lambda.1se, das größte \\(\\lambda\\), welches innerhalb einer Standardabweichung entfernt ist (rechte gestrichelte Linie).9 Wir berechnen die Schätzung für lambda.min.9 Die Wahl von lambda.1se ist eine Heuristik, welche die Schätzunsicherheit berücksichtigt und zu einem “sparsameren” Modell tendiert.\n\n(\n  ridge_coefs &lt;- coef(\n    object = ridge_cvfit, \n    s = ridge_cvfit$lambda.min\n  )\n)\n\n11 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s1\n(Intercept)  .        \nX1           4.1302194\nX2           1.0245661\nX3           0.3139297\nX4           0.5697498\nX5           0.2928664\nX6          -4.1693524\nX7          -0.7509305\nX8          -0.3844761\nX9          -0.3841997\nX10         -0.4078514\n\n\nWir schätzen das Modell nun mit KQ und vergleichen die Koeffizienten mit der Ridge-Schätzung.\n\n# KQ-Schätzung durchführen\nKQ_fit &lt;- lm(Y ~ X - 1)\n\n# Koeffizienten auslesen und transformieren:\ntibble(\n  Ridge = as.matrix(ridge_coefs)[2:11, ],\n  KQ = KQ_fit$coefficients\n) %&gt;% \n  mutate(j = factor(1:10)) %&gt;%\n  pivot_longer(\n    cols = Ridge:KQ, \n    names_to = \"Methode\", \n    values_to = \"Koeffizient\"\n  ) %&gt;%\n\n# Bar-Plot für Koeffizientenvergleich erzeugen  \n  ggplot(\n    mapping = aes(\n      x = j, \n      y = Koeffizient, \n      fill = Methode\n    )\n  ) +\n  geom_bar(\n    position = \"dodge\", \n    stat = \"identity\", \n    width = .5\n  )\n\n\n\nAbbildung 5.4: Koeffizientenvergleich: Ridge vs. KQ\n\n\n\nDer Vergleich anhand von Abbildung 5.4 zeigt deutlich, dass Ridge Regression im Vergleich mit KQ zu absolut kleineren Koeffizientenschätzungen tendiert. Inwiefern dies Konsequenzen für die Prognosegüte der Schätzung hat, können wir Anhand eines Testdatensatzes bestimmen. Hierzu vergleichen wir die mittleren Fehler (MSE) bei der Prognose von \\(Y\\) für die Beobachtungen im Testdatensatz. Für die Simulation des Testdatensatzes nutzen wir erneut die Vorschrift \\(\\eqref{eq:ridgedgp1}\\) um 80 neue Beobachtungen zu erzeugen.\n\n# Test-Datensatz erstellen\nset.seed(4321)\n# Regressoren\nnew_X &lt;- as.matrix(\n  genmvnorm(\n    k = k, \n    cor = rep(.85, (k^2-k)/2), \n    n = N\n  )\n)\n# Abh. Variable\nnew_Y &lt;- new_X %*% beta + rnorm(N)\n\nFür beide Methoden können wir predict() für die Prognosen von \\(Y\\) für den Testdatensatz (new_Y) nutzen.\n\n# Ridge: Vorhersage von new_Y für Test-Datensatz\nY_predict_ridge &lt;- predict(\n  object = ridge_cvfit, \n  newx = new_X, \n  s = ridge_cvfit$lambda.min\n)\n\n# Ridge: MSE für Test-Datensatz berechnen\nmean((Y_predict_ridge - new_Y)^2)\n\n[1] 1.288457\n\n\nDie Vorhersage für lm() benötigt dieselben Variablennamen wie im angepassten Modell, s. KQ_fit$coefficients.\n\n# Test-Datensatz für predict.lm() formatieren\nnew_X &lt;- as.data.frame(new_X)\ncolnames(new_X) &lt;- paste0(\"X\", 1:k)\n\n# KQ: Vorhersage von new_Y für Test-Datensatz\nY_predict_KQ &lt;- predict(\n  object = KQ_fit, \n  newdata = new_X\n)\n\n# KQ: MSE für Test-Datensatz berechnen\nmean((Y_predict_KQ - new_Y)^2)\n\n[1] 29.33797\n\n\nDie Ergebnisse zeigen, dass der Ridge-Schätzer trotz seiner Verzerrung einen deutlich geringeren mittleren Vorhersagefehler für die Testdaten erzielt als der KQ-Schätzer. Diese Eigenschaft der Koeffizientenschätzung kann die Prognosegüte von Ridge Regression gegenüber der KQ-Regression verbessern.\n\n5.1.3 Beispiel: Vorhersage von Abschlussnoten in Mathe\nZur Illustration von Ridge Regression nutzen wir den Datensatz SP aus Cortez und Silva (2008).10 SP enhält Beobachtungen zu Leistungen von insgesamt 100 Schülerinnen und Schülern im Fach Mathematik in der Sekundarstufe an zwei portugiesischen Schulen. Neben der Abschlussnote in Mathe (G3, Skala von 0 bis 20) beinhaltet SP diverse demografische, soziale und schulbezogene Merkmale, die mithilfe von Schulberichten und Fragebögen erhoben wurden. Ziel ist es, ein Modell für die Prognose von G3 anzupassen.10 Wir verwenden eine Auszug aus dem Orignaldatensatz, der nebst ausführlicher Variablenbeschreibung hier verfügbar ist.\nWir lesen zunächst die Daten (im .csv-Format) ein.\n\n# Daten einlesen\nSP &lt;- read_csv(file = \"datasets/SP.csv\")\n\nEin Überblick zeigt, dass der Großteil der Regressoren aus kategorialen Variablen mit sozio-ökonomischen Informationen besteht.\n\n# Überblick\nglimpse(SP)\n\nRows: 100\nColumns: 31\n$ school     &lt;chr&gt; \"GP\", \"GP\", \"GP\", \"MS\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\",…\n$ sex        &lt;chr&gt; \"M\", \"M\", \"F\", \"F\", \"M\", \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\",…\n$ age        &lt;dbl&gt; 17, 18, 19, 17, 16, 16, 19, 16, 16, 16, 18, 16, 15, 17, 17,…\n$ address    &lt;chr&gt; \"R\", \"R\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"R\", \"U\", \"U\",…\n$ famsize    &lt;chr&gt; \"GT3\", \"GT3\", \"LE3\", \"GT3\", \"LE3\", \"GT3\", \"GT3\", \"GT3\", \"GT…\n$ Pstatus    &lt;chr&gt; \"T\", \"T\", \"T\", \"T\", \"A\", \"T\", \"T\", \"T\", \"A\", \"T\", \"T\", \"T\",…\n$ Medu       &lt;dbl&gt; 1, 4, 3, 2, 3, 2, 0, 2, 3, 4, 4, 2, 1, 2, 2, 3, 3, 4, 4, 2,…\n$ Fedu       &lt;dbl&gt; 2, 3, 2, 2, 4, 3, 1, 1, 1, 4, 4, 2, 2, 3, 2, 3, 1, 3, 4, 2,…\n$ Mjob       &lt;chr&gt; \"at_home\", \"teacher\", \"services\", \"other\", \"services\", \"oth…\n$ Fjob       &lt;chr&gt; \"other\", \"services\", \"other\", \"at_home\", \"other\", \"other\", …\n$ reason     &lt;chr&gt; \"home\", \"course\", \"reputation\", \"home\", \"home\", \"reputation…\n$ guardian   &lt;chr&gt; \"mother\", \"mother\", \"other\", \"mother\", \"mother\", \"mother\", …\n$ traveltime &lt;dbl&gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1,…\n$ studytime  &lt;dbl&gt; 2, 3, 2, 3, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 3, 1, 2,…\n$ failures   &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ schoolsup  &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no…\n$ famsup     &lt;chr&gt; \"no\", \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", …\n$ paid       &lt;chr&gt; \"no\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"n…\n$ activities &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"y…\n$ nursery    &lt;chr&gt; \"yes\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\"…\n$ higher     &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes…\n$ internet   &lt;chr&gt; \"no\", \"yes\", \"yes\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", …\n$ romantic   &lt;chr&gt; \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"…\n$ famrel     &lt;dbl&gt; 3, 5, 4, 3, 5, 4, 3, 4, 2, 2, 1, 5, 4, 5, 3, 5, 4, 4, 5, 5,…\n$ freetime   &lt;dbl&gt; 1, 3, 2, 4, 3, 4, 4, 5, 3, 4, 4, 4, 3, 3, 4, 4, 5, 2, 3, 4,…\n$ goout      &lt;dbl&gt; 3, 2, 2, 3, 3, 3, 2, 2, 3, 4, 2, 4, 2, 3, 4, 2, 4, 2, 3, 4,…\n$ Dalc       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1,…\n$ Walc       &lt;dbl&gt; 5, 2, 2, 1, 1, 3, 1, 1, 2, 3, 2, 4, 1, 3, 3, 1, 3, 2, 1, 1,…\n$ health     &lt;dbl&gt; 3, 4, 1, 3, 5, 4, 5, 5, 4, 4, 1, 5, 5, 3, 5, 5, 1, 3, 5, 5,…\n$ absences   &lt;dbl&gt; 4, 9, 22, 8, 4, 6, 2, 20, 5, 6, 5, 0, 2, 2, 12, 0, 17, 0, 4…\n$ G3         &lt;dbl&gt; 10, 16, 11, 11, 11, 10, 9, 12, 7, 11, 16, 12, 9, 12, 12, 13…\n\n\nUm die Prognosegüte des Modells beurteilen zu können, partitionieren wir SP zufällig in einen Test- sowie einen Trainingsdatensatz (mit 30 und 70 Beobachtungen), jeweils für die Regressoren und die abhängige Variable.\n\n# ID für Beobachtungen im Testdatensatz zufällig erzeugen\nset.seed(1234)\nID &lt;- sample(1:nrow(SP), size = 30)\n\n# Regressoren aufteilen\nSP_test &lt;- SP[ID,]\nSP_train &lt;- SP[-ID,]\n\n# Abh. Variable aufteilen\nY_test &lt;- SP_test$G3\nY_train &lt;- SP_train$G3\n\nAls nächstes passen wir ein Ridge-Regressionsmodell für alle Regressoren in SP_train an und ermitteln ein optimales \\(\\lambda\\) mit Cross Validation. Beachte, dass cv.glmnet nicht für Regressoren im data.frame/tibble-Format ausgelegt ist, sondern ein matrix-Format erwartet. Wir transformieren SP_train daher mit data.matrix().\n\n# Ridge-Regression und CV für Trainingsdaten\nSP_fit_cv &lt;- cv.glmnet(\n  x = data.matrix(SP_train %&gt;% select(-G3)), \n  y = Y_train, \n  alpha = 0\n)\n\n# CV-Ergebnisse für lambda visualisieren\nSP_fit_cv %&gt;% \n  autoplot(label.n = 0)\n\n\n\n\nWie für das Beispiel mit simulierten Daten erhalten wir mit predict() Vorhersagen für die erzielte Punktzahl. Beachte, dass wir den MSE nicht für die Trainingsdaten SP_train, sondern für die Testdaten SP_test berechnen.\n\n# Prognose von G3 anhand des Ridge-Modells\nY_predict_ridge &lt;- predict(\n  object = SP_fit_cv, \n  newx = data.matrix(\n    SP_test %&gt;% \n      select(-G3)\n    ), \n  s = SP_fit_cv$lambda.min\n)\n\n# MSE für Testdaten berechnen\nmean((Y_predict_ridge - Y_test)^2)\n\n[1] 21.13249\n\n\nAuch in diesem empirischen Beispiel zeigt ein Vergleich der MSEs, dass Ridge Regression dem KQ-Schätzer hinsichtlich der Vorhersagegüte überlegen ist.\n\n# Modell mit KQ schätzen\nSP_fit_KQ &lt;- lm(G3 ~ ., SP_train)\n\n# Prognose\nY_predict_KQ &lt;- predict(\n  object = SP_fit_KQ, \n  newdata = SP_test %&gt;% \n    select(-G3)\n)\n\n# Testset-MSE berechnen\nmean((Y_predict_KQ - Y_test)^2)\n\n[1] 29.76893\n\n\nDer MSE für Ridge ist mit \\(21.13\\) deutlich kleiner als \\(29.77\\), der MSE für KQ.\nFür die Interpretation der Ridge-Schätzung erweitern den Code für die ggplot2-Grafik der Koeffizienten-Pfade um eine vertikale Linie des mit CV ermittelten \\(\\lambda\\) und fügen mit dem Paket ggrepel Labels für die Pfade der größten Koeffizienten hinzu.\n\nlibrary(ggrepel)\n\n# Lambda-Sequenz auslesen\nlambdas &lt;- SP_fit_cv$lambda\n\n# Ridge-Schätzung für Lambdas im langen Format \ndf &lt;- as.matrix(SP_fit_cv$glmnet.fit$beta) %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    Variable = rownames(SP_fit_cv$glmnet.fit$beta)\n  ) %&gt;%\n  pivot_longer(-Variable) %&gt;% \n  group_by(Variable) %&gt;% \n  mutate(lambda = lambdas) \n\n# Grafik mit ggplot erzeugen\ndf %&gt;%\n  ggplot(\n    mapping = aes(\n      x = lambda, \n      y = value, \n      col = Variable\n    )\n  ) + \n  geom_line() +\n  geom_label_repel(\n    data = df %&gt;% \n      filter(lambda == min(lambdas)),\n    mapping = aes(label = Variable), \n    seed = 1234,\n    size = 5, \n    max.overlaps = 8, \n    nudge_x = -.5) +\n  ylab(\"gesch. Koeffizienten\") +\n  scale_x_log10(\"log_10(lambda)\") +\n  geom_vline(\n    xintercept = SP_fit_cv$lambda.min, \n    col = \"red\", \n    lty = 2\n  ) +\n  theme(legend.position = \"none\")\n\n\n\nAbbildung 5.5: Lösungspfad für Ridge-Schätzung\n\n\n\nAbbildung 5.5 gibt Hinweise darauf, dass neben der Schulzugehörigkeit und Indikatoren für schulische Leistung (bspw. failures) sozio-ökonomische Prädiktoren wie internet (Internetzugang zuhause), Pstatus (Zusammenleben der Eltern) und address/traveltime (sozialer Status) relevante Variablen zu sein scheinen.\nDas optimale \\(\\lambda_\\mathrm{cv} \\approx 0.21\\) (gestrichelte rote Linie in Abbildung 5.5) führt zu deutlicher Shrinkage, was eine mögliche Erklärung für den besseren Testset-MSE von Ridge Regression ist: Die Koeffizienten von Variablen mit wenig Erklärungskraft werden durch die Regularisierung in Richtung 0 gezwungen und reduzieren so die Varianz der Vorhersage gegenüber der (idealerweise) unverzerrten KQ-Schätzung.\n\n\n\n\n\n\nKey Facts zu Ridge Regression\n\n\n\n\nRidge-Regression regularisiert den KQ-Schätzer mit der \\(\\ell_2\\)-Norm der Koeffizienten. Diese Form von Regularisierung ist eine Alternative für KQ in Anwendungen mit mehr Regressoren als Beobachtugen (\\(k\\geq n\\)) und/oder wenn KQ aufgrund starker Kollinearität eine hohe Varianz aufweist.\nDer Ridge-Schätzer \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_\\lambda\\) ist nicht erwartungstreu. Die geschätzten Koeffizienten sind auch für \\(n\\to\\infty\\) verzerrt.\nAufgrund der verzerrten Schätzung ist statistische Inferenz für Koeffizienten mit \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_\\lambda\\) problematisch. Anstatt für strukturelle Modelle oder die Schätzung kausaler Effekte wird Ridge Regression in der Praxis daher überwiegend für Prognosen verwendet.\nDie Wahl von \\(\\lambda\\) impliziert einen Tradeoff zwischen Verzerrung und Varianz: Große \\(\\lambda\\) schrumpfen die Koeffizientenschätzer Richtung 0 (mehr Verzerrung), führen aber zu einer kleineren Varianz der Schätzung. Entsprechend können Vorhersagen mit mehr Verzerrung aber weniger Varianz als mit KQ getroffen werden.\nRidge Regression kann in R mit dem Paket glmnet berechnet werden."
  },
  {
    "objectID": "RegReg.html#lasso-regression",
    "href": "RegReg.html#lasso-regression",
    "title": "\n5  Regularisierte Regression\n",
    "section": "\n5.2 Lasso Regression",
    "text": "5.2 Lasso Regression\nLeast Absolute Shrinkage and Selection Operator (Lasso) ist ein von Tibshirani (1996) vorgeschlagener Schätzer, der die Verlustfunktion des KQ-Schätzers um einen Strafterm für die Summe der (absoluten) Größe der Koeffizienten \\(\\boldsymbol\\beta = (\\beta_1, \\dots,\\beta_k)'\\) erweitert. Die Verlustfunktion des Lasso-Schätzers von \\(\\boldsymbol{\\beta}\\) lautet \\[\\begin{align}\n\\mathrm{RSS}(\\boldsymbol{\\beta},p=1,\\lambda) = \\mathrm{RSS}(\\boldsymbol{\\beta}) + \\lambda \\lVert\\boldsymbol{\\beta}\\rVert_1.\\label{eq:lassoloss}\n\\end{align}\\] Für den Strafterm wird also die \\(\\ell_1\\)-norm \\[\n\\lVert\\boldsymbol{\\beta}\\rVert_1 = \\sum_{j=1}^k \\lvert\\beta_j \\rvert\n\\] verwendet. Der Lasso-Schätzer \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{L}}_\\lambda\\) für \\(\\boldsymbol{\\beta}\\) minimiert \\(\\eqref{eq:lassoloss}\\), \\[\\begin{align}\n\\boldsymbol{\\beta}^{\\mathrm{L}}_\\lambda = \\arg\\min_{\\boldsymbol{\\beta}} \\ \\mathrm{RSS}(\\boldsymbol{\\beta},p=1,\\lambda).\n\\end{align}\\] Entsprechend erhalten wir in Abhängigkeit von \\(\\lambda\\) ein Kontinuum an Lösungen \\[\\begin{align}\n  \\left\\{\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{L}}_\\lambda\\right\\}_{\\lambda=0}^{\\lambda=\\infty},\\label{eq:LassoPath}\n\\end{align}\\] der sogenannte Lasso-Pfad.\nDas Optimierungsproblem \\(\\eqref{eq:lassoloss}\\) hat die äquivalente Darstellung \\[\\begin{align}\n  \\begin{split}\n    \\widehat{\\boldsymbol{\\beta}}^{\\mathrm{L}}_\\lambda =&\\, \\arg\\min_{\\boldsymbol{\\beta}} \\mathrm{RSS}(\\boldsymbol{\\beta}) + \\lambda\\left(\\lVert\\boldsymbol{\\beta}\\rVert_1 - t\\right)\\\\\n    =&\\, \\arg\\min_{\\lVert\\boldsymbol{\\beta}\\rVert_1\\leq t} \\mathrm{RSS}(\\boldsymbol{\\beta}),\n  \\end{split}\\label{eq:lassolagrange}\n\\end{align}\\] welche über den Lagrange-Ansatz unter der Nebenbedingung \\(\\lVert\\boldsymbol{\\beta}\\rVert_1 \\leq t\\) gelöst werden kann.\nÄhnlich wie der KQ-Schätzer ist der Lasso-Schätzer \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{L}}_\\lambda\\) durch Bedingungen 1. Ordnung bestimmt. Diese Bedingungen lassen sich komfortabel in Matrix-Schreibweise darstellen als \\[\\begin{align}\n  -2\\boldsymbol{X}_j'(\\boldsymbol{Y} - \\boldsymbol{X}\\boldsymbol{\\beta}) + \\lambda\\cdot\\mathrm{sgn}(\\beta_j) = 0, \\quad j = 1,\\dots,k.\\label{eq:LassoFOC}\n\\end{align}\\] Aus Gleichung \\(\\eqref{eq:LassoFOC}\\) folgt, dass der Lasso-Schätzer aufgrund des Strafterms im Allgemeinen nicht algebraisch bestimmt werden kann.1111 Zur Bestimmung des Schätzers werden Algorithmen der nicht-linearen Optimierung genutzt.\nIn Abhängigkeit von \\(\\lambda\\) zwingt der Lasso-Schätzer die KQ-Schätzung von \\(\\beta_j\\) zu einem (absolut) kleineren Wert: Ähnlich wie bei Ridge Regression bewirkt der \\(\\ell_1\\)-Strafterm eine mit \\(\\lambda\\) zunehmende Schrumpfung der geschätzen Koeffizienten in Richtung 0. Charakteristisch für die Lösung des Lasso-Schätzers ist, dass \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{L}}_j = 0\\), wenn die Bedingung \\[\\begin{align}\n  \\left\\lvert\\boldsymbol{X}_j'(\\boldsymbol{Y} - \\boldsymbol{X}\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{L}}_\\lambda)\\right\\rvert - \\lambda/2 \\leq 0 \\label{eq:lassoselection}\n\\end{align}\\] erfüllt ist. In Abhängigkeit von \\(\\lambda\\) kann der Lasso-Schätzer folglich geschätzte Regressionskoeffizienten nicht nur in Richtung \\(0\\), sondern diese auch exakt mit \\(0\\) schätzen und damit Variablenselektion betreiben. Aufgrund der mit \\(\\lambda\\) zunehmenden Shrinkage bis die Bedingung \\(\\eqref{eq:lassoselection}\\) erfüllt und der Koeffizient gleich \\(0\\) gesetzt wird, bezeichnet man Lasso auch als einen Soft Thresholding Operator. Im nächsten Abschnitt betrachten wir die Eigenschaften von Lasso-Regularisierung unter vereinfachten Annahmen bzgl. der Regressoren.\n\n5.2.1 Lasso ist Soft Thresholding\nWir betrachten nun eine mathematische Darstellung von Selektions- und Shrinkage-Eigenschaft des Lasso-Schätzers in einem vereinfachten Modell. Wenn die Regressoren \\(\\boldsymbol{X}\\) orthonormal zueinander sind, existiert eine analytische Lösung des Lasso-Schätzers, \\[\\begin{align}\n  \\widehat{\\boldsymbol{\\beta}}^{\\mathrm{L}}_\\lambda =\n  \\begin{cases}\n    \\widehat{\\boldsymbol{\\beta}}_j - \\lambda/2 &, \\ \\ \\widehat{\\boldsymbol{\\beta}}_j &gt; \\lambda/2\\\\\n    0 &, \\ \\ \\lvert\\widehat{\\boldsymbol{\\beta}}_j\\rvert\\leq\\lambda/2\\\\\n    \\widehat{\\boldsymbol{\\beta}}_j + \\lambda/2 &, \\ \\ \\widehat{\\boldsymbol{\\beta}}_j &lt; \\lambda/2\n  \\end{cases},\\label{eq:lassoST}\n\\end{align}\\] wobei \\(\\widehat{\\boldsymbol{\\beta}}_j\\) der KQ-Schätzer von \\(\\beta_j\\) ist. Anhand von \\(\\eqref{eq:lassoST}\\) können wir die Selektionseigenschaft sowie die Schrumpfung der KQ-Koeffizientenschätzung in Abhängigkeit der durch \\(\\lambda\\) regulierten \\(\\ell_1\\)-Strafe erkennen. Für eine Visualisierung implementieren wir \\(\\eqref{eq:lassoST}\\) als R-Funktion lasso_st() und zeichnen die resultierenden Koeffizientenschätzungen für die Parameterwerte \\(\\lambda\\in\\{0, 0.2, 0.4\\}\\).\nWir definieren zunächst die Funktion lasso_st().\n\nlibrary(tidyverse)\n\n# Funktion für Lasso soft-thresholding definieren\nlasso_st &lt;- function(KQ, lambda) {\n  case_when(\n    KQ &gt; lambda/2         ~ KQ - lambda/2,\n    abs(KQ) &lt;= lambda/2   ~ 0,\n    KQ &lt; -lambda/2        ~ KQ + lambda/2,\n  )\n}\n\nIm nächsten Schritt zeichnen wir lasso_st() für eine Sequenz von KQ-Schätzwerten gegeben \\(\\lambda\\).\n\n# Sequenz von KQ-Schätzwerten für Illustration definieren\ndat &lt;- tibble(\n  KQ = seq(-1, 1, .01)\n)\n\n# Lasso-Schätzer als Funktion des KQ-Schätzers plotten\nggplot(dat) +\n  geom_function(\n    fun = lasso_st, \n    args = list(lambda = 0), \n    lty = 2\n  ) + \n  geom_function(\n    fun = lasso_st, \n    args = list(lambda = .2),\n    col = \"red\"\n  ) + \n  geom_function(\n    fun = lasso_st, \n    args = list(lambda = .4), \n    col = \"blue\"\n  ) + \n  xlim(-.4, .4) +\n  xlab(\"KQ-Schätzer von beta_1\") +\n  ylab(\"Lasso-Schätzer von beta_1\")\n\n\n\nAbbildung 5.6: Shrinkage und Selektion von OLS-Koeffizienten mit Lasso\n\n\n\nAbbildung 5.6 zeigt, dass der \\(\\ell_1\\)-Strafterm des Lasso-Schätzers zu einem linearen Verlauf der auf den KQ-Schätzer (gezeichnet für \\(\\lambda = 0\\), gestrichelte Linie) applizierten Shrinkage führt: Der Lasso-Schätzer ist eine abschnittsweise-lineare Funktion des KQ-Schätzers in \\(\\lambda\\): Je größer der Parameter \\(\\lambda\\), desto größer ist das Intervall von KQ-Schätzwerten \\([-\\lambda/2,\\lambda/2]\\), wo der Lasso-Schätzer zu Variablenselektion führt, d.h. hier den Koeffizienten \\(\\beta_j\\) als \\(0\\) schätzt (rote bzw. blaue Linie).\nAnhand von Abbildung 5.6 kann abgeleitet werden, dass der Lasso-Schätzer nicht invariant gegenüber der Skalierung der Regressoren ist: Die Stärke der Regularisierung durch \\(\\lambda\\) ist hängt von der Magnitude des KQ-Schätzers ab. Daher müssen die Regressoren vor Berechnung der Schätzung standardsiert werden. Üblich ist hierbei eine Normierung auf einen Mittelwert von \\(0\\) und eine Varianz von \\(1\\).\nDie nachstehende interaktive Grafik illustriert das Lasso-Optimierungsproblem \\(\\eqref{eq:lassolagrange}\\) sowie den resultierenden Schätzer der Koeffizienten \\((\\beta_1, \\beta_2)\\) in einem multiplen Regressionsmodell mit korrelierten Regressoren \\(X_1\\) und \\(X_2\\).\n\nDie blaue Ellipse ist die Menge aller Schätzwerte \\(\\left(\\widehat\\beta_{1},\\, \\widehat\\beta_{2}\\right)\\) für den angegebenen Wert von \\(\\mathrm{RSS}\\). Im Zentrum der Ellipse liegt der KQ-Schätzer, welcher \\(\\mathrm{RSS}\\) minimiert.\nDas graue Quadrat ist die Menge aller Koeffizienten-Paare \\((\\beta_1, \\beta_2)\\), welche die Restriktion \\(\\lvert\\beta_1\\rvert+\\lvert\\beta_2\\rvert\\leq t\\) erfüllen. Beachte, dass die Größe dieser Region nur durch den Parameter \\(t\\) bestimmt wird.\nDer blaue Punkt ist der Lasso-Schätzer \\((\\widehat{\\boldsymbol{\\beta}}^L_{1,t},\\, \\widehat{\\boldsymbol{\\beta}}^L_{2,t})\\). Dieser ergibt sich als Schnittpunkt zwischen der blauen \\(\\mathrm{RSS}\\)-Ellipse und der Restriktionsregion und variiert mit \\(t\\). Die gestrichelte rote Linie zeigt den Lasso-Lösungspfad.\nFür kleine Werte, erhalten wir starke Shrinkage auf \\(\\widehat\\beta_{1,t}\\) bis zum Wertebereich \\(t\\leq50\\), wo \\(\\widehat{\\boldsymbol{\\beta}}^L_{1,t}=0\\). Hier erfolgt Variablenselektion: Die Regularisierung führt zu einem geschätzten Modell, das lediglich \\(X_2\\) als erklärende Variable enthält. In diesem Bereich von \\(t\\) bewirkt die Shrinkage, dass \\(\\widehat{\\boldsymbol{\\beta}}^L_{2,t}\\to0\\) für \\(t\\to0\\).\n\n\n\nBeachte, dass der rote Lasso-Pfad (die Menge aller Lasso-Lösungen) äquivalent als Funktion von \\(\\lambda\\) im Optimierungsproblem \\(\\eqref{eq:lassoloss}\\) dargestellt werden kann. Implementierungen mit statistischer Software berechnen die Lasso-Lösung häufig in Abhängigkeit von \\(\\lambda\\). Ein Algorithmus hierfür ist LARS.\n\n5.2.2 Berechnung der Lasso-Lösung mit dem LARS-Algorithmus\nFür die Berechnung des Lasso-Lösungspfads kann der LARS-Algorithmus von Efron u. a. (2004) im Lasso-Modus genutzt werden.12 Der Lasso-Lösungspfad beinhaltet geschätzte Koeffizienten über ein Intervall für \\(\\lambda\\), welches sämtliche Modellkomplexitäten zwischen der (trivialen) Lösung mit maximaler Shrinkage auf allen Koeffizienten (\\(\\lambda\\) groß, alle gesch. Koeffizienten sind \\(0\\)) und der unregularisierten Lösung (\\(\\lambda = 0\\), KQ-Schätzung) abbildet. Der LARS-Algorithmus erzeugt den Lösungspfad sequentiell, sodass die Schätzung als Funktion von \\(\\lambda\\) veranschaulicht werden kann, ähnlich wie bei Ridge Regression.12 LARS steht für Least Angle Regression.\nWir zeigen nun anhand simulierter Daten, wie Lasso-Lösungen mit dem R-Paket lars berechnet werden können. Hierfür erzeugen wir Daten gemäß der Vorschrift \\[\\begin{align}\n  \\begin{split}\n  Y_i =&\\, \\boldsymbol{X}_i' \\boldsymbol{\\beta}_v + u_i\\\\\n  \\\\\n  \\boldsymbol{\\beta}_v =&\\, (-1.25, -.75, 0, 0, 0, 0, 0, .75, 1.25)'\\\\\n  \\\\\n  \\boldsymbol{X}_i \\sim&\\, N(\\boldsymbol{0}, \\boldsymbol{I}_{9\\times9}), \\quad u_i \\overset{u.i.v.}{\\sim} N(0, 1), \\quad i = 1,\\dots,25.\n  \\end{split}\\label{eq:larsdgp}\n\\end{align}\\]\n\nlibrary(lars)\nset.seed(1234)\n\n# Parameter definieren\nN &lt;- 25\nbeta_v &lt;- c(-1.25, -.75, 0, 0, 0, 0, 0, .75, 1.25)\n\n# Beobachtungen simulieren\nX &lt;- matrix(rnorm(N * 9), ncol = 9)\nY &lt;- X %*% beta_v + rnorm(N)\n\nEntsprechend des DGP passen wir ein Modell ohne Konstante an. Damit lars::lars() den Lösungspfad des Lasso-Schätzers berechnet, muss type = \"lasso\" gewählt werden.1313 lars() standardisiert die Regressoren standardmäßig (aufgrund des DGPs hier nicht nötig).\n\n# Lösungen des Lasso-Schätzers mit LARS berechnen\n(\n  fit_lars &lt;- lars(\n    x = X, \n    y = Y, \n    intercept = F,\n    type = \"lasso\" # Wichtig: Lasso-Modus\n  )\n)\n\n\nCall:\nlars(x = X, y = Y, type = \"lasso\", intercept = F)\nR-squared: 0.858 \nSequence of LASSO moves:\n                      \nVar  9 2 8 1 3 5 4 7 6\nStep 1 2 3 4 5 6 7 8 9\n\n\nDie Zusammenfassung zeigt, dass der LARS-Algorithmus als erstes die (relevante) Variable \\(X_9\\) aktiviert.14 Mit abnehmender Regularisierung (kleinere \\(\\lambda\\)) werden in den nächsten 3 Schritten die übrigen relevanten Variablen \\(X_2\\), \\(X_8\\) und \\(X_1\\) aktiviert. Über die weiteren Schritte nähert der Algorithmus die Lösung an die saturierte Schätzung (das Modell mit allen neun Regressoren) an und aktiviert schrittweise die übrigen, irrelevanten Variablen.14 Aktivierung meint die Aufnahme einer Variable in der Modell gegeben eines hinreichend kleinen \\(\\lambda\\).\nWir visualisieren die geschätzen Koeffizienten an jedem Schritt des Lösungspfads als Funktion von \\(\\lambda\\). In der Praxis wird der Regularisierungsparameter häufig auf der natürlichen log-Skala dargestellt.\n\n# Transformation in ein weites Format\nfit_lars$beta %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    lambda = c(fit_lars$lambda, 1e-2)\n  ) %&gt;% \n  pivot_longer(\n    cols = 1:9, \n    names_to = \"Variable\", \n    values_to = \"gesch. Koeffizient\"\n  ) %&gt;% \n  \n# Visualisierung mit ggplot  \n  ggplot(\n    mapping = aes(\n      x = log(lambda), \n      y = `gesch. Koeffizient`, \n      color = Variable\n    )\n  ) + \n  geom_line() \n\n\n\nAbbildung 5.7: LARS-Lösungspfad für Lasso-Schätzung\n\n\n\nAbbildung 5.7 zeigt, dass die Shrinkage der geschätzten Koeffizienten nach der Aktivierung rasch abnimmt und sich für kleine Werte von \\(\\lambda\\) der KQ-Lösung annähert. Wir sehen auch, dass es einen Bereich von \\(\\lambda\\)-Werten gibt, für die das wahre Modell mit den Variablen \\(X_1\\), \\(X_2\\), \\(X_8\\) und \\(X_9\\) selektiert werden kann. Je nach Ziel der Analyse kann es sinnvoll sein, ein \\(\\lambda\\) in diesem Intervall zu schätzen.\n\n5.2.3 Wahl des Regularisierungsparameters \\(\\lambda\\) für den Lasso-Schätzer\nWie zuvor bei Ridge Regression muss in empirischen Anwendungen ein Wert für den Tuning-Parameter \\(\\lambda\\) gewählt werden. Hierbei besteht die Herausforderung darin, einen geeigneten Wert zu finden, der zu wünschenswerten Eigenschaften des resultierenden Modells führt. So ist für gute Vorhersagen wichtig, dass das Modell nicht zu sehr an die Daten angepasst ist (Overfitting), um eine gute Generalisierung auf neue Daten zu ermöglichen. Gleichzeitig muss das Modell flexibel genug sein, um wesentliche Eigenschaften des datenerzeugenden Prozesses hinreichend gut zu erfassen. In der Regel wird hierbei eine sparsame Modellierung angestrebt, die nur eine Teilmenge der Prädiktoren nutzt.\nIn der Praxis werden verschiedene Verfahren verwendet, um den Wert für den Tuning-Parameter \\(\\lambda\\) zu bestimmen. Gängige Methoden sind Cross Validation (CV) und Informationskriterien. In Abhängigkeit der Methode und der Daten ergeben sich ober- oder unterparameterisierte Modelle. Aufgrund der Implementierung im R-Paket lars betrachten wir CV.15 Wir zeigen nachfolgend anhand der simulierten Daten aus dem letzten Abschnitt, wie für die LARS-Schätzung ein optimales \\(\\lambda\\) mit leave-one-out CV (LOO-CV) bestimmt werden kann. Hierzu nutzen wir lars::cv.lars() unter Verwendung derselben Argumente wie zuvor im Aufruf von lars().15 Chetverikov, Liao, and Chernozhukov (2020) zeigen, dass CV zu konsistenter Modellselektion führen kann.\n\n# LARS-Lösungen mit CV evaluieren\nfit_lars_cv &lt;- cv.lars(\n  x = X, \n  y = Y, \n  intercept = F,\n  normalize = T,\n  type = \"lasso\", \n  plot.it = F, \n  K = N # für LOO-CV\n) \n\nDas Objekt fit_lars_cv ist eine Liste mit den CV-Ergebnissen. Wir können diese einfach mit ggplot visualisieren. index ist hierbei das Verhältnis der \\(\\ell_1\\)-Norm des Lasso-Schätzers für einen spezifischen Wert von \\(\\lambda\\) und der \\(\\ell_1\\)-Norm des KQ-Schätzers. Das optimale \\(\\lambda\\) wird so implizit geschätzt. cv.error ist der mit CV geschätzte MSE.\n\n# CV-MSE\nfit_lars_cv %&gt;% \n  as_tibble() %&gt;%\n\n  ggplot(\n    mapping = aes(\n      x = index, \n      y = cv.error\n    )\n  ) + \n  geom_line() +\n  xlab(\"|beta_lambda| / |beta|\") +\n  ylab(\"CV-MSE\")\n\n\n\nAbbildung 5.8: CV-MSE und relative Position von \\(\\lambda\\) auf dem Lassopfad\n\n\n\nIn der Grafik erkennen wir ein Minimum des CV-MSEs bei etwa 0.73.\n\n# CV-MSE-minimierendes Lambda bestimmen\nID &lt;- which.min(fit_lars_cv$cv.error) # Index\n\n(\n  fraction_opt &lt;- fit_lars_cv$index[ID]\n)\n\n[1] 0.7272727\n\n\nDie geschätzten Koeffizienten für die optimale Regularisierung können mit coef() ausgelesen werden.\n\n# LARS-Lasso-Fit für optimales lambda bestimmen\ncoef(\n  object = fit_lars, \n  s = fraction_opt, \n  mode = \"fraction\"\n)\n\n[1] -0.6513191 -0.6060906 -0.1946089  0.0000000  0.0000000  0.0000000  0.0000000\n[8]  0.4977908  1.3122407\n\n\nDas Ergebnis veranschaulicht die Selektionseigenschaft von Lasso: Gemäß DGP \\(\\eqref{eq:larsdgp}\\) sind die Variablen \\(X_3\\) bis \\(X_7\\) irrelevante Prädiktoren für \\(Y\\); ihre wahren Koeffizienten sind \\(0\\). In der kreuzvalidierten Lasso-Schätzung erreicht die Regularisierung, dass die Koeffizienten der Variablen \\(X_4\\) bis \\(X_7\\) tatsächlich mit 0 geschätzt werden. Wir schätzen für das mit CV bestimmte \\(\\lambda\\) also ein leicht überspezifiziertes Modell mit den Regressoren \\(X_1\\), \\(X_2\\), \\(X_3\\), \\(X_8\\) und \\(X_9\\). Beachte, dass die Lasso-Schätzung einen Kompromiss impliziert: Die Varianz der Schätzung ist geringer als die des KQ-Schätzers im Modell mit allen Variablen.16 Aufgrund der Regularisierung sind die mit Lasso geschätzten Koeffizienten der relevanten Variablen jedoch in Richtung \\(0\\) verzerrt.16 Wegen \\(N=25\\) verbleiben bei der KQ-Schätzung mit 9 Regressoren nur 16 Freiheitsgrade.\nEinen positiven Effekt dieses Kompromisses beobachten wir anhand des mittleren Vorhersagefehlers für Daten, die nicht zur Berechnung des Schätzers verwendet wurden. Wir vergleichen den Vorhersagefehler nachfolgend anhand eines solchen simulierten Test-Datensatzes mit 25 neuen Beobachtungen. Den Vorhersagefehler bestimmen wir als MSE zwischen den vorhergesagten und den tatsächlichen Ausprägungen für \\(Y\\).\n\n# Test-Datensatz erstellen\nset.seed(4321)\nnew_X &lt;- matrix(rnorm(N * 9), ncol = 9)\nnew_Y &lt;- new_X %*% beta_v + rnorm(N)\n\n# Lasso: Vorhersage von new_Y für Test-Datensatz\nY_predict_lars &lt;- predict(\n  object = fit_lars, \n  s = fraction_opt, \n  type = \"fit\", \n  mode = \"fraction\", \n  newx = new_X\n)$fit\n\n# Lasso: MSE für Test-Datensatz berechnen\nmean((Y_predict_lars - new_Y)^2)\n\n[1] 1.419817\n\n\nWir schätzen nun das große Modell mit allen 9 Variablen mit KQ und berechnen ebenfalls den MSE der Prognosen für den Test-Datensatz.\n\n# KQ-Schätzung des großen Modells durchführen\nKQ_fit &lt;- lm(Y ~ X - 1)\n\n# Test-Datensatz für predict.lm() formatieren\nnew_X &lt;- as.data.frame(new_X)\ncolnames(new_X) &lt;- paste0(\"X\", 1:9)\n\n# KQ: Vorhersage von new_Y für Test-Datensatz\nY_predict_KQ &lt;- predict(\n  object = KQ_fit, \n  newdata = new_X\n)\n\n# KQ: MSE für Test-Datensatz berechnen\nmean((Y_predict_KQ - new_Y)^2)\n\n[1] 9.851932\n\n\nOffenbar führt die Lasso-Schätzung zu einem deutlich geringeren MSE der Vorhersage von Y für den Test-Datensatz als die KQ-Schätzung und damit zu einer höheren Vorhersagegüte. Das “sparsame” mit Lasso-Regression geschätzte Modell ist dem “großen” mit KQ geschätztem Modell in dieser Hinsicht also überlegen.\n\n\n\n\n\n\nKey Facts zu Lasso-Regression\n\n\n\n\nLasso-Regression bestraft die Verlustfunktion des KQ-Schätzers mit der \\(\\ell_1\\)-Norm der Koeffizienten.\nNeben Koeffizientenschätzung mit Shrinkage in Richtung \\(0\\) kann der Lasso-Schätzer Variablenselektion durchführen: Regressionskoeffizienten können exakt mit \\(0\\) geschätzt und so ein “sparsames”, leichter zu interpretierendes Modell gewählt werden.\nWie bei Ridge Regression impliziert die Wahl von \\(\\lambda\\) einen Bias-Variance-Tradeoff, der für Vorhersagen nützlich ist: Für größere \\(\\lambda\\) wird mehr Verzerrung induziert und möglicherweise relevante Variablen mit kleinen Koeffizienten aus dem Modell entfernt. Ein solches sparsames Modell kann eine höhere Prognosegüte haben als ein komplexes, unregularisiertes Modell.\nDer Lasso-Schätzer \\(\\widehat{\\boldsymbol{\\beta}}_\\lambda^L\\) ist nicht erwartungstreu.\nLasso Regression kann bspw. mit dem LARS-Algorithmus (Paket lars) oder mit glmnet berechnet werden."
  },
  {
    "objectID": "RegReg.html#vergleich-von-lasso--und-ridge-regression-mit-simulation",
    "href": "RegReg.html#vergleich-von-lasso--und-ridge-regression-mit-simulation",
    "title": "\n5  Regularisierte Regression\n",
    "section": "\n5.3 Vergleich von Lasso- und Ridge-Regression mit Simulation",
    "text": "5.3 Vergleich von Lasso- und Ridge-Regression mit Simulation\nIn diesem Kapitel illustrieren wir Vor- und Nachteile von Lasso- und Ridge-Regression in Prognose-Anwendungen anhand von Monte-Carlo-Simulationen. Wir betrachten hierbei datenerzeugende Prozesse, die sich hinsichtlich der Anzahl relevanter Variablen sowie der Korrelation dieser Variablen unterscheiden.\nDie grundlegende Vorschrift für die Simulationen ist \\[\\begin{align*}\n  Y_i = \\sum_{j=1}^{k=40} \\beta_j X_{i,j} + u_i, \\quad u_i \\overset{u.i.v.}{\\sim} N(0,1), \\quad i=1,\\dots,100,\n\\end{align*}\\] wobei die Regressoren \\(X_j\\) eine Varianz von \\(1\\) haben und aus einer multivariaten Normalverteilung mit Korrelation \\[\\rho\\in(0,0.5,0.8)\\] gezogen werden.\nFür die Koeffizienten \\(\\boldsymbol{\\beta}\\) unterscheiden wir zwei Szenarien. In Szenario A ist \\[\\boldsymbol{\\beta} = (1,\\dots,1)',\\] d.h. alle Variablen sind relevant und haben denselben Einfluss auf \\(Y\\). In Szenario B erzeugen wir \\(\\boldsymbol{\\beta}\\) einmalig vorab so, dass \\[\\beta_j = \\begin{cases}1,\\quad \\text{mit Wsk.  }p\\\\ 0,\\quad \\text{mit Wsk.  }1-p, \\end{cases}\\] d.h. nur eine Teilmenge der Variablen beeinflusst \\(Y\\) jeweils mit demselben Effekt \\(\\beta_j = 1\\). Die übrigen Variablen sind irrelevant.\nWir schätzen und validieren die Modelle mit glmnet().\n\n5.3.1 Prognosegüte in diversen Szenarien\n\n# Simulationsparameter definieren\nrho &lt;- c(0, 0.5, 0.8)   # Korrelation\nk &lt;- 40                 # Anz. Regressoren\nN &lt;- 100                # Anz. Beobachtungen\nn_sim &lt;- 100            # Anz. Simulationen\n\nDamit der Code für die Simulation möglichst wenig repetitiv ist, definieren wir eine Funktion cv.glmnet_MSE(), die unter Angabe der Daten X und Y, des Trainingssets train sowie des Parameters alpha den gewünschten regularisierten Schätzer under Verwendung von Cross Validation anpasst und den Testset-MSE zurückgibt.\n\n# allg. Funktion für Testset-MSE nach CV\ncv.glmnet_MSE &lt;- function(X, Y, train, alpha) {\n  \n  # Modell mit glmnet schätzen; lambda per CV bestimmen\n  fit_cv &lt;- cv.glmnet(\n    x = X[train,],\n    y =Y[train],\n    alpha = alpha\n  )\n  \n  # Vorhersagen treffen\n  Y_pred &lt;- predict(\n    object = fit_cv, \n    s = fit_cv$lambda.min, \n    newx = X[-train,])\n  \n  return(\n    # Testset-MSE berechnen\n    mean(\n      (Y[-train] - Y_pred)^2\n      )\n  )\n}\n\nWir initialisieren zunächst Matrizen, in welche die MSEs aus den 100 Simulationsdurchläufen reihenweise geschrieben werden. lasso_mse und ridge_mse haben je eine Spalte für jede Korrelation in rho\n\n# Matrizen für simulierte MSEs initialisieren...\nlasso_mse &lt;- matrix(\n  data = NA, \n  nrow = n_sim, \n  ncol = length(rho)\n) \nridge_mse &lt;- lasso_mse\n\n# ... und benennen\ncolnames(lasso_mse) &lt;- paste0(\"Kor=\", rho)\ncolnames(ridge_mse) &lt;- colnames(lasso_mse)\n\nFür die Simulation iterieren wir mit purrr::walk über den Vektor rho sowie über die Laufvariable 1:n_sim. Beide Schleifen nutzen den Syntax für anonyme Funktionen:\n\n# Die anonyme Funktion\nfunction(x) return(x)\n# ist äquivalent definiert als\n\\(x) return(x)\n\nIn jeden Simulationsdurchlauf erzeugen wir den Datensatz entsprechend der obigen Vorschrift, teilen die Daten auf und berechnen MSEs für Lasso- und Ridge-Regression mit cv.glmnet_MSE().\nSzenario A\n\n# Koeffizienten-Vektor definieren\nbeta &lt;- rep(1, k) \n\n\nlibrary(mvtnorm)\nlibrary(tidyverse)\n\nset.seed(1234)\n\n# Simulation durchführen\nwalk(1:length(rho), \\(j) {\n  \n  # Korrelationsmatrix definieren\n  Sigma &lt;- matrix(\n    data = rho[j], \n    nrow = k, \n    ncol = k\n  )\n  diag(Sigma) &lt;- 1\n  \n  walk(1:n_sim, \\(i) {\n    \n  # Daten simulieren\n  X &lt;- rmvnorm(\n    n = N, \n    mean = rep(0, k), \n    sigma = Sigma\n  )\n  Y &lt;- X %*% beta + rnorm(N)\n    \n  # Trainingsdaten definieren\n  ID_train &lt;- sample(\n    x = c(1:N), \n    size = N/2\n  )\n    \n  # Modelle mit CV schätzen und MSEs berechnen\n  # Ridge-Regression\n  ridge_mse[i, j] &lt;&lt;- cv.glmnet_MSE(\n    X = X, \n    Y = Y, \n    train = ID_train, \n    alpha = 0\n  )\n  \n  # Lasso-Regression\n  lasso_mse[i, j] &lt;&lt;- cv.glmnet_MSE(\n    X = X, \n    Y = Y, \n    train = ID_train, \n    alpha = 1\n  )\n  \n  })\n  \n})\n\nBeachte, dass hier der Super-Assignment-Operator &lt;&lt;- genutzt wird, damit walk die Matrizen ridge_mse und lasso_mse in der globalen Umgebung überschreibt.1717 Dies folgt aus der Definition von walk. &lt;- bewirkt hier lediglich Assignment in der Funktionsumgebung.\nWir berechnen jeweils den mittleren MSEs, sammeln die Ergebnisse in einer tibble() und nutzen gt() für die tabellarische Darstellung.\n\nlibrary(gt)\n\n# Ergebnisse tabellarisch darstellen\ntibble(\n  Methode = c(\n    \"Lasso-Regression\", \n    \"Ridge-Regression\"\n  ),\n) %&gt;%\n  bind_cols(\n    bind_rows(\n      colMeans(lasso_mse),\n      colMeans(ridge_mse)  \n    )    \n  ) %&gt;%\n  gt() %&gt;%\n  tabopts\n\n\n\n\n\n\n\n  \nMethode\n      Kor=0\n      Kor=0.5\n      Kor=0.8\n    \n\n\nLasso-Regression\n7.17\n10.398\n7.581\n\n\nRidge-Regression\n4.841\n1.615\n1.517\n\n\n\nTabelle 5.1:  Durchschnittliche Testset-MSEs für Setting A \n\n\n\nTabelle 5.1 zeigt, dass Ridge-Regression gegenüber Lasso-Regression für jede der drei betrachteten Korrelationen überlegen ist. Insbesondere bei stärker korrelierten Regressoren ist Ridge vorteilhaft.\nFür Szenario B überschreiben wir beta nach Multiplikation mit einem zufälligen binären Vektor, sodass einige der Koeffizienten \\(0\\) und die zugehörigen Variablen irrelevant für \\(Y\\) sind.\nSzenario B\n\n# Wsk. für Relevanz einer Variable\np &lt;- .3\n\n# Koeffizienten-Vektor definieren\nset.seed(123)\nbeta &lt;- beta * sample(\n  x = 0:1, \n  size = k, \n  replace = T, \n  prob = c(1-p, p)\n)\n\n# Koeffizienten prüfen\nhead(beta, n = 10)\n\n [1] 0 1 0 1 1 0 0 1 0 0\n\n\nEine wiederholung der Simulation für die modifizierten Koeffizienten beta und liefert folgende tabellarische Auswertung.\n\n\n\n\n\n\n\n\n  \nMethode\n      Kor=0\n      Kor=0.5\n      Kor=0.8\n    \n\n\nLasso\n2.51\n2.143\n1.923\n\n\nRidge\n3.331\n2.562\n2.014\n\n\n\nTabelle 5.2:  Durchschnittliche Testset-MSEs für Szenario B \n\n\n\nDie Ergebnisse in Tabelle 5.2 zeigen, dass Ridge-Regression in Szenario B bis auf den Fall unkorrelierter Regressoren etwas schlechter abschneidet als in Szenario A. Die hohe Anzahl irrelevanter Variablen verbessert die Leistung von Lasso deutlich: Hier ist es plausibel, dass Lasso aufgrund der Thresholding-Eigenschaft die Koeffizienten einiger irrelevanten Variablen häufig exakt \\(0\\) setzt und damit ein sparsameres Modell schätzt als Ridge. Entsprechend erzielt Lasso in diesem Szenario insbesondere für \\(\\rho = 0\\) genauere Vorhersagen als Ridge Regression.\n\n5.3.2 Visualisierung des Bias-Variance-Tradeoffs bei Prognosen\nFür ein besseres Verständnis, wie sich der Regularisierungsparameter \\(\\lambda\\) auf den Bias-Variance-Tradeoff bei Prognosen mit Ridge- und Lasso-Regression auswirkt, vergleichen wir für beide Methoden nachfolgend die Abhängigkeit des MSEs der Prognose \\(\\widehat{Y}_0\\) für den Wert \\(Y_0\\) der abhängigen Variable eines Datenpunkts anhand seiner Regressoren \\(\\boldsymbol{X}_0'\\), wobei \\[\\begin{align}\n  \\text{MSE}(\\widehat{Y}_0) = \\text{Bias}(\\widehat{Y}_0)^2 + \\text{Var}(\\widehat{Y}_0) + \\text{Var}(Y_0) \\label{eq:pbvdecomp}\n\\end{align}\\] Beachte, dass \\(\\text{Var}(Y_0)\\) die durch den datenerzeugenden Prozess (und damit unvermeidbare) Varianz von \\(Y_0\\) ist, wohingegen \\(\\text{Bias}(\\widehat{Y}_0)^2\\) und \\(\\text{Var}(\\widehat{Y}_0)\\) von dem verwendeten Schätzer für \\(\\widehat{Y}_0\\) abhängt.\nFür die Simulation betrachten wir erneut Szenario A aus Kapitel 5.3.1 mit \\(50\\) Beobachtungen für ein Modell mit \\(40\\) unkorrelierten Regressoren. Wir legen zunächst die Simulationsparameter fest und erzeugen den vorherzusagenden Datenpunkt (X_0, Y_0).\n\n# Parameter festlegen\nset.seed(1234)\nn &lt;- 200 # Anz. Iterationen\nN &lt;- 50  # Anz. Beobachtungen\nk &lt;- 40  # Anz. Variablen\n\n# Korrelationsmatrix definieren\nSigma &lt;- diag(k) # Diagonalmatrix\nbeta &lt;- rep(x = 1, k)\n\n# Prognose-Ziel vorab zufällig generieren:\n\n# Regressoren\nX_0 &lt;- rmvnorm(\n  n = 1, \n  mean = rep(x = 0, k)\n)\n\n# Abh. Variable\nY_0 &lt;- X_0 %*% beta + rnorm(n = 1) %&gt;% \n  as.vector()\n\nAnhand der Simulationsergebnisse wollen wir die von der verwendeten Schätzfunktion abhängigen Komponenten von \\(\\eqref{eq:pbvdecomp}\\) untersuchen. Wir initialisieren hierzu die Listen ridge_fits und lasso_fits, in die unsere Simulationsergebnisse geschrieben werden.\n\n# Listen für Simulationsergebnisse initialisieren\nridge_fits &lt;- list()\nlasso_fits &lt;- list()\n\nWeiterhin definieren wir separate \\(\\lambda\\)-Sequenzen für Lasso- und Ridge-Schätzer.1818 Die Sequenzen haben wir in Abhängigkeit des DGP so gewählt, dass die Abhängigkeit der Prognosegüte von \\(\\lambda\\) gut visualisiert werden kann.\n\n# Lambda-Sequenzen festlegen\nlambdas_r &lt;- seq(.25, 2.5, length.out = 100)\nlambdas_l &lt;- seq(.05, 0.5, length.out = 100)\n\nFür die Simulation iterieren wir mit walk() über simulierte Datensätze und schreiben jeweils den vollständigen Output von glmnet() in die zuvor definierten Listen ridge_fits und lasso_fits.\n\n# Simulation\nwalk(1:n, \\(i) {\n  \n  # Daten simulieren\n  X &lt;- rmvnorm(\n    n = N, \n    mean = rep(0, k), \n    sigma = Sigma\n  )\n  Y &lt;- X %*% beta + rnorm(n = N, sd = 5)\n  \n  # Modelle mit glmnet schätzen\n  # Ridge-Regression\n  ridge_fits[[i]] &lt;&lt;- glmnet(\n    x = X, \n    y = Y, \n    alpha = 0, \n    intercept = F\n  )\n  # Lasso-Regression\n  lasso_fits[[i]] &lt;&lt;- glmnet(\n    x = X, \n    y = Y, \n    alpha = 1, \n    intercept = F\n  )\n  \n})\n\nWir nutzen Funktionen aus purrr und dplyr, um über die in den Simulationsdurchläufen angepassten Modelle zu iterieren. Mit predict() erhalten wir Punktvorhersagen für Y_0 für jedes \\(\\lambda\\) der zuvor definierten \\(\\lambda\\)-Sequenzen. Beachte, dass map() jeweils eine Liste mit 200 Punktvorhersagen für jedes der 100 zurückgibt. Mit list_rbind() können wir die Ergebnisse komfortabel jeweils in einer tibble sammeln.\n\n# Prognosen für Ridge-Regression\npred_r &lt;- map(\n  .x = ridge_fits, \n  .f = ~ as_tibble(\n    predict(\n      object = ., \n      s = lambdas_r, \n      newx = X_0\n    )\n  ) \n) %&gt;%\n  list_rbind() \n\n# Prognosen für Lasso-Regression\npred_l &lt;- map(\n  .x = lasso_fits, \n  .f = ~ as_tibble(\n    predict(\n      object = ., \n      s = lambdas_l, \n      newx = X_0)\n    ) \n) %&gt;%\n  list_rbind() \n\nFür die statistische Auswertung berechnen wir jeweils \\(\\text{MSE}(\\widehat{Y}_0)\\), \\(\\text{Bias}(\\widehat{Y}_0)^2\\) und \\(\\text{Var}(\\widehat{Y}_0)\\) und führen die Ergebnisse mit pivot_longer() in ein langes Format sim_data_r über. Wir berechnen weiterhin mit MSE_min_r das \\(\\lambda\\), für das wir über die Simulationsdurchläufe durchschnittlich den geringsten \\(\\text{MSE}\\) beobachten.\nRidge-Regression\n\n# Ergebnisse für Ridge-Regression zusammenfassen\nsim_data_r &lt;- tibble(\n  \n  lambda = lambdas_r,\n  \n  \"MSE\" = map_dbl(\n    .x = pred_r,  \n    .f = ~ mean((.x - Y_0)^2)\n  ),\n  \n  \"Bias^2\" = map_dbl(\n    .x = pred_r, \n    .f = ~ (mean(.x) - Y_0)^2\n  ),\n  \n  \"Varianz\" = map_dbl(\n    .x = pred_r, \n    .f = ~ var(.x)\n  )\n) %&gt;%\n  pivot_longer(\n    cols = -lambda, \n    values_to = \"Wert\",\n    names_to = \"Statistik\"\n  )\n\n# Lambda bei MSE-Minimum bestimmen\nMSE_min_r &lt;- sim_data_r %&gt;% \n  filter(\n    Statistik == \"MSE\",\n    Wert == min(Wert)\n  ) \n\nLasso-Regression\n\n# Ergebnisse zusammenfassen\nsim_data_l &lt;- tibble(\n  \n  lambda = lambdas_l,\n  \n  \"MSE\" = map_dbl(\n    .x = pred_l,  \n    .f = ~ mean((. - Y_0)^2)\n  ),\n  \n  \"Bias^2\" = map_dbl(\n    .x = pred_l, \n    .f = ~ (mean(.) - Y_0)^2\n  ),\n  \n  \"Varianz\" = map_dbl(\n    .x = pred_l, \n    .f = ~ var(.)\n  )\n) %&gt;%\n  pivot_longer(\n    cols = -lambda, \n    values_to = \"Wert\", \n    names_to = \"Statistik\"\n  )\n\n# Lambda bei MSE-Minimum bestimmen\nMSE_min_l &lt;- sim_data_l %&gt;% \n  filter(\n    Statistik == \"MSE\",\n    Wert == min(Wert)\n  ) \n\nDie Datensätze im langen Format, sim_data_r und sim_data_l, werden nun für die Visualisierung der Ergebnisse mit ggplo2 genutzt.\n\n# MSE, Bias^2 und Varianz gegen Lambda plotten\n\n# Ridge-Regression\nsim_data_r %&gt;%\n  ggplot(\n    mapping = aes(\n      x = lambda, \n      y = Wert, \n      color = Statistik\n    )\n  ) +\n  geom_line() +\n  geom_point(data = MSE_min_r)\n\n# Lasso-Regression\nsim_data_l %&gt;%\n  ggplot(\n    mapping = aes(\n      x = lambda, \n      y = Wert, \n      color = Statistik\n    )\n  ) +\n  geom_line() +\n  geom_point(data = MSE_min_l)\n\nAbbildung 5.9: Simulierte MSE-Komponenten in Abhängigkeit von Lambda\n\n\n\n\n(a) Ridge Regression\n\n\n\n\n\n\n\n(b) Lasso Regression\n\n\n\n\n\n\nAnhand von Abbildung 5.9 lässt sich der Bias-Variance-Tradeoff bei der Vorhersage von \\(Y_0\\) gut erkennen: Bereits für kleine \\(\\lambda\\) erzielen beide Methode eine deutliche Reduktion des MSE. Dies wir durch etwas zusätzlichen Bias, aber eine überproportionale Verringerung der Varianz erreicht. Der erkennbare funktionale Zusammenhang zeigt, dass der MSE eine konvexe Funktion von \\(\\lambda\\) ist. Damit existieren optimale \\(\\lambda\\) mit minimalem MSE (grüne Punkte), die wir mit Cross Validation schätzen können."
  },
  {
    "objectID": "RegReg.html#inferenz-für-treatment-effekt-schätzung-mit-vielen-variablen",
    "href": "RegReg.html#inferenz-für-treatment-effekt-schätzung-mit-vielen-variablen",
    "title": "\n5  Regularisierte Regression\n",
    "section": "\n5.4 Inferenz für Treatment-Effekt-Schätzung mit vielen Variablen",
    "text": "5.4 Inferenz für Treatment-Effekt-Schätzung mit vielen Variablen\nIn empirischen Studien des Effekts einer Behandlungsvariable \\(B\\) auf eine Outcome-Variable \\(Y\\) steht häufig eine Vielzahl potentieller Kontrollvariablen zur Verfügung. Häufig ist unklar, welche Variablen in das Modell aufgenommen werden sollten, um das Risiko einer verzerrten Schätzung durch ausgelassene Variablen zu vermindern und gleichzeitig eine Schätzung mit geringer Varianz zu gewährleisten. Ist der Beobachtungsumfang \\(N\\) relativ zur Variablenanzahl \\(k\\) groß, so kann die KQ-Schätzung einer langen Regression (ein Modell mit allen \\(k\\) Kontrollvariablen) gute Ergebnisse liefern. In der Praxis liegt diese wünschenswerte Situation jedoch oft nicht vor und es ist \\(k\\lesssim N\\) oder sogar \\(k&gt;N\\). Dann ist eine KQ-Schätzung des Behandlungseffekts anhand aller \\(k\\) Variablen mit hoher Varianz behaftet bzw. gar nicht möglich.19 Ein weiteres Szenario ist \\(k(N)&gt;N\\), d.h. die Anzahl der Regressoren kann mit dem Beobachtungsumfang wachsen.20 Lasso-Verfahren können dann hilfreich sein, um Determinanten von \\(Y\\) und \\(B\\) zu identifizieren und damit eine Menge an Kontrollvariablen zu selektieren, für die eine erwartungstreue und konsistente Schätzung des interessierenden Effekts wahrscheinlich ist.19 Beachte, dass der KQ-Schätzer bei \\(k&gt;N\\) nicht lösbar ist.20 Dieses Szenario wird unter Bedingungen bzgl. der Wachstumsrate und der Größe der Koeffizienten betrachet, s. (Belloni und Chernozhukov 2013).\nBetrachte zunächst das Modell mit allen Kontrollvariablen \\(X_j\\), \\[\\begin{align}\n  Y_i = \\beta_0 + \\alpha_0 B_i + \\sum_{j=1}^k \\beta_{j} X_{i,j} + u_i, \\label{eq:lassotmt}\n\\end{align}\\] wobei einige \\(\\beta_{j}=0\\) sind und wir annehmen, dass \\(B\\) lediglich mit ein paar der \\(X_j\\) korrelliert. Die Shrinkage der geschätzten Koeffizienten aus einer naiven Lasso-Regression von \\(\\eqref{eq:lassotmt}\\) führt grundsätzlich zu einer verzerrten Schätzung des Behandlungseffekts \\(\\alpha_0\\) und damit zu ungültiger Inferenz.2121 Hahn u. a. (2018) geben eine ausführliche Erläuterung dieser Problematik.\nDie Verzerrung von geschätzten Koeffizienten kann vermieden werden, indem Lasso lediglich zur Selektion von Kontrollvariablen verwendet wird. Dabei wird mit einer Lasso-Regression von \\(Y\\) auf die \\(X_j\\) eine Teilmenge von Regressoren \\(\\mathcal{S}\\) selektiert und der Treatment-Effekt anschließend mit der KQ-Schätzung von \\[\\begin{align}\n  Y_i = \\beta_0 + \\alpha_0 B_i + \\sum_{j\\in\\mathcal{S}} \\beta_{j} X_{i,j} + e_i,\n\\end{align}\\] basierend auf der Selektion \\(\\mathcal{S}\\) berechnet wird.22 Ein solcher Post-Lasso-Selection-Schätzer (Belloni und Chernozhukov 2013) ist jedoch im Allgemeinen und insbesondere in hoch-dimensionalen Settings nicht konsistent für \\(\\alpha_0\\) und nicht asymptotisch normalverteilt, da weiterhin die Gefahr einer verzerrten Schätzung durch in \\(\\mathcal{S}\\) ausgelassene Variablen besteht, die mit \\(B\\) korrelieren: Lasso selektiert Variablen \\(X_j\\), die “gut” \\(Y\\) erklären. Dabei kann nicht ausgeschlossen werden, das ein Modell gewählt wird, dass relevante Determinanten von \\(B\\) auslässt. Selbst wenn wir ein mit Lasso gewähltes Modell mit KQ (d.h. ohne Shrinkage) schätzen, würde \\(\\alpha_0\\) verzerrt geschätzt!22 Solche Verfahren werden Post-Selection-Schätzer gennant.\nBelloni, Chernozhukov, und Hansen (2014) schlagen ein alternatives Verfahren vor, dass auf Selektion der Determinanten \\(X_j\\) von \\(Y\\) und \\(B\\) basiert. Dieses Verfahren wird als Post-Double Selection bezeichnet und kann wiefolgt implementiert werden:\nPost-Double-Selection-Schätzer\n\nBestimme die Determinanten \\(X_j\\) von \\(Y\\) mit Lasso-Regression und bezeichne die Menge der selektierten Variablen als \\(\\mathcal{S}_Y\\).\nBestimme die Determinanten \\(X_j\\) von \\(B\\) mit Lasso-Regression und bezeichne die Menge der selektierten Variablen als \\(\\mathcal{S}_B\\).\nBestimme die Schnittmenge \\(\\mathcal{S}_{YB} = \\mathcal{S}_Y \\cap \\mathcal{S}_B\\). Schätze den Treatment-Effekt als \\(\\widehat{\\alpha}_0\\) in der KQ-Regression \\[\\begin{align}\n  Y_i = \\beta_0 + \\alpha_0 B_i + \\sum_{j\\in\\mathcal{S}_{YB}} \\beta_{j} X_{i,j} + v_i.\n\\end{align}\\]\n\nBelloni, Chernozhukov, und Hansen (2014) zeigen, dass \\(\\widehat{\\alpha}_0\\) aus diesem Verfahren ein asymptotisch normalverteiler Schätzer für \\(\\alpha_0\\) ist und herkömmliche t-Tests und Konfidenzintervalle gültige Inferenz erlauben.\nWir illustrieren die in diesem Abschnitt betrachteten Schätzer nun anhand simulierter Daten mit R. Die fiktive Problemstellung ist die Schätzung eines wahren Treatment-Effekts \\(\\alpha_0 = 2\\), wenn so viele potenzielle Kontrollvariablen vorliegen, dass der KQ-Schätzer gerade noch berechnet werden kann, aber aufgrund hoher Varianz unzuverlässig ist. Hierzu erzeugen wir \\(Y\\) gemäß der Vorschrift \\[\\begin{align*}\n  Y_i =&\\, \\alpha_0 B_i + \\sum_{j=1}^{k_Y} \\beta_{j}^Y X_{i,j}^Y + \\sum_{l=1}^{k_{YB}} \\beta_{l}^{YB} X_{i,l}^{YB} + u_i,\\\\\n  \\\\\n  \\beta_j^{YB} \\overset{u.i.v}{\\sim}&\\,N(10,1), \\quad \\beta_j^{Y} \\overset{u.i.v}{\\sim}U(0,1), \\quad u_i \\overset{u.i.v}{\\sim}N(0,1).\\\\\n  \\\\\n  i=&\\,1,\\dots,550\n\\end{align*}\\]\nDie Behandlungsvariable \\(B_i\\) entspricht der Vorschrift \\[\\begin{align*}\n  B_i =&\\, \\sum_{l=1}^{k_{YB}} \\beta_{l}^{YB} X_{i,l}^{YB} + e_i,\\\\\n  \\\\\n  \\beta_j^{YB} \\overset{u.i.v}{\\sim}&\\,N(2,0.2), \\quad e_i \\overset{u.i.v}{\\sim}N(0,1).\n\\end{align*}\\] Wir wählen \\(k_{YB} = k_{Y} = 25\\). Zusätzlich zu \\(B\\), den Determinanten von \\(Y\\) und \\(B\\) (\\(X^{YB}\\)) sowie den Variablen, die ausschließlich \\(Y\\) beeinflussen (\\(X^{Y}\\)) gibt es \\(k_U = 499\\) Variablen \\(X^U\\), die weder \\(Y\\) noch \\(B\\) beeinflussen und damit irrelevant für die Schätzung des Behandlungseffekts sind. Wir haben also \\(N=550\\) Beobachtungen und insgesamt \\(k = 1+k_{Y} + k_{YB} + k_{U} = 550\\) potenzielle Kontrollvariablen von denen \\(k_{YB} = 25\\) für eine unverzerrte Schätzung von \\(\\alpha_0\\) relevant sind.\nDer nachstehende Code generiert die Daten gemäß der Vorschrift.\n\nlibrary(mvtnorm)\nlibrary(tidyverse)\nset.seed(4321)\n\nn &lt;- 550      # Beobachtungen\np_Y &lt;- 25     # Determinanten Y\np_B &lt;- 25     # Determinanten B *und* Y\np_U &lt;- 499    # irrelevante Variablen \n\n# Variablen generieren\nXB &lt;- rmvnorm(n = n, sigma = diag(p_B))\nXU &lt;- rmvnorm(n = n, sigma = diag(p_U))\nXY &lt;- rmvnorm(n = n, sigma = diag(p_Y))\n\n# Stetige Behandlungsvariable erzeugen\nB &lt;- XB %*% rnorm(p_B, 2, sd = .2) + rnorm(n)\n\n# Abh. Variable erzeugen, Behandlungseffekt (ATE) ist 2\nY &lt;- 2 * B + \n  XB %*% rnorm(p_B, mean = 10) + \n  XY %*% runif(p_Y) + \n  rnorm(n)\n\n# Variablen in tibble sammeln\nX &lt;- cbind(B, XB, XU, XY) %&gt;% \n  as_tibble()\n\n# Namen zuweisen\ncolnames(X) &lt;- c(\n  \"B\", \n  paste0(\"XB\", 1:p_B), \n  paste0(\"XU\", 1:p_U),\n  paste0(\"XY\", 1:p_Y) \n)\n\nWünschenswert wäre die KQ-Schätzung des wahren Modells. Diese ergibt eine Schätzung nahe des wahren Treatment-Effekts \\(\\alpha_0 = 2\\). Unter realen Bedingungen wäre diese Regression jedoch nicht implementierbar, weil die relevanten Kovariablen XB unbekannt sind.\n\n# KQ: Wahres Modell schätzen\nlm(Y ~ B + XB - 1)$coefficients[\"B\"]\n\n       B \n1.937031 \n\n\nWir schätzen daher zunächst die “lange” Regression mit allen \\(k\\) verfügbaren Variablen mit KQ. Beachte, dass der KQ-Schätzer für \\(\\alpha_0\\) zwar implementierbar und erwartungstreu ist, jedoch eine hohe Varianz aufweist. Wegen \\(k=N=550\\) erhalten wir eine perfekte Anpassung an die Daten und können mangels Freiheitsgraden keine Hypothesentests durchführen.\n\n# KQ: Lange Regression schätzen\nlm(Y ~ . - 1, data = X)$coefficients[\"B\"]\n\n       B \n3.079497 \n\n\nDie KQ-Schätzung von \\(\\alpha_0\\) anhand der langen Regression weicht deutlich vom wahren Wert \\(\\alpha_0 = 2\\) ab.\nEine “kurze” KQ-Regression nur mit der Behandlungsvariable \\(B\\) führt wegen Korrelation mit den ausgelassenen Determinanten in XB zu einer deutlich verzerrten Schätzung.\n\n# KQ: Kurze Regression\nlm(Y ~ B - 1)$coefficients[\"B\"]\n\n       B \n6.716837 \n\n\nDie Methoden von Belloni und Chernozhukov (2013) und Belloni, Chernozhukov, und Hansen (2014) sind im R-Paket hdm implementiert. Mit den Funktionen hrm::rlasso() und hdm::rlassoEffect kann Lasso-Regression sowie Post- und Double-Post-Selection durchgeführt werden.2323 Diese Funktionen ermitteln ein optimales \\(\\lambda\\) mit dem in Belloni u. a. (2012) vorgeschlagenen Algorithmus.\nWir berechnen zunächst den naiven Lasso-Schätzer in einem Modell mit allen Variablen.\n\nlibrary(hdm)\n\n# Naiver Post-Lasso-Schätzer\nlasso &lt;- rlasso(\n  x = X, \n  y = Y, \n  intercept = F, \n  post = F\n)\n\n# Koeffizientenschätzer auslesen\nlasso$coefficients[\"B\"] \n\n       B \n6.368456 \n\n\nAuch dieser Schätzer ist deutlich verzerrt. Problematisch ist hier nicht nur die Shrinkage auf \\(\\widehat{\\alpha}_0\\), sondern die Selektion der Variablen in XB:\n\n# Welche Variablen in XB selektiert Lasso *nicht*?\nnselektiert &lt;- which(lasso$coef[1:26] == 0)   # ID\n\n# Namen auslesen\nnames(lasso$coef[1:26])[nselektiert]\n\n[1] \"XB8\"  \"XB10\" \"XB16\" \"XB18\" \"XB20\"\n\n\nDurch das Auslassen dieser Determinanten von \\(Y\\) und \\(B\\) leidet der Lasso-Schätzer unter OVB.\nAls nächstes berechnen wir den Post-Lasso-Selection-Schätzer.\n\n# Post-Lasso-Selection-Schätzer berechnen\np_lasso &lt;- rlasso(\n  x = X,\n  y = Y, \n  intercept = F, \n  post = T\n)\n\n# Schätzung für alpha_0\np_lasso$coef[\"B\"]\n\n       B \n6.362409 \n\n\nDie Ähnlichkeit der Post-Lasso-Schätzung von \\(\\alpha_0\\) zur Lasso-Schätzung zeigt deutlich, dass die Verzerrung des Lasso-Schätzers überwiegend durch ausgelassene Variablen anstatt durch Shrinkage verursacht wird.\nMit rlassoEffect() können wir den Post-Double-Selection-Schätzer berechnen.\n\n# Post-Double-Selection-Schätzer\npds_lasso &lt;- rlassoEffect(\n  x = X %&gt;% \n    dplyr::select(-B) %&gt;% \n    as.matrix(),\n  y = Y, \n  d = B, \n  method = \"double selection\"\n)\n\n# Schnittmenge der selektierten Determinanten \n# von Y und B\n(\n  S_BY &lt;- names(\n    which(pds_lasso$selection.index)\n  )\n)\n\n [1] \"XB1\"   \"XB2\"   \"XB3\"   \"XB4\"   \"XB5\"   \"XB6\"   \"XB7\"   \"XB8\"   \"XB9\"  \n[10] \"XB10\"  \"XB11\"  \"XB12\"  \"XB13\"  \"XB14\"  \"XB15\"  \"XB16\"  \"XB17\"  \"XB18\" \n[19] \"XB19\"  \"XB20\"  \"XB21\"  \"XB22\"  \"XB23\"  \"XB24\"  \"XB25\"  \"XU209\" \"XU241\"\n[28] \"XU295\" \"XY3\"   \"XY7\"   \"XY8\"   \"XY12\"  \"XY13\"  \"XY15\"  \"XY16\"  \"XY19\" \n[37] \"XY23\" \n\n\nDouble Selection führt ebenfalls zu einem Post-Lasso-KQ-Schätzer mit allen 25 relevaten Variablen in XB. Wir selektieren allerdings deutlich weniger irrelevante Variablen aus XU als mit Single Selection und dennoch einige Determinanten von \\(Y\\) aus XY. Double Selection führt also zu einer unverzerrten Schätzen mit geringerer Varianz. Mit summary() erhalten wir gültige Inferenz bzgl. des Treatment-Effekts.\n\nsummary(pds_lasso)\n\n[1] \"Estimates and significance testing of the effect of target variables\"\n   Estimate. Std. Error t value Pr(&gt;|t|)    \nd1   1.94977    0.07127   27.36   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDer Post-Double-Selection-Schätzer liefert unter den betrachteten Verfahren die beste Schätzung von \\(\\alpha_0\\) und erlaubt gülstige statistische Inferenz. Der geschätzte Effekt ist hoch-signifikant.\n\n\n\n\n\n\nKey Facts zum Post-Double-Selection-Schätzer\n\n\n\n\nDurch die sorgfältige Auswahl von Variablen, die mit Behandlung- und Outcome-Variable zusammenhängen, ermöglicht die Double-Selection eine bessere Kontrolle über das Risiko ausgelassender Variablen in Beobachtungsstudien und ermöglicht gültige (asymptotisch normale) Inferenz.\n\nDer Post-Double-Selection-Schätzer besteht aus drei Regressionen:\n\nEs werden Variablen mit Lasso selektiert, welche die Behandlungs-Variable erklären.\nEs werden Variablen mit Lasso selektiert, welche die Outcome-Variable erklären.\nDer Post-Double-Selection-Schätzer ist der KQ-Schätzer in einer Regression, die für die Schnittmenge der ausgewählten Variablen kontrolliert.\n\n\nDank der Selektion mit Lasso kann der Schätzer auch bei hoch-dimensionalen Daten (\\(k&gt;n\\)) angewendet werden.\nPost-Double-Selection-Schätzer für Behandlungseffekte sind im R-Paket hdm implementiert.\n\n\n\n\n5.4.1 Case Study: Makroökonomisches Wachstum\nZur Illustration des Post-Double-Selection Schätzers betrachten wir eine empirische Anwendung bzgl. der Validierung von makroökonomischer Wachstumtheorie. Aus neo-klassischen Ansätzen wie dem Solow-Swan-Modell kann die Hypothese, dass Volkswirtschaften zu einem gemeinsamen Wachstumspfad hin konvergieren, abgeleitet werden. Diese Konvergenzhypothese impliziert die Existenz von Aufholeffekten: Ärmere Volkswirtschaften müssen im mittel schneller Wachsen als die Wirschaft wohlhabender Länder. Die grundlegende Spezifikation eines entsprechenden Regressionsmodells lautet \\[\\begin{align}\n  \\text{WR}_{i} = \\alpha_0 \\text{BIP0}_i + u_i, \\label{eq:growthmodel1}\n\\end{align}\\] wobei \\(\\text{WR}_{i}\\) die Wachstumsrate des Pro-Kopf-BIP in Land \\(i\\) über einen Zeitraum (typischerweise berechnet als Log-Differenz zwischen zwei Perioden) und \\(\\text{BIP0}_i\\) das (logarithmierte) Pro-Kopf-BIP zu beginn der Referenzperiode ist. Gemäß der Konvergenzhypothese muss \\(\\alpha_0&lt;0\\) sein: Je wohlhabender eine Volkswirtschaft ist, desto geringer ist das Wirtschaftswachstum.\nUm Verzerrung durch ausgelassene Kovariablen zu vermeiden, sollte das Modell \\(\\eqref{eq:growthmodel1}\\) um länder-spezifische Regressoren \\(x_{i,j}\\), die sowohl das Ausgagnsniveau \\(\\text{BIP0}\\) sowie die Wachtumsrate beinflussen, erweitert werden. Zu der großen Menge potentieller Kovariablen gehören makro- und sozio-ökonomische Maße wie bspw. die Investitionstätigkeit des Staates, Offenheit der Volkswirtschaft, das politische Umfeld, das Bildungsniveau, die Demographie usw. Eine bevorzugte Spezifikation ist daher \\[\\begin{align}\n  \\text{WR}_{i} = \\alpha_0 \\text{BIP0}_i + \\sum_{j=1}^k \\beta_j x_{i,j} + u_i,\\label{eq:growthmodel2}\n\\end{align}\\] wobei \\(\\alpha_0\\) als Behandlungseffekt interpretiert werden kann. Beachte, dass \\(\\eqref{eq:growthmodel2}\\) eine Regression in der Form von \\(\\eqref{eq:lassotmt}\\) ist.\nWir illustrieren die Schätzung von und Inferenz bzgl. \\(\\alpha_0\\) in \\(\\eqref{eq:growthmodel2}\\) mit Post-Double-Selektion für einen 90 Länder umfassenden Auszug aus dem Datensatz von Barro und Lee (2013), der als Objekt GrowthData im R-Paket hdm verfügbar ist.2424 Eine ausführliche Beschreibung der Variablen ist hier einsehbar.\n\n# Datensatz in Arbeitsumgebung verfügbar machen\nlibrary(hdm)\ndata(GrowthData)\n\n# Anzahl Beobachtungen und Variablen\ndim(GrowthData)\n\n[1] 90 63\n\n\nDie Spalte Outcome ist die jeweilige Wachstumsrate des BIP zwischen den Perioden 1965-1975 und 1975-1985 und gdpsh465 ist das reale Pro-Kopf-BIP im Jahr 1965 zu Preisen von 1980.\nWir führen zunächst eine graphische Analyse hinsichtlich des Modells einfachen Modells \\(\\eqref{eq:growthmodel1}\\) durch, indem wir gdpsh465 gegen Outcome plotten und die geschätzte Regressionsgerade einzeichnen.\n\n# Einfache grafische Analyse mit ggplot2\nGrowthData %&gt;%\n  ggplot(\n    mapping = aes(\n      x = gdpsh465, \n      y = Outcome\n    )\n  ) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F)\n\n\n\nAbbildung 5.10: BIP-Wachstum: Einfache Regression\n\n\n\nAbbildung 5.10 zeigt einen geringen positiven geschätzten Effekt \\(\\widehat{\\alpha}_0\\). Eine Auswertung mit lm() ergibt, dass der Effekt \\(\\alpha_0\\) nicht signifikant von \\(0\\) verschieden ist.\n\n# Einfache Regression durchführen, \n# Inferenz für gdpsh465 erhalten\nlm(Outcome ~ gdpsh465, data = GrowthData) %&gt;%\n  summary() %&gt;%\n  coefficients() %&gt;% \n  .[2, ]\n\n   Estimate  Std. Error     t value    Pr(&gt;|t|) \n0.001316713 0.006102200 0.215776701 0.829661165 \n\n\nDer positive Effekt aus der einfachen Schätzung widerspricht der Konvergenzhypothese. Dieses Ergebnis könnte allerdings durch Auslassen relevanter Kovariablen ungültig sein. Beispielsweise ist es plausibel, dass das Bildungsniveau einer Volkswirtschaft sowohl mit dem BIP korreliert ist als auch die Wachstumsrate beeinflusst. Dann wäre das Bildungsniveau eine relevante Kovariable, deren Auslassen zu einer verzerrten Schätzung von \\(\\alpha_0\\) führt.\nEine “lange” Regression mit allen Kovariablen ist zwar möglich, aber problematisch: Das Verhältnis von Beobachtungen (90) zu Regressoren (62) bedeutet eine hohe Unsicherheit der Schätzung.\n\n# Inferenz für alpha_0 in langer Regression\nsummary(\n  lm(Outcome ~ . - 1 , data = GrowthData)\n  ) %&gt;% \n  coefficients() %&gt;% \n  .[2, ]\n\n    Estimate   Std. Error      t value     Pr(&gt;|t|) \n-0.009377989  0.029887726 -0.313773911  0.756018518 \n\n\nDer geschätzte Koeffizient \\(\\widehat{\\alpha}_0\\) ist nun zwar negativ, liefert jedoch weiterhin keine Evidenz, dass \\(\\alpha_0\\) von 0 verschieden ist. Ein Vergleich der Standardfehler zeigt aber, dass die KQ-Schätzung aufgrund Berücksichtigung aller potentiellen Kovariablen mit deutlich größerer Varianz behaftet ist als in der einfachen KQ-Regression \\(\\eqref{eq:growthmodel1}\\)\nPost-Double-Selection erlaubt gültige Inferenz bzgl. \\(\\alpha_0\\) nach Schätzung der Menge relevanter Kovariablen. Wir weisen die entsprechenden Variablen R-Objekten zu und berechnen den Schätzer.\n\n# Variablen für Post-Double-Selection vorbereiten\n\n# abh. Variable\ny &lt;- GrowthData %&gt;% \n  pull(Outcome)\n\n# \"Treatment\"\nd &lt;- GrowthData %&gt;% \n  pull(gdpsh465)\n\n# potentielle Regressoren\nX &lt;- GrowthData %&gt;% \n  dplyr::select(\n    -Outcome, -intercept, -gdpsh465\n  )\n\n\n# Post-Double-Selection-Schätzer berechnen\nGrowth_DS &lt;- \n  rlassoEffect(\n    x = X %&gt;% \n      as.matrix(), \n    y = y, \n    d = d, \n    method = \"double selection\"\n)\n\nPost-Double-Selection wählt aus der Menge potentieller Kovariablen lediglich sieben Regressoren aus.\n\n# Selektierte Variablen einsehen\n# ID\nSelektion &lt;- Growth_DS$selection.index\n\n# Namen auslesen\nnames(\n  which(Selektion == T)\n)\n\n[1] \"bmp1l\"    \"freetar\"  \"hm65\"     \"sf65\"     \"lifee065\" \"humanf65\" \"pop6565\" \n\n\nTabelle 5.3 zeigt die Definitionen der ausgewählten Variablen.\n\n\n\n\n\n\n\n\n  \nVariable\n      Beschreibung\n    \n\n\nbmp1l\nSchwarzmarktprämie d. Währung\n\n\nfreetar\nMaß für Zollbeschränkungen\n\n\nhm65\nEinschreibungsquote Uni (Männer) \n\n\nsf65\nBeschulungsquote Sekundarstufe (Frauen)\n\n\nlifee065\nLebenserwartung bei Geburt\n\n\nhumanf65\nDurschn. Bildung im Alter 25 (Frauen)\n\n\npop6565\nAnteil Bevölkerung ü. 65 Jahre\n\n\n\nTabelle 5.3:  Mit PDS selektierte Variablen aus GrowthData.\nReferenzjahr 1965. \n\n\n\n\n# Gültige Inferenz mit dem Post-Double-Selection-Schätzer\nsummary(Growth_DS)\n\n[1] \"Estimates and significance testing of the effect of target variables\"\n   Estimate. Std. Error t value Pr(&gt;|t|)   \nd1  -0.05001    0.01579  -3.167  0.00154 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDas Ergebnis der Post-Double-Selection-Schätzung unterstützt die (bedingte) Konvergenzhypothese mit einer signifikanten negativen Schätzung \\(\\widehat{\\alpha}_0\\approx-0.05\\).\n\n\n\n\n\n\nBarro, Robert J., und Jong Wha Lee. 2013. „A new data set of educational attainment in the world, 1950–2010“. Journal of Development Economics 104: 184–98. https://doi.org/https://doi.org/10.1016/j.jdeveco.2012.10.001.\n\n\nBelloni, Alexandre, Daniel Chen, Victor Chernozhukov, und Christian Hansen. 2012. „Sparse models and methods for optimal instruments with an application to eminent domain“. Econometrica 80 (6): 2369–429.\n\n\nBelloni, Alexandre, und Victor Chernozhukov. 2013. „Least squares after model selection in high-dimensional sparse models“. Bernoulli, 521–47.\n\n\nBelloni, Alexandre, Victor Chernozhukov, und Christian Hansen. 2014. „High-dimensional methods and inference on structural and treatment effects“. Journal of Economic Perspectives 28 (2): 29–50.\n\n\nCortez, Paulo, und Alice Maria Gonçalves Silva. 2008. „Using data mining to predict secondary school student performance“.\n\n\nEfron, Bradley, Trevor Hastie, Iain Johnstone, und Robert Tibshirani. 2004. „Least angle regression“.\n\n\nHahn, P Richard, Carlos M Carvalho, David Puelz, und Jingyu He. 2018. „Regularization and confounding in linear regression for treatment effect estimation“.\n\n\nHoerl, Arthur E, und Robert W Kennard. 1970. „Ridge regression: Biased estimation for nonorthogonal problems“. Technometrics 12 (1): 55–67.\n\n\nTibshirani, Robert. 1996. „Regression shrinkage and selection via the lasso“. Journal of the Royal Statistical Society Series B: Statistical Methodology 58 (1): 267–88."
  },
  {
    "objectID": "Literatur.html",
    "href": "Literatur.html",
    "title": "Literatur",
    "section": "",
    "text": "Abadie, Alberto, and Guido W. Imbens. 2008. “On the Failure of the\nBootstrap for Matching Estimators.” Econometrica. Journal of\nthe Econometric Society 76 (6): 1537–57. https://doi.org/10.3982/ECTA6474.\n\n\nAbadie, Alberto, and Jann Spiess. 2022. “Robust Post-Matching\nInference.” Journal of the American Statistical\nAssociation 117 (538): 983–95. https://doi.org/10.1080/01621459.2020.1840383.\n\n\nAustin, P. 2011. “An Introduction to Propensity Score Methods for\nReducing the Effects of Confounding in Observational Studies.”\nMultivariate Behavioral Research 46 (3): 399–424. https://doi.org/10.1080/00273171.2011.568786.\n\n\nAustin, Peter C., and Dylan S. Small. 2014. “The Use of\nBootstrapping When Using Propensity-Score Matching Without Replacement:\nA Simulation Study.” Statistics in Medicine 33 (24):\n4306–19. https://doi.org/10.1002/sim.6276.\n\n\nAustin, Peter C., and Elizabeth A. Stuart. 2017. “Estimating the\nEffect of Treatment on Binary Outcomes Using Full Matching on the\nPropensity Score.” Statistical Methods in Medical\nResearch 26 (6): 2505–25. https://doi.org/10.1177/0962280215601134.\n\n\nBarro, Robert J., and Jong Wha Lee. 2013. “A New Data Set of\nEducational Attainment in the World, 1950–2010.” Journal of\nDevelopment Economics 104: 184–98. https://doi.org/https://doi.org/10.1016/j.jdeveco.2012.10.001.\n\n\nBasten, Christoph, and Frank Betz. 2013. “Beyond Work Ethic:\nReligion, Individual, and Political Preferences.” American\nEconomic Journal: Economic Policy 5 (3): 67–91.\n\n\nBelloni, Alexandre, Daniel Chen, Victor Chernozhukov, and Christian\nHansen. 2012. “Sparse Models and Methods for Optimal Instruments\nwith an Application to Eminent Domain.” Econometrica 80\n(6): 2369–429.\n\n\nBelloni, Alexandre, and Victor Chernozhukov. 2013. “Least Squares\nAfter Model Selection in High-Dimensional Sparse Models.”\nBernoulli, 521–47.\n\n\nBelloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2014.\n“High-Dimensional Methods and Inference on Structural and\nTreatment Effects.” Journal of Economic Perspectives 28\n(2): 29–50.\n\n\nBodory, Hugo, Lorenzo Camponovo, Martin Huber, and Michael Lechner.\n2020. “The Finite Sample Performance of Inference Methods for\nPropensity Score Matching and Weighting Estimators.” Journal\nof Business & Economic Statistics. https://doi.org/10.2139/ssrn.2731969.\n\n\nCattaneo, Matias D, Michael Jansson, and Xinwei Ma. 2020. “Simple\nLocal Polynomial Density Estimators.” Journal of the American\nStatistical Association 115 (531): 1449–55.\n\n\nCortez, Paulo, and Alice Maria Gonçalves Silva. 2008. “Using Data\nMining to Predict Secondary School Student Performance.”\n\n\nEfron, Bradley, Trevor Hastie, Iain Johnstone, and Robert Tibshirani.\n2004. “Least Angle Regression.”\n\n\nGelman, Andrew, and Guido Imbens. 2019. “Why High-Order\nPolynomials Should Not Be Used in Regression Discontinuity\nDesigns.” Journal of Business & Economic Statistics\n37 (3): 447–56.\n\n\nHahn, P Richard, Carlos M Carvalho, David Puelz, and Jingyu He. 2018.\n“Regularization and Confounding in Linear Regression for Treatment\nEffect Estimation.”\n\n\nHájek, J. 1971. “Comment on ‘an Essay on the Logical\nFoundations of Survey Sampling’ by Basu, d.”\nFoundations of Statistical Inference 236.\n\n\nHill, Jennifer, and Jerome P. Reiter. 2006. “Interval Estimation\nfor Treatment Effects Using Propensity Score Matching. Statistics in\nMedicine.” Statistics in Medicine 25 (13): 2230–56. https://doi.org/10.1002/sim.2277.\n\n\nHirano, Keisuke, Guido Imbens, and Geert Ridder. 2003. “Efficient\nEstimation of Average Treatment Effects Using the Estimated Propensity\nScore.” Econometrica 71 (4): 1161–89. https://doi.org/10.1111/1468-0262.00442.\n\n\nHoerl, Arthur E, and Robert W Kennard. 1970. “Ridge regression: Biased estimation for nonorthogonal\nproblems.” Technometrics 12 (1): 55–67.\n\n\nImbens. 2016. “Matching on the Estimated Propensity Score.”\nEconometrica 84 (2): 781–807. https://doi.org/10.3982/ecta11293.\n\n\nImbens, G. W., and Thomas Lemieux. 2008. “Regression Discontinuity\nDesigns: A Guide to Practice.” Journal of Econometrics\n142 (2): 615–35.\n\n\nImbens, Guido, and Karthik Kalyanaraman. 2012. “Optimal Bandwidth\nChoice for the Regression Discontinuity Estimator.” The\nReview of Economic Studies 79 (3): 933–59.\n\n\nLee, David S. 2008. “Randomized Experiments from Non-Random\nSelection in US House Elections.” Journal of\nEconometrics 142 (2): 675–97.\n\n\nLove, Thomas. 2004. “Graphical Display of Covariate\nBalance.” Presentation.\n\n\nMcCrary, Justin. 2008. “Manipulation of the Running Variable in\nthe Regression Discontinuity Design: A Density Test.” Journal\nof Econometrics 142 (2): 698–714.\n\n\nRosenbaum, Paul R., and Donald R. Rubin. 1983. “The Central Role\nof the Propensity Score in Observational Studies for Causal\nEffects.” Biometrika 70 (1): 170–84. https://doi.org/10.1017/cbo9780511810725.016.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via\nthe Lasso.” Journal of the Royal Statistical Society Series\nB: Statistical Methodology 58 (1): 267–88.\n\n\nWeber, Max. 2004. Die Protestantische Ethik Und Der Geist Des\nKapitalismus. Vol. 1614. CH Beck."
  },
  {
    "objectID": "Matching.html#cluster-robuste-standardfehler",
    "href": "Matching.html#cluster-robuste-standardfehler",
    "title": "\n3  Matching\n",
    "section": "\n3.4 Cluster Robuste Standardfehler",
    "text": "3.4 Cluster Robuste Standardfehler\nFür Matching-Verfahren sind die mit summary() berechneten Standardfehler (und damit auch Konfidenzintervalle, t-Statistiken und p-Werte) für den Behandlungseffekt grundsätzlich ungültig. Je nach Matching-Verfahren liegen unterschiedliche Quellen von Schätzunsicherheit vor, die bei der Berechnung von Standardfehlern zusätzlich zu der “üblichen” Stichproben-Variabilität berücksichtig werden müssen, bspw. aufgrund der Schätzung von Propensity Scores und des Matching-Prozesses ansich. Wie Standardfehler für Matching-Schätzer von Behandlungseffekten berechnet werden sollten, ist Gegenstand aktueller Forschung. P. C. Austin und Small (2014) und Abadie und Spiess (2022) belegen die Gültigkeit von cluster-robusten Standardfehlern mit Clustering auf Ebene der Beobachtungsgruppen (subclass) bei Matching ohne Zurücklegen.\nFür Matching anhand von Propensity Scores (mit Zurücklegen) zeigen Imbens (2016), dass ignorieren der zusätzlichen Unsicherheit durch die Schätzung der Propensity Scores zu konservativer Inferenz für den ATE anhand eines cluster-robusten Standardfehlerschätzers führt, jedoch ungültige Inferenz für die Schätzung des ATT bedeuten kann. Ähnlich zu P. C. Austin und Small (2014) deuten die Ergebnisse der Simulationsstudie von Bodory u. a. (2020) auf grundsätzlich bessere Eigenschaften der Schätzung hin, wenn die Standardfehler nicht für die Schätzung der Propensity Scores adjustiert werden.\nWeiterhin ist die Kontrolle für weitere Kovariablen mit Erklärungskraft für die Outcome-Variable und für die Matching-Variablen (wie oben erfolgt) in Regressionen für die Schätzung des Behandlungseffekts nach Matching eine etablierte Strategie, vgl. Hill und Reiter (2006) und Abadie und Spiess (2022). So können die Varianz der Schätzung und das Risiko einer Verzerrung der Standardfehler aufgrund verbleibender Imbalance von Behandlungs- und Kontrollgruppe nach Matching verringert werden.\nFür (cluster)-robuste Inferenz und eine vereinfachte tabellarische Zusammenfassung nutzen wir die Pakete marginaleffects und modelsummary. Mit marginaleffects::avg_comparisons() können p-Werte und Kofindenzintervalle basierend unter Berücksichtigung robuster Standardfehler und Gewichte aus dem Matching-Verfahren berechnet werden.\n\nlibrary(marginaleffects)\n\n# Inferenz: Multiple Regression, ungematchter Datensatz\n(\n  sum_orig &lt;- avg_comparisons(\n    model = ATT_mod_org,\n    variables = \"dark_mode\",\n    # Heteroskedastie-robuste SE:\n    vcov = \"HC3\", \n    # Identifizierung der Kontrollgruppe:\n    newdata = subset(darkmode, dark_mode == 1) \n  )\n) \n\n\n      Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n dark_mode    1 - 0     1.39      0.537 2.58  0.00988 6.7 0.333   2.44\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\n\n# Inferenz: Multiple Regression bei CEM\n(\n  sum_CEM &lt;- avg_comparisons(\n  model = ATT_mod_CEM ,\n  variables = \"dark_mode\",\n  # Cluster-robuste SE\n  vcov = ~ subclass, \n  newdata = subset(darkmode_matched_CEM, dark_mode == 1),\n  wts = \"weights\"\n  )\n)\n\n\n      Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n dark_mode    1 - 0     1.65      0.549 3.01  0.00262 8.6 0.576   2.73\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\n\n# Inferenz: Multiple Regression bei PSM\n(\n  sum_PSC &lt;- avg_comparisons(\n    model = ATT_mod_PSC ,\n    variables = \"dark_mode\",\n    vcov = ~ subclass, \n    newdata = subset(darkmode_matched_PSC, dark_mode == 1),\n    wts = \"weights\"\n  )\n)\n\n\n      Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n dark_mode    1 - 0     1.63      0.565 2.89  0.00381 8.0 0.527   2.74\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nWir fassen die Ergebnisse mit modelsummary::modelsummary() tabellarisch zusammen.\n\nlibrary(modelsummary)\nmodelsummary(\n  models = list(\n   \"Kein Matching\" = sum_orig, \n   \"Coarsened Exact\" = sum_CEM, \n  \"Propensity Scores\" = sum_PSC\n  ),\n  stars = T, \n  gof_map = \"nobs\", \n  output = \"gt\"\n) %&gt;%\n  tabopts()\n\n\n\n\n\n\n \n      Kein Matching\n      Coarsened Exact\n      Propensity Scores\n    \n\n\ndark_mode\n1.386**\n1.653**\n1.635**\n\n\n\n(0.537)\n(0.549)\n(0.565)\n\n\nNum.Obs.\n300\n164\n208\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "Matching.html#bootstrap-schätzung-den-att-bei-matching",
    "href": "Matching.html#bootstrap-schätzung-den-att-bei-matching",
    "title": "\n3  Matching\n",
    "section": "\n3.5 Bootstrap-Schätzung den ATT bei Matching",
    "text": "3.5 Bootstrap-Schätzung den ATT bei Matching\nBei Matching mit Zurücklegen besteht zusätzliche Unsicherheit durch Zurücklegen, d.h. Beobachtungen aus der Kontroll-Gruppe können mehrfach als Match für Beobachtungen aus der Treatment-Gruppe genutzt werden. Mit summary() berechnete Standardfehler berücksichtigen dies nicht!\nEin Bootstrap-Verfahren generiert mit Resampling (wiederholtes Ziehen mit Zurücklegen) aus dem Original-Datensatz (viele) künstliche Datensätze, für die der Schätzer (d.h. das gesamte Verfahren inkl. Matching!) jeweils berechnet wird. Die Verteilung der so gewonnenen Bootstrap-Schätzwerte approximiert die wahre, unbekannte Stichprobenverteilung des Schätzers des Behandlungseffekts. Mit dieser simulierten Verteilung können wir Inferenz betreiben: Wir können einen Bootstrap-Punktschätzer des Behandlungseffekts (Stichprobenmittel der Bootstrap-Schätzungen) sowie Standardfehler (Standardabweichung der der Bootstrap-Schätzungen) und p-Werte berechnen.\nWir Implementieren nun einen Bootstrap-Schätzer des ATT als R-Funktion boot_fun().\n\nboot_fun &lt;- function(data, i) {\n  \n  boot_data &lt;- data[i, ]\n  \n  # 1:1 PS Matching _mit_ Zurücklegen\n  match_res &lt;- matchit(\n    dark_mode ~ age + hours + male,\n    estimand = \"ATT\",\n    distance = \"glm\", \n    method = \"nearest\", \n    caliper = .3,\n    data = boot_data,\n    # mit Zurücklegen:\n    replace = TRUE\n  ) \n  \n  # Gematchten Datensatz zuweisen\n  darkmode_matched &lt;- match.data(match_res, data = boot_data)\n  \n  # Outcome-Modell schätzen\n  ATT_mod &lt;- lm(\n    formula = read_time ~ age + male + hours + dark_mode,\n    data = darkmode_matched, \n    weights = weights # hier teilweise &gt; 1 wg. Matching mit Zurücklegen!\n  )\n  \n  #  ATT-Schätzer auslesen\n  return(\n    ATT_mod$coefficients[\"dark_mode\"]  \n  )\n}\n\nAbadie & Imbens (2008) zeigen analytisch, dass ein Standard-Bootstrap bei Matching grundsätzlich ungültig ist: Die unbekannte Varianz der Stichprobenverteilung des Matching-Schätzers (und damit der Standardfehler des Schätzers) kann durch den Bootstrap nicht repliziert werden. Problematisch hierbei sind grundsätzlich zu liberale (d.h. zu große) mit dem Bootstrap berechnete Standardfehler. Es gibt jedoch Simulationsnachweise die zeigen, dass Bootstrap-Standardfehler bei Matching mit Zurücklegen konservativ sind (Bodory et al., 2020), also tendentiell zu kleine Standardfehler produzieren und damit das gewünschte nominale Signifikanzniveau eines Bootstrap-Hypothesentests nicht überschritten wird.\nWir berechnen nun eine Bootstrap-Schätzung des ATT von dark_mode auf readingtime sowie den zugehörigen Standardfehler und ein 95%-KI mit der zuvor definierten Funktion boot_fun.\n\nlibrary(\"boot\")\nset.seed(4321)\nboot_out &lt;- boot(darkmode, boot_fun, R = 999)\n\nboot_out\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = darkmode, statistic = boot_fun, R = 999)\n\n\nBootstrap Statistics :\n    original    bias    std. error\nt1* 1.307298 -0.163572   0.8561828\n\n\n\n# Bootstrap-Schätzer für den Treatment-Effekt\nmean(boot_out$t) \n\n[1] 1.143725\n\n# = mean(t0) + bias = mean(Bootstrap_samples)\n# vgl. 't0 = boot_fun(darkmode, i = 1:1e3)'\n\n# Bootstrap-Standardfehler\nsd(boot_out$t)\n\n[1] 0.8561828\n\n# 95% Bootstrap-KI für den Treatment-Effekt\nboot.ci(boot_out, type = \"perc\")\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 999 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_out, type = \"perc\")\n\nIntervals : \nLevel     Percentile     \n95%   (-0.660,  2.721 )  \nCalculations and Intervals on Original Scale"
  },
  {
    "objectID": "Matching.html#bootstrap-schätzung-des-att-bei-matching",
    "href": "Matching.html#bootstrap-schätzung-des-att-bei-matching",
    "title": "\n3  Matching\n",
    "section": "\n3.4 Bootstrap-Schätzung des ATT bei Matching",
    "text": "3.4 Bootstrap-Schätzung des ATT bei Matching\nBei Matching mit Zurücklegen besteht zusätzliche Unsicherheit durch Zurücklegen, da Beobachtungen aus der Kontroll-Gruppe mehrfach als Match für Beobachtungen aus der Treatment-Gruppe genutzt werden. Dieser zusätzliche Faktor für die Unsicherheit bei der Schätzung des Behandlungseffekt wird in der von summary() verwendeten Formel für den Standardfehler ebenfalls nicht berücksichtigt.\nEin Bootstrap-Verfahren generiert mit Resampling (wiederholtes Ziehen mit Zurücklegen) aus dem Original-Datensatz (viele) künstliche Datensätze, für die der Schätzer (d.h. das gesamte Verfahren inkl. Matching!) jeweils berechnet wird. Die Verteilung der so gewonnenen Bootstrap-Schätzwerte approximiert die wahre, unbekannte Stichprobenverteilung des Schätzers des Behandlungseffekts. Mit dieser simulierten Verteilung können wir Inferenz betreiben: Wir können einen Bootstrap-Punktschätzer des Behandlungseffekts (Stichprobenmittel der Bootstrap-Schätzungen) sowie Standardfehler (Standardabweichung der der Bootstrap-Schätzungen) und p-Werte berechnen.\nAlgorithmus: Bootstrap Schätzer des ATT bei Propensity-Score-Matching\n\nGeneriere eine Bootstrap-Stichprobe durch \\(N\\) Züge mit Zurücklegen aus der \\(N\\)-elementigen originalen Stichprobe.\nWende das Matching-Verfahren für die Bootstrap-Stichprobe durch. Schätze den Behandlungseffekt \\(\\beta\\) anhand der resultierenden Stichprobe und speichere den Punktschätzer des Behandlungseffekts \\(\\widehat{\\beta}_b^*\\).\nFürhre die Schritte 1 und 2 für \\(b=1,\\dots,B\\) aus, wobei \\(B\\) eine hinreichend große Anzahl von Bootstrap-Replikationen ist.\nBerechne den Bootstrap-Schätzer des Behandlungseffekts \\(\\overline{\\beta}^* = \\frac{1}{B}\\sum_{b=1} \\widehat{\\beta}_b^*\\) und den Standardfehler \\(\\text{SE}(\\overline{\\beta}^*) = \\sqrt{\\frac{1}{B-1}\\sum_{b=1}^B(\\widehat{\\beta}_b^*-\\overline{\\beta}^*)^2}\\). Berechne Inferenz-Statistiken mit den üblichen Formeln.\n\nWir Implementieren nun einen Bootstrap-Schätzer des ATT als R-Funktion boot_fun().\n\nboot_fun &lt;- function(data, i) {\n  \n  boot_data &lt;- data[i, ]\n  \n  # 1:1 PS Matching _mit_ Zurücklegen\n  match_res &lt;- matchit(\n    dark_mode ~ age + hours + male,\n    estimand = \"ATT\",\n    distance = \"glm\", \n    method = \"nearest\", \n    caliper = .3,\n    data = boot_data,\n    # mit Zurücklegen:\n    replace = TRUE\n  ) \n  \n  # Gematchten Datensatz zuweisen\n  darkmode_matched &lt;- match.data(match_res, data = boot_data)\n  \n  # Outcome-Modell schätzen\n  ATT_mod &lt;- lm(\n    formula = read_time ~ age + male + hours + dark_mode,\n    data = darkmode_matched, \n    weights = weights # hier teilweise &gt; 1 wg. Matching mit Zurücklegen!\n  )\n  \n  #  ATT-Schätzer auslesen\n  return(\n    ATT_mod$coefficients[\"dark_mode\"]  \n  )\n}\n\nAbadie & Imbens (2008) zeigen analytisch, dass ein Standard-Bootstrap bei Matching grundsätzlich ungültig ist: Die unbekannte Varianz der Stichprobenverteilung des Matching-Schätzers (und damit der Standardfehler des Schätzers) kann durch den Bootstrap nicht repliziert werden. Problematisch hierbei sind grundsätzlich zu liberale (d.h. zu große) mit dem Bootstrap berechnete Standardfehler. Es gibt jedoch Simulationsnachweise die zeigen, dass Bootstrap-Standardfehler bei Matching mit Zurücklegen konservativ sind (Bodory et al., 2020), also tendentiell zu kleine Standardfehler produzieren und damit das gewünschte nominale Signifikanzniveau eines Bootstrap-Hypothesentests nicht überschritten wird.\nWir berechnen nun eine Bootstrap-Schätzung des ATT von dark_mode auf readingtime sowie den zugehörigen Standardfehler und ein 95%-KI mit der zuvor definierten Funktion boot_fun.\n\nlibrary(\"boot\")\nset.seed(4321)\nboot_out &lt;- boot(darkmode, boot_fun, R = 999)\n\nboot_out\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = darkmode, statistic = boot_fun, R = 999)\n\n\nBootstrap Statistics :\n    original    bias    std. error\nt1* 1.307298 -0.163572   0.8561828\n\n\n\n# Bootstrap-Schätzer für den Treatment-Effekt\nmean(boot_out$t) \n\n[1] 1.143725\n\n# = mean(t0) + bias = mean(Bootstrap_samples)\n# vgl. 't0 = boot_fun(darkmode, i = 1:1e3)'\n\n# Bootstrap-Standardfehler\nsd(boot_out$t)\n\n[1] 0.8561828\n\n# 95% Bootstrap-KI für den Treatment-Effekt\nboot.ci(boot_out, type = \"perc\")\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 999 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_out, type = \"perc\")\n\nIntervals : \nLevel     Percentile     \n95%   (-0.660,  2.721 )  \nCalculations and Intervals on Original Scale"
  },
  {
    "objectID": "Matching.html#bootstrap-schätzung-kausaler-effekte-bei-matching",
    "href": "Matching.html#bootstrap-schätzung-kausaler-effekte-bei-matching",
    "title": "\n3  Matching\n",
    "section": "\n3.4 Bootstrap-Schätzung kausaler Effekte bei Matching",
    "text": "3.4 Bootstrap-Schätzung kausaler Effekte bei Matching\nEin Bootstrap-Verfahren generiert mit Resampling (wiederholtes Ziehen mit Zurücklegen) aus dem Original-Datensatz (viele) künstliche Datensätze, für die der Schätzer (d.h. das gesamte Verfahren inkl. Matching!) jeweils berechnet wird. Die Verteilung der so gewonnenen Bootstrap-Schätzwerte approximiert die wahre, unbekannte Stichprobenverteilung des Schätzers des Behandlungseffekts. Mit dieser simulierten Verteilung können wir Inferenz betreiben: Wir können einen Bootstrap-Punktschätzer des Behandlungseffekts (Stichprobenmittel der Bootstrap-Schätzungen) sowie Standardfehler (Standardabweichung der der Bootstrap-Schätzungen) und p-Werte berechnen.\nDer Bootstrap kann hilfreich sein, wenn unklar ist, wie Standardfehler für die Unsicherheit des Matching-Prozesses zu adjustieren sind, um gültige Inferenzsstatistiken zu erhalten. Abadie und Imbens (2008) zeigen analytisch, dass der Standard-Bootstrap die Stichprobenverteilung für Schätzer kausaler Effekte anhand von gematchten Datensätzen (d.h. bei Zuordnung/Selektion von Beobachtungen mit Matching) nicht korrekt abbilden kann. Grundsätzlich problematisch hierbei ist, wenn der Bootstrap eine verzerrte Schätzung produziert und/oder zu kleine Standardfehler liefert. Abadie und Imbens (2008) belegen die Tendenz des Bootstraps zu konservative (d.h. zu große) Standardfehler zu produzieren. Simulationsnachweise (Bodory u. a. 2020; Hill und Reiter 2006; P. C. Austin und Small 2014; P. C. Austin und Stuart 2017) finden, dass Bootstrap-Standardfehler u.a. bei Propensity Score Matching mit Zurücklegen leicht konservativ sind somit das gewünschte nominale Signifikanzniveau eines Bootstrap-Hypothesentests nicht überschritten wird, weshalb der Standard-Bootstrap trotz seiner Schwächen in der empirischen Forschung oft angewendet wird.\nWir betrachen als nächstes einen Bootstrap-Algorithmus für Inferenz bezüglich eines kausalen Effekts nach Matching und demonstrieren die Schätzung anhand unseres Website-Beispiels für den ATT nach Propensity-Score-Matching.\nAlgorithmus: Bootstrap-Schätzer für Propensity-Score-Matching\n\nGeneriere eine Bootstrap-Stichprobe durch \\(N\\) Züge mit Zurücklegen aus der \\(N\\)-elementigen originalen Stichprobe.\nWende das Matching-Verfahren für die Bootstrap-Stichprobe an. Schätze den Behandlungseffekt \\(\\beta\\) anhand der gematchten Stichprobe mit Regression. Speichere den Punktschätzer des Behandlungseffekts \\(\\widehat{\\beta}_b^*\\).\nFürhre die Schritte 1 und 2 für \\(b=1,\\dots,B\\) aus, wobei \\(B\\) eine hinreichend große Anzahl von Bootstrap-Replikationen ist.\nBerechne den Bootstrap-Schätzer des Behandlungseffekts \\(\\overline{\\beta}^* = \\frac{1}{B}\\sum_{b=1} \\widehat{\\beta}_b^*\\) und den Standardfehler \\(\\text{SE}(\\overline{\\beta}^*) = \\sqrt{\\frac{1}{B-1}\\sum_{b=1}^B(\\widehat{\\beta}_b^*-\\overline{\\beta}^*)^2}\\). Berechne Inferenz-Statistiken mit den üblichen Formeln.\n\nWir Implementieren nun einen Bootstrap-Schätzer des ATT im Website-Beispiel für Propensity-Score-Matching. Hierzu definieren wir eine R-Funktion boot_fun() für die Schritte 1 und 2 im obigen Algorithmus.\n\n# Bootstrap-Funktion für Schritte 1 und 2\nboot_fun &lt;- function(\n    data, # originale Stichprobe\n    i     # Indexmenge f. Bootstrap-Stichprobe\n) {\n  \n  # Bootstrap-Stichprobe\n  boot_data &lt;- data[i, ]\n  \n  # 1:1 PS Matching\n  match_res &lt;- matchit(\n    dark_mode ~ age + hours + male,\n    estimand = \"ATT\",\n    distance = \"glm\", \n    method = \"nearest\", \n    caliper = .25,\n    data = boot_data\n  ) \n  \n  # Gematchten Datensatz zuweisen\n  darkmode_matched &lt;- match.data(\n    object = match_res, \n    data = boot_data\n  )\n  \n  # Outcome-Modell schätzen\n  ATT_mod &lt;- lm(\n    formula = read_time ~ age + male + hours + dark_mode,\n    data = darkmode_matched, \n    # Gewichte hier teilweise &gt; 1: Matching mit Zurücklegen!\n    weights = weights \n  )\n  \n  #  ATT-Schätzung zurückgeben\n  return(\n    ATT_mod$coefficients[\"dark_mode\"]  \n  )\n}\n\nWir berechnen nun eine Bootstrap-Schätzung des ATT von dark_mode auf readingtime sowie den zugehörigen Standardfehler und ein 95%-KI mit der zuvor definierten Funktion boot_fun.\n\nlibrary(\"boot\")\nset.seed(4321)\n\n# Anz. Bootstrap-Replikationen\nB &lt;- 999\n\n# Bootstrap durchführen\n(\n  boot_out &lt;- boot(darkmode, boot_fun, R = B)\n)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = darkmode, statistic = boot_fun, R = B)\n\n\nBootstrap Statistics :\n    original      bias    std. error\nt1* 1.634664 -0.08144129   0.5935746\n\n\nDen Bootstrap-Schätzer des ATT sowie den Bootstrap-Standardfehler berechnen wir mit mean() und sd() anhand der 999 Bootstrap-Replikationen in boot_out$t.\n\n# Bootstrap-Schätzer für den Treatment-Effekt\nmean(boot_out$t) \n\n[1] 1.553222\n\n# = mean(t0) + bias = mean(Bootstrap_samples)\n# vgl. 't0 = boot_fun(darkmode, i = 1:1e3)'\n\n# Bootstrap-Standardfehler\nsd(boot_out$t)\n\n[1] 0.5935746\n\n\nEin 95%-Konfidenzintervall für den kausalen Effekt erhalten wir mit boot::boot.ci().77 type = \"bca\" (bias-corrected accelerated) ist eine gängige Implementierung für die Berechnung des Konfidenz-Intervalls.\n\n# 95% Bootstrap-KI für den Treatment-Effekt\nboot.ci(\n  boot.out = boot_out, \n  type = \"bca\", \n  conf = .95\n)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 999 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_out, conf = 0.95, type = \"bca\")\n\nIntervals : \nLevel       BCa          \n95%   ( 0.527,  2.839 )  \nCalculations and Intervals on Original Scale\n\n\nBeachte, dass der Bootstrap-Standardfehler sowie das Bootstrap-Konfidenzintervall nahe der mit avg_comarisons berechneten Werte für sum_PSC sind."
  },
  {
    "objectID": "Matching.html#schätzung-und-inferenz-für-den-behandlungseffekt-nach-matching",
    "href": "Matching.html#schätzung-und-inferenz-für-den-behandlungseffekt-nach-matching",
    "title": "\n3  Matching\n",
    "section": "\n3.3 Schätzung und Inferenz für den Behandlungseffekt nach Matching",
    "text": "3.3 Schätzung und Inferenz für den Behandlungseffekt nach Matching\nWir schätzen nun den Behandlungseffekt von dark_mode auf read_time für die mit CEM und Propensity Score Matching ermittelten Datensätzen und vergleichen die Ergebniss anschließend mit einer Regressionsschätzung ohne Matching.\nWir kombinieren die Matching-Verfahren mit linearer Regression, d.h. wir Schätzen den Behandlungseffekt anhand es gematchten Datensatzes als Mittelwertdifferenz nach zusätzlicher Kontrolle für die Matching-Variablen. Diese Kombination von Matching mit Regression wird in der Literatur als Regression Adjustment bezeichnet und ist insbesondere hilfreich, wenn Backdoors mit Matching geschlossen werden sollen, der kausale Effekt jedoch nur unter Verwendung einer nicht-trivialen Regressionsfunktion ermittelt werden kann. Zum Beispiel kann bei einer kontinuierlichen Behandlungsvariable und einem nicht-linearen Zusammenhang mit \\(Y\\) der kausale Effekt nicht durch einen bloßen Mittelwertvergleich erfasst werden, sondern erfordert eine adäquate Modellierung dieses Zusammenhangs in der Regressionsfunktion. Die zusätzliche Kontrolle für Matching-Variablen kann die Varianz der Schätzung verringern und das Risiko einer verzerrten Schätzung abmildern, falls nach Matching noch Unterschiede in der Balance von Behandlungs- und Kontrollgruppe vorliegen.\n\n# ATT mit linearem Modell schätzen: CEM Datensatz\nATT_mod_CEM &lt;- lm(\n  formula = read_time ~ age + male + hours + dark_mode,\n  data = darkmode_matched_CEM, \n  weights = weights \n)\n\nsummary(ATT_mod_CEM)\n\n\nCall:\nlm(formula = read_time ~ age + male + hours + dark_mode, data = darkmode_matched_CEM, \n    weights = weights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.9310 -2.3856 -0.0194  2.5407 14.0020 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 16.6088953  1.5255529  10.887  &lt; 2e-16 ***\nage          0.0380982  0.0344699   1.105  0.27072    \nmale        -4.0915725  0.6858131  -5.966 1.53e-08 ***\nhours        0.0050129  0.0006977   7.185 2.45e-11 ***\ndark_mode    1.6532234  0.6149439   2.688  0.00794 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.937 on 159 degrees of freedom\nMultiple R-squared:  0.414, Adjusted R-squared:  0.3992 \nF-statistic: 28.08 on 4 and 159 DF,  p-value: &lt; 2.2e-16\n\n\n\n# Datensatz für Propensity Score Matching zuweisen\ndarkmode_matched_PSC &lt;- match.data(res_PSC)\n\n# ATT mit linearem Modell schätzen: PSM Datensatz\nATT_mod_PSC &lt;- lm(\n  formula = read_time ~ age + male + hours + dark_mode,\n  data = darkmode_matched_PSC, \n  weights = weights \n)\n\nsummary(ATT_mod_PSC)\n\n\nCall:\nlm(formula = read_time ~ age + male + hours + dark_mode, data = darkmode_matched_PSC, \n    weights = weights)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.972 -2.605 -0.044  2.587 14.951 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.3654519  1.2762648  13.606  &lt; 2e-16 ***\nage          0.0283941  0.0301484   0.942  0.34741    \nmale        -3.9858702  0.6480072  -6.151 4.03e-09 ***\nhours        0.0046119  0.0006685   6.899 6.53e-11 ***\ndark_mode    1.6346636  0.5769812   2.833  0.00507 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.141 on 203 degrees of freedom\nMultiple R-squared:  0.3171,    Adjusted R-squared:  0.3037 \nF-statistic: 23.57 on 4 and 203 DF,  p-value: 5.098e-16\n\n\n\n# ATT mit linearem Modell ohne Matching\nATT_mod_org &lt;- lm(\n  formula = read_time ~ age + male + hours + dark_mode,\n  data = darkmode\n)\n\nsummary(ATT_mod_org)\n\n\nCall:\nlm(formula = read_time ~ age + male + hours + dark_mode, data = darkmode)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.2697  -2.6710   0.0164   2.5909  14.5739 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 16.859075   1.082303  15.577  &lt; 2e-16 ***\nage          0.051332   0.022215   2.311  0.02154 *  \nmale        -4.485545   0.498957  -8.990  &lt; 2e-16 ***\nhours        0.004348   0.000516   8.427 1.58e-15 ***\ndark_mode    1.385810   0.523793   2.646  0.00859 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.03 on 295 degrees of freedom\nMultiple R-squared:  0.3434,    Adjusted R-squared:  0.3345 \nF-statistic: 38.57 on 4 and 295 DF,  p-value: &lt; 2.2e-16\n\n\nBeachte, dass für die gematchten Datensätze jeweils ein durchschnittlicher Behandlungseffekt für die Beobachtungen mit erfolgter Behandlung ermittelt wird: In sämtlichen oben gezeigten Matchibg-Verfahren werden mit estimand = \"ATT\" vergleichbarere Kontrollbeobachtungen für die behandelten Beobachtungen ermittelt. Wir schätzen den Effekt der Behandlung, indem wir die Ergebnisse von behandelten Personen mit denen von gematchten Personen vergleichen, die keine Behandlung erhalten haben. Diese Vergleichsgruppe dient als Ersatz für den hypothetischen Zustand der Behandlungsgruppe, wenn keine Behandlung erfolgt wäre. Dies entspricht der Definition eines ATT — ein average treatment effect on the treated.\n\n3.3.1 Cluster-robuste Standardfehler\nFür Matching-Verfahren sind die mit summary() berechneten Standardfehler (und damit auch Konfidenzintervalle, t-Statistiken und p-Werte) für den Behandlungseffekt grundsätzlich ungültig. Je nach Matching-Verfahren liegen unterschiedliche Quellen von Schätzunsicherheit vor, die bei der Berechnung von Standardfehlern zusätzlich zu der “üblichen” Stichproben-Variabilität berücksichtig werden müssen. Gründe hierfür sind der Matching-Prozess ansich oder weitere Unsicherheit durch die Schätzung zusätzlicher Parameter, etwa bei der Berechnung von Propensity Scores mit logistischer Regression. Eine weiterere Ursache zusätzlicher Variation durch den Matching-Prozess, die wir bisher nicht näher betrachtet haben ensteht durch Zurücklegen, d.h. wenn Beobachtungen mehrfach gematcht werden können. Auch dieser Faktor wird in der von summary() verwendeten Formel für den Standardfehler des Effekt-Schätzers nicht berücksichtigt.\nStandardfehlerberechnung für Matching-Schätzer von Behandlungseffekten ist Gegenstand aktueller Forschung. P. C. Austin und Small (2014) und Abadie und Spiess (2022) belegen die Gültigkeit von cluster-robusten Standardfehlern mit Clustering auf Ebene der Beobachtungsgruppen (subclass im output von match.data()) bei Matching ohne Zurücklegen. Für Matching anhand von Propensity Scores (auch mit Zurücklegen) zeigen Imbens (2016), dass ignorieren der zusätzlichen Unsicherheit durch die Schätzung der Propensity Scores zu konservativer Inferenz für den ATE anhand eines cluster-robusten Standardfehlerschätzers führt, jedoch ungültige Inferenz für die Schätzung des ATT bedeuten kann. Ähnlich zu P. C. Austin und Small (2014) deuten die Ergebnisse der Simulationsstudie von Bodory u. a. (2020) auf grundsätzlich bessere Eigenschaften der Schätzung hin, wenn die Standardfehler nicht für die Schätzung der Propensity Scores adjustiert werden.\nWeiterhin ist die Kontrolle für Kovariablen mit Erklärungskraft für die Outcome-Variable und für die Matching-Variablen (wie oben erfolgt) mit Regression Adjustment für die Schätzung des Behandlungseffekts nach Matching eine etablierte Strategie, vgl. Hill und Reiter (2006) und Abadie und Spiess (2022). So können die Varianz der Schätzung und das Risiko einer Verzerrung der Standardfehler aufgrund verbleibender Imbalance von Behandlungs- und Kontrollgruppe nach Matching verringert werden.\nZur Demonstration von (cluster)-robuster Inferenz und für eine tabellarische Zusammenfassung der Ergebnisse nutzen wir die Pakete marginaleffects und modelsummary. Mit marginaleffects::avg_comparisons() können p-Werte und Kofindenzintervalle unter Berücksichtigung von robuster Standardfehlern und der Gewichte aus dem Matching-Verfahren berechnet werden.\n\nlibrary(marginaleffects)\n\n# Inferenz: Multiple Regression, ungematchter Datensatz\n(\n  sum_orig &lt;- avg_comparisons(\n    model = ATT_mod_org,\n    variables = \"dark_mode\",\n    # Heteroskedastie-robuste SE:\n    vcov = \"HC3\", \n    # Identifizierung der Kontrollgruppe:\n    newdata = subset(darkmode, dark_mode == 1) \n  )\n) \n\n\n      Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n dark_mode    1 - 0     1.39      0.537 2.58  0.00988 6.7 0.333   2.44\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\n\n# Inferenz: Multiple Regression bei CEM\n(\n  sum_CEM &lt;- avg_comparisons(\n  model = ATT_mod_CEM ,\n  variables = \"dark_mode\",\n  # Cluster-robuste SE\n  vcov = ~ subclass, \n  newdata = subset(darkmode_matched_CEM, dark_mode == 1),\n  wts = \"weights\"\n  )\n)\n\n\n      Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n dark_mode    1 - 0     1.65      0.549 3.01  0.00262 8.6 0.576   2.73\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\n\n# Inferenz: Multiple Regression bei PSM\n(\n  sum_PSC &lt;- avg_comparisons(\n    model = ATT_mod_PSC ,\n    variables = \"dark_mode\",\n    vcov = ~ subclass, \n    newdata = subset(darkmode_matched_PSC, dark_mode == 1),\n    wts = \"weights\"\n  )\n)\n\n\n      Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n dark_mode    1 - 0     1.63      0.565 2.89  0.00381 8.0 0.527   2.74\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nWir fassen die Ergebnisse mit modelsummary::modelsummary() tabellarisch zusammen.\n\nlibrary(modelsummary)\n\n# Tabellarische Zusammenfassung erzeugen\nmodelsummary(\n  models = list(\n   \"Kein Matching\" = sum_orig, \n   \"Coarsened Exact\" = sum_CEM, \n   \"Propensity Scores\" = sum_PSC\n  ),\n  stars = T, \n  gof_map = \"nobs\", \n  output = \"gt\"\n) %&gt;%\n  tabopts()\n\n\n\n\n\n\n \n      Kein Matching\n      Coarsened Exact\n      Propensity Scores\n    \n\n\ndark_mode\n1.386**\n1.653**\n1.635**\n\n\n\n(0.537)\n(0.549)\n(0.565)\n\n\nNum.Obs.\n300\n164\n208\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "Matching.html#sec-bootmatching",
    "href": "Matching.html#sec-bootmatching",
    "title": "\n3  Matching\n",
    "section": "\n3.4 Bootstrap-Schätzung kausaler Effekte bei Matching",
    "text": "3.4 Bootstrap-Schätzung kausaler Effekte bei Matching\nEin Bootstrap-Verfahren generiert mit Resampling (wiederholtes Ziehen mit Zurücklegen) aus dem Original-Datensatz (viele) künstliche Datensätze, für die der Schätzer (d.h. das gesamte Verfahren inkl. Matching!) jeweils berechnet wird. Die Verteilung der so gewonnenen Bootstrap-Schätzwerte approximiert die wahre, unbekannte Stichprobenverteilung des Schätzers des Behandlungseffekts. Mit dieser simulierten Verteilung können wir Inferenz betreiben: Wir können einen Bootstrap-Punktschätzer des Behandlungseffekts (Stichprobenmittel der Bootstrap-Schätzungen) sowie Standardfehler (Standardabweichung der der Bootstrap-Schätzungen) und p-Werte berechnen.\nDer Bootstrap kann hilfreich sein, wenn unklar ist, wie Standardfehler für die Unsicherheit des Matching-Prozesses zu adjustieren sind, um gültige Inferenzsstatistiken zu erhalten. Abadie und Imbens (2008) zeigen analytisch, dass der Standard-Bootstrap die Stichprobenverteilung für Schätzer kausaler Effekte anhand von gematchten Datensätzen (d.h. bei Zuordnung/Selektion von Beobachtungen mit Matching) nicht korrekt abbilden kann. Grundsätzlich problematisch hierbei ist, wenn der Bootstrap eine verzerrte Schätzung produziert und/oder zu kleine Standardfehler liefert. Abadie und Imbens (2008) belegen die Tendenz des Bootstraps zu konservative (d.h. zu große) Standardfehler zu produzieren. Simulationsnachweise (Bodory u. a. 2020; Hill und Reiter 2006; P. C. Austin und Small 2014; P. C. Austin und Stuart 2017) finden, dass Bootstrap-Standardfehler u.a. bei Propensity Score Matching mit Zurücklegen leicht konservativ sind somit das gewünschte nominale Signifikanzniveau eines Bootstrap-Hypothesentests nicht überschritten wird, weshalb der Standard-Bootstrap trotz seiner Schwächen in der empirischen Forschung oft angewendet wird.\nWir betrachen als nächstes einen Bootstrap-Algorithmus für Inferenz bezüglich eines kausalen Effekts nach Matching und demonstrieren die Schätzung anhand unseres Website-Beispiels für den ATT nach Propensity-Score-Matching.\nAlgorithmus: Bootstrap-Schätzer für Propensity-Score-Matching\n\nGeneriere eine Bootstrap-Stichprobe durch \\(N\\) Züge mit Zurücklegen aus der \\(N\\)-elementigen originalen Stichprobe.\nWende das Matching-Verfahren für die Bootstrap-Stichprobe an. Schätze den Behandlungseffekt \\(\\beta\\) anhand der gematchten Stichprobe mit Regression. Speichere den Punktschätzer des Behandlungseffekts \\(\\widehat{\\beta}_b^*\\).\nFürhre die Schritte 1 und 2 für \\(b=1,\\dots,B\\) aus, wobei \\(B\\) eine hinreichend große Anzahl von Bootstrap-Replikationen ist.\nBerechne den Bootstrap-Schätzer des Behandlungseffekts \\(\\overline{\\beta}^* = \\frac{1}{B}\\sum_{b=1} \\widehat{\\beta}_b^*\\) und den Standardfehler \\(\\text{SE}(\\overline{\\beta}^*) = \\sqrt{\\frac{1}{B-1}\\sum_{b=1}^B(\\widehat{\\beta}_b^*-\\overline{\\beta}^*)^2}\\). Berechne Inferenz-Statistiken mit den üblichen Formeln.\n\nWir Implementieren nun einen Bootstrap-Schätzer des ATT im Website-Beispiel für Propensity-Score-Matching. Hierzu definieren wir eine R-Funktion boot_fun() für die Schritte 1 und 2 im obigen Algorithmus.\n\n# Bootstrap-Funktion für Schritte 1 und 2\nboot_fun &lt;- function(\n    data, # originale Stichprobe\n    i     # Indexmenge f. Bootstrap-Stichprobe\n) {\n  \n  # Bootstrap-Stichprobe\n  boot_data &lt;- data[i, ]\n  \n  # 1:1 PS Matching\n  match_res &lt;- matchit(\n    dark_mode ~ age + hours + male,\n    estimand = \"ATT\",\n    distance = \"glm\", \n    method = \"nearest\", \n    caliper = .25,\n    data = boot_data\n  ) \n  \n  # Gematchten Datensatz zuweisen\n  darkmode_matched &lt;- match.data(\n    object = match_res, \n    data = boot_data\n  )\n  \n  # Outcome-Modell schätzen\n  ATT_mod &lt;- lm(\n    formula = read_time ~ age + male + hours + dark_mode,\n    data = darkmode_matched, \n    weights = weights \n  )\n  \n  #  ATT-Schätzung zurückgeben\n  return(\n    ATT_mod$coefficients[\"dark_mode\"]  \n  )\n}\n\nWir berechnen nun eine Bootstrap-Schätzung des ATT von dark_mode auf readingtime nach Propensity Score Matching mit einem caliper von 0.25 sowie den zugehörigen Standardfehler und ein 95%-KI mit der zuvor definierten Funktion boot_fun.\n\nlibrary(\"boot\")\nset.seed(4321)\n\n# Anz. Bootstrap-Replikationen\nB &lt;- 999\n\n# Bootstrap durchführen\n(\n  boot_out &lt;- boot(darkmode, boot_fun, R = B)\n)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = darkmode, statistic = boot_fun, R = B)\n\n\nBootstrap Statistics :\n    original      bias    std. error\nt1* 1.634664 -0.08144129   0.5935746\n\n\nDen Bootstrap-Schätzer des ATT sowie den Bootstrap-Standardfehler berechnen wir mit mean() und sd() anhand der 999 Bootstrap-Replikationen in boot_out$t.\n\n# Bootstrap-Schätzer für den Treatment-Effekt\nmean(boot_out$t) \n\n[1] 1.553222\n\n# = mean(t0) + bias = mean(Bootstrap_samples)\n# vgl. 't0 = boot_fun(darkmode, i = 1:1e3)'\n\n# Bootstrap-Standardfehler\nsd(boot_out$t)\n\n[1] 0.5935746\n\n\nEin 95%-Konfidenzintervall für den kausalen Effekt erhalten wir mit boot::boot.ci().88 type = \"bca\" (bias-corrected accelerated) ist eine gängige Implementierung für die Berechnung des Konfidenz-Intervalls.\n\n# 95% Bootstrap-KI für den Treatment-Effekt\nboot.ci(\n  boot.out = boot_out, \n  type = \"bca\", \n  conf = .95\n)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 999 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_out, conf = 0.95, type = \"bca\")\n\nIntervals : \nLevel       BCa          \n95%   ( 0.527,  2.839 )  \nCalculations and Intervals on Original Scale\n\n\nBeachte, dass der Bootstrap-Standardfehler sowie das Bootstrap-Konfidenzintervall nahe der mit avg_comarisons berechneten Werte für sum_PSC sind.\nBootstrap-Standardfehler für IPW-Schätzer des ATE\nDie obige Bootstrap-Funktion boot_fun kann leicht für eine Schätzung des Standardfehlers für den IPW-Schätzer des ATE aus Kapitel 3.1.3 angepasst werden. Statt einer Matching-Prozedur berechnen wir hierzu für \\(B\\) Bootstrap-Stichproben den Schätzer \\(\\widehat{\\tau}^\\text{IPW}\\).\n\n# IPW estimation with regression adjustment\nipw_boot &lt;- function(\n    data, \n    i\n) {\n  \n  # Bootstrap-Stichprobe erstellen\n  data_boot  &lt;- data %&gt;% \n    slice(i)\n  \n  # Logistischee Regression\n  glm_fit &lt;- glm(\n    formula = dark_mode ~ age + hours + male,\n    data = data_boot, \n    family = binomial\n  )\n  \n  # Propensity Scores berechnen\n  data_boot &lt;- data_boot %&gt;%\n    mutate(\n      ps = predict(glm_fit, type = 'response')\n    )\n  \n  # Beobachtungen anhand Propensity Scores trimmen\n  data_boot &lt;- data_boot %&gt;%\n    filter(\n      between(\n        x = ps,\n        left = .2,\n        right = .7\n      )\n    )\n  \n  # IPW berechnen\n  data_boot &lt;- data_boot %&gt;%\n    mutate(\n      ipw = case_when(\n        dark_mode == 1 ~ 1 / ps,\n        dark_mode == 0 ~ 1 / (1 - ps))\n    )\n  \n  # Gewichtete Mittelwerte der Gruppen   \n  w_means &lt;- data_boot %&gt;%\n    group_by(dark_mode) %&gt;%\n    summarize(m = weighted.mean(read_time, w = ipw)) %&gt;%\n    arrange(dark_mode)\n  \n  # ATT-Schätzwert\n  return(w_means$m[2] - w_means$m[1])\n}\n\n\nb &lt;- boot(data = darkmode,\n          statistic = ipw_boot, \n          R = 999)\n\n\n# Bootstrap-Schätzer und Standardfehler\nmean(b$t)\n\n[1] 2.026179\n\nsd(b$t)\n\n[1] 0.6823184\n\n\n\n# 95% Bootstrap-KI für den Treatment-Effekt\nboot.ci(b, type = \"bca\")\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 999 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = b, type = \"bca\")\n\nIntervals : \nLevel       BCa          \n95%   ( 0.658,  3.225 )  \nCalculations and Intervals on Original Scale"
  },
  {
    "objectID": "Matching.html#selektierende-matching-verfahren",
    "href": "Matching.html#selektierende-matching-verfahren",
    "title": "\n3  Matching\n",
    "section": "\n3.2 Selektierende Matching-Verfahren",
    "text": "3.2 Selektierende Matching-Verfahren\nDas grundsätzliche Konzept von selektierendem Matching wird in der nachstehenden interaktiven Grafik veranschaulicht. Hier betrachten wir beobachtete Ausprägungen von zwei (unabhängig und identisch verteilten) Matching-Variablen für Subjekte in der Behandlungsgruppe (blau) sowie Kontrollgruppe (rot). Per Klick auf eine Beobachtung werden Matches aus der anderen Gruppe in cyan farblich kenntlich gemacht. Als Matches zählen sämtliche Beobachtungen der anderen Gruppe, deren Euklidische Distanz zu dem ausgewählten Punkt das über den Slider eingestellte Maximum Caliper nicht überschreitet.4 Diese Region wird durch den gestrichelten Kreis gekennzeichnet. Die Grafik illustriert inbesondere, dass Beobachtungen mehrfach (s.g. Matching mit zurücklegen) oder gar nicht gematcht werden können.4 Es handelt sich hierbei um einen Spezialfall von Matching anhand der Mahalanobis-Distanz.\n\n\nFür die nachfolgenden Code-Beispiele verwenden wir das R-Paket MatchIt. MatchIt::matchit() nutzt standardmäßig Eins-zu-Eins-Matching (ohne Zurücklegen) von Beobachtungen der Treatment-Gruppe mit Beobachtungen der Kontrollgruppe.5 Die für das Matching zu verwendenden Variablen werden über das Argument formula als Funktion des Behandlungsindikators definiert. matchit() bereitet das Objekt für eine Schätzung des ATT mit einer geeigneten Funktionen, s. ?matchit und hier insb. die Erläuterungen der Argumente replace = F, ratio = 1 und estimand = \"ATT\" für Details. Mit cobalt::balt.tab() erhalten wir eine balance table für den gematchten Datensatz.5 Dieses Schema zielt auf eine Schätzung des ATT ab.\nWir zeigen als nächstes, wie MatchIt::matchit() für Matching anhand der Regressoren age, hours, und male in unserem Website-Beispiel für unterschiedliche Varianten durchgeführt werden kann.\n\n3.2.1 Exaktes Matching\nExaktes Matching ordnet einem Subjekt aus der Behandlungsgruppe ein oder mehrere Subjekte aus der Kontrollgruppe zu, wenn die boebachteten Ausprägung der Matching-Variablen exakt übereinstimmen. Hierbei muss die ‘Distanz’ zwischen den Ausprägung der Matching-Variablen folglich \\(0\\) sein. Dieses Verfahren findet meist bei ausschließlich diskret verteilten Merkmalen Anwendung. Bei kontinuierlich verteilten Merkmalen (vgl. die obige interaktive Grafik) sind exakte Matches zwar theoretisch unmöglich, ergeben sich jedoch in der Praxis aus der Datenerfassung, bspw. durch Rundungsfehler. In matchit() erhalten wir exaktes Ein-zu-eins-Matching mit method = \"exact\".\n\nlibrary(MatchIt)\n\n# Exaktes Eins-zu-Eins-Matching durchführen\nres_em &lt;- matchit(\n  formula = dark_mode ~ age + male + hours, \n  data = darkmode,\n  estimand = \"ATT\",\n  method = \"exact\"\n)\n\nError in `matchit()`:\n! No exact matches were found.\n\nres_em\n\nError in eval(expr, envir, enclos): object 'res_em' not found\n\n\nAufgrund der kontinulierliche Verteilten Variable hours gibt es in unserem Website-Beispiel keine exakten Matches. Dieses Verfahren ist hier folglich ungeeignet.\n\n3.2.2 Coarsened Exact Matching\nBei dieser Methode werden kontinuierliche Matching-Variablen grob (Engl. coarse) klassiert, ähnlich wie bei einem Histogram. Diese Diskretisierung ermöglicht es exakte Übereinstimmungen zwischen Behandlungs- und Kontrollgruppenbeobachtungen hinsichtlich ihrer klassierten Ausprägungen zu finden. Sowohl Behandlungs- als auch Kontrollbeobachtungen die mindestents einen exakten Match haben, werden Teil des gematchten Datensatzes. In matchit() wird Coarsened Exact Matching mit method = \"cem\" durchgeführt. Über das Argument cutpoints geben wir an, dass hours in 6 Klassen und age in 4 Klassen eingeteilt werden soll.6 Mit k1k = TRUE erfolgt Eins-zu-eins-Matching: Bei mehreren exakten Matches wird die Beobachtung mit der geringsten Mahalanobis-Distanz (für die unklassierten Matching-Variablen) gewählt.6 Diese Werte wurden ad-hoc gewählt da sie zu einem guten Ergebnis führen.\n\n# Coarsened Exact Matching\nres_CEM &lt;- matchit(\n  formula = dark_mode ~ age + male + hours, \n  data = darkmode, \n  estimand = \"ATT\",\n  method = \"cem\", \n  k2k = TRUE,\n  cutpoints = list(\n    \"hours\" = 6, \n    \"age\" = 4\n  ) \n)\nres_CEM\n\nA matchit object\n - method: Coarsened exact matching\n - number of obs.: 300 (original), 164 (matched)\n - target estimand: ATT\n - covariates: age, male, hours\n\n\n\n# Balance-Table Coarsened Exact Matching\nbal.tab(res_CEM)\n\nBalance Measures\n         Type Diff.Adj\nage   Contin.   0.0106\nmale   Binary   0.0000\nhours Contin.  -0.0135\n\nSample sizes\n          Control Treated\nAll           151     149\nMatched        82      82\nUnmatched      69      67\n\n\nMit Coarsened Exact Matching erhalten wir einen Datensatz mit 82 Beobachtungen und guter Balance.\n\n3.2.3 Matching mit der Mahalanobis-Distanz\nDie Euklidische Distanz misst den direkten Abstand zwischen zwei Punkten und ist nicht invariant gegenüber Transformationen, insbesondere bei unterschiedlichen Skalierungen und bei Korrelation der Matching-Variablen. Die Mahalanobis-Distanz hingegen ist ein standardisiertes Distanzmaß, das unter Berücksichtigung der Varianz-Kovarianz-Struktur der Daten angibt, wie viele Standardabweichungen zwei Datenpunkte voneinander entfernt sind. Die Mahalanobis-Distanz ist invariant gegenüber linearen Transformationen (Skalierung, Translation und Rotation) der Daten und bietet ein genaueres Maß für die Unähnlichkeit zweier Beobachtungen hinsichtlich ihrer Ausprägungen der Matching-Variablen.\nBetrachte die Datenpunkte \\(P_1=(X_1,Y_1)'\\) und \\(P_2=(X_2,Y_2)'\\) für die Matching-Variablen \\(X\\) und \\(Y\\). Die Mahalanobis-Distanz zwischen \\(P_1\\) und \\(P_2\\) ist definiert als \\[\\begin{align*}\n  d_M(P_1,\\,P_2) = \\sqrt{(P_1 - P_2)'\\boldsymbol{S}^{-1} (P_1 - P_2)},\n\\end{align*}\\] wobei \\(\\boldsymbol{S}\\) die Varianz-Kovarianz-Matrix von \\(X\\) und \\(Y\\) ist. Die Mahalanobis-Distanz \\(d_M(\\cdot,\\cdot)\\) ist also die Euklidische Distanz zwischen den standardisierten Datenpunkten.\nFür beobachtete Daten ersetzen wir die Komponenten der Varianz-Kovarianz-Matrix durch Stichprobenmaße. Dies ergibt die Formel\n\\[\\begin{align*}\n  \\widehat{d}_M(P_1,\\,P_2) = \\sqrt{\n  \\begin{pmatrix}\n    X_1 - X_2\\\\\n    Y_1 - Y_2\n  \\end{pmatrix}'\n  \\begin{pmatrix}\n    \\widehat{\\text{Var}}(X^2) & \\widehat{\\text{Cov}}(X, Y) \\\\\n     \\widehat{\\text{Cov}}(X, Y) & \\widehat{\\text{Var}}(X^2)\n  \\end{pmatrix}^{-1}\n    \\begin{pmatrix}\n    X_1 - X_2\\\\\n    Y_1 - Y_2\n  \\end{pmatrix}\n}.\n\\end{align*}\\]\nDie nachstehende interaktive Grafik zeigt Beobachtungen zweier Matching-Variablen, die aus einer bivariaten Normalverteilung mit positiver Korrelation generiert wurden. Diese bivariate Verteilung ist identisch für Beobachtungen aus der Kontrollgruppe (rot) und Beobachtungen aus der Behandlungsgruppe (blau). Für die ausgewählte Beobachtung aus der Behandlungsgruppe (schwarzer Rand) werden potentielle Matches in der Kontrollgruppe innerhalb der vorgegebenen Mahalanobis-Distanz in Cyan kenntlich gemacht. Beachte, dass die Mahalanobis-Distanz Varianzen und Kovarianzen der Daten berücksichtigt, sodass die gematchten Beobachtungen in einem elliptischen Bereich um die betrachtete behandelte Beobachtung liegen. Eine Euklidische Distanz hingegen (gestrichelte Linie) ignoriert die Skalierung der Daten.\n\n\nFür Eins-zu-Eins-Matching im Website-Beispiel anhand der Mahalanobis-Distanz mit matchit() setzen wir distance = \"mahalanobis\" und wählen method = \"nearest\". Mit diesen Parametern wird jeder Behandlung aus der Behandlungsgruppe die gemäß \\(d_M\\) am ehesten vergleichbarste Beobachtung aus der Kontrollgruppe zugewiesen, wobei keine mehrfachen Matches zulässig sind.\n\n# 1:1 Mahalanobis-Distanz-Matching\nres_maha &lt;- matchit(\n  formula = dark_mode ~ age + male + hours, \n  data = darkmode, \n  estimand = \"ATT\",\n  distance = \"mahalanobis\", \n  method = \"nearest\"\n)\nres_maha\n\nA matchit object\n - method: 1:1 nearest neighbor matching without replacement\n - distance: Mahalanobis\n - number of obs.: 300 (original), 298 (matched)\n - target estimand: ATT\n - covariates: age, male, hours\n\n\n\n# Balance-Table für 1:1 Mahalanobis-Matching\nbal.tab(res_maha)\n\nBalance Measures\n         Type Diff.Adj\nage   Contin.  -0.5826\nmale   Binary   0.3154\nhours Contin.   0.0106\n\nSample sizes\n          Control Treated\nAll           151     149\nMatched       149     149\nUnmatched       2       0\n\n\nDie Ergebnisse zeigen, dass für sämtliche \\(149\\) Beobachtungen aus der Behandlungsgruppe ein individueller Match in der Kontrollgruppe gefunden werden konnte. Es werden lediglich \\(2\\) Beobachtungen der \\(151\\) Beobachtungen in der Kontrollgruppe nicht gematcht.\nEntsprechend zeigt die Balance-Table eine ähnliche Diskrepanz beider Gruppen hinsichtlich der Matching-Variablen an.\nMahalanobis-Distanz mit Caliper .25 für Propensity Scores basierend auf logistischer Regression\nFür eine strengeres Matching-Kriterium kann ein Caliper, d.h. eine maximal zulässige Distanz, herangezogen werden. Die Mahalanobis-Distanz hat jedoch keine einheitliche Skala: Ob eine Distanz als groß oder klein betrachten werden kann, hängt von der Anzahl der Matching-Variablen und dem Überlappungsgrad zwischen den Gruppen ab. Daher wird die Beschränkung durch einen Caliper nicht auf \\(\\widehat{d}_M\\) sondern auf Propensity Scores angewendet.\nIm nächsten Code-Beispiel spezifizieren wir mit distance = \"glm\", dass Propensity Scores gemäß der Vorschrift in formula geschätzt werden. Mit mahvars = ~ age + male + hours legen wir die Matching-Variablen für die Berechnung von \\(\\widehat{d}_M\\) fest. caliper = .25 legt fest, dass lediglich Beobachtungen der Kontrollgruppe bei einer absoluten Differenz der Propensity Scores von höchstens \\(0.25\\) Standardabweichungen als Match für eine Beobachtung in der Behandlungsgruppe qualifiziert sind.\n\n# Mahalanobis-Matchig mit PS-Caliper\nres_mahaC &lt;- matchit(\n  formula = dark_mode ~ age + male + hours, \n  data = darkmode, \n  distance = \"glm\",\n  estimand = \"ATT\",\n  method = \"nearest\",\n  mahvars = ~ age + male + hours,\n  caliper = .25\n)\nres_mahaC\n\nA matchit object\n - method: 1:1 nearest neighbor matching without replacement\n - distance: Mahalanobis [matching]\n             Propensity score [caliper]\n             - estimated with logistic regression\n - caliper: &lt;distance&gt; (0.058)\n - number of obs.: 300 (original), 208 (matched)\n - target estimand: ATT\n - covariates: age, male, hours\n\n\n\n# Balance Table\nbal.tab(res_mahaC)\n\nBalance Measures\n             Type Diff.Adj\ndistance Distance   0.1812\nage       Contin.  -0.1176\nmale       Binary   0.0481\nhours     Contin.  -0.0001\n\nSample sizes\n          Control Treated\nAll           151     149\nMatched       104     104\nUnmatched      47      45\n\n\nDie Balance-Table zeigt einen deutlichen Effekt der Beschränkung qualifizierter Beobachtungen durch caliper = .25: Aufgrund der oberen Grenze für die Propensity-Score-Differenz von \\(0.058\\) wird für lediglich \\(104\\) Beobachtungen aus der Behandlungsgruppe ein individueller Match in der Kontrollgruppe gefunden.7 Weiterhin finden wir eine verbesserte Balance für den gematchten Datensatz.7 Die durch caliper implizierte Obergrenze ergibt sich als .25 * sd(fitted(darkmode_ps_logit))).\n\n3.2.4 Propensity Score Matching\nEine gängige Variante ist Matching ausschließlich anhand von Propensity Scores innerhalb eines Calipers.\n\n# 1:1 Matching mit PS und Caliper\nres_PSC &lt;- matchit(\n  formula = dark_mode ~ age + male + hours, \n  data = darkmode, \n  estimand = \"ATT\",\n  distance = \"glm\", \n  method = \"nearest\", \n  caliper = .25\n)\nres_PSC\n\nA matchit object\n - method: 1:1 nearest neighbor matching without replacement\n - distance: Propensity score [caliper]\n             - estimated with logistic regression\n - caliper: &lt;distance&gt; (0.058)\n - number of obs.: 300 (original), 208 (matched)\n - target estimand: ATT\n - covariates: age, male, hours\n\n\n\n# Balance Table\nbal.tab(res_PSC)\n\nBalance Measures\n             Type Diff.Adj\ndistance Distance   0.1640\nage       Contin.  -0.0976\nmale       Binary   0.0481\nhours     Contin.   0.0134\n\nSample sizes\n          Control Treated\nAll           151     149\nMatched       104     104\nUnmatched      47      45\n\n\nLaut Balance-Table führt Eins-zu-Eins-Matching basierend auf Propensity Scores zu einem Datensatz mit \\(104\\) gematchten Beobachtungen in der Behandlungsgruppe. Hinsichtlich der standardisierten Mittelwertdifferenz (Diff.Adj) erzielt diese Methode die beste Balance unter den betrachteten Ansätzen.\nVergleich der Balance verschiedener Verfahren mit Love-Plot\nStandardisierte Mittelwertdifferenzen für verschiedene Matching-Verfahren können grafisch mit einem Love-Plot (Love 2004) veranschaulicht werden. Hierzu nutzen wir cobalt::love.plot() und übergeben die mit matchit() generierten Objekte im Argument weights.\n\n# Love-Plot für\nlove.plot(\n  x = dark_mode ~ age + male + hours, \n  weights = list(\n    CEM = res_CEM,\n    Mahalanobis = res_maha,\n    Mahalanobis_Cal = res_mahaC,\n    PSC = res_PSC\n  ),\n  data = darkmode, \n  line = T,\n  # absolute Mittelwertdifferenz plotten\n  abs = T\n)\n\n\n\n\nDie Grafik zeigt, dass Coarsened Exact Matching (CEM) unter allen betrachteten Verfahren die Stichprobe mit der besten Balance ergibt. Diesen gematchten Datensatz erhalten wir mit MatchIt::match.data().\n\n# gematchten Datensatz zuweisen\ndarkmode_matched_CEM &lt;- match.data(res_CEM)\nhead(darkmode_matched_CEM)\n\n# A tibble: 6 × 7\n  read_time dark_mode  male   age hours weights subclass\n      &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;   \n1      15.4         0     1    55  125.       1 26      \n2      20.9         1     0    23  643.       1 70      \n3      21.5         1     0    29  190.       1 79      \n4      22           1     0    18  334.       1 80      \n5      17.4         0     0    53  279.       1 11      \n6      20.4         0     0    43  138.       1 9       \n\n\ndarkmode_matched enthält Gewichte (weights) für die jeweilige Gruppe zu denen gemachte Beobachtungen gehören (subclass). Dies ist relevant, falls Beobachtungen mehrfach gematcht werden. Wegen Eins-zu-eins-Matching ohne Zurücklegen gibt es in unserem Beispiel 82 Beobachtungspaare und sämtliche Gewichte sind 1. Die Berücksichtigung der Gewicht in den nachfolgenden Aufrufen von Schätzfunktionen (bspw.lm()) ist daher nicht nötig und erfolgt lediglich zur Illustration der grundsätzlichen Vorgehensweise.\nEine Wiederholung der grafischen Analyse in Kapitel 3.1 zeigt eine deutlich verbesserte Vergleichbarkeit hinsichtlich der Verteilung der Matching-Variablen in darkmode_matched.\n\ndarkmode_matched_CEM %&gt;%\n  group_by(dark_mode) %&gt;%\n  select(age, hours) %&gt;%\n  mutate_all(scale) %&gt;%\n  pivot_longer(cols = c(-dark_mode)) %&gt;%\n  \n  ggplot(\n    aes(x = value, fill = as.factor(dark_mode))\n  ) +\n  geom_density( alpha = .5) + \n  facet_wrap(\n    facets = ~ name, \n    scales = \"free\", \n    nrow = 3\n  )\n\n\n\ndarkmode_matched_CEM %&gt;% \n  group_by(dark_mode) %&gt;%\n  mutate(\n    male = as.factor(male), \n    dark_mode = as.factor(dark_mode)\n  ) %&gt;%\n  \n  ggplot(\n    aes(x = dark_mode, fill = male)\n  ) +\n  geom_bar(position = \"fill\") +\n  ylab(\"Anteil\")\n\n\n\n\nWir beobachten eine bessere Balance bei age und hours. Inbesondere ist male für Kontroll- und Behandlungsgruppe ausgeglichen."
  },
  {
    "objectID": "Matching.html#doubly-robust-schätzung",
    "href": "Matching.html#doubly-robust-schätzung",
    "title": "\n3  Matching\n",
    "section": "\n3.5 Doubly-Robust-Schätzung",
    "text": "3.5 Doubly-Robust-Schätzung\nImplementieren und berechnen Sie einen Doubly-Robust-Schätzer des ATT, vgl. Wooldridge (2010).\n\n# IPW estimation with regression adjustment\nipwra &lt;- function(br, index = 1:nrow(br)) {\n    # slice bootstrapped observations\n    br &lt;- br %&gt;% slice(index)\n    \n    # estimate and predict propensity score\n    m &lt;- glm(\n      formula = dark_mode ~ age + hours + male,\n      data = br, \n      family = binomial(link = 'logit')\n    )\n    \n    br &lt;- br %&gt;%\n        mutate(ps = predict(m, type = 'response'))\n\n    br &lt;- br %&gt;%\n      filter(\n        between(\n          x = ps,\n          left = .2,\n          right = .7\n          )\n    )\n    \n    # compute IPWs\n    br &lt;- br %&gt;%\n      mutate(\n        ipw = case_when(\n          dark_mode == 1 ~ 1 / ps,\n          dark_mode == 0 ~ 1 / (1 - ps))\n      )\n    \n#    Simple _ATT_ estimate:\n    w_means &lt;- br %&gt;%\n        group_by(dark_mode) %&gt;%\n        summarize(m = weighted.mean(read_time, w = ipw)) %&gt;%\n        arrange(dark_mode)\n\n    # simple diff-in-means _ATT_ estimate\n     return(w_means$m[2] - w_means$m[1])\n    \n    # Do regression adjustment for _ATE_ estimate\n    # TE prediction for whole sample based on TG model\n    # mtreat &lt;- br %&gt;%\n    #   filter(dark_mode == 1) %&gt;%\n    #   lm(read_time ~ 1 + age + hours + male, data = ., weights = .$ipw) %&gt;%\n    #   predict(newdata = br) %&gt;%\n    #   mean()\n    # \n    # # TE prediction for whole sample based on CG model\n    # mcont &lt;- br %&gt;%\n    #   filter(dark_mode == 0) %&gt;%\n    #   lm(read_time ~ 1 + age + hours + male, data = ., weights = .$ipw) %&gt;%\n    #   predict(newdata = br) %&gt;%\n    #   mean()\n    # \n    # return(mtreat - mcont) # Regression adjusted _ATE_ estimate\n}\n\n\nb &lt;- boot(data = darkmode, ipwra, R = 999)\n# Bootstrap estimate and standard error\nmean(b$t)\n\n[1] 2.033055\n\nsd(b$t)\n\n[1] 0.6966973\n\n# 95% Bootstrap-KI für den Treatment-Effekt\nboot.ci(b, type = \"perc\")\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 999 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = b, type = \"perc\")\n\nIntervals : \nLevel     Percentile     \n95%   ( 0.600,  3.301 )  \nCalculations and Intervals on Original Scale\n\n\n\n\n\n\n\n\nAbadie, Alberto, und Guido W. Imbens. 2008. „On the Failure of the Bootstrap for Matching Estimators.“ Econometrica. Journal of the Econometric Society 76 (6): 1537–57. https://doi.org/10.3982/ECTA6474.\n\n\nAbadie, Alberto, und Jann Spiess. 2022. „Robust Post-Matching Inference.“ Journal of the American Statistical Association 117 (538): 983–95. https://doi.org/10.1080/01621459.2020.1840383.\n\n\nAustin, P. 2011. „An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies“. Multivariate Behavioral Research 46 (3): 399–424. https://doi.org/10.1080/00273171.2011.568786.\n\n\nAustin, Peter C., und Dylan S. Small. 2014. „The use of bootstrapping when using propensity-score matching without replacement: A simulation study.“ Statistics in Medicine 33 (24): 4306–19. https://doi.org/10.1002/sim.6276.\n\n\nAustin, Peter C., und Elizabeth A. Stuart. 2017. „Estimating the Effect of Treatment on Binary Outcomes Using Full Matching on the Propensity Score.“ Statistical Methods in Medical Research 26 (6): 2505–25. https://doi.org/10.1177/0962280215601134.\n\n\nBodory, Hugo, Lorenzo Camponovo, Martin Huber, und Michael Lechner. 2020. „The Finite Sample Performance of Inference Methods for Propensity Score Matching and Weighting Estimators.“ Journal of Business & Economic Statistics. https://doi.org/10.2139/ssrn.2731969.\n\n\nHainmueller, Jens. 2012. „Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies“. Political Analysis 20 (1): 25–46. https://doi.org/10.1093/pan/mpr025.\n\n\nHájek, J. 1971. „Comment on ‚An essay on the logical foundations of survey sampling‘ by Basu, D“. Foundations of Statistical Inference 236.\n\n\nHill, Jennifer, und Jerome P. Reiter. 2006. „Interval estimation for treatment effects using propensity score matching. Statistics in Medicine“. Statistics in Medicine 25 (13): 2230–56. https://doi.org/10.1002/sim.2277.\n\n\nHirano, Keisuke, Guido Imbens, und Geert Ridder. 2003. „Efficient Estimation of Average Treatment Effects Using the Estimated Propensity Score.“ Econometrica 71 (4): 1161–89. https://doi.org/10.1111/1468-0262.00442.\n\n\nImbens. 2016. „Matching on the Estimated Propensity Score.“ Econometrica 84 (2): 781–807. https://doi.org/10.3982/ecta11293.\n\n\nLove, Thomas. 2004. „Graphical display of covariate balance“. Presentation.\n\n\nRosenbaum, Paul R., und Donald R. Rubin. 1983. „The central role of the propensity score in observational studies for causal effects“. Biometrika 70 (1): 170–84. https://doi.org/10.1017/cbo9780511810725.016.\n\n\nWooldridge, Jeffrey. 2010. Econometric Analysis of Cross Section and Panel Data. Second edition. Cambridge, Massachusetts: MIT."
  }
]