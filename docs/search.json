[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kausalanalyse und Machinelles Lernen mit R",
    "section": "",
    "text": "1 Start"
  },
  {
    "objectID": "R_Einfuehrung.html#lange-weite-und-tidy-datenformate",
    "href": "R_Einfuehrung.html#lange-weite-und-tidy-datenformate",
    "title": "\n2  Statistische Programmierung mit R\n",
    "section": "\n2.1 Lange, weite und “tidy” Datenformate",
    "text": "2.1 Lange, weite und “tidy” Datenformate\nWir betrachten den in Tabelle 2.1 dargestellten Datensatz Klausurergebnisse.\n\n\n\n\n\n\n\n\n  \nName\n      Mikro\n      Makro\n      Mathe\n    \n\n\nTim\nNA\n1.3\n3\n\n\nLena\n1\n3\nNA\n\n\nRicarda\n2\n1.7\n1.3\n\n\nSimon\n2.3\n3.3\nNA\n\n\n\nTabelle 2.1:  Datensatz Klausurergebnisse \n\n\n\nDer Datensatz ist noch nicht in der R-Arbeitsumgebung verfügbar. Mit der Funktion tribble() können wir Tabelle 2.1 händisch als R-Objekt der Klasse tibble definieren\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nklausurergebnisse enhält die Klausurnoten der vier Studierenden (Boebachtungen) spaltenweise pro Modul, d.h. die Spaltennamen Mikro, Makro und Mathe sind Ausprägungen der Variable Modul. Der Datensatz liegt also nicht im s.g. Tidy-Format vor.\n\n\n\n\n\n\nTidy-Format\n\n\n\nTidy-Format: Jede Spalte ist eine Variable, jede Reihe ist eine Beobachtung und jede Zelle enthält einen einen Wert. Datensätze im Tidy-Format sind häufig lang: Die Zeilendimension ist größer als die Spaltendimension.\n\n\nDas Tidy-Format ist hilfreich für statistische Analysen mit tidyverse-Funktionen wie bspw. ggplot(). Wir nutzen die Funktion tidyr::pivot_longer(), um klausurergebnisse ein (langes) Tidy-Format zu transformieren.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nBeachte, dass die Spalte Name die Zugehörigkeit der Ausprägungen (Note) jeder Variable (Modul) zu einer Beobachtung identifiziert. Mit dieser Information können wir den langen Datensatz wieder in das ursprüngliche (weite) Format zurückführen. Wir nutzen hierzu tidyr::pivot_wider().\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nWenn die Zuweisung von Zwischenergebnissen in Variablen nicht benötigt wird, kann eine Verkettung von Funktionsaufrufen die Verständlichkeit des Codes verbessern. Hierzu wird der Pipe-Operator %&gt;% genutzt. Wir wiederholen die Transformationen mit den tidyr::pivot_*-Funktion bei Verwendung von %&gt;%.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nEin Beispiel für den Nachteil des weiten Formats im Umgang mit tidyverse-Paketen ist die Funktion tidyr::drop_na(). Diese entfernt sämtliche Zeilen eines Datensatzes, die NA-Einträge (d.h. fehlende Werte) aufweisen. Beachte, dass diese Operation im ursprünglichen weiten Format zum Entfernen ganzer Beobachtungen aus wide führt.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nIm Tidy-Format long hingegen bleiben die übrigen Informationen betroffener Beobachtungen erhalten.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte…."
  },
  {
    "objectID": "R_Einfuehrung.html#pinguine-und-pipes",
    "href": "R_Einfuehrung.html#pinguine-und-pipes",
    "title": "\n2  Statistische Programmierung mit R\n",
    "section": "\n2.2 Pinguine und Pipes",
    "text": "2.2 Pinguine und Pipes\nIn diesem Abschnitt zeigen wir die Verwendung häufig verwendeter dplyr-Funktionen (s.g. Verben) für die Transformation von Datensätzen: mutate(), select(), filter(),summarise() und arrange().\nFür die Illustration verwenden wir den Datensatz penguins aus dem R-Paket palmerpenguins. Dieser Datensatz wurde im Zeitraum 2007 bis 2009 von Dr. Kristen Gorman im Rahmen des Palmer Station Long Term Ecological Research Program zusammengetragen und enthält Größenmessungen für drei Pinguinarten, die auf den Inseln des Palmer-Archipels in der Antarktis beobachtet wurden.\n\n# Paket 'palmerpenguins' installieren\n# install.packages(\"palmerpenguins\")\n\n# Paket 'palmerpenguins' laden\nlibrary(palmerpenguins)\n\nMit data() wird der Datensatz in der Arbeitsumgebung verfügbar gemacht. Wir nutzen glimpse(), um einen Überblick zu erhalten.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n2.2.1 dplyr::mutate()\n\nMit mutate() können bestehende Variablen überschrieben oder neue Variablen als Funktion bestehender Variablen definiert werden. mutate() operiert in der Spaltendimension des Datensatz.\nWir definieren eine neue Variable body_mass_kg als Transformation body_mass_g/1000.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nMit across() kann die dieselbe Operation auf mehrere Variablen angewendet werden.\nIm nachstehenden Beispiel ändern wir den typ (type) der Variablen species, island, sex und year zu character.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\ntransmute() ist eine Variante von mutate(), die lediglich die transformierten Variablen beibehält.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n2.2.2 dplyr::select()\n\nMit select() werden Variablen aus dem Datensatz ausgewählt. Dies geschieht entweder über den Variablennamen…\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n… oder über eine Indexmenge.33 Hilfreich: dplyr::pull() selektiert eine Variable und wandelt diese in einen Vektor um.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nVariablen können anhand eines Muster im Namen selektiert werden. Die Selektion von ends_with(\"mm\") bezieht nur Variablen mit Endung mm im Namen ein:\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nMit where() können wir Variablen aufgrund bestimmter Eigenschaften ihrer Ausprägungen selektieren.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n2.2.3 dplyr::filter()\n\nDas Verb filter() filtert den Datensatz in der Zeilendimension. So können Beobachtungen ausgewält werden, deren Merkmalsausprägungen bestimmte Kriterien erfüllen. Hierzu muss filter() ein logischer (logical) Ausdruck übergeben werden. Häufig erfolgt dies über Vergleichsoperatoren.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nOft ist es praktisch, mehrere Kriterien zu kombinieren.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nAnalog: komma-getrennte Kriterien werden intern über den Und-Operator (&) verknüpft.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nÄhnlich wie bei select() verwenden wir häufig nützliche Funktionen, welche die Interpretation des Codes erleichtern. dplyr::between() erlaubt filtern innerhalb eines Intervals.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nMit diesen Verben sind wir bereits in der Lage, den Datensatz gemäß folgender Vorschrift zu bereinigen:\n\nEntfernen der Maßeinheiten aus den Variablennamen\nEntfernen von Pinguinen mit fehlenden Werten (NA)\nEntfernen von Pinguinen mit einem Gewicht oberhalb des 95%-Stichprobenquantils\n\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nDurch die Verkettung mit %&gt;% können wir sämtliche Schritte für die Bereinigung ohne das Abspeichern von Zwischenergebnissen durchführen.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n2.2.4 dplyr::summarise()\n\nDas Verb summarise() fasst Variablen über Beobachtungen hinweg zusammen. Der nachstehende Code-Chunk erzeugt eine Tabelle mit Stichprobenmittelwert und -standardabweichung von flipper_length_mm.4 Um zu vermeiden, dass die Auswertung aufgrund fehlender Werte (NA) in flipper_length_mm scheitert, lassen wir NAs mit na.rm = TRUE bei der Berechnung unberücksichtigt (wir verwenden weiterhin den unbereinigten Datensatz penguins).4 dplyr::summarise() darf nicht mit base::summary() verwechselt werden!\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nVarianten von summarise() können über mehrere Variablen angewendet werden. Wir verwenden across() und where(), um lediglich numerische Variablen mit den in der liste definierten Funktionen zusammenzufassen. Beachte, dass \\(x) mean(x) eine anonyme Funktion definiert.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n2.2.5 dplyr::arrange()\n\nMit arrange() können Datensätze in Abhängigkeit der beobachteten Ausprägungen von Variablen sortiert werden.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nDie Funktion dplyr::desc() kehrt die Reihenfolge zu einer absteigenden Sortierung um.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nKomplexe Sortier-Muster werden durch Übergabe von Variablennamen in der gewünschten Reihenfolge erreicht.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n2.2.6 Operationen mit gruppierten Datensätzen\nFür manche Transformationen ist eine Gruppierung der Daten hilfreich. Wir illustrieren nachfolgend die unterschiedlichen Verhaltensweisen ausgewählter Verben durch Vergleiche von gruppierten und nicht-gruppierten Anwendungen.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nspecies hat drei Ausprägungen. Entsprechend ist penguins_grouped nun in drei Gruppen eingeteilt.\nBei gruppierten Datensätzen fasst summarise() die Variablen pro Guppe zusammen.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nmutate() definiert bzw. transformiert für jede Gruppe separat. Im dies zu veranschaulichen, ziehen wir eine Zufallsstichprobe von 10 Pinguinen aus der Datensatz.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nFür den ungruppierten Datensatz berechnet mutate() das Stichprobenmittel von bill_length_mm über alle zehn Datenpunkte und weißt diesen Wert jeweils in der Variable mean zu.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nBei gruppierten Daten berechnet mutate() das Stichprobenmittel pro Gruppe und weist die Mittelwerte entsprechend zu."
  },
  {
    "objectID": "R_Einfuehrung.html#eine-explorative-analyse-mit-ggplot2",
    "href": "R_Einfuehrung.html#eine-explorative-analyse-mit-ggplot2",
    "title": "\n2  Statistische Programmierung mit R\n",
    "section": "\n2.3 Eine explorative Analyse mit ggplot2\n",
    "text": "2.3 Eine explorative Analyse mit ggplot2\n\nDer bereinigte Datensatz penguins_cleaned eignet sich gut für eine graphische Auswertung mit dem R-Paket ggplot2, welches Bestandteil des tidyverse ist. Nachfolgend untersuchen wir Zusammenhänge zwischen den Körpermaßen der Pinguine.\nWir erstellen zunächst einen einfachen Punkteplot des Gewichts (body_mass) und der Schnabeltiefe (bill_depth).\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nDie Grafik zeigt einen positiven Zusammenhang zwischen dem Gewicht und der Schnabeltiefe. Als nächstes passen wir den Code so an, dass die Datenpunkte entsprechend der Art (species) eingefärbt sind.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nOffenbar gibt es deutliche Unterschiede in der (gemeinsamen) Verteilung von Gewicht und Schnabeltiefe zwischen den verschiedenen Arten.\nUm den Zusammenhang zwischen Gewicht und Schnabeltiefe zu untersuchen, schätzen wir lineare Regressionen \\[body\\_mass = \\beta_0 + \\beta_1 bill\\_depth + u\\] separat für jede der drei Pinguinarten mit der KQ-Methode. Anschließend zeichnen wir die geschätzten Regressionsgeraden ein.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nDie Schätzungen bekräftigen die Vermutung, dass der lineare Zusammenhang zwischen Gewicht und Schnabeltiefe sich nicht zwischen den verschiedenen Pinguinarten unterscheidet: Pinguine der Art Gentoo sind im Mittel schwerer als Pinguine der übrigen Arten, haben jedoch eine geringere Schnabeltiefe.\nDer nachfolgende Code fügt der Grafik eine Regressionsline über alle Arten hinzu. Wir setzen hierbei das Argment inherit_aes = FALSE und legen damit fest, dass die Regression für body_mass und bill_depth ohne Differenzierung per species durchgeführt wird.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nOffenbar ist die vorherige Analyse per Spezies sinnvoller: Die Regression über alle Arten suggeriert einen negativen Zusammenhang zwischen Gewicht und Schnabeltiefe.\nFacetting mit facet_wrap() erlaubt eine Untersuchung des Zusammenhangs je Insel (island), auf der die Messung erfolgt ist.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nWir sehen, dass es hinsichtlich des Zusammenhangs von Gewicht und Schnabeltiefe keine wesentlichen Diskrepanzen zwischen den drei Inseln gibt. Darüber hinaus lässt sich anhand der Facetten leicht erkennen, wie die drei Arten über die Inseln verteilt sind."
  },
  {
    "objectID": "Matching.html",
    "href": "Matching.html",
    "title": "\n3  Matching\n",
    "section": "",
    "text": "4 Literatur\nAbadie, Alberto, and Guido W. Imbens. (2008). On the Failure of the Bootstrap for Matching Estimators.Econometrica 76 (6): 1537–57.\nBodory, H., Camponovo, L., Huber, M., & Lechner, M. (2020). The Finite Sample Performance of Inference Methods for Propensity Score Matching and Weighting Estimators. Journal of Business & Economic Statistics, 38(1), 183–200.\nWooldridge, J. M. (2010). Econometric analysis of cross section and panel data. MIT press."
  },
  {
    "objectID": "Matching.html#einfluss-von-darkmode-auf-blog-lesezeit",
    "href": "Matching.html#einfluss-von-darkmode-auf-blog-lesezeit",
    "title": "\n3  Matching\n",
    "section": "\n3.1 Einfluss von Darkmode auf Blog-Lesezeit",
    "text": "3.1 Einfluss von Darkmode auf Blog-Lesezeit\nWir illustrieren eine Auswahl an Methoden für die Schätzung eines Behandlungseffekts mit Matching anhand einer folgender (fiktiver) Fallstudie.\nDer Lehrstuhl für Ökonometrie an der Universität Duisburg-Essen verwaltet die Website betreibt einen Ökonometrie-Blog und interessiert sich für den kausalen Effekt der Einführung eines darkmode auf die Verweildauer der User auf der Webseite. Die Webseite ist zwar nicht-kommerziell, hat sich allerdings insb. für die Aquise internationaler Studierender für den Studiengang MSc. Econometrics als wichtiges Marketing-Instrument erwiesen. Ein anprechendes Design des Blogs wird daher als hoch-relevant erachtet.\nIdealerweise sollte der Effekt des Design-Relaunches auf die Nutzungsintensität in einem kontrollierten randomisierten Experiment untersucht werden. Hierbei würden wir Nutzern zufällig das neue oder das alte Design zuweisen und den Effekt als Differnz des durchschnittlichen Verweildauer für die Gruppen bestimmen. Eine solche Studie ist jedoch aus technischen und finanziellen Gründen nicht realisierbar, sodass die Auswirkungen des darkmode mit vorliegenden nicht-experimenellen Nutzungsstatistiken für die Webseite geschätzt werden sollen.\nDie Nutzungsstatistiken sind im Datensatz darkmode.csv enthalten und sollen der Analyse des Effekts des darkmode (dark_mode) auf die Verweildauer der Leser auf der Webseite (read_time) dienen.\nTabelle 3.1 zeigt die Definitionen der Variablen in darkmode.csv.\n\n\n\n\n\n\n\n\n  \nVariable\n      Beschreibung\n    \n\n\nread_time\nLesezeit (Minuten/Woche)\n\n\ndark_mode\nIndikator: Beobachtung nach Einführung darkmode\n\n\nmale\nIndikator: Individuum männlich\n\n\nage\nAlter (in Jahren)\n\n\nhours\nBisherige Verweildauer auf der Seite\n\n\n\nTabelle 3.1:  Variablen im Datensatz darkmode \n\n\n\nDa die User sich beim Aufrufen der Seite aktiv für oder gegen den das neue Design entscheiden müssen (und somit selektieren, ob Sie in der Behandlungs- oder Kontrollgruppe landen), liegt wahrscheinlich Confounding vor: Unsere Hypothese ist, dass männliche User eine durchschnittlich längere Lesezeit aufweisen als nicht-männliche Leser und mit größerer Wahrscheinlichkeit auf das neue Design wechseln. Weiterhin scheint plausibel, dass das Alter der Nutzer sowohl die Akzeptanz des Design-Updates als auch die Lesezeit beeinflusst. Die bisherige Verweildauer ist ebenfalls eine plausible Determinante der Lesezeit.\nDer angenommene DGP ist in Abbildung Abbildung 3.1 dargestellt, wobei Backdoor-Pfade mit roten Pfeilen gekennzeichnet sind.\n\n\n\n\n\nread_time\n\nread_time\ndark_mode\n\ndark_mode\ndark_mode-&gt;read_time\n\n\nmale\n\nmale\nmale-&gt;read_time\n\n\nmale-&gt;dark_mode\n\n\nage\n\nage\nage-&gt;read_time\n\n\nage-&gt;dark_mode\n\n\nhours\n\nhours\nhours-&gt;read_time\n\n\n\nAbbildung 3.1: Vermuteter DGP im Website-Design-Bespiel\n\n\n\nFür die Analyse lesen wir zunächst den Datensatz darkmode.csv mit readr::read_csv() ein und verschaffen uns einen Überblick über die verfügbaren Variablen.\n\n# Paket `tidyverse` laden\nlibrary(tidyverse)\n\n# Datensatz 'darkmode' einlesen\ndarkmode &lt;- read_csv(\n  file = \"datasets/darkmode.csv\"\n)\n\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\ndark_mode hat den Typ logical. Mit dplyr::mutate_all() können wir komfortabel alle Spalten in den Typ numeric transformieren.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nInverse probability weights (IPWs) anhand der PS können schnell anhand der Vorschrift \\[\\texttt{ipw} = \\texttt{dark\\_mode} / \\texttt{propensity} + (1 - \\texttt{dark\\_mode}) / (1 - \\texttt{propensity}), \\quad \\texttt{dark\\_mode} \\in\\{0,1\\}\\] berechnet werden.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nVerteilung der Propensity Scores nach Behandlungs-Indikator:\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nBeobachtungen aus der Kontroll-Gruppe entfernen, die außerhalb des Supports der Treatment-Gruppe liegen.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nDie Abdeckung können wir erneut mit einer Grafik geschätzter Dichtefunktionen vergleichen.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nWir finden etwas weniger Wahrscheinlichkeits-Masse nahe 0 für die Kontroll-Gruppe nach filtern von (Kontroll-)Beobachtungen mit PS in der Spannweite der PS in der Behandlungs-Gruppe. Als nächstes schätzen wir den ATE mit linearer Regression.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nUnsere Schätzung des ATE ist der geschätzte Koeffizient von dark_mode. Die ausgegebenen Standardfehler und Inferenzstatistiken sind ungültig aufgrund der Gewichtung mit IPWs, inversen geschätzten Wahrscheinlichkeiten für eine Behandlung. Der Grund hierfür ist, dass die Standardformel in summary() die zusätzliche Unsicherheit durch die IPW-Schätzung nicht berücksichtigt!\nDie Vergleichbarkeit der Nutzer in Kontroll- und Behandlungsgruppe-Gruppe für die Variablen age, hours und male können wir graphisch und anhand einer balance table vergleichen.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nWir zeigen als nächstes, wie MatchIt::matchit() für Nearest-neighbor-Matching anhand der Regressoren age, hours, und male in unterschiedlichen Varianten durchgeführt werden kann.\nMatchIt::matchit() führt standardmäßig 1:1-Matching (ohne Zurücklegen) von Beobachtungen der Treatment-Gruppe mit Beobachtungen der Kontrollgruppe druch. Das Objekt wird für eine Schätzung des ATT mit einer geeigneten Funktionen vorbereitet, s. ?matchit, und hier insb. die Argumente replace = F, ratio = 1 und estimand = \"ATT\" für Details.\nMit cobalt::balt.tab() erhalten wir eine balance table für den gematchten Datensatz.\nExaktes Matching\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nEins-zu-Eins-Matching: Mahalanobis-Distanz\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nEins-zu-Eins-Matching: Mahalanobis-Distanz mit Caliper 0.25 für propensity scores basierend auf logistischer Regression\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n4. Eins-zu-Eins-Matching: Propensity scores basierend auf logistischer Regression mit Caliper 0.25\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nDie Vergleichbarkeit der Nutzer in Kontroll- und Treatment-Gruppe hinsichtlich der Variablen age, hours und male können wir graphisch und anhand einer balance table vergleichen. Wir berechnen die balance table mit cobalt::bal.tab() für den anhand von Variante 4 gematchten Datensatz.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nWir beobachten eine bessere Balance bei age und hours. Am wichtigsten: gender (male) ist nahezu ausgeglichen für Kontroll- und Treatment-Gruppe!\nWir schätzen nun den ATT von dark_mode auf read_time mit linearer Regression für den gematchten Datensatz aus sowie für den ursprünglichen Datensatz und berechnen jeweils ein robustes 95%-Konfidenzintervall für den ATT.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nAchtung: Für Matching-Verfahren (ATT_mod) sind die von summary() berechneten Standardfehler (und damit KI, t-Statistiken und p-Werte) für den ATT grundsätzlich ungültig Wir haben 3 Quellen von Schätzunsicherheit, die bei der Berechnung von Standardfehlern berücksichtigt werden müssen: Die Schätzung der PS, der Matching-Prozess und die “übliche” Stichproben-Variabilität. Wir nutzen daher nachfolgende Funktionen gem. Empfehlungen aus der aktuellen Forschung für Standardfehlerberechnung. S. auch Aufgabe 5 (a).\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte…."
  },
  {
    "objectID": "Matching.html#inferenz-für-attate-propensity-score-matching-mit-bootstrap",
    "href": "Matching.html#inferenz-für-attate-propensity-score-matching-mit-bootstrap",
    "title": "\n3  Matching\n",
    "section": "\n3.2 Inferenz für ATT/ATE: Propensity-Score-Matching mit Bootstrap",
    "text": "3.2 Inferenz für ATT/ATE: Propensity-Score-Matching mit Bootstrap\nBei Matching mit Zurücklegen besteht zusätzliche Unsicherheit durch Zurücklegen, d.h. Beobachtungen aus der Kontroll-Gruppe können mehrfach als Match für Beobachtungen aus der Treatment-Gruppe genutzt werden. Mit summary() berechnete Standardfehler berücksichtigen dies nicht!\nEin Bootstrap-Verfahren generiert mit Resampling (wiederholtes Ziehen mit Zurücklegen) aus dem Original-Datensatz (viele) künstliche Datensätze, für die der Schätzer (d.h. das gesamte Verfahren inkl. Matching!) jeweils berechnet wird. Die Verteilung der so gewonnenen Bootstrap-Schätzwerte approximiert die wahre, unbekannte Stichprobenverteilung des Schätzers des Behandlungseffekts. Mit dieser simulierten Verteilung können wir Inferenz betreiben: Wir können einen Bootstrap-Punktschätzer des Behandlungseffekts (Stichprobenmittel der Bootstrap-Schätzungen) sowie Standardfehler (Standardabweichung der der Bootstrap-Schätzungen) und p-Werte berechnen.\nWir Implementieren nun einen Bootstrap-Schätzer des ATT als R-Funktion boot_fun().\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\nAbadie & Imbens (2008) zeigen analytisch, dass ein Standard-Bootstrap bei Matching grundsätzlich ungültig ist: Die unbekannte Varianz der Stichprobenverteilung des Matching-Schätzers (und damit der Standardfehler des Schätzers) kann durch den Bootstrap nicht repliziert werden. Problematisch hierbei sind grundsätzlich zu liberale (d.h. zu große) mit dem Bootstrap berechnete Standardfehler. Es gibt jedoch Simulationsnachweise die zeigen, dass Bootstrap-Standardfehler bei Matching mit Zurücklegen konservativ sind (Bodory et al., 2020), also tendentiell zu kleine Standardfehler produzieren und damit das gewünschte nominale Signifikanzniveau eines Bootstrap-Hypothesentests nicht überschritten wird.\nWir berechnen nun eine Bootstrap-Schätzung des ATT von dark_mode auf readingtime sowie den zugehörigen Standardfehler und ein 95%-KI mit der zuvor definierten Funktion boot_fun.\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte…."
  },
  {
    "objectID": "Matching.html#doubly-robust-schätzer-für-attate",
    "href": "Matching.html#doubly-robust-schätzer-für-attate",
    "title": "\n3  Matching\n",
    "section": "\n3.3 Doubly-Robust-Schätzer für ATT/ATE",
    "text": "3.3 Doubly-Robust-Schätzer für ATT/ATE\nImplementieren und berechnen Sie einen Doubly-Robust-Schätzer des ATT (vgl. Wooldridge, 2010) für den kausalen Effekt in Aufgabe 5. Vergleichen Sie mit den Ergebnissen der Aufgaben 1 (d), 4 (f) und 5 (d).\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte….\n    \n    \n      \n    \n  \n  \n  \n\n\n\n  \n    \n    \n      \n      R lädt. Etwas Geduld bitte…."
  },
  {
    "objectID": "RDD.html#sharp-regression-discontinuity-design",
    "href": "RDD.html#sharp-regression-discontinuity-design",
    "title": "\n4  Regression Discontiniuty Designs\n",
    "section": "\n4.1 Sharp Regression Discontinuity Design",
    "text": "4.1 Sharp Regression Discontinuity Design\nModell und funktionale Form\nDie korrekte Spezifikation der funktionalen Form für ein RDD ist wichtig, um eine verzerrte Schätzung des Effekts zu vermeiden. Die einfachste Form eines SRDD kann anhand der linearen Regression \\[\\begin{align}\nY_i = \\beta_0 + \\beta_1 B_i + \\beta_2 X_i + u_i\\label{eq-simpleSRDD}\n\\end{align}\\] geschätzt werden, wobei \\(B_i\\) eine Dummy-Variable für das Überschreiten des Schwellenwertes \\(c\\) ist, d.h. \\[\\begin{align*}\n  B_i=\\begin{cases}\n    0 & X_i &lt; c\\\\\n    1 & X_i \\geq c.\n  \\end{cases}\n\\end{align*}\\] Damit ist \\(B_i\\) eine deterministische Funktion der Laufvariable \\(X_i\\) und zeigt die Zugehörigkeit zur Behandlungs- oder Treatmentgruppe an. Der Koeffizient \\(\\beta_1\\) misst den Behandlungseffekt.\nDas Modell \\(\\eqref{eq-simpleSRDD}\\) unterstellt, dass \\(X\\) links- und rechtsseitig von \\(c\\) denselben Effekt auf \\(Y\\) hat. Diese Annahme ist restriktiv. Eine Alternative ist ein lineares Interaktionsmodell \\[\\begin{align}\nY_i = \\beta_0 + \\beta_1 B_i + \\beta_2 (X_i - c) + \\beta_3(X_i - c)\\times B_i + u_i.\\label{eq:linearSRDD}\n\\end{align}\\] Das Modell \\(\\eqref{eq:linearSRDD}\\) kann unterschiedliche lineare Effekte von \\(X\\) auf \\(Y\\) unterhalb (\\(\\beta_2\\)) und oberhalb (\\(\\beta_2 + \\beta_3\\)) von \\(c\\) abbilden. Beachte, dass \\((X_i - c)\\) die um den Schwellenwert zentrierte Laufvariable ist, sodass \\(\\beta_1\\) wie in \\(\\eqref{eq-simpleSRDD}\\) den Unterschied des Effekts von \\(X\\) auf \\(Y\\) für Beoabachtungen am Schwellenwert erfasst.\nUm unterschiedliche nicht-lineare Zusammenhänge von \\(X\\) und \\(Y\\) unterhalb und oberhalb von \\(c\\) abzubilden, können (interargierte) Polynom-Terme in \\(X\\) verwendet werden. Häufig wird eine quadratische Regressionsfunktion genutzt, \\[\\begin{align}\n  Y_i =&\\, \\beta_0 + \\beta_1 B_i + \\beta_2 (X_i - c) + \\beta_3 (X_i - c)^2\\\\\n       &+\\, \\beta_4(X_i - c)\\times B_i + \\beta_5(X_i - c)\\times B_i + u_i.\\label{eq:quadSRDD}\n\\end{align}\\] Gelman und Imbens (2019) zeigen, dass Polynome höherer Ordnung zu verzerrten Schätzern und hoher Varianz führen können.2 Die Authoren empfehlen stattdessen die Schätzung mit lokaler Regression.2 Ursachen sind Überanpassung an die Daten sowie instabiles Verhalten der Schätzung nahe des Schwellenwertes.\nNicht-parametrische Schätzung und Bandweite\nAktuelle Studien nutzen nicht-parametrische Schätzer, die den Behandlungseffekt als Differenz der geschätzten Regressionsfunktionen am Schwellenwert \\(c\\) berechnen. Um auch nicht-lineare Regressionsfunktionen abzubilden zu können, wird häufig lokale Regression verwendet. Dieses Verfahren liefert eine “lokale” Schätzung der Regressionsfunktionen am Schwellenwert, bei der nur Beobachtungen nahe \\(X = c\\) für die Schätzung berücksichtigt werden. Hinreichende Nähe wird hierbei durch eine sogenannte Bandweite \\(h\\) festgelegt, wobei \\[\\begin{align}\n  \\lvert(X_i-c)\\rvert\\leq h \\label{eq:bwc}\n\\end{align}\\] das Kriterium für eine Berücksichtigung von Beobachtung \\(i\\) bei der Schätzung ist.\nUnter Verwendung einer Bandweite \\(h\\) wird der Regressionsansatz \\(\\eqref{eq:linearSRDD}\\) als lokale lineare Regression mit Uniform-Kernelfunktion bezeichnet. Der Uniform-Kernel gibt allen Beobachtungen, innerhalb der Bandweite \\(h\\) dasselbe Gewicht. Ist \\(h\\) so groß, dass der gesamte Datensatz in die Schätzung einbezogen wird, entspricht der lokale lineare Regressions-Schätzer mit Uniform-Kernel dem (globalen) KQ-Schätzer in einem linearen Interaktionsmodell anhand aller Beobachtungen. Neben dem Uniform-Kernel ist der Triangular-Kernel eine in der Praxis häufig genutzte lineare Kernelfunktion. Der nachstehende Code plottet die Uniform- (grün) sowie die Triangular-Kernelfunktion (blau), siehe Abbildung 4.2.\n\nCodelibrary(ggplot2)\nlibrary(cowplot)\n\n# Kernelfunktionen zeichnen\nggplot() + \n    geom_function(\n      fun = ~ ifelse(\n        test = abs(.) &lt;= 1,\n        yes =  1/2, \n        no = 0\n      ), \n      col = \"green\", \n      n = 1000\n      ) + \n    geom_function(\n      fun = ~ ifelse(\n        test = abs(.) &lt;= 1, \n        yes = 1 - abs(.), \n        no = 0\n      ), \n      col = \"blue\", \n      n = 100\n      ) + \n    scale_x_continuous(\n      name = \"x\", \n      limits = c(-1.5, 1.5), \n      breaks = c(-1, 0, 1)\n    ) +\n    scale_y_continuous(\n      name = \"K(x)\", \n      breaks = c(0, 1), \n      limits = c(0, 1.25)\n    ) +\n    theme_cowplot()\n\n\n\nAbbildung 4.2: Kernelfunktionen auf [-1, 1]\n\n\n\nIn empirischen Studien wird als Basis-Spezifikation oft eine lokale lineare Regression anhand von \\(\\eqref{eq:linearSRDD}\\) mit einer linearen Kernelfunktionen und geringer bandweite \\(h\\) genutzt. Anschließend wird die Robustheit der Ergebnisse anhand flexiblerer Spezifikationen, die Nicht-Linearitäten in der Regressionsfunktion besser abbilden können, geprüft.\nDie nachstehende Visualisierung zeigt die Schätzung des kausalen Effektes der Behandlung \\(B_i\\) anhand lokaler linearer Regression mit einem Uniform-Kernel für wiefolgt simulierte Daten: \\[\\begin{align*}\n  Y_i =&\\, \\beta_1 X_i + \\beta_2 B + \\beta_3 X_i^2 \\times B_i + u_i,\\\\\n  \\\\\n  u_i \\sim&\\, N(0, 0.5), \\quad X_i \\sim U(0, 10), \\quad B = \\mathbb{I}(X_i \\geq c = 5)\\\\\n  \\beta_1 =&\\, .5, \\quad \\beta_2 = 1.5, \\quad \\beta_3 = -0.15\n\\end{align*}\\]\nDiese Vorschrift ist schnell mit R umgesetzt:\n\nset.seed(1234)\n# Anz.Beobachtungen\nn &lt;- 750\n\n# Parameter definieren\nc &lt;- 5\nbeta_1 &lt;- .5\nbeta_2 &lt;- 1.5\nbeta_3 &lt;- -.15\n\n# Regressionsfunktion definieren\nf &lt;- function(X) {\n  beta_1 * (X - c) + beta_2 * B + beta_3 * B * (X - c)^2\n}\n\n# Daten erzeugen\nX &lt;- runif(n, 0, 11)\nB &lt;- ifelse(X - c &gt;= 0, 1, 0)\nY &lt;- f(X) + rnorm(n, sd = .5)\n\n# Beoabchtungen sammeln\ndat &lt;- data.frame(\n  Y = Y, X = X - c, B = B\n)\n\n\nhtml`\n&lt;style&gt;\n.regression {\n  fill: none;\n  stroke: #000;\n  stroke-width: 1.5px;\n}\n.axis line {\n  stroke: #ddd;\n}\n.axis .baseline line {\n  stroke: #555;\n}\n.axis .domain {\n  display: none;\n} \n&lt;/style&gt;\n&lt;link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css\"&gt;\n`\n\n\n\n\n\n\n\nd3 = require(\"d3-array@3\", \"d3-axis@3\", \"d3-regression@1\", \"d3-scale@4\", \"d3-shape@3\", \"d3-selection@3\", \"d3-format\")\n\nmargin = ({left: 55, right: 8, top: 13, bottom: 24});\nbase = Math.min(width, 500);\ninnerWidth = base - margin.left - margin.right;\ninnerHeight = base-100 - margin.top - margin.bottom;\n\n\nviewof bw_daten_LLRU = Inputs.range([.1, 6], {\n  label: \"Bandweite (h)\",\n  step: .1,\n  value: 1.3\n});\n\nxScaleLLRU = d3.scaleLinear()\n   .domain([-6, 6])\n   .range([0, innerWidth]);\n   \nyScaleLLRU = d3.scaleLinear()\n  .domain([-4, 6])\n  .range([innerHeight, 0]);\n\nlineLLRU = d3.line()\n  .x(d =&gt; xScaleLLRU(d[0]))\n  .y(d =&gt; yScaleLLRU(d[1]));\n  \nxAxisLLRU = d3.axisBottom(xScaleLLRU)\n  .tickSize(innerHeight + 10)\n  .tickValues([-6, -4, -2, 0, 2, 4, 6])\n  .tickFormat(d =&gt; d);\n\nyAxisLLRU = d3.axisLeft(yScaleLLRU)\n  .tickSize(innerWidth + 10)\n  .tickFormat(d =&gt; d);\n\nLLRURegression = d3.regressionLinear()\n  .x(d =&gt; d.X)\n  .y(d =&gt; d.Y);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  const svg = d3.select(DOM.svg(innerWidth + margin.left + margin.right + 20, innerHeight + margin.top + margin.bottom + 20))\n  \n  const g = svg.append(\"g\")\n      .attr(\"transform\", `translate(${margin.left}, ${margin.top})`);\n\n  g.append(\"g\")\n      .attr(\"class\", \"axis\")\n      .call(xAxisLLRU);\n\n  g.append(\"g\")\n    .attr(\"class\", \"axis\")\n    .attr(\"transform\", `translate(${innerWidth})`)\n    .call(yAxisLLRU);\n\n  // Add X axis label:\n  g.append(\"text\")\n    .attr(\"text-anchor\", \"end\")\n    .attr(\"font-size\", 13)\n    .attr(\"x\", innerWidth)\n    .attr(\"y\", innerHeight + margin.top + 25)\n    .text(\"X - c\");\n\n  // Y axis label:\n  g.append(\"text\")\n    .attr(\"text-anchor\", \"end\")\n    .attr(\"transform\", \"rotate(-90)\")\n    .attr(\"font-size\", 13)\n    .attr(\"y\", -margin.left+10)\n    .attr(\"x\", -margin.top+10)\n    .text(\"Y\");\n\n  g.selectAll(\"circle\")\n    .data(transpose(SRDD))\n    .enter().append(\"circle\")\n    .attr(\"r\", 2)\n    .attr(\"cx\", d =&gt; xScaleLLRU(d.X))\n    .attr(\"cy\", d =&gt; yScaleLLRU(d.Y));\n\ng.selectAll(\"circle\")\n .filter( function(d){ return Math.abs(d.X) &lt;= bw_daten_LLRU } )\n .attr(\"fill\", \"orange\")\n .attr(\"stroke\", \"none\");\n \n  &lt;!-- trf --&gt;\nvar  line = d3.line()\n           .x(function(d) { return xScaleLLRU(d.X); }) \n           .y(function(d) { return yScaleLLRU(d.Y_true); }) \n           .curve(d3.curveLinear); \n\n  g.append(\"path\")\n    .datum(transpose(SRDD))\n    .attr(\"d\", line)\n    .attr(\"fill\", \"none\")\n    .attr(\"stroke\", \"red\")\n    .attr(\"stroke-width\", 2);\n\n\nfunction b(d) { return LLRURegression(\n        transpose(SRDD).filter(function(d){ return d.X &lt;= 0 & d.X &gt;= -bw_daten_LLRU })\n    ); }\n\nfunction a(d) { return LLRURegression(\n      transpose(SRDD).filter(function(d){ return d.X &gt; 0 & d.X &lt;= bw_daten_LLRU })\n    ); }\n\n  g.append(\"path\")\n      .attr(\"class\", \"regression\")\n      .datum(b)\n      .attr(\"d\", lineLLRU)\n      .attr(\"stroke-width\", 2)\n      .style(\"stroke\", \"#39FF14\");\n\n  g.append(\"path\")\n      .attr(\"class\", \"regression\")\n      .datum(a)\n      .attr(\"d\", lineLLRU)\n      .attr(\"stroke-width\", 2)\n      .style(\"stroke\", \"#39FF14\");\n  \n  g.append(\"line\")\n  .attr(\"x1\", xScaleLLRU(0))\n  .attr(\"y1\", yScaleLLRU((b().slice(-1))[0][1]))\n  .attr(\"x2\", xScaleLLRU(0))\n  .attr(\"y2\", yScaleLLRU((a().slice(0))[0][1]))\n  .attr(\"stroke\", \"#39FF14\")\n  .attr(\"stroke-width\", 2);\n  \n   g.append(\"text\")\n    .attr(\"x\", d =&gt; xScaleLLRU(-2.75))\n    .attr(\"y\", d =&gt; yScaleLLRU(4.5))\n    .attr(\"dy\", \".35em\")\n    .attr(\"fill\", \"#39FF14\")\n    .text(\n    d3.format(\",.2f\")( (a().slice(0))[0][1] - (b().slice(-1))[0][1] ) \n    );\n  \n  /* dashed line data bw upper */\n  g.append(\"line\")\n  .attr(\"x1\", xScaleLLRU(bw_daten_LLRU))\n  .attr(\"y1\", 0)\n  .attr(\"x2\", xScaleLLRU(bw_daten_LLRU))\n  .attr(\"y2\", innerHeight)\n  .style(\"stroke\", \"blue\")\n  .style(\"stroke-dasharray\", \"4\")\n  .style(\"stroke-width\", \"1\");\n  \n  /* dashed line data bw lower */\n  g.append(\"line\")\n  .attr(\"x1\", xScaleLLRU(-bw_daten_LLRU))\n  .attr(\"y1\", 0)\n  .attr(\"x2\", xScaleLLRU(-bw_daten_LLRU))\n  .attr(\"y2\", innerHeight)\n  .style(\"stroke\", \"blue\")\n  .style(\"stroke-dasharray\", \"4\")\n  .style(\"stroke-width\", \"1\");\n\n  return svg.node();\n}\n\n\n\n\nAbbildung 4.3: Nicht-parametrische Regression auf beiden Seiten des Cut-offs\n\n\nDer interssierende Effekt am Schwellenwert \\(c=5\\) beträgt \\(\\beta_2 = 1.5\\). Beachte, dass aufgrund des Terms \\(\\beta_3 X_i^2 \\times B_i\\) ein quadratischer Zusammenhang von \\(Y\\) und \\(X\\) oberhalb von \\(X_i = c\\) vorliegt. Es können folgende Eigenschaften der Schätzung in Abhängigkeit von der Bandweite \\(h\\) beobachtet werden:\n\nFür die voreingestellte Bandweite \\(h = 1.3\\) liefert die lokale lineare Regression eine gute Approximation des Regressionszusammenhangs auf beiden Seiten des Schwellenwertes und die Schätzung des Behandlungseffekts liegt nahe beim wahren Wert \\(\\beta_2 = 1.5\\).\nFür kleinere Bandweiten verringert sich die Datenbasis der Schätzung. Die Varianz der Schätzung nimmt zu und die Approximation der Regressionsfunktion verschlechtert sich. Wir beobachten eine mit \\(h\\to0\\) zunehmende Verzerrung bei der Schätzung des Behandlungseffekts.\nGrößere Bandweiten \\(h\\) erhöhen die Datenbasis der Schätzung, führen aber zu einer Annäherung der lokalen Schätzung an die globale KQ-Schätzung. Linksseitig des Schwellenwertes erzielen wir damit eine Schätzung mit hoher Güte. Rechsseitig von \\(X_i = c\\) verschlechtert sich die lokale Anpassung am Schwellenwert deutlich, weil die lineare Schätzung den tatsächlichen (nicht-linearen) Zusammenhang nicht adäquat abbilden kann. Die Schätzung des Behandlungseffekts ist hier deutlich verzerrt.\n\nDie Wahl der Bandweite ist also eine wichtige Komponenten der RDD-Schätzung: Kleine Bandweiten erlauben eine Schätzung der Regressionsfunktion nahe des Schwellenwertes mit wenig Verzerrung. Allerdings kann diese Schätzung unpräzise sein, wenn nur wenige Beobachtungen \\(\\eqref{eq:bwc}\\) erfüllen. In der Praxis wird \\(h\\) daher mit einem analytischen Schätzer (vgl. G. Imbens und Kalyanaraman 2012) oder anhand von Cross Validation (bspw. G. W. Imbens und Lemieux 2008) bestimmt. Die später in diesem Kapitel betrachteten R-Pakete halten diese Methoden bereit."
  },
  {
    "objectID": "RDD.html#manipulation-am-schwellenwert",
    "href": "RDD.html#manipulation-am-schwellenwert",
    "title": "\n4  Regression Discontiniuty Designs\n",
    "section": "\n4.2 Manipulation am Schwellenwert",
    "text": "4.2 Manipulation am Schwellenwert\nEine wichtige Annahmen für die Gültigkeit einer RDD-Schätzung ist, dass keine Manipulation der Gruppenzugehörigkeit am Schwellenwert vorliegt. Wenn sich Subjekte nahe des Schwellenwertes \\(c\\) — d.h. in Abhängigkeit der Laufvariable \\(X\\) — systematisch in den Confoundern \\(Z\\) unterscheiden, können wir den Backdoor-Pfad Oberhalb C → Behandlung B → Y nicht isolieren. Wir erhalten dann eine verzerrte Schätzung des Behandlungseffekts.\nIn empirischen Studien mit Individuen kann Selbstselektion auftreten: Menschen mit \\(X&lt;c\\) aber nahe \\(c\\) (hier Kontrollgruppe) könnten aufgrund unbeobachtbarer Eigenschaften \\(Z\\) die Ausprägung ihrer Laufvariable zu \\(X&gt;c\\) (hier Behandlungsgruppe) manipulieren. Wenn \\(Z\\) die Outcome-Variable beeinflusst, bleibt der Backdoor-Pfad Oberhalb C → Behandlung B → Y so bestehen.\nManipulation resultiert in Häufung von Beobachtungen am Schwellenwert. Dei Verteilung der Laufvariable kann auf diese Unregelmäßigkeit hin untersucht werden. McCrary (2008) schlägt hierfür einen Verfahren vor, das die Kontinuität der Dichtefunktion von \\(X\\) am Schwellenwert testet.\nDer Test von McCrary (2008) ist in rdd::DCdensity() implementiert. Wir zeigen die Anwendung des Tests anhand der oben simulierten Daten. Beachte, dass \\(X_i\\sim U(0, 10)\\), d.h. die Laufvariable ist bei \\(X_i = c\\) kontinuierlich verteilt. Die Nullhypothese (keine Manipulation) gilt für die simulierten Daten\n\n# McCrary-Test durchführen\np_mccrary &lt;- rdd::DCdensity(\n  runvar = X, \n  cutpoint = c, \n  plot = F\n)\n\n# p-Wert\np_mccrary\n\n[1] 0.5013939\n\n\nDer p-Wert 0.5 ist größer als jedes übliche Signifikanzniveau. Damit liegt starke Evidenz für die Nullhypothese (keine Diskontinuität) und gegen Manipulation am Schwellenwert vor.\nCattaneo, Jansson, und Ma (2020) (CMJ) schlagen eine Weiterentwicklung des McCrary-Tests vor, die höhere statistische Macht gegenüber Diskontinuitäten hat am Schwellenwert hat. Der CJM-Test ist im Paket rddensity implementiert.\n\nlibrary(rddensity)\n\n# CJM Schätzer berechnen\nCJM &lt;- rddensity(X, c = 5)\n\nMit der Funktion rddensity::rdplotdensity() erzeugen wir Abbildung 4.4.\n\n# Plot für Dichtefunktion erstellen\nplot &lt;- rdplotdensity(\n  rdd = CJM, \n  X = X, \n  # für Punkte- und Linienplots:\n  type = \"both\" \n)\n\n\n\nAbbildung 4.4: CJM-Test – geschätzte Dichtefunktionen der Laufvariable auf beiden Seiten des Schwellenwerts c = 5\n\n\n\nAbbildung 4.4 zeigt die geschätzten Dichtefunktionen. Erwartungsgemäß finden wir eine große Überlappung der zugehörigen Konfidenzbänder (schattierte Flächen) am Schwellenwert \\(c=5\\).\nMit summary() erhalten wir eine detaillierte Zusammenfassung des Tests.\n\n# Statistische Zusammenfassung des CJM-Tests\nsummary(CJM)\n\n\nManipulation testing using local polynomial density estimation.\n\nNumber of obs =       750\nModel =               unrestricted\nKernel =              triangular\nBW method =           estimated\nVCE method =          jackknife\n\nc = 5                 Left of c           Right of c          \nNumber of obs         329                 421                 \nEff. Number of obs    133                 154                 \nOrder est. (p)        2                   2                   \nOrder bias (q)        3                   3                   \nBW est. (h)           1.918               2.124               \n\nMethod                T                   P &gt; |T|             \nRobust                -0.3338             0.7385              \n\n\nP-values of binomial tests (H0: p=0.5).\n\nWindow Length              &lt;c     &gt;=c    P&gt;|T|\n0.346     + 0.346          20      21    1.0000\n0.521     + 0.544          34      37    0.8126\n0.696     + 0.742          44      57    0.2323\n0.870     + 0.939          54      64    0.4075\n1.045     + 1.137          62      77    0.2349\n1.220     + 1.334          73      98    0.0661\n1.394     + 1.532          86     106    0.1701\n1.569     + 1.729          96     124    0.0685\n1.743     + 1.927         119     140    0.2139\n1.918     + 2.124         133     154    0.2377\n\n\nGemäß des p-Werts (P &gt; |T|) von 0.74 spricht der CJM-Test noch deutlicher gegen eine Diskontinuität als der McCrary-Test.\n\n4.2.1 Case Study: Amtsinhaber-Vorteil (Lee 2008)\n\nLee (2008) untersucht den Einfluss des Amtsinhaber-Vorteils auf die Wahl von Mitgliedern des US-Repräsentantenhaus. In den meisten Wahlkreisen entfallen große Anteile der Stimmen (oder gar ausschließlich) auf demokratische und republikanische Kanditat*innen, sodass sich die Studie auf diese Parteien beschränkt. Entfällt die Mehrheit der Stimmen auf eine*n Kandiat*in, gewinnt diese*r den Sitz für den Wahlkreis. Durch die Analyse der 6558 Wahlen im Zeitraum 1946-1998 mit einem SRDD kommt die Studie zu dem Ergebnis, dass Amtsinhabende im Durchschnitt einen Vorteil von etwa 8% bis 10% bei der Wahl haben. Dieses Ergebnis kann verschiedene Ursachen haben, bspw. dass die amtierende Partei höhere finanzielle Ressourcen besitzt und von einer besseren Organisation und durch Instrumenalisierung staatlicher Strukturen für die eigenen Zwecke profitiert.\nAnhand der Datensätze house und house_binned illustrieren wir nachfolgend die Schätzung von SRDD-Modellen für den Wahlerfolg der demokratischen Partei, wenn diese Amtsinhaber ist. Wir lesen hierfür zunächst die Datensätze house und house_binned ein und verschaffen uns einen Überblick.\n\nlibrary(tidyverse)\nlibrary(modelsummary)\n\n# Daten einlesen\nhouse &lt;- read_csv(\"datasets/house.csv\")\n# Gruppierter Datensatz\nhouse_binned &lt;- read_csv(\"datasets/house_binned.csv\")\n\n# Überblick verschaffen\nglimpse(house)\n\nRows: 6,558\nColumns: 2\n$ StimmenTm1 &lt;dbl&gt; 0.1049, 0.1393, -0.0736, 0.0868, 0.3994, 0.1681, 0.2516, 0.…\n$ StimmenT   &lt;dbl&gt; 0.5810, 0.4611, 0.5434, 0.5846, 0.5803, 0.6244, 0.4873, 0.5…\n\nglimpse(house_binned)\n\nRows: 100\nColumns: 2\n$ StimmenT   &lt;dbl&gt; 0.5995600, 0.5657000, 0.4272554, 0.5637456, 0.6868627, 0.60…\n$ StimmenTm1 &lt;dbl&gt; 0.104764444, 0.135005263, -0.075690769, 0.084570886, 0.3951…\n\n\nDer Datensatz house enthält die Stimmenanteile demokratischer Kandidat*innen bei der Wahl zum Zeitpunkt \\(T\\) (\\(StimmenT\\)) sowie die Differenz zwischen demokratischen und republikanischen Stimmenanteilen bei der vorherigen Wahl, d.h. zum Zeitpunkt \\(T-1\\) (\\(StimmenTm1\\)). Der Schwellenwert für einen Wahlsieg liegt bei Stimmengleichheit, d.h. \\(StimmenTm1 = 0\\).\nhouse_binned ist eine aggregierte Version von house mit Mittelwerten von jeweils 50 gleichgroßen Intervallen oberhalb und unterhalb der Schwelle von \\(StimmenTm1 = 0\\). Dieser Datensatz eignet sich, um einen ersten Eindruck des funktionalen Zusammenhangs auf beiden Seiten zu erhalten. Wir stellen zunächst diese klassierten Daten mit ggplot2 graphisch dar.\n\n# Klassierte Daten plotten\nhouse_binned %&gt;%\n  ggplot(\n    aes(x = StimmenTm1, y = StimmenT)\n    ) +\n  geom_point() +\n  geom_vline(xintercept = 0, lty = 2)\n\n\n\nAbbildung 4.5: Klassierte Daten aus Lee (2008)\n\n\n\nDie Grafik zeigt eindeutig einen Sprung von \\(StimmenT\\) bei \\(StimmenTm1 = 0\\). Weiterhin erkennen wir, dass der Zusammenhang nahe \\(0\\) vermutlich jeweils gut durch eine lineare Funktion approximiert werden kann. Eine Modell-Spezifikation mit gleicher Steigung auf beiden Seiten des Schwellenwertes scheint hingegen weniger gut geeignet. Wir vergleichen diese Spezifikationen nachfolgend.\nZunächst fügen wir dem Datensatz eine Dummyvariable B hinzu. Diese dient als Indikator für den Wahlgewinn in der letzten Wahl und zeigt die Amtsinhaberschaft (Behandlung) an.\n\n# Behandlungsindikator B hinzufügen\nhouse &lt;- house %&gt;% \n  mutate(B = StimmenTm1 &gt; 0)\n\nglimpse(house)\n\nRows: 6,558\nColumns: 3\n$ StimmenTm1 &lt;dbl&gt; 0.1049, 0.1393, -0.0736, 0.0868, 0.3994, 0.1681, 0.2516, 0.…\n$ StimmenT   &lt;dbl&gt; 0.5810, 0.4611, 0.5434, 0.5846, 0.5803, 0.6244, 0.4873, 0.5…\n$ B          &lt;lgl&gt; TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n\n\nWir überprüfen die Laufvariable mit dem CJM-Test auf Manipulation am Schwellenwert \\(c=0\\).\n\n# CJM-Test durchführen\nCJM_Lee &lt;- rddensity(X = house$StimmenTm1)\n\n# Zusammenfassung anzeigen\nsummary(CJM_Lee)\n\n\nManipulation testing using local polynomial density estimation.\n\nNumber of obs =       6558\nModel =               unrestricted\nKernel =              triangular\nBW method =           estimated\nVCE method =          jackknife\n\nc = 0                 Left of c           Right of c          \nNumber of obs         2740                3818                \nEff. Number of obs    1297                1360                \nOrder est. (p)        2                   2                   \nOrder bias (q)        3                   3                   \nBW est. (h)           0.236               0.243               \n\nMethod                T                   P &gt; |T|             \nRobust                1.4346              0.1514              \n\n\nP-values of binomial tests (H0: p=0.5).\n\nWindow Length / 2          &lt;c     &gt;=c    P&gt;|T|\n0.004                      21      24    0.7660\n0.007                      38      46    0.4452\n0.011                      50      60    0.3909\n0.014                      73      77    0.8066\n0.018                      91     104    0.3902\n0.022                     124     132    0.6618\n0.025                     149     149    1.0000\n0.029                     163     174    0.5860\n0.032                     176     202    0.1984\n0.036                     197     223    0.2225\n\n\n\n# CJM-Plot\nplot &lt;- rdplotdensity(\n  rdd = CJM_Lee,\n  X = house$StimmenTm1, \n  type = \"both\", \n)\n\n\n\nAbbildung 4.6: CJM-Test – geschätzte Dichtefunktionen der Laufvariable\n\n\n\nAbbildung 4.6 und der p-Wert von \\(0.15\\) sind Evidenz gegen eine Manipulation am Schwellenwert.\nUm den Behandlungseffekt anhand eines SRDDs zu ermitteln, schätzen wir das Interaktionsmodell \\[\\begin{align*}\n  \\text{StimmenT}_i =&\\, \\beta_0 + \\beta_1 B_i + \\beta_2 (\\text{StimmenTm1}_i - 50)\\\\\n  +&\\, \\beta_3(\\text{StimmenTm1}_i - 50)\\times B_i + u_i\n\\end{align*}\\] zunächst für eine Bandweite von \\(h = 0.5\\). Aufgrund der Skalierung der Daten (Wahlergebnisse in %) bedeutet dies die Verwendung des gesamten Datensatzes für die Schätzung.\n\n# Interaktionsmodell schätzen\nhouse_llr1 &lt;- lm(\n  formula = StimmenT ~ B * StimmenTm1, \n  data = house\n)\n\n# Zusammenfassung anzeigen  \nmodelsummary(\n  models = house_llr1, \n  vcov = \"HC1\", # robuste Standardfehler\n  stars = T, \n  gof_map = \"nobs\", \n  output = \"gt\"\n) %&gt;% \n  tabopts\n\n\n\n\n\n\n \n      (1)\n    \n\n\n(Intercept)\n0.433***\n\n\n\n(0.004)\n\n\nBTRUE\n0.118***\n\n\n\n(0.006)\n\n\nStimmenTm1\n0.297***\n\n\n\n(0.016)\n\n\nBTRUE × StimmenTm1\n0.046*\n\n\n\n(0.018)\n\n\nNum.Obs.\n6558\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n    \n\n\n\n\nDer geschätzte Koeffizient von \\(B\\) (BTRUE) beträgt etwa \\(0.12\\) und ist hochsignifikant. Übereinstimmend mit Abbildung 4.5 erhalten wir also eine positive Schätzung des Behandlungseffekts. Die Interpretation ist, dass die amtierenden Demokraten bei der Wahl von einem Amtsinhabervorteil profitieren. Dieser Effekt schlägt sich als Stimmenbonus von geschätzten 12% nieder. Diese Schätzung des Behandlungseffekts könnte jedoch verzerrt sein:\n\nDie (implizite) Wahl von \\(h=0.5\\) in unserer Schätzung macht die Isolation des relevanten Frontdoor-Paths (\\(c=0\\) → Treatment → StimmenT) wenig plausibel. \\(h\\) sollte mit einer datengetriebenen Methode gewählt werden.\nWeiterhin könnte die lineare funktionale Form der Regression inadäquat sein: Die lineare Approximation der wahren Regressionsfunktion nahe des Schwellenwerts \\(0\\) könnte unzureichend sein und in einer verzerrten Schätzung des Effekts resultieren. Zur Überprüfung der Robustheit der Ergebnisse sollte mit Schätzungen anhand nicht-linearer Spezifikationen verglichen werden.\n\nUm diesen Gefahren für die Validität der Studie zu begegnen, schätzen wir nun weitere Spezifikationen. Im Folgenden verwenden wir eine Bandweitenschätzung gemäß G. Imbens und Kalyanaraman (2012).\n\n# Bandweite mit Schätzer von IK (2012) berechnen\n(\nIK_BW &lt;- \n  rdd::IKbandwidth(\n    X = house$StimmenTm1, \n    Y = house$StimmenT\n  )\n)\n\n[1] 0.2685123\n\n\nWir schätzen zunächst erneut das lineare Interaktionsmodell, diesmal jedoch mit der Bandweite IK_BW.\n\n# Lineares Interaktionsmodelle mit IK-Bandweite\nhouse_llin_IK &lt;- lm(\n  formula = StimmenT ~ B * StimmenTm1, \n  data = house %&gt;% \n    filter(\n      abs(StimmenTm1) &lt;= IK_BW\n    )\n)\n\nFür den Vergleich mit einer nicht-linearen Spezifikation schätzen wir auch ein quadratisches Interaktionsmodell.\n\n# Quadratisches Interaktionsmodell mit IK-Bandweite\nhouse_poly_IK &lt;- update(\n  object = house_llin_IK,\n  formula = StimmenT ~ B * poly(StimmenTm1, degree = 2, raw = T)\n)\n\nFür eine Gegenüberstellung der Ergebnisse verwenden wir modelsummary().\n\n# Tabellarischer Modellvergleich\nmodelsummary(\n  models = list(\n    \"Linear int.\" = house_llin_IK, \n    \"Quadratisch int.\" = house_poly_IK\n  ),  \n  vcov = \"HC1\", \n  stars = T,\n  gof_map = \"nobs\", \n  output = \"gt\"\n) %&gt;% \n  tabopts\n\n\n\n\n\n\n\n  \n \n      Linear int.\n      Quadratisch int.\n    \n\n\n(Intercept)\n0.450***\n0.460***\n\n\n\n(0.005)\n(0.008)\n\n\nBTRUE\n0.085***\n0.068***\n\n\n\n(0.008)\n(0.012)\n\n\nStimmenTm1\n0.360***\n\n\n\n\n(0.036)\n\n\n\nBTRUE × StimmenTm1\n0.055\n\n\n\n\n(0.059)\n\n\n\npoly(StimmenTm1, degree = 2, raw = T)1\n\n0.573***\n\n\n\n\n(0.138)\n\n\npoly(StimmenTm1, degree = 2, raw = T)2\n\n0.798\n\n\n\n\n(0.493)\n\n\nBTRUE × poly(StimmenTm1, degree = 2, raw = T)1\n\n0.036\n\n\n\n\n(0.219)\n\n\nBTRUE × poly(StimmenTm1, degree = 2, raw = T)2\n\n-1.529+\n\n\n\n\n(0.834)\n\n\nNum.Obs.\n2956\n2956\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n    \n\nTabelle 4.1:  Vergleich von SRDD-Interaktionsmodellen für Lee (2008) \n\n\n\nDie Spalte (1) in Tabelle 4.1 zeigt die lokale Schätzung mit einem linearen Interaktionsmodell. Wir erhalten damit einen Behandlungseffekt von etwa \\(8.5\\%\\). Der Schätzwert fällt also etwas geringer aus als für die globale KQ-Schätzung des linearen Interaktionsmodells. Für das Modell (2) mit quadratischer Spezifikation liegt der Schätzwert mit \\(6.8\\%\\) in der selben Größenordnung. Beide Schätzungen ergeben einen signifikant von \\(0\\) verschieden Effekt. Weiterhin fällt auf, dass in beiden Modellen keine Evidenz für unterschiedliche Formen der Regressionsfunktionen auf beiden Seiten des Schwellenwerts vorliegen: sämtliche Koeffizientenschätzwerte der Interaktionsterme haben hohe Standardfehler und sind nicht signifikant. Im quadratischen Modell hat auch der Term \\(StimmenTm1^2\\) keinen signifikanten Effekt. Diese Ergebnisse deuten darauf hin, dass eine lineare Spezifikation ausreichend ist.\nSRDD-Schätzung mit LOESS\nWir illustrieren nachfolgend die Schätzung des Behandlungseffekts mit einer flexiblen und in der Praxis häufig verwendeten Methode für lokale Regression. Die nachfolgende interaktive Grafik zeigt die klassierten Daten aus Lee (2008) auf dem Intervall \\([-0.5,0.5]\\) gemeinsam mit einer nicht-parametrischen Schätzung des Zusammenhangs von StimmenT und StimmenTm1 mittels LOESS.3 Diese Implementierung von lokaler Regression nutzt einen tricube kernel. Über den Input kann eine Bandweite \\(l\\in(0,1]\\) für den LOESS-Schätzer auf beiden Seiten des Schwellenwerts \\(0\\) gewählt werden. Die Bandweite ist hier der Anteil der Beobachtungen an der gesamten Anzahl an Beobachtungen, die in die Schätzung einbezogen werden sollen.3 LOESS ist eine Variante von lokaler Polynom-Regression.\nFür die Schätzung am Schwellenwert berücksichtigte Daten sind in orange kenntlich gemacht. Die rote linie zeigt die geschätzte Regressionsfunktion über gleichmäßig verteilte Werte von StimmenTm1 auf \\([-0.5,0.5]\\). Die Grafik verdeutlicht, dass die LOESS-Methode flexibel genug ist, um lineare und nicht-lineare Zusammenhänge abbilden zu können. Wie zuvor ist eine adäquate Wahl der Bandweite wichtig:\n\nDer mit LOESS geschätzte Zusammenhang auf beiden Seiten des Schwellenwerts ist etwa linear für den voreingestellten Parameter (\\(l = 0.28\\)).\nFür größere Werte von \\(l\\) nähert sich die Schätzung weiter einem linearen Verlauf an. Die Schätzung des Effekts bleibt vergleichbar mit den Ergebnissen des linearen Interaktionsmodell (s. oben).\nFür kleinere \\(l\\) erhalten wir eine stärkere Anpassung der Schätzung an die Daten. Zu kleine Werte führen zu einer Überanpassung (overfitting). Insbesondere tendiert die geschätzte Funktion zu extremer Steigung nahe des Schwellenwerts → stark verzerrte Schätzung des Effekts!\n\n\nhtml`\n&lt;style&gt;\n.regression {\n  fill: none;\n  stroke: #000;\n  stroke-width: 1.5px;\n}\n.axis line {\n  stroke: #ddd;\n}\n.axis .baseline line {\n  stroke: #555;\n}\n.axis .domain {\n  display: none;\n} \n&lt;/style&gt;\n&lt;link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css\"&gt;\n`\n\n\n\n\n\n\n\nviewof bandwidth = Inputs.range([.01, 1], {\n  label: \"Bandweite LOESS (l)\",\n  step: .01,\n  value: .28\n});\n\nxScaleLoess = d3.scaleLinear()\n   .domain([-.55, .55])\n   .range([0, innerWidth]);\n   \nyScaleLoess = d3.scaleLinear()\n  .domain([.2, .8])\n  .range([innerHeight, 0]);\n\nlineLoess = d3.line()\n  .x(d =&gt; xScaleLoess(d[0]))\n  .y(d =&gt; yScaleLoess(d[1]));\n  \nxAxisLoess = d3.axisBottom(xScaleLoess)\n  .tickSize(innerHeight + 10)\n  .tickValues([-.5, -.25, 0, .25, .5])\n  .tickFormat(d =&gt; d);\n\nyAxisLoess = d3.axisLeft(yScaleLoess)\n  .tickSize(innerWidth + 10)\n  .tickValues([.2, .35, .5, .65, .8])\n  .tickFormat(d =&gt; d);\n\nloessRegression = d3.regressionLoess()\n  .x(d =&gt; d.StimmenTm1)\n  .y(d =&gt; d.StimmenT)\n  .bandwidth(bandwidth);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  const svg = d3.select(DOM.svg(innerWidth + margin.left + margin.right + 20, innerHeight + margin.top + margin.bottom + 20))\n  \n  const g = svg.append(\"g\")\n      .attr(\"transform\", `translate(${margin.left}, ${margin.top})`);\n\n  g.append(\"g\")\n      .attr(\"class\", \"axis\")\n      .call(xAxisLoess);\n\n  g.append(\"g\")\n    .attr(\"class\", \"axis\")\n    .attr(\"transform\", `translate(${innerWidth})`)\n    .call(yAxisLoess);\n\n  // Add X axis label:\n  g.append(\"text\")\n    .attr(\"text-anchor\", \"end\")\n    .attr(\"font-size\", 13)\n    .attr(\"x\", innerWidth)\n    .attr(\"y\", innerHeight + margin.top + 25)\n    .text(\"Differenz Stimmenanteil Demokraten letzte Wahl zur 50%-Schwelle\");\n\n  // Y axis label:\n  g.append(\"text\")\n    .attr(\"text-anchor\", \"end\")\n    .attr(\"transform\", \"rotate(-90)\")\n    .attr(\"font-size\", 13)\n    .attr(\"y\", -margin.left+10)\n    .attr(\"x\", -margin.top+10)\n    .text(\"Stimmenanteil Demokraten\");\n\n  g.selectAll(\"circle\")\n    .data(transpose(house_binned))\n    .enter().append(\"circle\")\n    .attr(\"r\", 2)\n    .attr(\"cx\", d =&gt; xScaleLoess(d.StimmenTm1))\n    .attr(\"cy\", d =&gt; yScaleLoess(d.StimmenT));\n\n  g.selectAll(\"circle\")\n   .filter( function(d){ return Math.abs(d.StimmenTm1) &lt;= bandwidth/2 } )\n   .attr(\"fill\", \"orange\")\n   .attr(\"stroke\", \"none\");\n\nfunction b(d) { return loessRegression(\n        transpose(house).filter(function(d){ return d.StimmenTm1 &lt;= 0 & d.StimmenTm1 &gt;= -.5 })\n    ); }\n\nfunction a(d) { return loessRegression(\n      transpose(house).filter(function(d){ return d.StimmenTm1 &gt; 0 & d.StimmenTm1 &lt;= .5  })\n    ); }\n\n  g.append(\"path\")\n      .attr(\"class\", \"regression\")\n      .datum(b)\n      .attr(\"d\", lineLoess)\n      .style(\"stroke\", \"red\");\n\n  g.append(\"path\")\n      .attr(\"class\", \"regression\")\n      .datum(a)\n      .attr(\"d\", lineLoess)\n      .style(\"stroke\", \"red\");\n  \n  g.append(\"text\")\n    .attr(\"x\", d =&gt; xScaleLoess(-.24))\n    .attr(\"y\", d =&gt; yScaleLoess(.55))\n    .attr(\"dy\", \".35em\")\n    .attr(\"fill\", \"#39FF14\")\n    .text(d3.format(\",.2f\")((a().slice(0))[0][1] - (b().slice(-1))[0][1]));\n  \n  g.append(\"line\")\n  .attr(\"x1\", xScaleLoess(0))\n  .attr(\"y1\", yScaleLoess((b().slice(-1))[0][1]))\n  .attr(\"x2\", xScaleLoess(0))\n  .attr(\"y2\", yScaleLoess((a().slice(0))[0][1]))\n  .attr(\"stroke\", \"#39FF14\")\n  .attr(\"stroke-width\", 2);\n  \n  /* dashed line at cutoff */\n  g.append(\"line\")\n  .attr(\"x1\", xScaleLoess(0))\n  .attr(\"y1\", 0)\n  .attr(\"x2\", xScaleLoess(0))\n  .attr(\"y2\", innerHeight)\n  .style(\"stroke\", \"black\")\n  .style(\"stroke-dasharray\", \"1\")\n  .style(\"stroke-width\", \"1\");\n  \n  /* dashed line data bw upper */\n  g.append(\"line\")\n  .attr(\"x1\", xScaleLoess(bandwidth/2))\n  .attr(\"y1\", 0)\n  .attr(\"x2\", xScaleLoess(bandwidth/2))\n  .attr(\"y2\", innerHeight)\n  .style(\"stroke\", \"blue\")\n  .style(\"stroke-dasharray\", \"4\")\n  .style(\"stroke-width\", \"1\");\n  \n  /* dashed line data bw lower */\n  g.append(\"line\")\n  .attr(\"x1\", xScaleLoess(-bandwidth/2))\n  .attr(\"y1\", 0)\n  .attr(\"x2\", xScaleLoess(-bandwidth/2))\n  .attr(\"y2\", innerHeight)\n  .style(\"stroke\", \"blue\")\n  .style(\"stroke-dasharray\", \"4\")\n  .style(\"stroke-width\", \"1\");\n\n  return svg.node();\n}\n\n\n\n\nAbbildung 4.7: Nicht-parametrische Regression auf beiden Seiten des Cut-offs."
  },
  {
    "objectID": "RDD.html#fuzzy-regression-discontinuity-design",
    "href": "RDD.html#fuzzy-regression-discontinuity-design",
    "title": "\n4  Regression Discontiniuty Designs\n",
    "section": "\n4.3 Fuzzy Regression Discontinuity Design",
    "text": "4.3 Fuzzy Regression Discontinuity Design\n\n\n\n\n\nX\n\nX\nY\n\nY\nX-&gt;Y\n\n\noberhalb c\n\noberhalb c\nX-&gt;oberhalb c\n\n\nBehandlung\n\nBehandlung\nX-&gt;Behandlung\n\n\noberhalb c-&gt;Behandlung\n\n\nZ\n\nZ\nZ-&gt;X\n\n\nZ-&gt;Y\n\n\nZ-&gt;Behandlung\n\n\nBehandlung-&gt;Y\n\n\n\nAbbildung 4.8: Kausales Diagram für FRDD\n\n\n\nEin FRDD liegt vor, wenn die Zuweisung der Behandlung \\(B\\) durch die Laufvariable \\(X\\) (und möglicherweise weitere Variablen \\(Z\\)) beeinflusst wird. Im Vergleich zum SRDD ist die Behandlung dann also nicht ausschließlich durch Überschreiten des Schwellenwerts \\(X = c\\) bestimmt.\nAbbildung 4.8 zeigt den grundsätzlichen Zusammenhang. Hier genügt es weiterhin für \\(X\\) (und ggf. \\(Z\\)) zu kontrollieren, um den Pfad oberhalb \\(C\\) → Behandlung \\(B\\) → \\(Y\\) zu isolieren. Der so für Behandlung \\(B\\) ermittelte Effekt auf \\(Y\\) entspricht jedoch nicht dem “vollständigen” Behandlungseffekt, da bei \\(c\\) die Zuweisung der Behandlung nicht von \\(0\\) auf \\(100\\%\\) springt. Die Schätzung des FRDD berücksichtigt dies und skaliert den geschätzten Effekt entsprechend.\nWir betrachten zunächst den Zusammenhang \\[\\begin{align}\n  Y_i = \\beta_0 + \\beta_1 B_i + \\beta_2 (X_i - c) + u_i.\\label{eq-simpleFRDD}\n\\end{align}\\] In einem FRDD springt die Behandlungswahrscheinlichkeit am Schwellenwert \\(c\\) um \\(\\Delta p&lt;1\\). Wir können \\(B\\) also nicht als deterministische Funktion von \\(X\\), welche die Zuweisung zu Behandlungs- bzw. Kontrollgruppe am Schwellenwert \\(c\\) anzeigt (wie im SRDD), definieren. Stattdessen betrachten wir \\[\\begin{align}\n  P(B_i=1\\vert X_i) =\n  \\begin{cases}\n    g_{X_i&lt;c}(X_i), & X_i &lt; c \\\\\n    g_{X_i\\geq c}(X_i) & X_i \\geq c\n  \\end{cases}\\,. \\label{eq-BFRDD}\n\\end{align}\\] Die Funktionen \\(g_{X_i&lt;c}\\) und \\(g_{X_i\\geq c}\\) können verschieden sein. Es muss jedoch \\[g_{X_i&lt;c}(X_i = c) \\neq g_{X_i\\geq c}(X_i = c)\\] gelten. Die Behandlungsvariable \\(B_i\\) ist im FRDD also eine (binäre) Zufallsvariable, deren bedingte Wahrscheinlichkeitsfunktion \\(P(B_i=1\\vert X_i)\\) am Schwellenwert \\(c\\) eine Diskontinuität aufweist. Abbildung 4.9 zeigt heispielhafte Verläufe nicht-linearer bedingter Wahrscheinlichkeitsfunktion für die Behandlung mit einer Diskontinuität bei \\(X_i = c\\).\n\nCodelibrary(ggplot2)\nlibrary(cowplot)\n\n# Bedingte Behandlungswahrscheinlichkeit im FRDD illustrieren\nggplot() + \n  geom_function(\n    fun = ~ ifelse(\n      . &lt; 0, \n      -.1 * .^2 + .25, \n      -.1 * (.-1.5)^2 + 1\n    ), \n    n = 1000\n  ) + \n    geom_function(\n    fun = ~ ifelse(\n      . &lt; 0, \n     .35, \n     .65\n    ),\n    n = 1000, \n    lty = 2, \n    col = \"red\"\n  ) + \n  scale_x_continuous(\n    name = \"Laufvariable X\", \n    limits = c(-1.5, 1.5),\n    labels = NULL,\n    breaks = NULL\n  ) +\n  scale_y_continuous(\n    name = \"P(D=1|X)\", \n    breaks = c(0, 1), \n    limits = c(0, 1)\n  ) +\n  theme_cowplot()\n\n\n\nAbbildung 4.9: Bedingte Behandlungswahrscheinlichkeiten im FRDD\n\n\n\nDefinition \\(\\eqref{eq-BFRDD}\\) bedeutet, dass eine KQ-Schätzung von \\(\\beta_1\\) anhand \\(\\eqref{eq-simpleFRDD}\\) eine verzerrte Schätzung des Behandlungseffekts ist: Der in \\(\\widehat{\\beta}_1\\) erfasste Effekt auf \\(Y\\) ist auf einen Sprung der Behandlungswahrscheinlichkeit bei \\(X_i = c\\) um weniger als \\(100\\%\\) zurückzuführen. Der wahre Behandlungseffekt wird also unterschätzt. Daher muss \\(\\widehat{\\beta}_1\\) skaliert werden, sodass die Schätzung als Effekt einer Änderung der Behandlungswahrscheinlichkei um \\(100\\%\\) interpretiert werden kann — der erwartete Effekt, wenn ausschließlich Subjekte mit \\(X_i\\geq c\\) behandelt würden. Diese skalierte Schätzung erhalten wir mit IV-Regression (vgl. Kapitel XYZ). Hierfür nutzen wir für \\(B_i\\) die Instrumentvariable \\[\\begin{align*}\n  D_i = \\begin{cases}\n    0, & X_i &lt; c \\\\\n    1, & X_i \\geq c.\n  \\end{cases}\n\\end{align*}\\]\nAngenommen \\(g_{X_i\\geq c}(X_i) = \\alpha_0\\) und \\(g_{X_i&lt;c}(X_i) = \\alpha_0 + \\alpha_1\\) mit \\(\\alpha_0 + \\alpha_1 &lt; 1\\) (vgl. rote Funktion in Abbildung 4.8). Der FRDD-Schätzer des Behandlungseffekts ist dann \\(\\widehat{\\gamma}_\\textup{FRDD}\\) im 2SLS-Verfahren mit den Regressionen \\[\\begin{align}\n  \\begin{split}\n  (\\mathrm{I})\\qquad B_i =&\\, \\alpha_0 + \\alpha_1 D_i + \\alpha_2 (X_i - c) + e_i,\\\\\n  (\\mathrm{II})\\qquad Y_i =&\\, \\gamma_0 + \\gamma_1 \\widehat{B}_i + \\gamma_2 (X_i - c) + \\epsilon_i,\n  \\end{split}\\label{eq:FRDD_simpleIV}\n\\end{align}\\] wobei \\(\\widehat{B}_i\\) die angepassten Werte aus Stufe \\((\\mathrm I)\\) und \\(e_i\\) sowie \\(\\epsilon_i\\) Fehlterterme sind.\nAnalog zum SRDD müssen in empirischen Anwendungen geeignete Spezifikationen für die Regressionsfunktionen \\(\\eqref{eq-simpleFRDD}\\) und \\(\\eqref{eq-BFRDD}\\) gewählt und der 2SLS-Schätzer \\(\\eqref{eq:FRDD_simpleIV}\\) entsprechend angepasst werden. Ein einfaches Interaktionsmodell wäre \\[\\begin{align}\n  \\begin{split}\n  (\\mathrm{I})\\qquad B_i =&\\, \\alpha_0 + \\alpha_1 D_i + \\alpha_2 (X_i - c)\\\\\n  +&\\, \\alpha_3 (X_i - c) \\times D_i + e_i,\\\\\n  \\\\\n  (\\mathrm{II})\\qquad Y_i =&\\, \\gamma_0 + \\gamma_1 \\widehat{B}_i\\\\\n  +&\\, \\gamma_2 (X_i - c) + \\gamma_3 (X_i-c)\\times\\widehat{B}_i, \\epsilon_i\n  \\end{split},\\label{eq:FRDD_lintIV}\n\\end{align}\\] d.h. wir instrumentieren \\(B_i\\) mit \\(D_i\\) und dem Interaktionsterm \\((X_i-c)\\times D_i\\).\nWie im SRDD werden die IV-Ansätze für das FRDD \\(\\eqref{eq:FRDD_simpleIV}\\) und \\(\\eqref{eq:FRDD_lintIV}\\) in empirischen Studien unter Berücksichtigung einer Bandweite (i.d.R. dieselbe Bandweite für beide Stufen) angewendet."
  },
  {
    "objectID": "RDD.html#case-study-protestantische-arbeitsethik",
    "href": "RDD.html#case-study-protestantische-arbeitsethik",
    "title": "\n4  Regression Discontiniuty Designs\n",
    "section": "\n4.4 Case Study: Protestantische Arbeitsethik",
    "text": "4.4 Case Study: Protestantische Arbeitsethik\n\n\n\nDie Studie Beyond Work Ethic: Religion, Individual, and Political Preferences (Basten und Betz 2013) untersucht den Zusammenhang zwischen Religion, individuellen Merkmalen und politischen Präferenzen. Das Hauptaugenmerk ist die Rolle von Religiosität als Einflussfaktor auf politische Einstellungen. Die Hypothese der Autoren ist, dass Religiosität eines Individuums über den traditionellen Rahmen von Moralvorstellungen und sozialen Normen hinaus auch die politischen Präferenzen beeinflusst. Eine entsprechende Theorie wurde zu Beginn des 20. Jahrhunderts entwickelt und prominent von Max Weber (vgl. Weber 2004) vertreten. Weber argumentiert, dass die protestantische Arbeitsethik einen entscheidenden Einfluss auf die Entwicklung des Kapitalismus hatte. Laut Weber führte der protestantische Glaube an harte Arbeit, ein sparsames Leben und ethisches Verhalten zur einer in den damaligen Gesellschaften weit verbreiteten Geisteshaltung, die wirtschaftliches Wachstum förderte und den Aufstieg des Kapitalismus begünstigte.\nBasten und Betz (2013) nutzen Wahlergebnisse sowie geo- und soziodemographische Datensätze für schweizer Gemeinden, um den Zusammenhang zwischen Religiosität und politischen Präferenzen wie links-rechts-Ausrichtung, Einstellungen zur Umverteilung und Einwanderung zu untersuchen. Hierfür verwenden die Autoren ein FRDD, dass eine historisch bedingte Diskontinuität der geographischen Verteilung von evanglischer bzw. katholischer Religionszugehörigkeit zwischen den Kantonen Freiburg (überwiegend dunkelrote Region, frz. Fribourg) und Waadt (kleinere hellrote Region, frz. Vaud) ausnutzt. Die historische Verteilung der Konfessionen in der betrachteten Region im 16. Jahrhundert durch Abspaltung des Kantons Freiburg ist in Abbildung 4.10 dargestellt.\nAufgrund von Bevölkerungsbewegungen ist die Verteilung der Konfessionen zwar nicht mehr eindeutig durch die Kantonsgrenze bestimmt, jedoch sind die Gemeinden der betrachteten Kantone auch heute noch mehrheitlich protestantisch bzw. katholisch. Es ist plausibel, dass eine Prägung gemäß Webers Theorie vorliegt, sich die Gemeinden nahe der Grenz aber hinsichtlich anderer Charakteristika (insb. der Bevölkerungsstruktur) nicht systematisch unterscheiden. Somit liegt ein quai\n\n\n\n\nAbbildung 4.10: Historische Verteilung von Religionszugehörigkeit in Schweizer Gemeinden im 16. Jahrhundert. Quelle: Basten und Betz (2013).\n\n\n\nDie Ergebnisse der Studie zeigen einen signifikanten Einfluss von Protestantismus auf politische Präferenzen, die über traditionelle Moralvorstellungen hinausgehen: Die Autoren finden Hinweise, dass Einwohner evangelisch geprägter Gemeinden eher konservative soziale und politische Ansichten vertreten. Eine mögliche Erklärung für diesen Effekt ist, dass religiöse Institutionen auch eine soziale und politische Agenda verfolgen, die von den Gläubigen internalisiert wird.\n\n4.4.1 Aufbereitung der Daten\nIn diesem Kapitel zeigen wir, wie die Kernergebnisse der Studie mit R reproduziert werden können. Hierfür werden folgende Pakete benötigt.\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(vtable)\nlibrary(rdrobust)\n\nDas Papier sowie der Datensatz BastenBetz.dta sind auf der Übersichtsseite der AEA verfügbar und liegt im STATA-Format .dta vor.44 Siehe alternativ das working paper, falls kein Abbonement für AEA-Journals vorliegt.\n\n# Datensatz einlesen\nBastenBetz &lt;- read_dta('BastenBetz.dta')\n\nDer Datensatz BastenBetz enthält Beobachtungen zu 509 schweizer Gemeinden. Eine Vielzahl an Variablen ist lediglich für Robustheits-Checks relevant. Für die Reproduktion der Kernergebnisse erstellen wir zunächst einen reduzierten Datensatz und transformieren einige Variablen.\n\n# Reduzierten Datensatz erstellen\nBastenBetz &lt;- BastenBetz %&gt;%\n  transmute(\n    gini = Ecoplan_gini,\n    prot = prot1980s,\n    bord = borderdis, \n    vaud,\n    pfl, \n    pfr, \n    pfi\n  )\n\nDie Definitionen der Variablen sind in Tabelle 4.2 gegeben. Die Präferenzen pfl, pfr und pfi basieren auf Wahlergebnissen auf Gemeindeebene zu Volksentscheiden.\n\n\n\n\n\n\n\n\nVariable\nDefinition\n\n\n\nprot\nAnteil Prothestanten im Jahr 1980 (%)\n\n\ngini\nGini-Koeffizient\n\n\nbord\nLaufdistanz zur Kantonsgrenze (Km)\n\n\nvaud\nDummyvariable: Gemeine im Kanton Waadt\n\n\npfl\nPräferenz für Freizeit (%)\n\n\npfr\nPräferenz für Umverteilung (%)\n\n\npfi\nPräferenz für wirtschaftliche Intervention des Staats (%)\n\n\n\nTabelle 4.2: BastenBetz – Variablen und Definitionen\nFür die Berechnung der optimalen Bandweite des FRDD verwenden wir einen MSE-optimalen Schätzer, der in der Funktion rdrobust::rdbwselect() implementiert ist.55 Basten und Betz (2013) setzen BW = 5.01, den Durchschnitt von IK-Schätzungen über Modelle sämtlicher betrachteter Outcome-Variablen. Diese Bandweite liegt nahe des Ergebnisses von rdbwselect. Wir verwenden nachfolgend die Schätzung OB.\n\n# Bandweite schätzen (Bsp. für Freizeitpräferenz)\nbw_selection &lt;- rdbwselect(\n  y = BastenBetz$pfl,\n  x = BastenBetz$bord,\n  fuzzy = BastenBetz$prot, \n  bwselect = \"mserd\", \n  kernel = \"uniform\"\n) \n\n# Bandweite auslesen und zuweisen\n(OB &lt;- bw_selection$bws[1])\n\n[1] 5.078001\n\n\n\n4.4.2 Deskriptive Statistiken\nZur Reproduktion von Tabelle 1 aus Basten und Betz (2013) erzeugen wir eine nach Kantonen gruppierte Zusammenfassung der Daten und berechnen deskriptive Statistiken. Wie im Paper berücksichtigen wir hierbei nur Gemeinden innerhalb der geschätzten optimalen Bandweite OB.\n\n# Datensatz für Reproduktion von Table 1 formatieren\nT1 &lt;- BastenBetz %&gt;%\n  filter(abs(bord) &lt; OB) %&gt;%\n  mutate(\n    vaud = ifelse(\n      test = vaud == 1, \n      yes = \"Waadt\", \n      no = \"Freiburg\"\n    ),\n    prot = prot * 100\n  ) %&gt;%\n  group_by(vaud) %&gt;%\n  summarise(\n    across(\n      everything(), \n      list(\n        Mean = mean, \n        SD = sd, \n        N = length\n      )\n    )\n  ) %&gt;%\n  pivot_longer(\n    cols = -vaud,\n    names_to = c(\"variable\", \"statistic\"), \n    names_sep = \"_\"\n  )\n\nFür die tabellarische Darstellung transformieren wir in ein weites Format, sodass die Tabelle die deskriptive Statistiken spaltenweise für die Kantone zeigt.\n\n# Daten in weites Format überführen\nT1_wider &lt;- T1 %&gt;% \n  pivot_wider(\n    names_from = c(\"vaud\", \"statistic\")\n  )\n\nDie Tabelle erzeugen wir mit gt::gt().\n\n# Tabelle mit gt() erzeugen\nT1_wider %&gt;%\n  gt(rowname_col = \"Variable\") %&gt;% \n  tab_spanner_delim(\n    delim = \"_\",\n  ) %&gt;%\n tabopts\n\n\n\n\n\n\n\n  \n\nvariable\n      \n        Freiburg\n      \n      \n        Waadt\n      \n    \n\nMean\n      SD\n      N\n      Mean\n      SD\n      N\n    \n\n\n\ngini\n0.302\n0.029\n49\n0.367\n0.052\n84\n\n\nprot\n9.428\n5.695\n49\n83.245\n11.411\n84\n\n\nbord\n−2.327\n1.274\n49\n2.493\n1.201\n84\n\n\npfl\n48.239\n4.774\n49\n39.508\n5.723\n84\n\n\npfr\n43.049\n2.634\n49\n39.19\n5.025\n84\n\n\npfi\n52.642\n2.94\n49\n47.086\n3.368\n84\n\n\n\nTabelle 4.3:  Datensatz BastenBetz – Zusammenfassende Statistiken \n\n\n\nDie Statistiken in Tabelle 4.3 scheinen konsistent mit der (historischen) Verteilung der Religionszugehörigkeit und politischen Einstellung gemäß der Hypothese: Im überwiegend katholischen Freiburg finden wir eine größere Einkommensungleichkeit und höhere aus Wahlergebnissen abgeleitete Präferenzen für Freizeit, Umverteilung sowie staatliche Interventionen.\n\n4.4.3 Modellspezifikation und First-Stage-Ergebnisse\nDie Kantone Waadt und Freiburg haben bis heute mehrheitlich protestantische bzw. katholische Gemeinden. Die Verteilung von Protestantismus ist also, u.a. aufgrund von Bevölkerungsbewegungen, nicht mehr deterministisch. An der Kantonsgrenze besteht jedoch eine deutliche Diskontinuität im Anteil protestantischer Einwohner, die auf die historische Verteilung der Religionszugehörigkeit zurückzuführen ist. Damit kann ein FRDD implementiert werden, bei dem die Distanz zur Grenze (bord) die zentrierte Laufvariable ist und die Zugehörigkeit zum Kanton Waadt (vaud) ein Instrument für die Behandlungsvariable (prot) ist.\nWir nutzen die Funktion rdrobust::rdplot um diesen Zusammenhang für verschiedene Bandweiten anhand des linearen Interaktionsmodells \\[\\begin{align}\n  \\begin{split}\n  prot_i =&\\, \\alpha_0 + \\alpha_1 vaud_i + \\alpha_2 bord_i \\\\\n  +&\\, \\alpha_3 bord_i \\times vaud_i + u_i\n  \\end{split}\\label{eq:BBFSR}\n\\end{align}\\] grafisch darzustellen. Dies ist die First-Stage-Regression für die 2SLS-Schätzung der Behandlungseffekte.\n\n# Reproduktion von Abbildung 3 in Basten und Betz (2013)\nplots_BB &lt;- list(\n  # gesch. optimale Bandweite\n  p_OB = rdplot(\n    y = BastenBetz$prot, \n    x = BastenBetz$bord, \n    h = c(OB, OB), \n    x.label = \"Distanz zur Grenze (bord)\",\n    y.label = \"Anteil Protestanten (prot)\", \n    title = \"Gesch. Bandweite\",\n    p = 1, \n    nbins = c(6, 14), \n    masspoints = \"off\"\n  ),\n  \n  # Bandweite 10\n  p_BW10 = rdplot(\n    y = BastenBetz$prot, \n    x = BastenBetz$bord, \n    h = c(10, 10), \n    x.label = \"Distanz zur Grenze (bord)\",\n    y.label = \"Anteil Protestanten  (prot)\", \n    title = \"Bandweite = 10\",\n    p = 1, \n    nbins = c(6, 14),\n    masspoints = \"off\"\n  ),\n  \n  # Bandweite 20\n  p_BW20 = rdplot(\n    y = BastenBetz$prot, \n    x = BastenBetz$bord, \n    h = c(20, 20), \n    x.label = \"Distanz zur Grenze (bord)\",\n    y.label = \"Anteil Protestanten  (prot)\", \n    title = \"Bandweite = 20\",\n    p = 1, \n    nbins = c(6, 14),\n    masspoints = \"off\"\n  ),\n  \n  # Gesamter Datensatz\n  p_G = rdplot(\n    y = BastenBetz$prot, \n    x = BastenBetz$bord,\n    x.label = \"Distanz zur Grenze (bord)\",\n    y.label = \"Anteil Protestanten\", \n    title = \"Ges. Datensatz\",\n    p = 1, \n    nbins = c(6, 14),\n    masspoints = \"off\"\n  )\n)\n\nWir sammeln die Ergebnisse in einem Plot-Gitter mit cowplot::plot_grid().\n\n# Reproduktion von Abbildung 3 in Basten und Betz (2013)\nplot_grid(\n  plotlist = map(plots_BB, ~ .$rdplot), ncol = 2\n)\n\n\n\nAbbildung 4.11: First-Stage-Regressionen\n\n\n\nDie Grafiken in Abbildung 4.11 zeigen deutliche Hinweise auf die Diskontinuität in prot nahe der Kantonsgrenze. Die Größe des geschätzten Sprungs scheint nur wenig sensitiv gegenüber der gewählten Bandweite zu sein. Die Signifikanz des Effekts können wir anhand der jeweiligen KQ-Regressionen beurteilen.66 Wir nutzen update() um die Regression mit weniger Code für verschiedene Bandweiten zu schätzen.\n\n# Reproduktion der First-Stage-Regressionen\n# s. Tabelle 2 in Basten und Betz (2013)\n\n# (1) BW = OB\nFS1 &lt;- lm(\n  formula = prot ~ vaud + bord + vaud * bord, \n  data = BastenBetz %&gt;% \n    filter(\n      abs(bord) &lt;= OB\n    )\n)\n\n# (2) BW = 10\nFS2 &lt;- update(\n  FS1,\n  data = BastenBetz %&gt;% \n    filter(\n      abs(bord) &lt;= 10\n    )\n)\n\n# (3) BW = 20\nFS3 &lt;- update(\n  FS1,\n  data = BastenBetz %&gt;%\n    filter(\n      abs(bord) &lt;= 20\n    )\n)\n\n# (4) Ges. Datensatz\nFS4 &lt;- update(\n  object = FS1,\n  data = BastenBetz\n)\n\n\n# Tabellarische Darstellung\nmodelsummary(\n  list(\n    \"BW = OB\"= FS1, \n    \"BW = 10\" = FS2, \n    \"BW=20\" = FS3, \n    \"Ges. Datensatz\" = FS4\n  ), \n  vcov = \"HC1\", \n  stars = T, \n  gof_map = \"nobs\", \n  output = \"gt\"\n) %&gt;%\n  tabopts()\n\n\n\n\n\n\n\n  \n \n      BW = OB\n      BW = 10\n      BW=20\n      Ges. Datensatz\n    \n\n\n(Intercept)\n0.134***\n0.100***\n0.103***\n0.109***\n\n\n\n(0.017)\n(0.013)\n(0.010)\n(0.009)\n\n\nvaud\n0.671***\n0.726***\n0.756***\n0.710***\n\n\n\n(0.034)\n(0.022)\n(0.018)\n(0.014)\n\n\nbord\n0.017**\n0.001\n0.001\n0.002*\n\n\n\n(0.006)\n(0.003)\n(0.001)\n(0.001)\n\n\nvaud × bord\n-0.006\n-0.001\n-0.009***\n-0.004***\n\n\n\n(0.012)\n(0.005)\n(0.003)\n(0.001)\n\n\nNum.Obs.\n133\n207\n312\n509\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n    \n\nTabelle 4.4:  First-Stage Regressionen \n\n\n\nFür die geschätze Bandweite schätzen wir einen hochsignifikanten Sprung in prot von etwa 67% an der Kantonsgrenze. Auch für größere Bandweiten von 10km und 20km sowie für den gesamten Datensatz finden wir vergleichbare signifikante Effekte, was eine bei zunehmender Distanz zur Grenze persistente Diskrepanz der Religionszugehörigkeit bestätigt.\n\n4.4.4 Second-Stage-Ergebnisse\nWir schätzen nun den LATE von Protestantismus für die Outcome-Variablen gini, pfl, pfi und pfr, vgl. Tabelle 4.2. Die Spezifikation für die Second-Stage-Regression der FRDD-Schätzung ist \\[\\begin{align}\n  \\begin{split}\n    Y_i = \\gamma_0 + \\gamma_1 \\widehat{prot}_i +  \\gamma_2 bord_i + \\gamma_3 bord_i  \\times vaud_i + e_i\n  \\end{split},\n\\end{align}\\] wobei \\(\\widehat{prot}_i\\) angepasste Werte aus der KQ-Schätzung von \\(\\eqref{eq:BBFSR}\\) mit Bandweite OB sind. Dazu erzeugen wir zunächst eine angepasste Version des Objekts BastenBetz, welche nur Gemeinden innerhalb der Bandweite enthält.\n\n# Gemeinden innerhalb der Bandweite filtern\nBastenBetz_OB &lt;- BastenBetz %&gt;% \n  filter(\n    abs(bord) &lt;= OB\n  )\n\nZur Illustration schätzen wir nun die Second-Stage-Regression für \\(Y = pfl\\).\n\n# Second-Stage-Regression für `pfl`\nBastenBetz_OB %&gt;% \n  mutate(\n    prot_fitted = fitted(FS1)\n    ) %&gt;%\n\nlm(\n  pfl ~ prot_fitted + bord + vaud:bord, \n  data = .\n) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = pfl ~ prot_fitted + bord + vaud:bord, data = .)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.8870  -3.8621  -0.0423   3.4993  12.1636 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  50.5275     1.9721  25.621  &lt; 2e-16 ***\nprot_fitted -13.4600     3.1749  -4.240 4.24e-05 ***\nbord          0.4380     0.6528   0.671    0.503    \nbord:vaud    -0.3636     0.7939  -0.458    0.648    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.433 on 129 degrees of freedom\nMultiple R-squared:  0.383, Adjusted R-squared:  0.3686 \nF-statistic: 26.69 on 3 and 129 DF,  p-value: 1.704e-13\n\n\nDer Koeffizient prot_fitted ist der gesuchte Behandlungseffekt. Beachte, dass die von summary() berechneten Standardfehler ungültig sind, weil diese die zusätzliche Unsicherheit durch die Berechnung von \\(\\widehat{prot}\\) über die First-Stage-Regression nicht berücksichtigen. Nachfolgend nutzen wir AER::ivreg(), um komfortabel gültige (heteroskedastie-robuste) Inferenz betreiben zu können.77 Die Autoren geben an, robuste SEs zu nutzen. Das scheint nicht der Fall zu sein, denn vcov = \"HC0\" liefert die Ergebinsse im Paper. Die von Stata berechneten HC1-SEs weichen ab. Dies ändert allerdings nichts an der Signifikanz der Koeffizienten. Wir nutzen vcov = \"HC1\".\n\n# Schätzung mit 2SlS\n# s. Tabelle 4 in Basten und Betz (2013)\n#\n# Wir instrumentieren Treatment (`prot1980s`) mit dem Schwellenindikator (`vaud`)\n# ivreg: exogene Variablen instrumentieren sich selbst, daher\n# ' | vaud * borderdis '\nlibrary(AER)\n# (1) Präferenz für Freizeit\nSS_pfl &lt;- ivreg(\n  formula = pfl ~ prot + bord:vaud + bord | vaud * bord,\n  data = BastenBetz_OB\n)\n\n# (2) Präferenz für Umverteilung\nSS_pfr &lt;- update(\n  object = SS_pfl,\n  formula = pfr ~ prot + bord:vaud + bord | vaud * bord,\n)\n\n# (3) Präferenz für Intervention\nSS_pfi &lt;- update(\n  object = SS_pfl,\n  formula = pfi ~ prot + bord:vaud + bord | vaud * bord,\n)\n\n# (4) Einkommensungleichheit\nSS_gini &lt;- update(\n  object = SS_pfl,\n  formula = pfi ~ prot + bord:vaud + bord | vaud * bord,\n)\n\n\n# Tabellarische Darstellung\nmodelsummary(\n  list(\n    \"(1) Freizeit\"= SS_pfl, \n    \"(2) Umverteilung\" = SS_pfr, \n    \"(3) Intervention\" = SS_pfi, \n    \"(4) Ungleichheit\" = SS_gini\n  ), \n  vcov = \"HC1\", \n  stars = T, \n  gof_map = \"nobs\", \n  output = \"gt\"\n) %&gt;%\n  tabopts()\n\n\n\n\n\n\n\n  \n \n      (1) Freizeit\n      (2) Umverteilung\n      (3) Intervention\n      (4) Ungleichheit\n    \n\n\n(Intercept)\n50.528***\n44.560***\n52.871***\n52.871***\n\n\n\n(1.918)\n(0.950)\n(1.063)\n(1.063)\n\n\nprot\n-13.460***\n-5.061*\n-6.487***\n-6.487***\n\n\n\n(3.161)\n(2.161)\n(1.738)\n(1.738)\n\n\nbord\n0.438\n0.444\n-0.165\n-0.165\n\n\n\n(0.639)\n(0.357)\n(0.332)\n(0.332)\n\n\nbord × vaud\n-0.364\n-0.909\n0.011\n0.011\n\n\n\n(0.811)\n(0.561)\n(0.432)\n(0.432)\n\n\nNum.Obs.\n133\n133\n133\n133\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n    \n\nTabelle 4.5:  Ergebnisse der Second-Stage-Regressionen \n\n\n\nDie Koeffizienten von prot in Tabelle 4.5 sind die mit 2SLS ermittelten erwarteten Behandlungseffekte einer 100%-Reformation (d.h. von 100% katholisch zu 100% protestantisch) für eine durchschnittliche Gemeine nahe der Kantonsgrnze. Es handelt sich jeweils um einen lokalen durchschnittlichen Behandlungseffekt (LATE). Gem. der Definition der abhängigen Variablen, interpretieren wir die Koeffizienten von prot in de Regressionen (1), (2) und (3) als erwartete Prozentänderung durch Reformation. Der Koeffizient in Regression (4) gibt die erwartete Änderung des Gini-Index an. Sämtliche geschätzte Effekte sind signifikant und haben ein mit der Hypothese der Autoren konsistentes negatives Vorzeichen.\nDie Ergebnisse sind Evidenz, dass Protestantismus zu verringerter Präferenz für Freizeit, Umverteilung sowie wirtschaftspolitische Intervention seitens des Staats führt. Auch die ökonomische Ungleichheit ist signifikant geringer, als in einer durchschnittlichen vollständig katholischen Gemeinde.\n\n4.4.5 Addendum: FRDD-Schätzung mit rdrobust()\n\nDie Funktion rdrobust::rdrobust() erlaubt die Schätzung von SRDD und FRDD mit einer Vielzahl von Optionen, s. ?rdrobust. Dies erleichtert die Schätzung mehrerer Modellspezifikationenen und Bandweiten. Mit dem nachstehenden Befehl schätzen wir den LATE von Reformation auf die Präferenz für Umverteilung anhand lokaler quadratischer Regression. Der Output gibt einen Überblick der Bandweitenschätzung sowie der 2 Stufen des 2SLS-Schätzers, inkl. robuster Inferenzstatistiken.\n\npfr_rdr &lt;- rdrobust(\n  y = BastenBetz$pfr,\n  x = BastenBetz$bord,\n  fuzzy = BastenBetz$prot, \n  p = 2,\n  kernel = \"uniform\",\n  vce = \"HC1\"\n) \n\npfr_rdr %&gt;% \n  summary()\n\nFuzzy RD estimates using local polynomial regression.\n\nNumber of Obs.                  509\nBW type                       mserd\nKernel                      Uniform\nVCE method                      HC1\n\nNumber of Obs.                  127          382\nEff. Number of Obs.              85          131\nOrder est. (p)                    2            2\nOrder bias  (q)                   3            3\nBW est. (h)                  10.796       10.796\nBW bias (b)                  22.271       22.271\nrho (h/b)                     0.485        0.485\nUnique Obs.                      97          261\n\nFirst-stage estimates.\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     0.701     0.039    17.782     0.000     [0.624 , 0.778]     \n        Robust         -         -    15.837     0.000     [0.599 , 0.768]     \n=============================================================================\n\nTreatment effect estimates.\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional    -5.047     2.254    -2.239     0.025    [-9.464 , -0.629]    \n        Robust         -         -    -2.210     0.027   [-10.114 , -0.607]    \n=============================================================================\n\n\nAuch für die quadratische Spezifikation erhalten wir mit -5.047 ein vergleichbares signifikantes Ergebnis für den LATE von Protestantismus auf Umverteilung, vgl. Spalte (2) in Tabelle 4.5.\nMit der Option bwselect = \"msetwo\" kann die Bandweite jeweils für die lokale Regression links- und rechtssetig des Schwellenwerts geschätzt werden.\n\npfr_rdr %&gt;% \n  update(bwselect = \"msetwo\") %&gt;%\n  summary()\n\nFuzzy RD estimates using local polynomial regression.\n\nNumber of Obs.                  509\nBW type                      msetwo\nKernel                      Uniform\nVCE method                      HC1\n\nNumber of Obs.                  127          382\nEff. Number of Obs.              51          134\nOrder est. (p)                    2            2\nOrder bias  (q)                   3            3\nBW est. (h)                   5.340       11.387\nBW bias (b)                  13.917       22.330\nrho (h/b)                     0.384        0.510\nUnique Obs.                      97          261\n\nFirst-stage estimates.\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     0.649     0.046    14.216     0.000     [0.560 , 0.739]     \n        Robust         -         -    11.970     0.000     [0.534 , 0.743]     \n=============================================================================\n\nTreatment effect estimates.\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional    -7.487     3.378    -2.216     0.027   [-14.109 , -0.866]    \n        Robust         -         -    -2.156     0.031   [-14.750 , -0.704]    \n=============================================================================\n\n\nTrotz Diskrepanz der geschätzten Bandweiten erhalten wir eine größere aber vergleichbare Schätzung für einen negativen Effekt.\n\n\n\n\n\n\nBasten, Christoph, und Frank Betz. 2013. „Beyond work ethic: Religion, individual, and political preferences“. American Economic Journal: Economic Policy 5 (3): 67–91.\n\n\nCattaneo, Matias D, Michael Jansson, und Xinwei Ma. 2020. „Simple local polynomial density estimators“. Journal of the American Statistical Association 115 (531): 1449–55.\n\n\nGelman, Andrew, und Guido Imbens. 2019. „Why high-order polynomials should not be used in regression discontinuity designs“. Journal of Business & Economic Statistics 37 (3): 447–56.\n\n\nImbens, G. W., und Thomas Lemieux. 2008. „Regression discontinuity designs: A guide to practice“. Journal of econometrics 142 (2): 615–35.\n\n\nImbens, Guido, und Karthik Kalyanaraman. 2012. „Optimal bandwidth choice for the regression discontinuity estimator“. The Review of economic studies 79 (3): 933–59.\n\n\nLee, David S. 2008. „Randomized experiments from non-random selection in US House elections“. Journal of Econometrics 142 (2): 675–97.\n\n\nMcCrary, Justin. 2008. „Manipulation of the running variable in the regression discontinuity design: A density test“. Journal of Econometrics 142 (2): 698–714.\n\n\nWeber, Max. 2004. Die protestantische Ethik und der Geist des Kapitalismus. Bd. 1614. CH Beck."
  },
  {
    "objectID": "RegReg.html#ridge-regression",
    "href": "RegReg.html#ridge-regression",
    "title": "\n5  Regularisierte Regression\n",
    "section": "\n5.1 Ridge Regression",
    "text": "5.1 Ridge Regression\nRidge Regression wurde von Hoerl und Kennard (1970) als Alternative zur KQ-Schätzung bei hoch-korrelierten Regressoren eingeführt. Die Verlustfunktion lautet \\[\\begin{align}\n  \\mathrm{RSS}(\\boldsymbol{\\beta},p=2,\\lambda) = \\mathrm{RSS}(\\boldsymbol{\\beta}) + \\lambda \\lVert\\boldsymbol{\\beta}\\rVert_2,\\label{eq:ridgeloss}\n\\end{align}\\] d.h. der Parameter \\(\\lambda\\) reguliert den Einfluss eines \\(\\ell_2\\)-Strafterms \\[\\begin{align*}\n  \\lVert\\boldsymbol{\\beta}\\rVert_2 = \\sqrt{\\sum_{j=1}^k\\beta_j^2}\n\\end{align*}\\] auf die Verlustfunktion \\(\\mathrm{RSS}(\\boldsymbol{\\beta},p=2,\\lambda)\\). Der Ridge-Schätzer ergibt sich als \\[\\begin{align}\n  \\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_\\lambda := \\arg\\min_{\\boldsymbol{\\beta}}\\mathrm{RSS}(\\boldsymbol{\\beta}) + \\lambda \\lVert\\boldsymbol{\\beta}\\rVert_2.\\label{eq:ridgereg}\n\\end{align}\\]\nFür Das Optimierungsproblem \\(\\eqref{eq:ridgereg}\\) kann wir aus den Bedingungen 1. Ordnung \\[\\begin{align}\n  -2\\boldsymbol{X}'(\\boldsymbol{Y} - \\boldsymbol{X}\\boldsymbol{\\beta}) + 2\\lambda\\boldsymbol{\\beta} = \\boldsymbol{0}\n\\end{align}\\] die analytische Lösung \\[\\begin{align}\n  \\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_\\lambda = (\\boldsymbol{X}'\\boldsymbol{X} + \\lambda\\boldsymbol{I}_p)^{-1}\\boldsymbol{X}'\\boldsymbol{Y},\\label{eq:ridgecf}\n\\end{align}\\] bestimmt werden, wobei \\(\\boldsymbol{I}_k\\) die \\(k\\times k\\) Einheitsmatrix ist. Aus Gleichung \\(\\eqref{eq:ridgecf}\\) kann die Wirkungsweise des Strafterms \\(\\lambda \\lVert\\boldsymbol{\\beta}\\rVert_2\\) abgeleitet werden: Ridge Regression modifiziert die Diagonale der zu invertierenden Matrix \\(\\boldsymbol{X}'\\boldsymbol{X}\\) durch Addition von \\(\\lambda&gt;0\\). Dies ist hilfreich, wenn\n\n\\(k\\geq n\\) und damit \\(\\boldsymbol{X}'\\boldsymbol{X}\\) nicht invertiertbar (singulär) ist. Dann kann der KQ-Schätzer nicht berechnet werden.3 Die Inverse \\((\\boldsymbol{X}'\\boldsymbol{X} + \\lambda\\boldsymbol{I}_p)^{-1}\\) hingegen existiert unter milden Bedingungen.\nhohe Kollinearität vorliegt, sodass \\((\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\) zwar existiert, aber zu einer instablilen KQ-Schätzung mit hoher Varianz führt.\n\n3 Beispiel: X &lt;- matrix(rnorm(100), ncol = 10). Vergleiche solve(t(X) %*% X) und solve(t(X) %*% X + diag(.01, nrow = 10))Für eine grafische Betrachtung des Optimierungskalküls \\(\\eqref{eq:ridgereg}\\) betrachten wir die äquivalente Darstellung als Lagrange-Problem \\[\\begin{align}\n  \\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_\\lambda := \\arg\\min_{\\lVert\\boldsymbol{\\beta}\\rVert&lt;t}\\mathrm{RSS}(\\boldsymbol{\\beta}).\\label{eq:ridgeLg}\n\\end{align}\\] In der folgenden interaktiven Grafik illustrieren wir das Optimierungsproblem \\(\\eqref{eq:ridgeLg}\\) sowie den resultierenden Schätzer der Koeffizienten \\((\\beta_1, \\beta_2)\\) in einem multiplen Regressionsmodell mit den Regressoren \\(X_1\\) und \\(X_2\\).\n\nDie blaue Ellipse ist die Menge aller Schätzwerte \\(\\left(\\widehat\\beta_{1},\\, \\widehat\\beta_{2}\\right)\\) für den angegebenen Wert von \\(\\mathrm{RSS}\\). Im Zentrum der Ellipse liegt der KQ-Schätzer, welcher \\(\\mathrm{RSS}\\) minimiert.\nDer blaue Kreis ist die Menge aller Koeffizienten-Paare \\((\\beta_1, \\beta_2)\\), welche die Restriktion \\(\\beta_1^2 + \\beta_2^2\\leq t\\) erfüllen. Beachte, dass die Größe des Kreises nur durch den Parameter \\(t\\) bestimmt wird, welcher für einen vorgegebenen Wertebereich variiert werden kann.\nDer blaue Punkt ist der Ridge-Schätzer \\((\\widehat\\beta^R_{1,t},\\, \\widehat\\beta^R_{2,t})\\). Dieser ergibt sich als Schnittpunkt zwischen der blauen \\(\\mathrm{RSS}\\)-Ellipse und der Restriktionsregion und variiert mit \\(t\\). Die gestrichelte rote Kurve zeigt den Ridge-Lösungspfad.\nFür kleine Werte \\(t\\) drückt die Shrinkage die geschätzten Koeffizienten Richtung 0, wobei der Lösungspfad i.d.R. nicht-linear verläuft, d.h. die Shrinkage auf den Koeffizienten ist grundsätzlich unterschiedlich. Die Lösung \\((\\widehat\\beta^R_{1,t},\\, \\widehat\\beta^R_{2,t}) = (0,0)\\) existiert nur als Grenzwert für \\(t\\to0\\).\nBeachte, dass der Effekt von \\(t\\) auf die Schätzung umgekehrt für \\(\\lambda\\) verläuft: Größere \\(\\lambda\\) führen zu stärkerer Regularisierung.\n\n\n\n\n5.1.1 Eigenschaften des Schätzers\nDer Ridge-Schätzer \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_\\lambda\\) ist nicht invariant gegenüber der Skalierung der Regressoren. Für empirische Daten sollte daher vorab eine Standardisierung der erklärenden Variablen durchgeführt werden.4 Um die Eigenschaften des Ridge-Schätzers besser zu verstehen, betrachten wir hier den Fall orthonormaler Regressoren \\(\\boldsymbol{X}_j\\).5 Dann ist \\[\\begin{align}\n  \\widehat{\\beta}^{\\mathrm{R}}_{\\lambda,\\,j} = (1+\\lambda)^{-1} \\cdot\\widehat{\\beta}_j,\\quad j = 1,\\dots,k,\\label{eq:ridgeortho}\n\\end{align}\\] d.h. der Ridge-Schätzer skaliert die KQ-Lösung mit einem von \\(\\lambda\\) abhängigen Faktor.64 Bspw. mit der Funktion scale().5 Orthonormalität heißt \\(\\boldsymbol{X}_i'\\boldsymbol{X}_j = 1\\) für \\(i=j\\) und \\(0\\) sonst. Dann ist \\(\\boldsymbol{X}\\)’\\(\\boldsymbol{X} = \\boldsymbol{I}_k\\).6 \\((1+\\lambda)^{-1}\\) wird auch als Shrinkage-Faktor bezeichnet.\nWir illustrieren dies, indem wir den Zusammenhang zwischen KQ- und Ridge-Schätzer im orthonormalen Fall als R-Funktion ridge_ortho() implementieren und für die Parameterwerte \\(\\lambda\\in\\{0,0.5,2\\}\\) plotten.\n\nlibrary(tidyverse)\n\n# Funktion für Rige Regression bei orthonormalen Regressoren\nridge_ortho &lt;- function(KQ, lambda) {\n  1/(1 + lambda) * KQ\n}\n\n\n# KQ-Schätzer gegen Ridge-Schätzer plotten\ndat &lt;- tibble(KQ = seq(-1, 1, .01))\n\nggplot(dat) +\n  geom_function(fun = ridge_ortho, \n                args = list(lambda =  0), \n                lty = 2) + \n  geom_function(fun = ridge_ortho, \n                args = list(lambda = .5), \n                col = \"red\") + \n  geom_function(fun = ridge_ortho, \n                args = list(lambda = 2), \n                col = \"blue\") + \n  xlim(-.4, .4) +\n  xlab(\"KQ-Schätzer von beta_1\") +\n  ylab(\"Ridge-Schätzer von beta_1\")\n\n\n\nAbbildung 5.1: Shrinkage des OLS-Schätzers bei Ridge Regression\n\n\n\nAbbildung 5.1 zeigt, dass der Ridge-Schätzer eine lineare Transformation des KQ-Schätzers (gestrichelte Linie) ist. Größere Werte des Regularisierungsparameters \\(\\lambda\\) führen zu stärkerer Shrinkage des Koeffizientenschätzers in Richtung 0. Die \\(\\ell_2\\)-Norm führt zu proportional zum Absolutwert des KQ-Schätzers verlaufender Shrinkage: Größere Koeffizienten werden stärker bestraft als kleine Koeffizienten.\nDie Eigenschaft \\[\\mathrm{E}\\left(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_{\\lambda,\\,j}\\right) = (1+\\lambda)^{-1} \\cdot \\beta_j\\] zeigt, dass \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_{\\lambda,\\,j}\\) (für fixes \\(\\lambda&gt;0\\)) nicht erwartungstreu für \\(\\beta_j\\) ist. Weiterhin ist \\[\\begin{align*}\n  \\mathrm{Var}\\left(\\widehat{\\beta}^{\\mathrm{R}}_{\\lambda,\\,j}\\right) =&\\,\n  \\mathrm{Var}\\left(\\widehat{\\beta}_j\\right) \\cdot \\left(\\frac{\\lambda}{1+\\lambda^2}\\right)\\\\\n    =&\\, \\sigma^2\\cdot \\left(\\frac{\\lambda}{1+\\lambda^2}\\right),\n\\end{align*}\\] wobei \\(\\sigma^2\\) die Varianz des Regressionsfehlers \\(u\\) ist. Wegen \\(\\lambda&lt;(1+\\lambda)^2\\) für \\(\\lambda&gt;0\\) gilt \\[\\mathrm{Var}\\left(\\widehat{\\beta}^{\\mathrm{R}}_{\\lambda,\\,j}\\right)&lt;\\mathrm{Var}\\left(\\widehat{\\beta}_j\\right).\\] Der Ridge-Schätzer hat also eine kleinere Varianz als der KQ-Schätzer. Diese Eigenschaften können auch für korrelierte Regressoren gezeigt werden.\n\n5.1.2 Ridge Regression mit glmnet\n\nWir zeigen nun anhand simulierter Daten, wie der Ridge-Lösungspfad mit dem R-Paket glmnet berechnet werden kann. Wir erzeugen zunächst Daten gemäß der Vorschrift \\[\\begin{align}\n  \\begin{split}\n  Y_i =&\\, \\boldsymbol{X}_i' \\boldsymbol{\\beta} + u_i,\\\\\n  \\\\\n  \\beta_j =&\\,  \\frac{5}{j^2}, \\qquad\\qquad\\ j=1,\\dots,5,\\\\\n  \\beta_j =&\\, -\\frac{5}{(j-5)^2}, \\quad j=6,\\dots,10,\\\\\n  \\\\\n  \\boldsymbol{X}_i \\sim&\\, N(\\boldsymbol{0}, \\boldsymbol{\\Sigma}), \\quad u_i \\overset{u.i.v.}{\\sim} N(0, 1), \\quad i = 1,\\dots,25.\n  \\end{split} \\label{eq:ridgedgp1}\n\\end{align}\\] Hierbei wird \\(\\boldsymbol{\\Sigma}\\) so definiert, dass jeder Regressor \\(N(0,1)\\)-verteilt ist und eine Korrelation von \\(0.8\\) mit allen anderen Regressoren aufweist. Mit der Vorschrift für die \\(\\beta_j\\) stellen wir sicher, dass es wenige Variablen gibt, die \\(Y\\) stark beeinflussen, da der Absolutbetrag der Koeffizienten in \\(j\\) abnimmt.77 Für bessere Interpretierbarkeit der Grafischen Auswertung, wählen wir positive und negative Koeffizienten mit gleichem Bertag.\n\nlibrary(gendata)\nset.seed(1234)\n\n# Parameter definieren\nN &lt;- 80\nk &lt;- 10\n\ncoefs &lt;- 5/(1:(k/2))^2\nbeta &lt;- c(coefs, -coefs)\n\n# Beobachtungen simulieren\nX &lt;- as.matrix(\n  genmvnorm(\n    k = k, \n    cor = rep(.8, (k^2-k)/2), \n    n = N)\n  )\nY &lt;- X %*% beta + rnorm(N)\n\nWir schätzen nun ein Modell mit allen 10 Regressoren mit glmnet. Beachte, dass für den Ridge-Strafterm alpha = 0 gesetzt werden muss.88 alpha ist ein Mischparameter im Algorithmus für elastic net, siehe ?glmnet.\n\nlibrary(glmnet)\n\n# Ridge-Regression anpassen\nridge_fit &lt;- glmnet(\n  x = X, \n  y = Y, \n  alpha = 0 # für Ridge-Strafterm\n)\n\nDer Lösungspfad der Ridge-Schätzung kann nach Transformation der geschätzen Koeffizienten und der zugehörigen \\(\\lambda\\)-Werte in ein langes Format überführt und komfortabel mit ggplot2 dargestellt werden.\n\n# Lambda-Sequenz auslesen\nlambdas &lt;- ridge_fit$lambda\n\n# Ridge-Schätzung für Lambdas im langen Format \nas.matrix(ridge_fit$beta) %&gt;% \n  as_tibble() %&gt;% \n  rownames_to_column(\"Variable\") %&gt;%\n  pivot_longer(-Variable) %&gt;% \n  group_by(Variable) %&gt;% \n  mutate(lambda = lambdas) %&gt;%\n  \n  # Grafik mit ggplot erzeugen\n  ggplot(\n    mapping = aes(\n      x = lambda, \n      y = value, \n      col = Variable\n    )\n  ) + \n  geom_line() +\n  ylab(\"gesch. Koeffizienten\") +\n  scale_x_log10(\"log_10(lambda)\")\n\n\n\nAbbildung 5.2: Lösungspfad für Ridge-Schätzung\n\n\n\nAbbildung 5.2 zeigt den nicht-linearen Verlauf der Shrinkage auf den geschätzten Modellkoeffizienten. Die Koeffizienten werden mit zunehmendem \\(\\lambda\\) von der KQ-Lösung ausgehend (linkes Ende der Skala) in Richtung 0 gezwungen.\nÜber die Funktion cv.glmnet() kann ein optimales \\(\\lambda\\) mit Cross Validation (CV) ermittelt werden. Ähnlich wie bei glmnet() wird für die Validierung automatisch eine \\(\\lambda\\)-Sequenz erzeugt. Wir nutzen autoplot() aus dem R-Paket ggfortify für die Visualisierung der Ergebnisse mit ggplot2.\n\nlibrary(ggfortify)\n\n# Cross-validierte Bestimmung von lambda\nridge_cvfit &lt;- cv.glmnet(\n  y = Y, \n  x = X, \n  intercept = F,\n  alpha = 0\n) \n\n# Ergebnisse plotten\nridge_cvfit %&gt;% \n  autoplot(label.n = 0)\n\n\n\nAbbildung 5.3: Lösungspfad für Ridge-Schätzung\n\n\n\nAbbildung 5.3 zeigt ridge_cvfit$lambda.min, das optimale \\(\\lambda\\) mit dem geringsten CV Mean-Squarred-Error (linke gestrichelte Linie) und ridge_cvfit$lambda.1se, das größte \\(\\lambda\\), welches innerhalb einer Standardabweichung entfernt ist (rechte gestrichelte Linie).9 Wir berechnen die Schätzung für lambda.min.9 Die Wahl von lambda.1se ist eine Heuristik, welche die Schätzunsicherheit berücksichtigt und zu einem “sparsameren” Modell tendiert.\n\n(\n  ridge_coefs &lt;- coef(\n    object = ridge_cvfit, \n    s = ridge_cvfit$lambda.min\n  )\n)\n\n11 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s1\n(Intercept)  .        \nX1           4.1302194\nX2           1.0245661\nX3           0.3139297\nX4           0.5697498\nX5           0.2928664\nX6          -4.1693524\nX7          -0.7509305\nX8          -0.3844761\nX9          -0.3841997\nX10         -0.4078514\n\n\nWir schätzen das Modell nun mit KQ und vergleichen die Koeffizienten mit der Ridge-Schätzung.\n\n# KQ-Schätzung durchführen\nKQ_fit &lt;- lm(Y ~ X - 1)\n\n# Koeffizienten auslesen und transformieren:\ntibble(\n  Ridge = as.matrix(ridge_coefs)[2:11, ],\n  KQ = KQ_fit$coefficients\n) %&gt;% \n  mutate(j = factor(1:10)) %&gt;%\n  pivot_longer(\n    cols = Ridge:KQ, \n    names_to = \"Methode\", \n    values_to = \"Koeffizient\"\n  ) %&gt;%\n\n# Bar-Plot für Koeffizientenvergleich erzeugen  \n  ggplot(\n    mapping = aes(\n      x = j, \n      y = Koeffizient, \n      fill = Methode\n    )\n  ) +\n  geom_bar(\n    position = \"dodge\", \n    stat = \"identity\", \n    width = .5\n  )\n\n\n\nAbbildung 5.4: Koeffizientenvergleich: Ridge vs. KQ\n\n\n\nDer Vergleich anhand von Abbildung 5.4 zeigt deutlich, dass Ridge Regression im Vergleich mit KQ zu absolut kleineren Koeffizientenschätzungen tendiert. Inwiefern dies Konsequenzen für die Prognosegüte der Schätzung hat, können wir Anhand eines Testdatensatzes bestimmen. Hierzu vergleichen wir die mittleren Fehler (MSE) bei der Prognose von \\(Y\\) für die Beobachtungen im Testdatensatz. Für die Simulation des Testdatensatzes nutzen wir erneut die Vorschrift \\(\\eqref{eq:ridgedgp1}\\) um 80 neue Beobachtungen zu erzeugen.\n\n# Test-Datensatz erstellen\nset.seed(4321)\n# Regressoren\nnew_X &lt;- as.matrix(\n  genmvnorm(\n    k = k, \n    cor = rep(.85, (k^2-k)/2), \n    n = N\n  )\n)\n# Abh. Variable\nnew_Y &lt;- new_X %*% beta + rnorm(N)\n\nFür beide Methoden können wir predict() für die Prognosen von \\(Y\\) für den Testdatensatz (new_Y) nutzen.\n\n# Ridge: Vorhersage von new_Y für Test-Datensatz\nY_predict_ridge &lt;- predict(\n  object = ridge_cvfit, \n  newx = new_X, \n  s = ridge_cvfit$lambda.min\n)\n\n# Ridge: MSE für Test-Datensatz berechnen\nmean((Y_predict_ridge - new_Y)^2)\n\n[1] 1.288457\n\n\nDie Vorhersage für lm() benötigt dieselben Variablennamen wie im angepassten Modell, s. KQ_fit$coefficients.\n\n# Test-Datensatz für predict.lm() formatieren\nnew_X &lt;- as.data.frame(new_X)\ncolnames(new_X) &lt;- paste0(\"X\", 1:k)\n\n# KQ: Vorhersage von new_Y für Test-Datensatz\nY_predict_KQ &lt;- predict(\n  object = KQ_fit, \n  newdata = new_X\n)\n\n# KQ: MSE für Test-Datensatz berechnen\nmean((Y_predict_KQ - new_Y)^2)\n\n[1] 29.33797\n\n\nDie Ergebnisse zeigen, dass der Ridge-Schätzer trotz seiner Verzerrung einen deutlich geringeren mittleren Vorhersagefehler für die Testdaten erzielt als der KQ-Schätzer. Diese Eigenschaft der Koeffizientenschätzung kann die Prognosegüte von Ridge Regression gegenüber der KQ-Regression verbessern.\n\n5.1.3 Beispiel: Vorhersage von Abschlussnoten in Mathe\nZur Illustration von Ridge Regression nutzen wir den Datensatz SP aus Cortez und Silva (2008).10 SP enhält Beobachtungen zu Leistungen von insgesamt 100 Schülerinnen und Schülern im Fach Mathematik in der Sekundarstufe an zwei portugiesischen Schulen. Neben der Abschlussnote in Mathe (G3, Skala von 0 bis 20) beinhaltet SP diverse demografische, soziale und schulbezogene Merkmale, die mithilfe von Schulberichten und Fragebögen erhoben wurden. Ziel ist es, ein Modell für die Prognose von G3 anzupassen.10 Wir verwenden eine Auszug aus dem Orignaldatensatz, der nebst ausführlicher Variablenbeschreibung hier verfügbar ist.\nWir lesen zunächst die Daten (im .csv-Format) ein.\n\n# Daten einlesen\nSP &lt;- read_csv(file = \"datasets/SP.csv\")\n\nEin Überblick zeigt, dass der Großteil der Regressoren aus kategorialen Variablen mit sozio-ökonomischen Informationen besteht.\n\n# Überblick\nglimpse(SP)\n\nRows: 100\nColumns: 31\n$ school     &lt;chr&gt; \"GP\", \"GP\", \"GP\", \"MS\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\",…\n$ sex        &lt;chr&gt; \"M\", \"M\", \"F\", \"F\", \"M\", \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\",…\n$ age        &lt;dbl&gt; 17, 18, 19, 17, 16, 16, 19, 16, 16, 16, 18, 16, 15, 17, 17,…\n$ address    &lt;chr&gt; \"R\", \"R\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"R\", \"U\", \"U\",…\n$ famsize    &lt;chr&gt; \"GT3\", \"GT3\", \"LE3\", \"GT3\", \"LE3\", \"GT3\", \"GT3\", \"GT3\", \"GT…\n$ Pstatus    &lt;chr&gt; \"T\", \"T\", \"T\", \"T\", \"A\", \"T\", \"T\", \"T\", \"A\", \"T\", \"T\", \"T\",…\n$ Medu       &lt;dbl&gt; 1, 4, 3, 2, 3, 2, 0, 2, 3, 4, 4, 2, 1, 2, 2, 3, 3, 4, 4, 2,…\n$ Fedu       &lt;dbl&gt; 2, 3, 2, 2, 4, 3, 1, 1, 1, 4, 4, 2, 2, 3, 2, 3, 1, 3, 4, 2,…\n$ Mjob       &lt;chr&gt; \"at_home\", \"teacher\", \"services\", \"other\", \"services\", \"oth…\n$ Fjob       &lt;chr&gt; \"other\", \"services\", \"other\", \"at_home\", \"other\", \"other\", …\n$ reason     &lt;chr&gt; \"home\", \"course\", \"reputation\", \"home\", \"home\", \"reputation…\n$ guardian   &lt;chr&gt; \"mother\", \"mother\", \"other\", \"mother\", \"mother\", \"mother\", …\n$ traveltime &lt;dbl&gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1,…\n$ studytime  &lt;dbl&gt; 2, 3, 2, 3, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 3, 1, 2,…\n$ failures   &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ schoolsup  &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no…\n$ famsup     &lt;chr&gt; \"no\", \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", …\n$ paid       &lt;chr&gt; \"no\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"n…\n$ activities &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"y…\n$ nursery    &lt;chr&gt; \"yes\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\"…\n$ higher     &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes…\n$ internet   &lt;chr&gt; \"no\", \"yes\", \"yes\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", …\n$ romantic   &lt;chr&gt; \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"…\n$ famrel     &lt;dbl&gt; 3, 5, 4, 3, 5, 4, 3, 4, 2, 2, 1, 5, 4, 5, 3, 5, 4, 4, 5, 5,…\n$ freetime   &lt;dbl&gt; 1, 3, 2, 4, 3, 4, 4, 5, 3, 4, 4, 4, 3, 3, 4, 4, 5, 2, 3, 4,…\n$ goout      &lt;dbl&gt; 3, 2, 2, 3, 3, 3, 2, 2, 3, 4, 2, 4, 2, 3, 4, 2, 4, 2, 3, 4,…\n$ Dalc       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1,…\n$ Walc       &lt;dbl&gt; 5, 2, 2, 1, 1, 3, 1, 1, 2, 3, 2, 4, 1, 3, 3, 1, 3, 2, 1, 1,…\n$ health     &lt;dbl&gt; 3, 4, 1, 3, 5, 4, 5, 5, 4, 4, 1, 5, 5, 3, 5, 5, 1, 3, 5, 5,…\n$ absences   &lt;dbl&gt; 4, 9, 22, 8, 4, 6, 2, 20, 5, 6, 5, 0, 2, 2, 12, 0, 17, 0, 4…\n$ G3         &lt;dbl&gt; 10, 16, 11, 11, 11, 10, 9, 12, 7, 11, 16, 12, 9, 12, 12, 13…\n\n\nUm die Prognosegüte des Modells beurteilen zu können, partitionieren wir SP zufällig in einen Test- sowie einen Trainingsdatensatz (mit 30 und 70 Beobachtungen), jeweils für die Regressoren und die abhängige Variable.\n\n# ID für Beobachtungen im Testdatensatz zufällig erzeugen\nset.seed(1234)\nID &lt;- sample(1:nrow(SP), size = 30)\n\n# Regressoren aufteilen\nSP_test &lt;- SP[ID,]\nSP_train &lt;- SP[-ID,]\n\n# Abh. Variable aufteilen\nY_test &lt;- SP_test$G3\nY_train &lt;- SP_train$G3\n\nAls nächstes passen wir ein Ridge-Regressionsmodell für alle Regressoren in SP_train an und ermitteln ein optimales \\(\\lambda\\) mit Cross Validation. Beachte, dass cv.glmnet nicht für Regressoren im data.frame/tibble-Format ausgelegt ist, sondern ein matrix-Format erwartet. Wir transformieren SP_train daher mit data.matrix().\n\n# Ridge-Regression und CV für Trainingsdaten\nSP_fit_cv &lt;- cv.glmnet(\n  x = data.matrix(SP_train %&gt;% select(-G3)), \n  y = Y_train, \n  alpha = 0\n)\n\n# CV-Ergebnisse für lambda visualisieren\nSP_fit_cv %&gt;% \n  autoplot(label.n = 0)\n\n\n\n\nWie für das Beispiel mit simulierten Daten erhalten wir mit predict() Vorhersagen für die erzielte Punktzahl. Beachte, dass wir den MSE nicht für die Trainingsdaten SP_train, sondern für die Testdaten SP_test berechnen.\n\n# Prognose von G3 anhand des Ridge-Modells\nY_predict_ridge &lt;- predict(\n  object = SP_fit_cv, \n  newx = data.matrix(\n    SP_test %&gt;% \n      select(-G3)\n    ), \n  s = SP_fit_cv$lambda.min\n)\n\n# MSE für Testdaten berechnen\nmean((Y_predict_ridge - Y_test)^2)\n\n[1] 21.13249\n\n\nAuch in diesem empirischen Beispiel zeigt ein Vergleich der MSEs, dass Ridge Regression dem KQ-Schätzer hinsichtlich der Vorhersagegüte überlegen ist.\n\n# Modell mit KQ schätzen\nSP_fit_KQ &lt;- lm(G3 ~ ., SP_train)\n\n# Prognose\nY_predict_KQ &lt;- predict(\n  object = SP_fit_KQ, \n  newdata = SP_test %&gt;% \n    select(-G3)\n)\n\n# Testset-MSE berechnen\nmean((Y_predict_KQ - Y_test)^2)\n\n[1] 29.76893\n\n\nDer MSE für Ridge ist mit \\(21.13\\) deutlich kleiner als \\(29.77\\), der MSE für KQ.\nFür die Interpretation der Ridge-Schätzung erweitern den Code für die ggplot2-Grafik der Koeffizienten-Pfade um eine vertikale Linie des mit CV ermittelten \\(\\lambda\\) und fügen mit dem Paket ggrepel Labels für die Pfade der größten Koeffizienten hinzu.\n\nlibrary(ggrepel)\n\n# Lambda-Sequenz auslesen\nlambdas &lt;- SP_fit_cv$lambda\n\n# Ridge-Schätzung für Lambdas im langen Format \ndf &lt;- as.matrix(SP_fit_cv$glmnet.fit$beta) %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    Variable = rownames(SP_fit_cv$glmnet.fit$beta)\n  ) %&gt;%\n  pivot_longer(-Variable) %&gt;% \n  group_by(Variable) %&gt;% \n  mutate(lambda = lambdas) \n\n# Grafik mit ggplot erzeugen\ndf %&gt;%\n  ggplot(\n    mapping = aes(\n      x = lambda, \n      y = value, \n      col = Variable\n    )\n  ) + \n  geom_line() +\n  geom_label_repel(\n    data = df %&gt;% \n      filter(lambda == min(lambdas)),\n    mapping = aes(label = Variable), \n    seed = 1234,\n    size = 5, \n    max.overlaps = 8, \n    nudge_x = -.5) +\n  ylab(\"gesch. Koeffizienten\") +\n  scale_x_log10(\"log_10(lambda)\") +\n  geom_vline(\n    xintercept = SP_fit_cv$lambda.min, \n    col = \"red\", \n    lty = 2\n  ) +\n  theme(legend.position = \"none\")\n\n\n\nAbbildung 5.5: Lösungspfad für Ridge-Schätzung\n\n\n\nAbbildung 5.5 gibt Hinweise darauf, dass neben der Schulzugehörigkeit und Indikatoren für schulische Leistung (bspw. failures) sozio-ökonomische Prädiktoren wie internet (Internetzugang zuhause), Pstatus (Zusammenleben der Eltern) und address/traveltime (sozialer Status) relevante Variablen zu sein scheinen.\nDas optimale \\(\\lambda_\\mathrm{cv} \\approx 0.21\\) (gestrichelte rote Linie in Abbildung 5.5) führt zu deutlicher Shrinkage, was eine mögliche Erklärung für den besseren Testset-MSE von Ridge Regression ist: Die Koeffizienten von Variablen mit wenig Erklärungskraft werden durch die Regularisierung in Richtung 0 gezwungen und reduzieren so die Varianz der Vorhersage gegenüber der (idealerweise) unverzerrten KQ-Schätzung.\n\n\n\n\n\n\nKey Facts zu Ridge Regression\n\n\n\n\nRidge-Regression regularisiert den KQ-Schätzer mit der \\(\\ell_2\\)-Norm der Koeffizienten. Diese Form von Regularisierung ist eine Alternative für KQ in Anwendungen mit mehr Regressoren als Beobachtugen (\\(k\\geq n\\)) und/oder wenn KQ aufgrund starker Kollinearität eine hohe Varianz aufweist.\nDer Ridge-Schätzer \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_\\lambda\\) ist nicht erwartungstreu. Die geschätzten Koeffizienten sind auch für \\(n\\to\\infty\\) verzerrt.\nAufgrund der verzerrten Schätzung ist statistische Inferenz für Koeffizienten mit \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_\\lambda\\) problematisch. Anstatt für strukturelle Modelle oder die Schätzung kausaler Effekte wird Ridge Regression in der Praxis daher überwiegend für Prognosen verwendet.\nDie Wahl von \\(\\lambda\\) impliziert einen Tradeoff zwischen Verzerrung und Varianz: Große \\(\\lambda\\) schrumpfen die Koeffizientenschätzer Richtung 0 (mehr Verzerrung), führen aber zu einer kleineren Varianz der Schätzung. Entsprechend können Vorhersagen mit mehr Verzerrung aber weniger Varianz als mit KQ getroffen werden.\nRidge Regression kann in R mit dem Paket glmnet berechnet werden."
  },
  {
    "objectID": "RegReg.html#lasso-regression",
    "href": "RegReg.html#lasso-regression",
    "title": "\n5  Regularisierte Regression\n",
    "section": "\n5.2 Lasso Regression",
    "text": "5.2 Lasso Regression\nLeast Absolute Shrinkage and Selection Operator (Lasso) ist ein von Tibshirani (1996) vorgeschlagener Schätzer, der die Verlustfunktion des KQ-Schätzers um einen Strafterm für die Summe der (absoluten) Größe der Koeffizienten \\(\\boldsymbol\\beta = (\\beta_1, \\dots,\\beta_k)'\\) erweitert. Die Verlustfunktion des Lasso-Schätzers von \\(\\boldsymbol{\\beta}\\) lautet \\[\\begin{align}\n\\mathrm{RSS}(\\boldsymbol{\\beta},p=1,\\lambda) = \\mathrm{RSS}(\\boldsymbol{\\beta}) + \\lambda \\lVert\\boldsymbol{\\beta}\\rVert_1.\\label{eq:lassoloss}\n\\end{align}\\] Für den Strafterm wird also die \\(\\ell_1\\)-norm \\[\n\\lVert\\boldsymbol{\\beta}\\rVert_1 = \\sum_{j=1}^k \\lvert\\beta_j \\rvert\n\\] verwendet. Der Lasso-Schätzer \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{L}}_\\lambda\\) für \\(\\boldsymbol{\\beta}\\) minimiert \\(\\eqref{eq:lassoloss}\\), \\[\\begin{align}\n\\boldsymbol{\\beta}^{\\mathrm{L}}_\\lambda = \\arg\\min_{\\boldsymbol{\\beta}} \\ \\mathrm{RSS}(\\boldsymbol{\\beta},p=1,\\lambda).\n\\end{align}\\] Entsprechend erhalten wir in Abhängigkeit von \\(\\lambda\\) ein Kontinuum an Lösungen \\[\\begin{align}\n  \\left\\{\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{L}}_\\lambda\\right\\}_{\\lambda=0}^{\\lambda=\\infty},\\label{eq:LassoPath}\n\\end{align}\\] der sogenannte Lasso-Pfad.\nDas Optimierungsproblem \\(\\eqref{eq:lassoloss}\\) hat die äquivalente Darstellung \\[\\begin{align}\n  \\begin{split}\n    \\widehat{\\boldsymbol{\\beta}}^{\\mathrm{L}}_\\lambda =&\\, \\arg\\min_{\\boldsymbol{\\beta}} \\mathrm{RSS}(\\boldsymbol{\\beta}) + \\lambda\\left(\\lVert\\boldsymbol{\\beta}\\rVert_1 - t\\right)\\\\\n    =&\\, \\arg\\min_{\\lVert\\boldsymbol{\\beta}\\rVert_1\\leq t} \\mathrm{RSS}(\\boldsymbol{\\beta}),\n  \\end{split}\\label{eq:lassolagrange}\n\\end{align}\\] welche über den Lagrange-Ansatz unter der Nebenbedingung \\(\\lVert\\boldsymbol{\\beta}\\rVert_1 \\leq t\\) gelöst werden kann.\nÄhnlich wie der KQ-Schätzer ist der Lasso-Schätzer \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{L}}_\\lambda\\) durch Bedingungen 1. Ordnung bestimmt. Diese Bedingungen lassen sich komfortabel in Matrix-Schreibweise darstellen als \\[\\begin{align}\n  -2\\boldsymbol{X}_j'(\\boldsymbol{Y} - \\boldsymbol{X}\\boldsymbol{\\beta}) + \\lambda\\cdot\\mathrm{sgn}(\\beta_j) = 0, \\quad j = 1,\\dots,k.\\label{eq:LassoFOC}\n\\end{align}\\] Aus Gleichung \\(\\eqref{eq:LassoFOC}\\) folgt, dass der Lasso-Schätzer aufgrund des Strafterms im Allgemeinen nicht algebraisch bestimmt werden kann.1111 Zur Bestimmung des Schätzers werden Algorithmen der nicht-linearen Optimierung genutzt.\nIn Abhängigkeit von \\(\\lambda\\) zwingt der Lasso-Schätzer die KQ-Schätzung von \\(\\beta_j\\) zu einem (absolut) kleineren Wert: Ähnlich wie bei Ridge Regression bewirkt der \\(\\ell_1\\)-Strafterm eine mit \\(\\lambda\\) zunehmende Schrumpfung der geschätzen Koeffizienten in Richtung 0. Charakteristisch für die Lösung des Lasso-Schätzers ist, dass \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{L}}_j = 0\\), wenn die Bedingung \\[\\begin{align}\n  \\left\\lvert\\boldsymbol{X}_j'(\\boldsymbol{Y} - \\boldsymbol{X}\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{L}}_\\lambda)\\right\\rvert - \\lambda/2 \\leq 0 \\label{eq:lassoselection}\n\\end{align}\\] erfüllt ist. In Abhängigkeit von \\(\\lambda\\) kann der Lasso-Schätzer folglich geschätzte Regressionskoeffizienten nicht nur in Richtung \\(0\\), sondern diese auch exakt mit \\(0\\) schätzen und damit Variablenselektion betreiben. Aufgrund der mit \\(\\lambda\\) zunehmenden Shrinkage bis die Bedingung \\(\\eqref{eq:lassoselection}\\) erfüllt und der Koeffizient gleich \\(0\\) gesetzt wird, bezeichnet man Lasso auch als einen Soft Thresholding Operator. Im nächsten Abschnitt betrachten wir die Eigenschaften von Lasso-Regularisierung unter vereinfachten Annahmen bzgl. der Regressoren.\n\n5.2.1 Lasso ist Soft Thresholding\nWir betrachten nun eine mathematische Darstellung von Selektions- und Shrinkage-Eigenschaft des Lasso-Schätzers in einem vereinfachten Modell. Wenn die Regressoren \\(\\boldsymbol{X}\\) orthonormal zueinander sind, existiert eine analytische Lösung des Lasso-Schätzers, \\[\\begin{align}\n  \\widehat{\\boldsymbol{\\beta}}^{\\mathrm{L}}_\\lambda =\n  \\begin{cases}\n    \\widehat{\\boldsymbol{\\beta}}_j - \\lambda/2 &, \\ \\ \\widehat{\\boldsymbol{\\beta}}_j &gt; \\lambda/2\\\\\n    0 &, \\ \\ \\lvert\\widehat{\\boldsymbol{\\beta}}_j\\rvert\\leq\\lambda/2\\\\\n    \\widehat{\\boldsymbol{\\beta}}_j + \\lambda/2 &, \\ \\ \\widehat{\\boldsymbol{\\beta}}_j &lt; \\lambda/2\n  \\end{cases},\\label{eq:lassoST}\n\\end{align}\\] wobei \\(\\widehat{\\boldsymbol{\\beta}}_j\\) der KQ-Schätzer von \\(\\beta_j\\) ist. Anhand von \\(\\eqref{eq:lassoST}\\) können wir die Selektionseigenschaft sowie die Schrumpfung der KQ-Koeffizientenschätzung in Abhängigkeit der durch \\(\\lambda\\) regulierten \\(\\ell_1\\)-Strafe erkennen. Für eine Visualisierung implementieren wir \\(\\eqref{eq:lassoST}\\) als R-Funktion lasso_st() und zeichnen die resultierenden Koeffizientenschätzungen für die Parameterwerte \\(\\lambda\\in\\{0, 0.2, 0.4\\}\\).\nWir definieren zunächst die Funktion lasso_st().\n\nlibrary(tidyverse)\n\n# Funktion für Lasso soft-thresholding definieren\nlasso_st &lt;- function(KQ, lambda) {\n  case_when(\n    KQ &gt; lambda/2         ~ KQ - lambda/2,\n    abs(KQ) &lt;= lambda/2   ~ 0,\n    KQ &lt; -lambda/2        ~ KQ + lambda/2,\n  )\n}\n\nIm nächsten Schritt zeichnen wir lasso_st() für eine Sequenz von KQ-Schätzwerten gegeben \\(\\lambda\\).\n\n# Sequenz von KQ-Schätzwerten für Illustration definieren\ndat &lt;- tibble(\n  KQ = seq(-1, 1, .01)\n)\n\n# Lasso-Schätzer als Funktion des KQ-Schätzers plotten\nggplot(dat) +\n  geom_function(\n    fun = lasso_st, \n    args = list(lambda = 0), \n    lty = 2\n  ) + \n  geom_function(\n    fun = lasso_st, \n    args = list(lambda = .2),\n    col = \"red\"\n  ) + \n  geom_function(\n    fun = lasso_st, \n    args = list(lambda = .4), \n    col = \"blue\"\n  ) + \n  xlim(-.4, .4) +\n  xlab(\"KQ-Schätzer von beta_1\") +\n  ylab(\"Lasso-Schätzer von beta_1\")\n\n\n\nAbbildung 5.6: Shrinkage und Selektion von OLS-Koeffizienten mit Lasso\n\n\n\nAbbildung 5.6 zeigt, dass der \\(\\ell_1\\)-Strafterm des Lasso-Schätzers zu einem linearen Verlauf der auf den KQ-Schätzer (gezeichnet für \\(\\lambda = 0\\), gestrichelte Linie) applizierten Shrinkage führt: Der Lasso-Schätzer ist eine abschnittsweise-lineare Funktion des KQ-Schätzers in \\(\\lambda\\): Je größer der Parameter \\(\\lambda\\), desto größer ist das Intervall von KQ-Schätzwerten \\([-\\lambda/2,\\lambda/2]\\), wo der Lasso-Schätzer zu Variablenselektion führt, d.h. hier den Koeffizienten \\(\\beta_j\\) als \\(0\\) schätzt (rote bzw. blaue Linie).\nAnhand von Abbildung 5.6 kann abgeleitet werden, dass der Lasso-Schätzer nicht invariant gegenüber der Skalierung der Regressoren ist: Die Stärke der Regularisierung durch \\(\\lambda\\) ist hängt von der Magnitude des KQ-Schätzers ab. Daher müssen die Regressoren vor Berechnung der Schätzung standardsiert werden. Üblich ist hierbei eine Normierung auf einen Mittelwert von \\(0\\) und eine Varianz von \\(1\\).\nDie nachstehende interaktive Grafik illustriert das Lasso-Optimierungsproblem \\(\\eqref{eq:lassolagrange}\\) sowie den resultierenden Schätzer der Koeffizienten \\((\\beta_1, \\beta_2)\\) in einem multiplen Regressionsmodell mit korrelierten Regressoren \\(X_1\\) und \\(X_2\\).\n\nDie blaue Ellipse ist die Menge aller Schätzwerte \\(\\left(\\widehat\\beta_{1},\\, \\widehat\\beta_{2}\\right)\\) für den angegebenen Wert von \\(\\mathrm{RSS}\\). Im Zentrum der Ellipse liegt der KQ-Schätzer, welcher \\(\\mathrm{RSS}\\) minimiert.\nDas graue Quadrat ist die Menge aller Koeffizienten-Paare \\((\\beta_1, \\beta_2)\\), welche die Restriktion \\(\\lvert\\beta_1\\rvert+\\lvert\\beta_2\\rvert\\leq t\\) erfüllen. Beachte, dass die Größe dieser Region nur durch den Parameter \\(t\\) bestimmt wird.\nDer blaue Punkt ist der Lasso-Schätzer \\((\\widehat{\\boldsymbol{\\beta}}^L_{1,t},\\, \\widehat{\\boldsymbol{\\beta}}^L_{2,t})\\). Dieser ergibt sich als Schnittpunkt zwischen der blauen \\(\\mathrm{RSS}\\)-Ellipse und der Restriktionsregion und variiert mit \\(t\\). Die gestrichelte rote Linie zeigt den Lasso-Lösungspfad.\nFür kleine Werte, erhalten wir starke Shrinkage auf \\(\\widehat\\beta_{1,t}\\) bis zum Wertebereich \\(t\\leq50\\), wo \\(\\widehat{\\boldsymbol{\\beta}}^L_{1,t}=0\\). Hier erfolgt Variablenselektion: Die Regularisierung führt zu einem geschätzten Modell, das lediglich \\(X_2\\) als erklärende Variable enthält. In diesem Bereich von \\(t\\) bewirkt die Shrinkage, dass \\(\\widehat{\\boldsymbol{\\beta}}^L_{2,t}\\to0\\) für \\(t\\to0\\).\n\n\n\nBeachte, dass der rote Lasso-Pfad (die Menge aller Lasso-Lösungen) äquivalent als Funktion von \\(\\lambda\\) im Optimierungsproblem \\(\\eqref{eq:lassoloss}\\) dargestellt werden kann. Implementierungen mit statistischer Software berechnen die Lasso-Lösung häufig in Abhängigkeit von \\(\\lambda\\). Ein Algorithmus hierfür ist LARS.\n\n5.2.2 Berechnung der Lasso-Lösung mit dem LARS-Algorithmus\nFür die Berechnung des Lasso-Lösungspfads kann der LARS-Algorithmus von Efron u. a. (2004) im Lasso-Modus genutzt werden.12 Der Lasso-Lösungspfad beinhaltet geschätzte Koeffizienten über ein Intervall für \\(\\lambda\\), welches sämtliche Modellkomplexitäten zwischen der (trivialen) Lösung mit maximaler Shrinkage auf allen Koeffizienten (\\(\\lambda\\) groß, alle gesch. Koeffizienten sind \\(0\\)) und der unregularisierten Lösung (\\(\\lambda = 0\\), KQ-Schätzung) abbildet. Der LARS-Algorithmus erzeugt den Lösungspfad sequentiell, sodass die Schätzung als Funktion von \\(\\lambda\\) veranschaulicht werden kann, ähnlich wie bei Ridge Regression.12 LARS steht für Least Angle Regression.\nWir zeigen nun anhand simulierter Daten, wie Lasso-Lösungen mit dem R-Paket lars berechnet werden können. Hierfür erzeugen wir Daten gemäß der Vorschrift \\[\\begin{align}\n  \\begin{split}\n  Y_i =&\\, \\boldsymbol{X}_i' \\boldsymbol{\\beta}_v + u_i\\\\\n  \\\\\n  \\boldsymbol{\\beta}_v =&\\, (-1.25, -.75, 0, 0, 0, 0, 0, .75, 1.25)'\\\\\n  \\\\\n  \\boldsymbol{X}_i \\sim&\\, N(\\boldsymbol{0}, \\boldsymbol{I}_{9\\times9}), \\quad u_i \\overset{u.i.v.}{\\sim} N(0, 1), \\quad i = 1,\\dots,25.\n  \\end{split}\\label{eq:larsdgp}\n\\end{align}\\]\n\nlibrary(lars)\nset.seed(1234)\n\n# Parameter definieren\nN &lt;- 25\nbeta_v &lt;- c(-1.25, -.75, 0, 0, 0, 0, 0, .75, 1.25)\n\n# Beobachtungen simulieren\nX &lt;- matrix(rnorm(N * 9), ncol = 9)\nY &lt;- X %*% beta_v + rnorm(N)\n\nEntsprechend des DGP passen wir ein Modell ohne Konstante an. Damit lars::lars() den Lösungspfad des Lasso-Schätzers berechnet, muss type = \"lasso\" gewählt werden.1313 lars() standardisiert die Regressoren standardmäßig (aufgrund des DGPs hier nicht nötig).\n\n# Lösungen des Lasso-Schätzers mit LARS berechnen\n(\n  fit_lars &lt;- lars(\n    x = X, \n    y = Y, \n    intercept = F,\n    type = \"lasso\" # Wichtig: Lasso-Modus\n  )\n)\n\n\nCall:\nlars(x = X, y = Y, type = \"lasso\", intercept = F)\nR-squared: 0.858 \nSequence of LASSO moves:\n                      \nVar  9 2 8 1 3 5 4 7 6\nStep 1 2 3 4 5 6 7 8 9\n\n\nDie Zusammenfassung zeigt, dass der LARS-Algorithmus als erstes die (relevante) Variable \\(X_9\\) aktiviert.14 Mit abnehmender Regularisierung (kleinere \\(\\lambda\\)) werden in den nächsten 3 Schritten die übrigen relevanten Variablen \\(X_2\\), \\(X_8\\) und \\(X_1\\) aktiviert. Über die weiteren Schritte nähert der Algorithmus die Lösung an die saturierte Schätzung (das Modell mit allen neun Regressoren) an und aktiviert schrittweise die übrigen, irrelevanten Variablen.14 Aktivierung meint die Aufnahme einer Variable in der Modell gegeben eines hinreichend kleinen \\(\\lambda\\).\nWir visualisieren die geschätzen Koeffizienten an jedem Schritt des Lösungspfads als Funktion von \\(\\lambda\\). In der Praxis wird der Regularisierungsparameter häufig auf der natürlichen log-Skala dargestellt.\n\n# Transformation in ein weites Format\nfit_lars$beta %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    lambda = c(fit_lars$lambda, 1e-2)\n  ) %&gt;% \n  pivot_longer(\n    cols = 1:9, \n    names_to = \"Variable\", \n    values_to = \"gesch. Koeffizient\"\n  ) %&gt;% \n  \n# Visualisierung mit ggplot  \n  ggplot(\n    mapping = aes(\n      x = log(lambda), \n      y = `gesch. Koeffizient`, \n      color = Variable\n    )\n  ) + \n  geom_line() \n\n\n\nAbbildung 5.7: LARS-Lösungspfad für Lasso-Schätzung\n\n\n\nAbbildung 5.7 zeigt, dass die Shrinkage der geschätzten Koeffizienten nach der Aktivierung rasch abnimmt und sich für kleine Werte von \\(\\lambda\\) der KQ-Lösung annähert. Wir sehen auch, dass es einen Bereich von \\(\\lambda\\)-Werten gibt, für die das wahre Modell mit den Variablen \\(X_1\\), \\(X_2\\), \\(X_8\\) und \\(X_9\\) selektiert werden kann. Je nach Ziel der Analyse kann es sinnvoll sein, ein \\(\\lambda\\) in diesem Intervall zu schätzen.\n\n5.2.3 Wahl des Regularisierungsparameters \\(\\lambda\\) für den Lasso-Schätzer\nWie zuvor bei Ridge Regression muss in empirischen Anwendungen ein Wert für den Tuning-Parameter \\(\\lambda\\) gewählt werden. Hierbei besteht die Herausforderung darin, einen geeigneten Wert zu finden, der zu wünschenswerten Eigenschaften des resultierenden Modells führt. So ist für gute Vorhersagen wichtig, dass das Modell nicht zu sehr an die Daten angepasst ist (Overfitting), um eine gute Generalisierung auf neue Daten zu ermöglichen. Gleichzeitig muss das Modell flexibel genug sein, um wesentliche Eigenschaften des datenerzeugenden Prozesses hinreichend gut zu erfassen. In der Regel wird hierbei eine sparsame Modellierung angestrebt, die nur eine Teilmenge der Prädiktoren nutzt.\nIn der Praxis werden verschiedene Verfahren verwendet, um den Wert für den Tuning-Parameter \\(\\lambda\\) zu bestimmen. Gängige Methoden sind Cross Validation (CV) und Informationskriterien. In Abhängigkeit der Methode und der Daten ergeben sich ober- oder unterparameterisierte Modelle. Aufgrund der Implementierung im R-Paket lars betrachten wir CV.15 Wir zeigen nachfolgend anhand der simulierten Daten aus dem letzten Abschnitt, wie für die LARS-Schätzung ein optimales \\(\\lambda\\) mit leave-one-out CV (LOO-CV) bestimmt werden kann. Hierzu nutzen wir lars::cv.lars() unter Verwendung derselben Argumente wie zuvor im Aufruf von lars().15 Chetverikov, Liao, and Chernozhukov (2020) zeigen, dass CV zu konsistenter Modellselektion führen kann.\n\n# LARS-Lösungen mit CV evaluieren\nfit_lars_cv &lt;- cv.lars(\n  x = X, \n  y = Y, \n  intercept = F,\n  normalize = T,\n  type = \"lasso\", \n  plot.it = F, \n  K = N # für LOO-CV\n) \n\nDas Objekt fit_lars_cv ist eine Liste mit den CV-Ergebnissen. Wir können diese einfach mit ggplot visualisieren. index ist hierbei das Verhältnis der \\(\\ell_1\\)-Norm des Lasso-Schätzers für einen spezifischen Wert von \\(\\lambda\\) und der \\(\\ell_1\\)-Norm des KQ-Schätzers. Das optimale \\(\\lambda\\) wird so implizit geschätzt. cv.error ist der mit CV geschätzte MSE.\n\n# CV-MSE\nfit_lars_cv %&gt;% \n  as_tibble() %&gt;%\n\n  ggplot(\n    mapping = aes(\n      x = index, \n      y = cv.error\n    )\n  ) + \n  geom_line() +\n  xlab(\"|beta_lambda| / |beta|\") +\n  ylab(\"CV-MSE\")\n\n\n\nAbbildung 5.8: CV-MSE und relative Position von \\(\\lambda\\) auf dem Lassopfad\n\n\n\nIn der Grafik erkennen wir ein Minimum des CV-MSEs bei etwa 0.73.\n\n# CV-MSE-minimierendes Lambda bestimmen\nID &lt;- which.min(fit_lars_cv$cv.error) # Index\n\n(\n  fraction_opt &lt;- fit_lars_cv$index[ID]\n)\n\n[1] 0.7272727\n\n\nDie geschätzten Koeffizienten für die optimale Regularisierung können mit coef() ausgelesen werden.\n\n# LARS-Lasso-Fit für optimales lambda bestimmen\ncoef(\n  object = fit_lars, \n  s = fraction_opt, \n  mode = \"fraction\"\n)\n\n[1] -0.6513191 -0.6060906 -0.1946089  0.0000000  0.0000000  0.0000000  0.0000000\n[8]  0.4977908  1.3122407\n\n\nDas Ergebnis veranschaulicht die Selektionseigenschaft von Lasso: Gemäß DGP \\(\\eqref{eq:larsdgp}\\) sind die Variablen \\(X_3\\) bis \\(X_7\\) irrelevante Prädiktoren für \\(Y\\); ihre wahren Koeffizienten sind \\(0\\). In der kreuzvalidierten Lasso-Schätzung erreicht die Regularisierung, dass die Koeffizienten der Variablen \\(X_4\\) bis \\(X_7\\) tatsächlich mit 0 geschätzt werden. Wir schätzen für das mit CV bestimmte \\(\\lambda\\) also ein leicht überspezifiziertes Modell mit den Regressoren \\(X_1\\), \\(X_2\\), \\(X_3\\), \\(X_8\\) und \\(X_9\\). Beachte, dass die Lasso-Schätzung einen Kompromiss impliziert: Die Varianz der Schätzung ist geringer als die des KQ-Schätzers im Modell mit allen Variablen.16 Aufgrund der Regularisierung sind die mit Lasso geschätzten Koeffizienten der relevanten Variablen jedoch in Richtung \\(0\\) verzerrt.16 Wegen \\(N=25\\) verbleiben bei der KQ-Schätzung mit 9 Regressoren nur 16 Freiheitsgrade.\nEinen positiven Effekt dieses Kompromisses beobachten wir anhand des mittleren Vorhersagefehlers für Daten, die nicht zur Berechnung des Schätzers verwendet wurden. Wir vergleichen den Vorhersagefehler nachfolgend anhand eines solchen simulierten Test-Datensatzes mit 25 neuen Beobachtungen. Den Vorhersagefehler bestimmen wir als MSE zwischen den vorhergesagten und den tatsächlichen Ausprägungen für \\(Y\\).\n\n# Test-Datensatz erstellen\nset.seed(4321)\nnew_X &lt;- matrix(rnorm(N * 9), ncol = 9)\nnew_Y &lt;- new_X %*% beta_v + rnorm(N)\n\n# Lasso: Vorhersage von new_Y für Test-Datensatz\nY_predict_lars &lt;- predict(\n  object = fit_lars, \n  s = fraction_opt, \n  type = \"fit\", \n  mode = \"fraction\", \n  newx = new_X\n)$fit\n\n# Lasso: MSE für Test-Datensatz berechnen\nmean((Y_predict_lars - new_Y)^2)\n\n[1] 1.419817\n\n\nWir schätzen nun das große Modell mit allen 9 Variablen mit KQ und berechnen ebenfalls den MSE der Prognosen für den Test-Datensatz.\n\n# KQ-Schätzung des großen Modells durchführen\nKQ_fit &lt;- lm(Y ~ X - 1)\n\n# Test-Datensatz für predict.lm() formatieren\nnew_X &lt;- as.data.frame(new_X)\ncolnames(new_X) &lt;- paste0(\"X\", 1:9)\n\n# KQ: Vorhersage von new_Y für Test-Datensatz\nY_predict_KQ &lt;- predict(\n  object = KQ_fit, \n  newdata = new_X\n)\n\n# KQ: MSE für Test-Datensatz berechnen\nmean((Y_predict_KQ - new_Y)^2)\n\n[1] 9.851932\n\n\nOffenbar führt die Lasso-Schätzung zu einem deutlich geringeren MSE der Vorhersage von Y für den Test-Datensatz als die KQ-Schätzung und damit zu einer höheren Vorhersagegüte. Das “sparsame” mit Lasso-Regression geschätzte Modell ist dem “großen” mit KQ geschätztem Modell in dieser Hinsicht also überlegen.\n\n\n\n\n\n\nKey Facts zu Lasso-Regression\n\n\n\n\nLasso-Regression bestraft die Verlustfunktion des KQ-Schätzers mit der \\(\\ell_1\\)-Norm der Koeffizienten.\nNeben Koeffizientenschätzung mit Shrinkage in Richtung \\(0\\) kann der Lasso-Schätzer Variablenselektion durchführen: Regressionskoeffizienten können exakt mit \\(0\\) geschätzt und so ein “sparsames”, leichter zu interpretierendes Modell gewählt werden.\nWie bei Ridge Regression impliziert die Wahl von \\(\\lambda\\) einen Bias-Variance-Tradeoff, der für Vorhersagen nützlich ist: Für größere \\(\\lambda\\) wird mehr Verzerrung induziert und möglicherweise relevante Variablen mit kleinen Koeffizienten aus dem Modell entfernt. Ein solches sparsames Modell kann eine höhere Prognosegüte haben als ein komplexes, unregularisiertes Modell.\nDer Lasso-Schätzer \\(\\widehat{\\boldsymbol{\\beta}}_\\lambda^L\\) ist nicht erwartungstreu.\nLasso Regression kann bspw. mit dem LARS-Algorithmus (Paket lars) oder mit glmnet berechnet werden."
  },
  {
    "objectID": "RegReg.html#vergleich-von-lasso--und-ridge-regression-mit-simulation",
    "href": "RegReg.html#vergleich-von-lasso--und-ridge-regression-mit-simulation",
    "title": "\n5  Regularisierte Regression\n",
    "section": "\n5.3 Vergleich von Lasso- und Ridge-Regression mit Simulation",
    "text": "5.3 Vergleich von Lasso- und Ridge-Regression mit Simulation\nIn diesem Kapitel illustrieren wir Vor- und Nachteile von Lasso- und Ridge-Regression in Prognose-Anwendungen anhand von Monte-Carlo-Simulationen. Wir betrachten hierbei datenerzeugende Prozesse, die sich hinsichtlich der Anzahl relevanter Variablen sowie der Korrelation dieser Variablen unterscheiden.\nDie grundlegende Vorschrift für die Simulationen ist \\[\\begin{align*}\n  Y_i = \\sum_{j=1}^{k=40} \\beta_j X_{i,j} + u_i, \\quad u_i \\overset{u.i.v.}{\\sim} N(0,1), \\quad i=1,\\dots,100,\n\\end{align*}\\] wobei die Regressoren \\(X_j\\) eine Varianz von \\(1\\) haben und aus einer multivariaten Normalverteilung mit Korrelation \\[\\rho\\in(0,0.5,0.8)\\] gezogen werden.\nFür die Koeffizienten \\(\\boldsymbol{\\beta}\\) unterscheiden wir zwei Szenarien. In Szenario A ist \\[\\boldsymbol{\\beta} = (1,\\dots,1)',\\] d.h. alle Variablen sind relevant und haben denselben Einfluss auf \\(Y\\). In Szenario B erzeugen wir \\(\\boldsymbol{\\beta}\\) einmalig vorab so, dass \\[\\beta_j = \\begin{cases}1,\\quad \\text{mit Wsk.  }p\\\\ 0,\\quad \\text{mit Wsk.  }1-p, \\end{cases}\\] d.h. nur eine Teilmenge der Variablen beeinflusst \\(Y\\) jeweils mit demselben Effekt \\(\\beta_j = 1\\). Die übrigen Variablen sind irrelevant.\nWir schätzen und validieren die Modelle mit glmnet().\n\n5.3.1 Prognosegüte in diversen Szenarien\n\n# Simulationsparameter definieren\nrho &lt;- c(0, 0.5, 0.8)   # Korrelation\nk &lt;- 40                 # Anz. Regressoren\nN &lt;- 100                # Anz. Beobachtungen\nn_sim &lt;- 100            # Anz. Simulationen\n\nDamit der Code für die Simulation möglichst wenig repetitiv ist, definieren wir eine Funktion cv.glmnet_MSE(), die unter Angabe der Daten X und Y, des Trainingssets train sowie des Parameters alpha den gewünschten regularisierten Schätzer under Verwendung von Cross Validation anpasst und den Testset-MSE zurückgibt.\n\n# allg. Funktion für Testset-MSE nach CV\ncv.glmnet_MSE &lt;- function(X, Y, train, alpha) {\n  \n  # Modell mit glmnet schätzen; lambda per CV bestimmen\n  fit_cv &lt;- cv.glmnet(\n    x = X[train,],\n    y =Y[train],\n    alpha = alpha\n  )\n  \n  # Vorhersagen treffen\n  Y_pred &lt;- predict(\n    object = fit_cv, \n    s = fit_cv$lambda.min, \n    newx = X[-train,])\n  \n  return(\n    # Testset-MSE berechnen\n    mean(\n      (Y[-train] - Y_pred)^2\n      )\n  )\n}\n\nWir initialisieren zunächst Matrizen, in welche die MSEs aus den 100 Simulationsdurchläufen reihenweise geschrieben werden. lasso_mse und ridge_mse haben je eine Spalte für jede Korrelation in rho\n\n# Matrizen für simulierte MSEs initialisieren...\nlasso_mse &lt;- matrix(\n  data = NA, \n  nrow = n_sim, \n  ncol = length(rho)\n) \nridge_mse &lt;- lasso_mse\n\n# ... und benennen\ncolnames(lasso_mse) &lt;- paste0(\"Kor=\", rho)\ncolnames(ridge_mse) &lt;- colnames(lasso_mse)\n\nFür die Simulation iterieren wir mit purrr::walk über den Vektor rho sowie über die Laufvariable 1:n_sim. Beide Schleifen nutzen den Syntax für anonyme Funktionen:\n\n# Die anonyme Funktion\nfunction(x) return(x)\n# ist äquivalent definiert als\n\\(x) return(x)\n\nIn jeden Simulationsdurchlauf erzeugen wir den Datensatz entsprechend der obigen Vorschrift, teilen die Daten auf und berechnen MSEs für Lasso- und Ridge-Regression mit cv.glmnet_MSE().\nSzenario A\n\n# Koeffizienten-Vektor definieren\nbeta &lt;- rep(1, k) \n\n\nlibrary(mvtnorm)\nlibrary(tidyverse)\n\nset.seed(1234)\n\n# Simulation durchführen\nwalk(1:length(rho), \\(j) {\n  \n  # Korrelationsmatrix definieren\n  Sigma &lt;- matrix(\n    data = rho[j], \n    nrow = k, \n    ncol = k\n  )\n  diag(Sigma) &lt;- 1\n  \n  walk(1:n_sim, \\(i) {\n    \n  # Daten simulieren\n  X &lt;- rmvnorm(\n    n = N, \n    mean = rep(0, k), \n    sigma = Sigma\n  )\n  Y &lt;- X %*% beta + rnorm(N)\n    \n  # Trainingsdaten definieren\n  ID_train &lt;- sample(\n    x = c(1:N), \n    size = N/2\n  )\n    \n  # Modelle mit CV schätzen und MSEs berechnen\n  # Ridge-Regression\n  ridge_mse[i, j] &lt;&lt;- cv.glmnet_MSE(\n    X = X, \n    Y = Y, \n    train = ID_train, \n    alpha = 0\n  )\n  \n  # Lasso-Regression\n  lasso_mse[i, j] &lt;&lt;- cv.glmnet_MSE(\n    X = X, \n    Y = Y, \n    train = ID_train, \n    alpha = 1\n  )\n  \n  })\n  \n})\n\nBeachte, dass hier der Super-Assignment-Operator &lt;&lt;- genutzt wird, damit walk die Matrizen ridge_mse und lasso_mse in der globalen Umgebung überschreibt.1717 Dies folgt aus der Definition von walk. &lt;- bewirkt hier lediglich Assignment in der Funktionsumgebung.\nWir berechnen jeweils den mittleren MSEs, sammeln die Ergebnisse in einer tibble() und nutzen gt() für die tabellarische Darstellung.\n\nlibrary(gt)\n\n# Ergebnisse tabellarisch darstellen\ntibble(\n  Methode = c(\n    \"Lasso-Regression\", \n    \"Ridge-Regression\"\n  ),\n) %&gt;%\n  bind_cols(\n    bind_rows(\n      colMeans(lasso_mse),\n      colMeans(ridge_mse)  \n    )    \n  ) %&gt;%\n  gt() %&gt;%\n  tabopts\n\n\n\n\n\n\n\n  \nMethode\n      Kor=0\n      Kor=0.5\n      Kor=0.8\n    \n\n\nLasso-Regression\n7.17\n10.398\n7.581\n\n\nRidge-Regression\n4.841\n1.615\n1.517\n\n\n\nTabelle 5.1:  Durchschnittliche Testset-MSEs für Setting A \n\n\n\nTabelle 5.1 zeigt, dass Ridge-Regression gegenüber Lasso-Regression für jede der drei betrachteten Korrelationen überlegen ist. Insbesondere bei stärker korrelierten Regressoren ist Ridge vorteilhaft.\nFür Szenario B überschreiben wir beta nach Multiplikation mit einem zufälligen binären Vektor, sodass einige der Koeffizienten \\(0\\) und die zugehörigen Variablen irrelevant für \\(Y\\) sind.\nSzenario B\n\n# Wsk. für Relevanz einer Variable\np &lt;- .3\n\n# Koeffizienten-Vektor definieren\nset.seed(123)\nbeta &lt;- beta * sample(\n  x = 0:1, \n  size = k, \n  replace = T, \n  prob = c(1-p, p)\n)\n\n# Koeffizienten prüfen\nhead(beta, n = 10)\n\n [1] 0 1 0 1 1 0 0 1 0 0\n\n\nEine wiederholung der Simulation für die modifizierten Koeffizienten beta und liefert folgende tabellarische Auswertung.\n\n\n\n\n\n\n\n\n  \nMethode\n      Kor=0\n      Kor=0.5\n      Kor=0.8\n    \n\n\nLasso\n2.51\n2.143\n1.923\n\n\nRidge\n3.331\n2.562\n2.014\n\n\n\nTabelle 5.2:  Durchschnittliche Testset-MSEs für Szenario B \n\n\n\nDie Ergebnisse in Tabelle 5.2 zeigen, dass Ridge-Regression in Szenario B bis auf den Fall unkorrelierter Regressoren etwas schlechter abschneidet als in Szenario A. Die hohe Anzahl irrelevanter Variablen verbessert die Leistung von Lasso deutlich: Hier ist es plausibel, dass Lasso aufgrund der Thresholding-Eigenschaft die Koeffizienten einiger irrelevanten Variablen häufig exakt \\(0\\) setzt und damit ein sparsameres Modell schätzt als Ridge. Entsprechend erzielt Lasso in diesem Szenario insbesondere für \\(\\rho = 0\\) genauere Vorhersagen als Ridge Regression.\n\n5.3.2 Visualisierung des Bias-Variance-Tradeoffs bei Prognosen\nFür ein besseres Verständnis, wie sich der Regularisierungsparameter \\(\\lambda\\) auf den Bias-Variance-Tradeoff bei Prognosen mit Ridge- und Lasso-Regression auswirkt, vergleichen wir für beide Methoden nachfolgend die Abhängigkeit des MSEs der Prognose \\(\\widehat{Y}_0\\) für den Wert \\(Y_0\\) der abhängigen Variable eines Datenpunkts anhand seiner Regressoren \\(\\boldsymbol{X}_0'\\), wobei \\[\\begin{align}\n  \\text{MSE}(\\widehat{Y}_0) = \\text{Bias}(\\widehat{Y}_0)^2 + \\text{Var}(\\widehat{Y}_0) + \\text{Var}(Y_0) \\label{eq:pbvdecomp}\n\\end{align}\\] Beachte, dass \\(\\text{Var}(Y_0)\\) die durch den datenerzeugenden Prozess (und damit unvermeidbare) Varianz von \\(Y_0\\) ist, wohingegen \\(\\text{Bias}(\\widehat{Y}_0)^2\\) und \\(\\text{Var}(\\widehat{Y}_0)\\) von dem verwendeten Schätzer für \\(\\widehat{Y}_0\\) abhängt.\nFür die Simulation betrachten wir erneut Szenario A aus Kapitel 5.3.1 mit \\(50\\) Beobachtungen für ein Modell mit \\(40\\) unkorrelierten Regressoren. Wir legen zunächst die Simulationsparameter fest und erzeugen den vorherzusagenden Datenpunkt (X_0, Y_0).\n\n# Parameter festlegen\nset.seed(1234)\nn &lt;- 200 # Anz. Iterationen\nN &lt;- 50  # Anz. Beobachtungen\nk &lt;- 40  # Anz. Variablen\n\n# Korrelationsmatrix definieren\nSigma &lt;- diag(k) # Diagonalmatrix\nbeta &lt;- rep(x = 1, k)\n\n# Prognose-Ziel vorab zufällig generieren:\n\n# Regressoren\nX_0 &lt;- rmvnorm(\n  n = 1, \n  mean = rep(x = 0, k)\n)\n\n# Abh. Variable\nY_0 &lt;- X_0 %*% beta + rnorm(n = 1) %&gt;% \n  as.vector()\n\nAnhand der Simulationsergebnisse wollen wir die von der verwendeten Schätzfunktion abhängigen Komponenten von \\(\\eqref{eq:pbvdecomp}\\) untersuchen. Wir initialisieren hierzu die Listen ridge_fits und lasso_fits, in die unsere Simulationsergebnisse geschrieben werden.\n\n# Listen für Simulationsergebnisse initialisieren\nridge_fits &lt;- list()\nlasso_fits &lt;- list()\n\nWeiterhin definieren wir separate \\(\\lambda\\)-Sequenzen für Lasso- und Ridge-Schätzer.1818 Die Sequenzen haben wir in Abhängigkeit des DGP so gewählt, dass die Abhängigkeit der Prognosegüte von \\(\\lambda\\) gut visualisiert werden kann.\n\n# Lambda-Sequenzen festlegen\nlambdas_r &lt;- seq(.25, 2.5, length.out = 100)\nlambdas_l &lt;- seq(.05, 0.5, length.out = 100)\n\nFür die Simulation iterieren wir mit walk() über simulierte Datensätze und schreiben jeweils den vollständigen Output von glmnet() in die zuvor definierten Listen ridge_fits und lasso_fits.\n\n# Simulation\nwalk(1:n, \\(i) {\n  \n  # Daten simulieren\n  X &lt;- rmvnorm(\n    n = N, \n    mean = rep(0, k), \n    sigma = Sigma\n  )\n  Y &lt;- X %*% beta + rnorm(n = N, sd = 5)\n  \n  # Modelle mit glmnet schätzen\n  # Ridge-Regression\n  ridge_fits[[i]] &lt;&lt;- glmnet(\n    x = X, \n    y = Y, \n    alpha = 0, \n    intercept = F\n  )\n  # Lasso-Regression\n  lasso_fits[[i]] &lt;&lt;- glmnet(\n    x = X, \n    y = Y, \n    alpha = 1, \n    intercept = F\n  )\n  \n})\n\nWir nutzen Funktionen aus purrr und dplyr, um über die in den Simulationsdurchläufen angepassten Modelle zu iterieren. Mit predict() erhalten wir Punktvorhersagen für Y_0 für jedes \\(\\lambda\\) der zuvor definierten \\(\\lambda\\)-Sequenzen. Beachte, dass map() jeweils eine Liste mit 200 Punktvorhersagen für jedes der 100 zurückgibt. Mit list_rbind() können wir die Ergebnisse komfortabel jeweils in einer tibble sammeln.\n\n# Prognosen für Ridge-Regression\npred_r &lt;- map(\n  .x = ridge_fits, \n  .f = ~ as_tibble(\n    predict(\n      object = ., \n      s = lambdas_r, \n      newx = X_0\n    )\n  ) \n) %&gt;%\n  list_rbind() \n\n# Prognosen für Lasso-Regression\npred_l &lt;- map(\n  .x = lasso_fits, \n  .f = ~ as_tibble(\n    predict(\n      object = ., \n      s = lambdas_l, \n      newx = X_0)\n    ) \n) %&gt;%\n  list_rbind() \n\nFür die statistische Auswertung berechnen wir jeweils \\(\\text{MSE}(\\widehat{Y}_0)\\), \\(\\text{Bias}(\\widehat{Y}_0)^2\\) und \\(\\text{Var}(\\widehat{Y}_0)\\) und führen die Ergebnisse mit pivot_longer() in ein langes Format sim_data_r über. Wir berechnen weiterhin mit MSE_min_r das \\(\\lambda\\), für das wir über die Simulationsdurchläufe durchschnittlich den geringsten \\(\\text{MSE}\\) beobachten.\nRidge-Regression\n\n# Ergebnisse für Ridge-Regression zusammenfassen\nsim_data_r &lt;- tibble(\n  \n  lambda = lambdas_r,\n  \n  \"MSE\" = map_dbl(\n    .x = pred_r,  \n    .f = ~ mean((.x - Y_0)^2)\n  ),\n  \n  \"Bias^2\" = map_dbl(\n    .x = pred_r, \n    .f = ~ (mean(.x) - Y_0)^2\n  ),\n  \n  \"Varianz\" = map_dbl(\n    .x = pred_r, \n    .f = ~ var(.x)\n  )\n) %&gt;%\n  pivot_longer(\n    cols = -lambda, \n    values_to = \"Wert\",\n    names_to = \"Statistik\"\n  )\n\n# Lambda bei MSE-Minimum bestimmen\nMSE_min_r &lt;- sim_data_r %&gt;% \n  filter(\n    Statistik == \"MSE\",\n    Wert == min(Wert)\n  ) \n\nLasso-Regression\n\n# Ergebnisse zusammenfassen\nsim_data_l &lt;- tibble(\n  \n  lambda = lambdas_l,\n  \n  \"MSE\" = map_dbl(\n    .x = pred_l,  \n    .f = ~ mean((. - Y_0)^2)\n  ),\n  \n  \"Bias^2\" = map_dbl(\n    .x = pred_l, \n    .f = ~ (mean(.) - Y_0)^2\n  ),\n  \n  \"Varianz\" = map_dbl(\n    .x = pred_l, \n    .f = ~ var(.)\n  )\n) %&gt;%\n  pivot_longer(\n    cols = -lambda, \n    values_to = \"Wert\", \n    names_to = \"Statistik\"\n  )\n\n# Lambda bei MSE-Minimum bestimmen\nMSE_min_l &lt;- sim_data_l %&gt;% \n  filter(\n    Statistik == \"MSE\",\n    Wert == min(Wert)\n  ) \n\nDie Datensätze im langen Format, sim_data_r und sim_data_l, werden nun für die Visualisierung der Ergebnisse mit ggplo2 genutzt.\n\n# MSE, Bias^2 und Varianz gegen Lambda plotten\n\n# Ridge-Regression\nsim_data_r %&gt;%\n  ggplot(\n    mapping = aes(\n      x = lambda, \n      y = Wert, \n      color = Statistik\n    )\n  ) +\n  geom_line() +\n  geom_point(data = MSE_min_r)\n\n# Lasso-Regression\nsim_data_l %&gt;%\n  ggplot(\n    mapping = aes(\n      x = lambda, \n      y = Wert, \n      color = Statistik\n    )\n  ) +\n  geom_line() +\n  geom_point(data = MSE_min_l)\n\nAbbildung 5.9: Simulierte MSE-Komponenten in Abhängigkeit von Lambda\n\n\n\n\n(a) Ridge Regression\n\n\n\n\n\n\n\n(b) Lasso Regression\n\n\n\n\n\n\nAnhand von Abbildung 5.9 lässt sich der Bias-Variance-Tradeoff bei der Vorhersage von \\(Y_0\\) gut erkennen: Bereits für kleine \\(\\lambda\\) erzielen beide Methode eine deutliche Reduktion des MSE. Dies wir durch etwas zusätzlichen Bias, aber eine überproportionale Verringerung der Varianz erreicht. Der erkennbare funktionale Zusammenhang zeigt, dass der MSE eine konvexe Funktion von \\(\\lambda\\) ist. Damit existieren optimale \\(\\lambda\\) mit minimalem MSE (grüne Punkte), die wir mit Cross Validation schätzen können."
  },
  {
    "objectID": "RegReg.html#inferenz-für-treatment-effekt-schätzung-mit-vielen-variablen",
    "href": "RegReg.html#inferenz-für-treatment-effekt-schätzung-mit-vielen-variablen",
    "title": "\n5  Regularisierte Regression\n",
    "section": "\n5.4 Inferenz für Treatment-Effekt-Schätzung mit vielen Variablen",
    "text": "5.4 Inferenz für Treatment-Effekt-Schätzung mit vielen Variablen\nIn empirischen Studien des Effekts einer Behandlungsvariable \\(B\\) auf eine Outcome-Variable \\(Y\\) steht häufig eine Vielzahl potentieller Kontrollvariablen zur Verfügung. Häufig ist unklar, welche Variablen in das Modell aufgenommen werden sollten, um das Risiko einer verzerrten Schätzung durch ausgelassene Variablen zu vermindern und gleichzeitig eine Schätzung mit geringer Varianz zu gewährleisten. Ist der Beobachtungsumfang \\(N\\) relativ zur Variablenanzahl \\(k\\) groß, so kann die KQ-Schätzung einer langen Regression (ein Modell mit allen \\(k\\) Kontrollvariablen) gute Ergebnisse liefern. In der Praxis liegt diese wünschenswerte Situation jedoch oft nicht vor und es ist \\(k\\lesssim N\\) oder sogar \\(k&gt;N\\). Dann ist eine KQ-Schätzung des Behandlungseffekts anhand aller \\(k\\) Variablen mit hoher Varianz behaftet bzw. gar nicht möglich.19 Ein weiteres Szenario ist \\(k(N)&gt;N\\), d.h. die Anzahl der Regressoren kann mit dem Beobachtungsumfang wachsen.20 Lasso-Verfahren können dann hilfreich sein, um Determinanten von \\(Y\\) und \\(B\\) zu identifizieren und damit eine Menge an Kontrollvariablen zu selektieren, für die eine erwartungstreue und konsistente Schätzung des interessierenden Effekts wahrscheinlich ist.19 Beachte, dass der KQ-Schätzer bei \\(k&gt;N\\) nicht lösbar ist.20 Dieses Szenario wird unter Bedingungen bzgl. der Wachstumsrate und der Größe der Koeffizienten betrachet, s. (Belloni und Chernozhukov 2013).\nBetrachte zunächst das Modell mit allen Kontrollvariablen \\(X_j\\), \\[\\begin{align}\n  Y_i = \\beta_0 + \\alpha_0 B_i + \\sum_{j=1}^k \\beta_{j} X_{i,j} + u_i, \\label{eq:lassotmt}\n\\end{align}\\] wobei einige \\(\\beta_{j}=0\\) sind und wir annehmen, dass \\(B\\) lediglich mit ein paar der \\(X_j\\) korrelliert. Die Shrinkage der geschätzten Koeffizienten aus einer naiven Lasso-Regression von \\(\\eqref{eq:lassotmt}\\) führt grundsätzlich zu einer verzerrten Schätzung des Behandlungseffekts \\(\\alpha_0\\) und damit zu ungültiger Inferenz.2121 Hahn u. a. (2018) geben eine ausführliche Erläuterung dieser Problematik.\nDie Verzerrung von geschätzten Koeffizienten kann vermieden werden, indem Lasso lediglich zur Selektion von Kontrollvariablen verwendet wird. Dabei wird mit einer Lasso-Regression von \\(Y\\) auf die \\(X_j\\) eine Teilmenge von Regressoren \\(\\mathcal{S}\\) selektiert und der Treatment-Effekt anschließend mit der KQ-Schätzung von \\[\\begin{align}\n  Y_i = \\beta_0 + \\alpha_0 B_i + \\sum_{j\\in\\mathcal{S}} \\beta_{j} X_{i,j} + e_i,\n\\end{align}\\] basierend auf der Selektion \\(\\mathcal{S}\\) berechnet wird.22 Ein solcher Post-Lasso-Selection-Schätzer (Belloni und Chernozhukov 2013) ist jedoch im Allgemeinen und insbesondere in hoch-dimensionalen Settings nicht konsistent für \\(\\alpha_0\\) und nicht asymptotisch normalverteilt, da weiterhin die Gefahr einer verzerrten Schätzung durch in \\(\\mathcal{S}\\) ausgelassene Variablen besteht, die mit \\(B\\) korrelieren: Lasso selektiert Variablen \\(X_j\\), die “gut” \\(Y\\) erklären. Dabei kann nicht ausgeschlossen werden, das ein Modell gewählt wird, dass relevante Determinanten von \\(B\\) auslässt. Selbst wenn wir ein mit Lasso gewähltes Modell mit KQ (d.h. ohne Shrinkage) schätzen, würde \\(\\alpha_0\\) verzerrt geschätzt!22 Solche Verfahren werden Post-Selection-Schätzer gennant.\nBelloni, Chernozhukov, und Hansen (2014) schlagen ein alternatives Verfahren vor, dass auf Selektion der Determinanten \\(X_j\\) von \\(Y\\) und \\(B\\) basiert. Dieses Verfahren wird als Post-Double Selection bezeichnet und kann wiefolgt implementiert werden:\nPost-Double-Selection-Schätzer\n\nBestimme die Determinanten \\(X_j\\) von \\(Y\\) mit Lasso-Regression und bezeichne die Menge der selektierten Variablen als \\(\\mathcal{S}_Y\\).\nBestimme die Determinanten \\(X_j\\) von \\(B\\) mit Lasso-Regression und bezeichne die Menge der selektierten Variablen als \\(\\mathcal{S}_B\\).\nBestimme die Schnittmenge \\(\\mathcal{S}_{YB} = \\mathcal{S}_Y \\cap \\mathcal{S}_B\\). Schätze den Treatment-Effekt als \\(\\widehat{\\alpha}_0\\) in der KQ-Regression \\[\\begin{align}\n  Y_i = \\beta_0 + \\alpha_0 B_i + \\sum_{j\\in\\mathcal{S}_{YB}} \\beta_{j} X_{i,j} + v_i.\n\\end{align}\\]\n\nBelloni, Chernozhukov, und Hansen (2014) zeigen, dass \\(\\widehat{\\alpha}_0\\) aus diesem Verfahren ein asymptotisch normalverteiler Schätzer für \\(\\alpha_0\\) ist und herkömmliche t-Tests und Konfidenzintervalle gültige Inferenz erlauben.\nWir illustrieren die in diesem Abschnitt betrachteten Schätzer nun anhand simulierter Daten mit R. Die fiktive Problemstellung ist die Schätzung eines wahren Treatment-Effekts \\(\\alpha_0 = 2\\), wenn so viele potenzielle Kontrollvariablen vorliegen, dass der KQ-Schätzer gerade noch berechnet werden kann, aber aufgrund hoher Varianz unzuverlässig ist. Hierzu erzeugen wir \\(Y\\) gemäß der Vorschrift \\[\\begin{align*}\n  Y_i =&\\, \\alpha_0 B_i + \\sum_{j=1}^{k_Y} \\beta_{j}^Y X_{i,j}^Y + \\sum_{l=1}^{k_{YB}} \\beta_{l}^{YB} X_{i,l}^{YB} + u_i,\\\\\n  \\\\\n  \\beta_j^{YB} \\overset{u.i.v}{\\sim}&\\,N(10,1), \\quad \\beta_j^{Y} \\overset{u.i.v}{\\sim}U(0,1), \\quad u_i \\overset{u.i.v}{\\sim}N(0,1).\\\\\n  \\\\\n  i=&\\,1,\\dots,550\n\\end{align*}\\]\nDie Behandlungsvariable \\(B_i\\) entspricht der Vorschrift \\[\\begin{align*}\n  B_i =&\\, \\sum_{l=1}^{k_{YB}} \\beta_{l}^{YB} X_{i,l}^{YB} + e_i,\\\\\n  \\\\\n  \\beta_j^{YB} \\overset{u.i.v}{\\sim}&\\,N(2,0.2), \\quad e_i \\overset{u.i.v}{\\sim}N(0,1).\n\\end{align*}\\] Wir wählen \\(k_{YB} = k_{Y} = 25\\). Zusätzlich zu \\(B\\), den Determinanten von \\(Y\\) und \\(B\\) (\\(X^{YB}\\)) sowie den Variablen, die ausschließlich \\(Y\\) beeinflussen (\\(X^{Y}\\)) gibt es \\(k_U = 499\\) Variablen \\(X^U\\), die weder \\(Y\\) noch \\(B\\) beeinflussen und damit irrelevant für die Schätzung des Behandlungseffekts sind. Wir haben also \\(N=550\\) Beobachtungen und insgesamt \\(k = 1+k_{Y} + k_{YB} + k_{U} = 550\\) potenzielle Kontrollvariablen von denen \\(k_{YB} = 25\\) für eine unverzerrte Schätzung von \\(\\alpha_0\\) relevant sind.\nDer nachstehende Code generiert die Daten gemäß der Vorschrift.\n\nlibrary(mvtnorm)\nlibrary(tidyverse)\nset.seed(4321)\n\nn &lt;- 550      # Beobachtungen\np_Y &lt;- 25     # Determinanten Y\np_B &lt;- 25     # Determinanten B *und* Y\np_U &lt;- 499    # irrelevante Variablen \n\n# Variablen generieren\nXB &lt;- rmvnorm(n = n, sigma = diag(p_B))\nXU &lt;- rmvnorm(n = n, sigma = diag(p_U))\nXY &lt;- rmvnorm(n = n, sigma = diag(p_Y))\n\n# Stetige Behandlungsvariable erzeugen\nB &lt;- XB %*% rnorm(p_B, 2, sd = .2) + rnorm(n)\n\n# Abh. Variable erzeugen, Behandlungseffekt (ATE) ist 2\nY &lt;- 2 * B + \n  XB %*% rnorm(p_B, mean = 10) + \n  XY %*% runif(p_Y) + \n  rnorm(n)\n\n# Variablen in tibble sammeln\nX &lt;- cbind(B, XB, XU, XY) %&gt;% \n  as_tibble()\n\n# Namen zuweisen\ncolnames(X) &lt;- c(\n  \"B\", \n  paste0(\"XB\", 1:p_B), \n  paste0(\"XU\", 1:p_U),\n  paste0(\"XY\", 1:p_Y) \n)\n\nWünschenswert wäre die KQ-Schätzung des wahren Modells. Diese ergibt eine Schätzung nahe des wahren Treatment-Effekts \\(\\alpha_0 = 2\\). Unter realen Bedingungen wäre diese Regression jedoch nicht implementierbar, weil die relevanten Kovariablen XB unbekannt sind.\n\n# KQ: Wahres Modell schätzen\nlm(Y ~ B + XB - 1)$coefficients[\"B\"]\n\n       B \n1.937031 \n\n\nWir schätzen daher zunächst die “lange” Regression mit allen \\(k\\) verfügbaren Variablen mit KQ. Beachte, dass der KQ-Schätzer für \\(\\alpha_0\\) zwar implementierbar und erwartungstreu ist, jedoch eine hohe Varianz aufweist. Wegen \\(k=N=550\\) erhalten wir eine perfekte Anpassung an die Daten und können mangels Freiheitsgraden keine Hypothesentests durchführen.\n\n# KQ: Lange Regression schätzen\nlm(Y ~ . - 1, data = X)$coefficients[\"B\"]\n\n       B \n3.079497 \n\n\nDie KQ-Schätzung von \\(\\alpha_0\\) anhand der langen Regression weicht deutlich vom wahren Wert \\(\\alpha_0 = 2\\) ab.\nEine “kurze” KQ-Regression nur mit der Behandlungsvariable \\(B\\) führt wegen Korrelation mit den ausgelassenen Determinanten in XB zu einer deutlich verzerrten Schätzung.\n\n# KQ: Kurze Regression\nlm(Y ~ B - 1)$coefficients[\"B\"]\n\n       B \n6.716837 \n\n\nDie Methoden von Belloni und Chernozhukov (2013) und Belloni, Chernozhukov, und Hansen (2014) sind im R-Paket hdm implementiert. Mit den Funktionen hrm::rlasso() und hdm::rlassoEffect kann Lasso-Regression sowie Post- und Double-Post-Selection durchgeführt werden.2323 Diese Funktionen ermitteln ein optimales \\(\\lambda\\) mit dem in Belloni u. a. (2012) vorgeschlagenen Algorithmus.\nWir berechnen zunächst den naiven Lasso-Schätzer in einem Modell mit allen Variablen.\n\nlibrary(hdm)\n\n# Naiver Post-Lasso-Schätzer\nlasso &lt;- rlasso(\n  x = X, \n  y = Y, \n  intercept = F, \n  post = F\n)\n\n# Koeffizientenschätzer auslesen\nlasso$coefficients[\"B\"] \n\n       B \n6.368456 \n\n\nAuch dieser Schätzer ist deutlich verzerrt. Problematisch ist hier nicht nur die Shrinkage auf \\(\\widehat{\\alpha}_0\\), sondern die Selektion der Variablen in XB:\n\n# Welche Variablen in XB selektiert Lasso *nicht*?\nnselektiert &lt;- which(lasso$coef[1:26] == 0)   # ID\n\n# Namen auslesen\nnames(lasso$coef[1:26])[nselektiert]\n\n[1] \"XB8\"  \"XB10\" \"XB16\" \"XB18\" \"XB20\"\n\n\nDurch das Auslassen dieser Determinanten von \\(Y\\) und \\(B\\) leidet der Lasso-Schätzer unter OVB.\nAls nächstes berechnen wir den Post-Lasso-Selection-Schätzer.\n\n# Post-Lasso-Selection-Schätzer berechnen\np_lasso &lt;- rlasso(\n  x = X,\n  y = Y, \n  intercept = F, \n  post = T\n)\n\n# Schätzung für alpha_0\np_lasso$coef[\"B\"]\n\n       B \n6.362409 \n\n\nDie Ähnlichkeit der Post-Lasso-Schätzung von \\(\\alpha_0\\) zur Lasso-Schätzung zeigt deutlich, dass die Verzerrung des Lasso-Schätzers überwiegend durch ausgelassene Variablen anstatt durch Shrinkage verursacht wird.\nMit rlassoEffect() können wir den Post-Double-Selection-Schätzer berechnen.\n\n# Post-Double-Selection-Schätzer\npds_lasso &lt;- rlassoEffect(\n  x = X %&gt;% \n    dplyr::select(-B) %&gt;% \n    as.matrix(),\n  y = Y, \n  d = B, \n  method = \"double selection\"\n)\n\n# Schnittmenge der selektierten Determinanten \n# von Y und B\n(\n  S_BY &lt;- names(\n    which(pds_lasso$selection.index)\n  )\n)\n\n [1] \"XB1\"   \"XB2\"   \"XB3\"   \"XB4\"   \"XB5\"   \"XB6\"   \"XB7\"   \"XB8\"   \"XB9\"  \n[10] \"XB10\"  \"XB11\"  \"XB12\"  \"XB13\"  \"XB14\"  \"XB15\"  \"XB16\"  \"XB17\"  \"XB18\" \n[19] \"XB19\"  \"XB20\"  \"XB21\"  \"XB22\"  \"XB23\"  \"XB24\"  \"XB25\"  \"XU209\" \"XU241\"\n[28] \"XU295\" \"XY3\"   \"XY7\"   \"XY8\"   \"XY12\"  \"XY13\"  \"XY15\"  \"XY16\"  \"XY19\" \n[37] \"XY23\" \n\n\nDouble Selection führt ebenfalls zu einem Post-Lasso-KQ-Schätzer mit allen 25 relevaten Variablen in XB. Wir selektieren allerdings deutlich weniger irrelevante Variablen aus XU als mit Single Selection und dennoch einige Determinanten von \\(Y\\) aus XY. Double Selection führt also zu einer unverzerrten Schätzen mit geringerer Varianz. Mit summary() erhalten wir gültige Inferenz bzgl. des Treatment-Effekts.\n\nsummary(pds_lasso)\n\n[1] \"Estimates and significance testing of the effect of target variables\"\n   Estimate. Std. Error t value Pr(&gt;|t|)    \nd1   1.94977    0.07127   27.36   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDer Post-Double-Selection-Schätzer liefert unter den betrachteten Verfahren die beste Schätzung von \\(\\alpha_0\\) und erlaubt gülstige statistische Inferenz. Der geschätzte Effekt ist hoch-signifikant.\n\n\n\n\n\n\nKey Facts zum Post-Double-Selection-Schätzer\n\n\n\n\nDurch die sorgfältige Auswahl von Variablen, die mit Behandlung- und Outcome-Variable zusammenhängen, ermöglicht die Double-Selection eine bessere Kontrolle über das Risiko ausgelassender Variablen in Beobachtungsstudien und ermöglicht gültige (asymptotisch normale) Inferenz.\n\nDer Post-Double-Selection-Schätzer besteht aus drei Regressionen:\n\nEs werden Variablen mit Lasso selektiert, welche die Behandlungs-Variable erklären.\nEs werden Variablen mit Lasso selektiert, welche die Outcome-Variable erklären.\nDer Post-Double-Selection-Schätzer ist der KQ-Schätzer in einer Regression, die für die Schnittmenge der ausgewählten Variablen kontrolliert.\n\n\nDank der Selektion mit Lasso kann der Schätzer auch bei hoch-dimensionalen Daten (\\(k&gt;n\\)) angewendet werden.\nPost-Double-Selection-Schätzer für Behandlungseffekte sind im R-Paket hdm implementiert.\n\n\n\n\n5.4.1 Case Study: Makroökonomisches Wachstum\nZur Illustration des Post-Double-Selection Schätzers betrachten wir eine empirische Anwendung bzgl. der Validierung von makroökonomischer Wachstumtheorie. Aus neo-klassischen Ansätzen wie dem Solow-Swan-Modell kann die Hypothese, dass Volkswirtschaften zu einem gemeinsamen Wachstumspfad hin konvergieren, abgeleitet werden. Diese Konvergenzhypothese impliziert die Existenz von Aufholeffekten: Ärmere Volkswirtschaften müssen im mittel schneller Wachsen als die Wirschaft wohlhabender Länder. Die grundlegende Spezifikation eines entsprechenden Regressionsmodells lautet \\[\\begin{align}\n  \\text{WR}_{i} = \\alpha_0 \\text{BIP0}_i + u_i, \\label{eq:growthmodel1}\n\\end{align}\\] wobei \\(\\text{WR}_{i}\\) die Wachstumsrate des Pro-Kopf-BIP in Land \\(i\\) über einen Zeitraum (typischerweise berechnet als Log-Differenz zwischen zwei Perioden) und \\(\\text{BIP0}_i\\) das (logarithmierte) Pro-Kopf-BIP zu beginn der Referenzperiode ist. Gemäß der Konvergenzhypothese muss \\(\\alpha_0&lt;0\\) sein: Je wohlhabender eine Volkswirtschaft ist, desto geringer ist das Wirtschaftswachstum.\nUm Verzerrung durch ausgelassene Kovariablen zu vermeiden, sollte das Modell \\(\\eqref{eq:growthmodel1}\\) um länder-spezifische Regressoren \\(x_{i,j}\\), die sowohl das Ausgagnsniveau \\(\\text{BIP0}\\) sowie die Wachtumsrate beinflussen, erweitert werden. Zu der großen Menge potentieller Kovariablen gehören makro- und sozio-ökonomische Maße wie bspw. die Investitionstätigkeit des Staates, Offenheit der Volkswirtschaft, das politische Umfeld, das Bildungsniveau, die Demographie usw. Eine bevorzugte Spezifikation ist daher \\[\\begin{align}\n  \\text{WR}_{i} = \\alpha_0 \\text{BIP0}_i + \\sum_{j=1}^k \\beta_j x_{i,j} + u_i,\\label{eq:growthmodel2}\n\\end{align}\\] wobei \\(\\alpha_0\\) als Behandlungseffekt interpretiert werden kann. Beachte, dass \\(\\eqref{eq:growthmodel2}\\) eine Regression in der Form von \\(\\eqref{eq:lassotmt}\\) ist.\nWir illustrieren die Schätzung von und Inferenz bzgl. \\(\\alpha_0\\) in \\(\\eqref{eq:growthmodel2}\\) mit Post-Double-Selektion für einen 90 Länder umfassenden Auszug aus dem Datensatz von Barro und Lee (2013), der als Objekt GrowthData im R-Paket hdm verfügbar ist.2424 Eine ausführliche Beschreibung der Variablen ist hier einsehbar.\n\n# Datensatz in Arbeitsumgebung verfügbar machen\nlibrary(hdm)\ndata(GrowthData)\n\n# Anzahl Beobachtungen und Variablen\ndim(GrowthData)\n\n[1] 90 63\n\n\nDie Spalte Outcome ist die jeweilige Wachstumsrate des BIP zwischen den Perioden 1965-1975 und 1975-1985 und gdpsh465 ist das reale Pro-Kopf-BIP im Jahr 1965 zu Preisen von 1980.\nWir führen zunächst eine graphische Analyse hinsichtlich des Modells einfachen Modells \\(\\eqref{eq:growthmodel1}\\) durch, indem wir gdpsh465 gegen Outcome plotten und die geschätzte Regressionsgerade einzeichnen.\n\n# Einfache grafische Analyse mit ggplot2\nGrowthData %&gt;%\n  ggplot(\n    mapping = aes(\n      x = gdpsh465, \n      y = Outcome\n    )\n  ) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F)\n\n\n\nAbbildung 5.10: BIP-Wachstum: Einfache Regression\n\n\n\nAbbildung 5.10 zeigt einen geringen positiven geschätzten Effekt \\(\\widehat{\\alpha}_0\\). Eine Auswertung mit lm() ergibt, dass der Effekt \\(\\alpha_0\\) nicht signifikant von \\(0\\) verschieden ist.\n\n# Einfache Regression durchführen, \n# Inferenz für gdpsh465 erhalten\nlm(Outcome ~ gdpsh465, data = GrowthData) %&gt;%\n  summary() %&gt;%\n  coefficients() %&gt;% \n  .[2, ]\n\n   Estimate  Std. Error     t value    Pr(&gt;|t|) \n0.001316713 0.006102200 0.215776701 0.829661165 \n\n\nDer positive Effekt aus der einfachen Schätzung widerspricht der Konvergenzhypothese. Dieses Ergebnis könnte allerdings durch Auslassen relevanter Kovariablen ungültig sein. Beispielsweise ist es plausibel, dass das Bildungsniveau einer Volkswirtschaft sowohl mit dem BIP korreliert ist als auch die Wachstumsrate beeinflusst. Dann wäre das Bildungsniveau eine relevante Kovariable, deren Auslassen zu einer verzerrten Schätzung von \\(\\alpha_0\\) führt.\nEine “lange” Regression mit allen Kovariablen ist zwar möglich, aber problematisch: Das Verhältnis von Beobachtungen (90) zu Regressoren (62) bedeutet eine hohe Unsicherheit der Schätzung.\n\n# Inferenz für alpha_0 in langer Regression\nsummary(\n  lm(Outcome ~ . - 1 , data = GrowthData)\n  ) %&gt;% \n  coefficients() %&gt;% \n  .[2, ]\n\n    Estimate   Std. Error      t value     Pr(&gt;|t|) \n-0.009377989  0.029887726 -0.313773911  0.756018518 \n\n\nDer geschätzte Koeffizient \\(\\widehat{\\alpha}_0\\) ist nun zwar negativ, liefert jedoch weiterhin keine Evidenz, dass \\(\\alpha_0\\) von 0 verschieden ist. Ein Vergleich der Standardfehler zeigt aber, dass die KQ-Schätzung aufgrund Berücksichtigung aller potentiellen Kovariablen mit deutlich größerer Varianz behaftet ist als in der einfachen KQ-Regression \\(\\eqref{eq:growthmodel1}\\)\nPost-Double-Selection erlaubt gültige Inferenz bzgl. \\(\\alpha_0\\) nach Schätzung der Menge relevanter Kovariablen. Wir weisen die entsprechenden Variablen R-Objekten zu und berechnen den Schätzer.\n\n# Variablen für Post-Double-Selection vorbereiten\n\n# abh. Variable\ny &lt;- GrowthData %&gt;% \n  pull(Outcome)\n\n# \"Treatment\"\nd &lt;- GrowthData %&gt;% \n  pull(gdpsh465)\n\n# potentielle Regressoren\nX &lt;- GrowthData %&gt;% \n  dplyr::select(\n    -Outcome, -intercept, -gdpsh465\n  )\n\n\n# Post-Double-Selection-Schätzer berechnen\nGrowth_DS &lt;- \n  rlassoEffect(\n    x = X %&gt;% \n      as.matrix(), \n    y = y, \n    d = d, \n    method = \"double selection\"\n)\n\nPost-Double-Selection wählt aus der Menge potentieller Kovariablen lediglich sieben Regressoren aus.\n\n# Selektierte Variablen einsehen\n# ID\nSelektion &lt;- Growth_DS$selection.index\n\n# Namen auslesen\nnames(\n  which(Selektion == T)\n)\n\n[1] \"bmp1l\"    \"freetar\"  \"hm65\"     \"sf65\"     \"lifee065\" \"humanf65\" \"pop6565\" \n\n\nTabelle 5.3 zeigt die Definitionen der ausgewählten Variablen.\n\n\n\n\n\n\n\n\n  \nVariable\n      Beschreibung\n    \n\n\nbmp1l\nSchwarzmarktprämie d. Währung\n\n\nfreetar\nMaß für Zollbeschränkungen\n\n\nhm65\nEinschreibungsquote Uni (Männer) \n\n\nsf65\nBeschulungsquote Sekundarstufe (Frauen)\n\n\nlifee065\nLebenserwartung bei Geburt\n\n\nhumanf65\nDurschn. Bildung im Alter 25 (Frauen)\n\n\npop6565\nAnteil Bevölkerung ü. 65 Jahre\n\n\n\nTabelle 5.3:  Mit PDS selektierte Variablen aus GrowthData.\nReferenzjahr 1965. \n\n\n\n\n# Gültige Inferenz mit dem Post-Double-Selection-Schätzer\nsummary(Growth_DS)\n\n[1] \"Estimates and significance testing of the effect of target variables\"\n   Estimate. Std. Error t value Pr(&gt;|t|)   \nd1  -0.05001    0.01579  -3.167  0.00154 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDas Ergebnis der Post-Double-Selection-Schätzung unterstützt die (bedingte) Konvergenzhypothese mit einer signifikanten negativen Schätzung \\(\\widehat{\\alpha}_0\\approx-0.05\\).\n\n\n\n\n\n\nBarro, Robert J., und Jong Wha Lee. 2013. „A new data set of educational attainment in the world, 1950–2010“. Journal of Development Economics 104: 184–98. https://doi.org/https://doi.org/10.1016/j.jdeveco.2012.10.001.\n\n\nBelloni, Alexandre, Daniel Chen, Victor Chernozhukov, und Christian Hansen. 2012. „Sparse models and methods for optimal instruments with an application to eminent domain“. Econometrica 80 (6): 2369–429.\n\n\nBelloni, Alexandre, und Victor Chernozhukov. 2013. „Least squares after model selection in high-dimensional sparse models“. Bernoulli, 521–47.\n\n\nBelloni, Alexandre, Victor Chernozhukov, und Christian Hansen. 2014. „High-dimensional methods and inference on structural and treatment effects“. Journal of Economic Perspectives 28 (2): 29–50.\n\n\nCortez, Paulo, und Alice Maria Gonçalves Silva. 2008. „Using data mining to predict secondary school student performance“.\n\n\nEfron, Bradley, Trevor Hastie, Iain Johnstone, und Robert Tibshirani. 2004. „Least angle regression“.\n\n\nHahn, P Richard, Carlos M Carvalho, David Puelz, und Jingyu He. 2018. „Regularization and confounding in linear regression for treatment effect estimation“.\n\n\nHoerl, Arthur E, und Robert W Kennard. 1970. „Ridge regression: Biased estimation for nonorthogonal problems“. Technometrics 12 (1): 55–67.\n\n\nTibshirani, Robert. 1996. „Regression shrinkage and selection via the lasso“. Journal of the Royal Statistical Society Series B: Statistical Methodology 58 (1): 267–88."
  },
  {
    "objectID": "Literatur.html",
    "href": "Literatur.html",
    "title": "Literatur",
    "section": "",
    "text": "Barro, Robert J., and Jong Wha Lee. 2013. “A New Data Set of\nEducational Attainment in the World, 1950–2010.” Journal of\nDevelopment Economics 104: 184–98. https://doi.org/https://doi.org/10.1016/j.jdeveco.2012.10.001.\n\n\nBasten, Christoph, and Frank Betz. 2013. “Beyond Work Ethic:\nReligion, Individual, and Political Preferences.” American\nEconomic Journal: Economic Policy 5 (3): 67–91.\n\n\nBelloni, Alexandre, Daniel Chen, Victor Chernozhukov, and Christian\nHansen. 2012. “Sparse Models and Methods for Optimal Instruments\nwith an Application to Eminent Domain.” Econometrica 80\n(6): 2369–429.\n\n\nBelloni, Alexandre, and Victor Chernozhukov. 2013. “Least Squares\nAfter Model Selection in High-Dimensional Sparse Models.”\nBernoulli, 521–47.\n\n\nBelloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2014.\n“High-Dimensional Methods and Inference on Structural and\nTreatment Effects.” Journal of Economic Perspectives 28\n(2): 29–50.\n\n\nCattaneo, Matias D, Michael Jansson, and Xinwei Ma. 2020. “Simple\nLocal Polynomial Density Estimators.” Journal of the American\nStatistical Association 115 (531): 1449–55.\n\n\nCortez, Paulo, and Alice Maria Gonçalves Silva. 2008. “Using Data\nMining to Predict Secondary School Student Performance.”\n\n\nEfron, Bradley, Trevor Hastie, Iain Johnstone, and Robert Tibshirani.\n2004. “Least Angle Regression.”\n\n\nGelman, Andrew, and Guido Imbens. 2019. “Why High-Order\nPolynomials Should Not Be Used in Regression Discontinuity\nDesigns.” Journal of Business & Economic Statistics\n37 (3): 447–56.\n\n\nHahn, P Richard, Carlos M Carvalho, David Puelz, and Jingyu He. 2018.\n“Regularization and Confounding in Linear Regression for Treatment\nEffect Estimation.”\n\n\nHoerl, Arthur E, and Robert W Kennard. 1970. “Ridge regression: Biased estimation for nonorthogonal\nproblems.” Technometrics 12 (1): 55–67.\n\n\nImbens, G. W., and Thomas Lemieux. 2008. “Regression Discontinuity\nDesigns: A Guide to Practice.” Journal of Econometrics\n142 (2): 615–35.\n\n\nImbens, Guido, and Karthik Kalyanaraman. 2012. “Optimal Bandwidth\nChoice for the Regression Discontinuity Estimator.” The\nReview of Economic Studies 79 (3): 933–59.\n\n\nLee, David S. 2008. “Randomized Experiments from Non-Random\nSelection in US House Elections.” Journal of\nEconometrics 142 (2): 675–97.\n\n\nMcCrary, Justin. 2008. “Manipulation of the Running Variable in\nthe Regression Discontinuity Design: A Density Test.” Journal\nof Econometrics 142 (2): 698–714.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via\nthe Lasso.” Journal of the Royal Statistical Society Series\nB: Statistical Methodology 58 (1): 267–88.\n\n\nWeber, Max. 2004. Die Protestantische Ethik Und Der Geist Des\nKapitalismus. Vol. 1614. CH Beck."
  }
]