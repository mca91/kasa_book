[
  {
    "objectID": "Matching.html",
    "href": "Matching.html",
    "title": "\n6  Matching\n",
    "section": "",
    "text": "6.1 Balance: Vergleichbarkeit von Behandlungs- und Kontrollgruppe\nDer Lehrstuhl für Ökonometrie an der Universität Duisburg-Essen betreibt einen Ökonometrie-Blog und interessiert sich für den kausalen Effekt der Einführung eines darkmode auf die Verweildauer der Leser auf der Webseite. Die Webseite ist zwar nicht-kommerziell, hat sich allerdings insb. für die Aquise internationaler Studierender für den Studiengang MSc. Econometrics als wichtiges Marketing-Instrument erwiesen. Ein anprechendes Design wird daher als hoch-relevant erachtet.\nIdealerweise sollte der Effekt des Design-Relaunches auf die Nutzungsintensität in einem kontrollierten randomisierten Experiment untersucht werden. Hierbei würden wir Nutzern zufällig das neue oder das alte Design zuweisen und den Effekt als Differnz des durchschnittlichen Verweildauer für die Gruppen bestimmen. Eine solche Studie ist jedoch aus technischen und finanziellen Gründen nicht realisierbar, sodass die Auswirkungen des darkmode mit vorliegenden nicht-experimenellen Nutzungsstatistiken für die Webseite geschätzt werden sollen.\nDie Nutzungsstatistiken sind im Datensatz darkmode.csv enthalten und sollen der Analyse des Effekts des darkmode (dark_mode) auf die Verweildauer der Leser auf der Webseite (read_time) dienen.\nTabelle 6.1 zeigt die Definitionen der Variablen in darkmode.csv.\nVariable\nBeschreibung\n\n\n\nread_time\nLesezeit (Minuten/Woche)\n\n\ndark_mode\nIndikator: Beobachtung nach Einführung darkmode\n\n\nmale\nIndikator: Individuum männlich\n\n\nage\nAlter (in Jahren)\n\n\nhours\nBisherige Verweildauer auf der Seite\n\n\n\n\n\n\n\nTabelle 6.1: Variablen im Datensatz darkmode\nFür die Analyse lesen wir zunächst den Datensatz darkmode.csv mit readr::read_csv() ein und verschaffen uns einen Überblick über die verfügbaren Variablen.\ndark_mode hat den Typ logical. Mit dplyr::mutate_all() können wir komfortabel alle Spalten in den Typ numeric transformieren.\nEine naive Schätzung des durchschnittlichen Behandlungseffekts (ATE) \\(\\widehat{\\tau}^{\\text{naiv}}\\) erhalten wir als Mittelwertdifferenz von read_time für die Behandlungsgruppe (dark_mode == 1) und die Kontrollgruppe (dark_mode == 0) \\[\\begin{align}\n  \\widehat{\\tau}^{\\text{naiv}} = \\overline{\\text{read\\_time}}_{\\text{Behandlung}} - \\overline{\\text{read\\_time}}_{\\text{Kontrolle}}.\\label{eq:naivATEdarkmode}\n\\end{align}\\]\nDiese Berechnung ist schnell mit R durchgeführt.\nDie Schätzung ergibt einen negativen Behandlungseffekt, mit der Interpreation, dass das neue Design zu einer Reduktion der Lesezeit um etwa 0.44 Minuten pro Woche führt. Dieses Ergebnis ist allerdings zweifelhaft, weil eine Isolierung des Behandlungseffekts aufgrund von Backdoor-Pfaden im DGP vermutlich nicht gewähleistet ist. Ein Indikator hierfür sind systematische Unterschiede hinsichtlich von (möglicherweise unbeobachtbaren) Charakteristika von Kontrollgruppe und Behandlungsgruppe.\nDa die User sich beim Aufrufen der Seite aktiv für oder gegen den das neue Design entscheiden müssen (und somit selektieren, ob Sie in der Behandlungs- oder Kontrollgruppe landen), liegt wahrscheinlich Confounding vor: Unsere Hypothese ist zunächst, dass männliche User eine durchschnittlich längere Lesezeit aufweisen und mit größerer Wahrscheinlichkeit auf das neue Design wechseln als nicht-männliche Leser. Dann ist male eine Backdoor-Variable. Diese Situation ist unter der Annahme, dass nur diese Faktoren den DGP bestimmen, in Abbildung 6.1 dargestellt.\nread_time\nread_timedark_mode\ndark_modedark_mode-&gt;read_time\nmale\nmalemale-&gt;read_time\nmale-&gt;dark_mode\n\n\n\n\nAbbildung 6.1: Backdoor durch ‘male’ im Website-Design-Bespiel\nDer DGP in Abbildung 6.1 führt zu einer verzerrten Schätzung des kausalen Effekts von dark_mode auf read_time mit \\(\\eqref{eq:naivATEdarkmode}\\), wenn das Verhältnis von männlichen und nicht-männlichen Usern in Bahandlungs- und Kontrollgruppe nicht ausgeglichen ist. Wir überprüfen dies mit R.\nDie Zusammenfassung anteile_m zeigt, dass der Anteil männlicher User in der Behandlungsgruppe deutlich höher ist als in der Kontrollgruppe.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matching</span>"
    ]
  },
  {
    "objectID": "Matching.html#sec-balance",
    "href": "Matching.html#sec-balance",
    "title": "\n6  Matching\n",
    "section": "",
    "text": "6.1.1 Matching durch Gewichtung\nMatching eliminiert die Variation von male zwischen den Gruppen. Eine Möglichkeit hierfür ist die Gewichtung der Beobachtungen in der Kontrollgruppe entsprechend der Anteile von Männern und Nicht-Männern in der Behandlungsgruppe, sodass die Vergleichbarkeit mit der Behandlungsgruppe hinsichtlich des Geschlechts gewährleistet ist. Dies wird in der Literatur als Balance bezeichnet. Der Behandlungseffekt wird dann analog zu \\(\\eqref{eq:naivATEdarkmode}\\) geschätzt.\nDie Gewichte für Beobachtungen in der Kontrollgruppe \\(w_i\\) werden berechnet als \\[\\begin{align}\n  w_i =\n  \\begin{cases}\n    \\text{ant\\_m}_B/\\text{anz\\_m}_{K}, & \\text{falls } \\text{male}_i = 1\\\\\n        \\text{ant\\_nm}_B/\\text{anz\\_nm}_{K}, & \\text{sonst.}\\\\\n  \\end{cases}\\label{eq:darkmodeweights}\n\\end{align}\\] Anhand der Formel für einen gewichteten Durchschnitt, \\[\\begin{align}\n  \\overline{X}_w = \\frac{\\sum_i w_i \\cdot X_i}{\\sum_i w_i},\n\\end{align}\\] berechnen wir die gewichteten Mittelwerte für male und read_time in der Kontrollgruppe.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEin Vergleich des gewichteten Mittelwertes von male in der Kontrollgruppe mit dem Mittelwert in der Behandlungsgruppe (male_k) zeigt, dass die Gewichte die Variation in male zwischen beiden Gruppen eliminieren, sodass die Backdoor durch male geschlossen ist. Mit wmean_read_time_K haben wir einen entsprechend gewichteten Mittelwert der Verweildauer für die Kontrollgruppe berechnet. Wir schätzen den Behandlungseffekt nun als \\[\\begin{align}\n  \\widehat{\\tau}^{\\text{w}} = \\overline{\\text{read\\_time}}_{B} - \\overline{\\text{read\\_time}}_{w,K}.\\label{eq:weightedATEdarkmode}\n\\end{align}\\]\n\n\n\n\n\n\n\n\nEntgegen der naiven Schätzung andhand von \\(\\eqref{eq:naivATEdarkmode}\\) erhalten wir nach Matching für male eine positive Schätzung des Behandlungseffekts.\nDie Schätzung des Behandlungseffekts anhand von \\(\\eqref{eq:weightedATEdarkmode}\\) entspricht dem geschätzten Koeffizienten \\(\\widehat{\\beta}_1\\) aus einer gewichteten KQ-Regression im Modell \\[\\begin{align*}\n  \\text{read\\_time} = \\beta_0 + \\beta_1 \\text{dark\\_mode} + u,\n\\end{align*}\\] wobei die Beobachtungen der Kontrollgruppe wie in \\(\\eqref{eq:darkmodeweights}\\) gewichtet werden und \\(w_i=1\\) für Beobachtungen der Behandlungsgruppe ist. Wir überprüfen dies mit R.\n\n\n\n\n\n\n\n\nDer geschätzte Koeffizient von dark_mode entspricht \\(\\widehat{\\tau}^w\\).\nDa male eine binäre Variable ist, reduziert sich eine Beurteilung der Vergleichbarkeit der Verteilungen von male in Behandlungs- und Kontrollgruppe auf einen simplen Vergleich des Männeranteils beider Gruppen. In der Praxis gibt es meist eine Vielzahl potentieller Backdoor-Variablen, die zudem kontinuierlich verteilt sind. Es scheint plausibel, dass das Alter der Nutzer sowohl die Akzeptanz des Design-Updates als auch die Lesezeit beeinflusst. Die bisherige Verweildauer ist mindestens eine plausible Determinante der Lesezeit.\nDer erweiterte DGP ist in Abbildung Abbildung 6.2 dargestellt, wobei der zusätzliche Backdoor-Pfad durch age ebenfalls mit roten Pfeilen gekennzeichnet sind.\n\n\n\n\n\nread_time\nread_timedark_mode\ndark_modedark_mode-&gt;read_time\nmale\nmalemale-&gt;read_time\nmale-&gt;dark_mode\nage\nageage-&gt;read_time\nage-&gt;dark_mode\nhours\nhourshours-&gt;read_time\n\n\n\n\nAbbildung 6.2: Erweiterter DGP im Website-Design-Beispiel\n\n\n\n\nDie Beurteilung der Balance von Kontrollgruppe und Behandlungsgruppe kann durch eine grafische Gegenüberstellung der empirischen Verteilungen der Kovariablen beider Gruppen erfolgen. Wir visualisieren die empirischen Verteilungen mit ggplot2. Hierzu standardisieren wir age und hours zunächst mit scale().\n\n\n\n\n\n\n\n\nFür age und hours eignen sich die geschätzten Dichtefunktionen für einen Vergleich der Verteilungen in Behandlungs- und Kontrollgruppe.\n\n\n\n\n\n\n\n\nDie graphische Analyse zeigt deutliche Unterschiede in den Verteilungen von age zwischen Kontroll- und Behandlungsgruppe. Für einen Beurteilung mit deskriptiven Statistiken wird häufig eine sogenannte Balance Table herangezogen. Wir berechnen diese für age, hours und male mit cobalt::bal.tab()\n\n\n\n\n\n\n\n\nDie Einträge M.0.Un und M.1.Un zeigen die jeweiligen Stichprobenmittelwerte der Variablen für Kontroll- und Behandlungsgruppe. Diff.Un gibt eine standardisierte Mittelwertdifferenz \\(SMD\\) an, wobei \\[\\begin{align*}\n  SMD_j := \\left(\\overline{X}_{j,B} - \\overline{X}_{j,K}\\right) \\bigg/ \\sqrt{\\frac{1}{2}\\left(\\widehat{\\text{Var}}(X_{j,B}) + \\widehat{\\text{Var}}(X_{j,K})\\right)},\n\\end{align*}\\] mit Stichprobenmitteln \\(\\overline{X}_{j,B}\\) und \\(\\overline{X}_{j,K}\\) und Stichprobenvarianzen \\(\\widehat{\\text{Var}}(X_{j,B})\\) und \\(\\widehat{\\text{Var}}(X_{j,K})\\) für eine kontinuierliche Kovariable \\(j\\).2 Obwohl es keinen einheitlichen Schwellenwert für die standardisierte Differenz gibt, der ein erhebliches Ungleichgewicht anzeigt, gilt für kontinuierliche Variablen eine standardisierte (absolute) Differenz von weniger als \\(0.1\\) als Hinweis auf einen vernachlässigbaren Unterschied zwischen den Gruppen.\n2 Siehe P. Austin (2011) für einen Überblick zu Balance-Statistiken.Die Balance Table weist also auf einen vernachlässigbaren Unterschied für hours hin und bestätigt den aus den Grafiken abgeleiteten Eindruck einer relevanten Differenzen für age.\n\n6.1.2 Entropy Balancing\nEntropy Balancing (Hainmueller 2012) ist eine weitere Methode zur Gewährleistung der Vergleichbarkeit von Behandlungs- und Kontrollgruppe anhand von Gewichten. Das Verfahren nutzt Konzepte aus der Informationstheorie um die Gewichte für Subjekte in der Kontrollgruppe so anzupassen, dass die Verteilung der Matchingvariablen in der Kontrollgruppe die Verteilung in der Behandlungsgruppe möglichst gut approximiert. Dies geschieht unter der Restriktion, dass bestimmte empirische Momente (meist Mittelwerte und Varianzen) der Matchingvariablen exakt übereinstimmen. Mathematisch werden die Gewichte für Kontrollgruppenbeobachtungen durch Minimierung der Kullback-Leibler-Divergenz zwischen den Verteilungen gefunden, wobei die Divergenz ein Maß für den Unterschied von Wahrscheinlichkeitsverteilungen ist.\nEntropy Balancing ist im R-Paket WeightIt implementiert. Wir zeigen, wie die benötigten Gewichte für eine Schätzungen des ATT im Website-Beispiel mit WeightIt::wightit() bestimmt werden können. Über das Argument moments legen wir fest, dass die Gewichte unter der Restriktion übereinstimmender Mittelwerte aller Matching-Variablen zwischen Behandlungs- und Kontrollgruppe erfolgen soll.\n\n\n\n\n\n\n\n\nWir schätzen den Behandlungseffekt nach Entropy Balancing mit gewichteter Regression.\n\n\n\n\n\n\n\n\nBeachte, dass die von summary() berechneten Standardfehler bei Entropy Balancing ungültig sind. In Abschnitt Kapitel 6.4 erläutern wir die Berechnung von Standardfehlern für Matching-Schätzer mit dem Bootstrap.\n\n6.1.3 Mehrere Matching-Variablen und der Propensity Score\nBei mehreren Backdoor-Variablen kann eine Gewichtung anhand der Behandlungswahrscheinlichkeit (Treatment Propensity) erfolgen. Die Idee hierbei ist, dass der DGP wie in Abbildung 6.3 dargestellt werden kann.\n\n\n\n\n\nread_time\nread_timedark_mode\ndark_modedark_mode-&gt;read_time\nTreatmentPropensity\nTreatmentPropensityTreatmentPropensity-&gt;dark_mode\nmale\nmalemale-&gt;read_time\nmale-&gt;TreatmentPropensity\nage\nageage-&gt;read_time\nage-&gt;TreatmentPropensity\nhours\nhourshours-&gt;read_time\n\n\n\n\nAbbildung 6.3: Propensity im Website-Design-Beispiel\n\n\n\n\nHierbei beeinflussen die Backdoor-Variablen age und male die Behandlungsvariable dark_mode lediglich durch die Behandlungswahrscheinlichkeit Treatment Propensity. Diese Darstellung zeigt, das die mehrdimensionale Information bzgl. der Ähnlichkeit von Subjekten hinsichtlich der beobachteten Kovariablen in einer einzigen Variable zusammengefasst werden kann. Die Backdoor-Pfade können daher geschlossen werden, indem wir Subjekte anhand von Treatment Propensity derart gewichten, dass beide Gruppen hinsichtlich der Verteilung der Behandlungswahrscheinlichkeit vergleichbar sind. Betrachte erneut \\(\\eqref{eq:cia}\\) und beachte, dass \\[\\begin{align}\n  Y_i = Y_i^{(1)} D_i + Y_i^{(0)} (1-D_i).\n\\end{align}\\] Rosenbaum und Rubin (1983) zeigen, dass es hinsichtlich \\(\\eqref{eq:cia}\\) äquivalent ist für die Treatment Propensity \\(P_i(X_i):=P(B_i=1\\vert X_i = x)\\) zu kontrollieren, d.h. \\[\\begin{align}\n  \\left\\{Y_i^{(1)},Y_i^{(0)}\\right\\} \\perp B_i\\vert X_i \\quad\\Leftrightarrow\\quad \\left\\{Y_i^{(1)},Y_i^{(0)}\\right\\} \\perp B_i\\vert P_i(X_i).\n\\end{align}\\]\nDer Behandlungseffekt kann so als Differenz von gewichteten Gruppenmittelwerten berechnet werden, mit inversem Wahrscheinlichkeitsgewicht (IPW) \\(w_{i,B} = 1/P_i(X_i)\\) für Beobachtungen in der Behandlungsgruppe und \\(w_{i,K} = 1/(1-P_i(X_i))\\) für Beobachtungen in der Kontrollgruppe, \\[\\begin{align}\n  \\tau^{\\text{IPW}} = \\frac{1}{n}\\sum_{i=1}^n \\left[\\frac{B_i Y_i}{P_i(X_i)} - \\frac{(1-B_i)Y_i}{1-P_i(X_i)} \\right].\\label{eq:tauipw}\n\\end{align}\\]\nGrundsätzlich ist TreatmentPropensity eine nicht beobachtbare Variable und muss daher aus den Daten geschätzt werden. Eine geschätzte Behandlungswahrscheinlichkeiten \\(\\widehat{P}_i(X_i)\\) wird als Propensity Score bezeichnet. In der Praxis erfolgt die Schätzung von Propensity Scores meist mit logistischer Regression. Ein erwartungstreuer Schätzer des ATE ist \\[\\begin{align}\n  \\widehat{\\tau}^{\\text{IPW}} = \\frac{1}{n}\\sum_{i=1}^n \\left[\\frac{B_i Y_i}{\\widehat{P}_i(X_i)} - \\frac{(1-B_i)Y_i}{1-\\widehat{P}_i(X_i)} \\right].\\label{eq:hattauipw}\n\\end{align}\\] Hirano, Imbens, und Ridder (2003) diskutieren Alternativen zu \\(\\eqref{eq:hattauipw}\\) für die Schätzung anderer Typen von Behandlungseffekten.\nWir schätzen nachfolgend die Propensity Scores für unser Anwendungsbeispiel, erläutern die Berechnung der Gewichte sowie die Schätzung von Behandlungseffekten mit gewichteter Regression. Hierbei betrachten wir eine Variante von \\(\\eqref{eq:hattauipw}\\) mit normalisierten Gewichten \\(\\tilde{w}_{i,B} = w_{i,B}/\\sum_i w_{i,B}\\) und \\(\\tilde{w}_{i,K} = w_{i,K}/\\sum_i w_{i,K}\\) die sich jeweils zu 1 summieren.3 Dies ergibt den Hájek-Schätzer4 \\[\\begin{align}\n    \\widehat{\\tau}_N^{\\text{IPW}} = \\frac{\\sum_i\\tilde{w}_{i,B}Y_i}{\\sum_i\\tilde{w}_{i,B}} -  \\frac{\\sum_i\\tilde{w}_{i,K}Y_i}{\\sum_i\\tilde{w}_{i,K}}.\\label{eq:hattauhajek}\n\\end{align}\\]\n3 Eine Normalisierung der Gewichte reduziert die Varianz des Schätzers, vgl. Hirano, Imbens, und Ridder (2003)4 Siehe Hájek (1971).Zunächst Schätzen wir ein logistisches Regressionsmodell mit age, male und hours als erklärende Variablen für dark_mode.\n\n\n\n\n\n\n\n\nDie Propensity Scores erhalten wir als angepasste Werte aus der Regression darkmode_ps_logit mit fitted(). Wir erweitern den Datensatz mit den Ergebnissen.\n\n\n\n\n\n\n\n\nZur Beurteilung der Überlappung (vgl. Annahme \\(\\eqref{eq:overlap}\\) können wir die Verteilung der Propensity Scores nach Behandlungs-Indikator mit Histogrammen visualisieren.\n\n\n\n\n\n\n\n\nEin Vergleich der Histogramme zeigt, dass die Überlappung der Propensity Scores in der linken Flanken der Verteilungen der Kontrollgruppe und in der rechten Flanke der Behandlungsgruppe schlechter wird. Wir entfernen zunächst Beobachtungen aus der Stichprobe deren Propensity Scores wenig bzw. keine Überlappung aufweisen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIPWs anhand der Propensity Scores können schnell mit der Vorschrift \\[\\begin{align}\n  \\text{IPW} = \\frac{\\text{dark\\_mode}}{\\text{PS}} + \\frac{1 - \\text{dark\\_mode}}{1 - \\text{PS}},\n\\end{align}\\] berechnet werden.\n\n\n\n\n\n\n\n\nEine Schätzung des durchschnittlichen Behandlungseffekts gemäß \\(\\eqref{eq:hattauhajek}\\) implementieren wir mit dplyr.\n\n\n\n\n\n\n\n\nDiese Schätzung des Behandlungseffekts ist äquivalent zur gewichteten KQ-Schätzung anhand eines einfachen linearen Regressionsmodells.\n\n\n\n\n\n\n\n\nUnsere Schätzung des ATE ist der geschätzte Koeffizient von dark_mode. Die ausgegebenen Standardfehler und Inferenzstatistiken sind jedoch ungültig aufgrund der Gewichtung mit IPWs, den inversen geschätzten Wahrscheinlichkeiten für eine Behandlung. Der Grund hierfür ist, dass die Berechnung der Standardfehler in summary() die zusätzliche Unsicherheit durch die geschätzen Propensity Scores nicht berücksichtigt! Später im Kapitel erläutern wir die Berechnung gültiger Standardfehler für IPW-Schätzer basierend auf Propensity Scores mit dem Bootstrap.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matching</span>"
    ]
  },
  {
    "objectID": "Matching.html#selektierende-matching-verfahren",
    "href": "Matching.html#selektierende-matching-verfahren",
    "title": "\n6  Matching\n",
    "section": "\n6.2 Selektierende Matching-Verfahren",
    "text": "6.2 Selektierende Matching-Verfahren\nDas grundsätzliche Konzept von selektierendem Matching wird in der nachstehenden interaktiven Grafik veranschaulicht. Hier betrachten wir beobachtete Ausprägungen von zwei (unabhängig und identisch verteilten) Matching-Variablen für Subjekte in der Behandlungsgruppe (blau) sowie Kontrollgruppe (rot). Als Matches qualifizieren sich sämtliche Beobachtungen der anderen Gruppe, deren Euklidische Distanz zu dem ausgewählten Punkt das über den Slider eingestellte Maximum Caliper nicht überschreitet.5 Diese Region wird durch den gestrichelten Kreis gekennzeichnet und kann über den zugehörigen Slider angepasst werden. Per Klick auf eine Beobachtung werden Matches aus der anderen Gruppe durch eine verbindende Linie und farbliches Hervorheben kenntlich gemacht. Mit dem Slider für k wird festgelegt, dass nur die nahesten k qualifizierten Beobachtungen als Matches behandelt werden. Die Grafik illustriert inbesondere, dass Beobachtungen in Abhängigkeit von k und Caliper falls gewünscht mehrfach (s.g. Matching mit Zurücklegen) oder gar nicht gematcht werden können.\n5 Es handelt sich hierbei um einen Spezialfall von Matching anhand der Mahalanobis-Distanz, siehe Kapitel 6.2.3.\n\n\n\n\n\n\n\n\n\nFür die nachfolgenden Code-Beispiele verwenden wir das R-Paket MatchIt. MatchIt::matchit() nutzt standardmäßig Eins-zu-Eins-Matching (ohne Zurücklegen) von Beobachtungen der Treatment-Gruppe mit Beobachtungen der Kontrollgruppe.6 Die für das Matching zu verwendenden Variablen werden über das Argument formula als Funktion des Behandlungsindikators definiert. matchit() bereitet das Objekt für eine Schätzung des ATT mit einer geeigneten Funktionen, s. ?matchit und hier insb. die Erläuterungen der Argumente replace = F, ratio = 1 und estimand = \"ATT\" für Details. Mit cobalt::balt.tab() erhalten wir eine balance table für den gematchten Datensatz.\n6 Dieses Schema zielt auf eine Schätzung des ATT ab.Wir zeigen als nächstes, wie MatchIt::matchit() für Matching anhand der Regressoren age, hours, und male in unserem Website-Beispiel für unterschiedliche Varianten durchgeführt werden kann.\n\n6.2.1 Exaktes Matching\nExaktes Matching ordnet einem Subjekt aus der Behandlungsgruppe ein oder mehrere Subjekte aus der Kontrollgruppe zu, wenn die boebachteten Ausprägung der Matching-Variablen exakt übereinstimmen. Hierbei muss die ‘Distanz’ zwischen den Ausprägung der Matching-Variablen folglich \\(0\\) sein. Dieses Verfahren findet meist bei ausschließlich diskret verteilten Merkmalen Anwendung. Bei kontinuierlich verteilten Merkmalen (vgl. die obige interaktive Grafik) sind exakte Matches zwar theoretisch unmöglich, ergeben sich jedoch in der Praxis aus der Datenerfassung, bspw. durch Rundungsfehler. In matchit() erhalten wir exaktes Ein-zu-eins-Matching mit method = \"exact\".\n\n\n\n\n\n\n\n\nAufgrund der kontinulierlich verteilten Variable hours gibt es in unserem Website-Beispiel keine exakten Matches. Dieses Verfahren ist hier folglich ungeeignet.\n\n6.2.2 Coarsened Exact Matching\nBei dieser Methode werden kontinuierliche Matching-Variablen grob (Engl. coarse) klassiert, ähnlich wie bei einem Histogram. Diese Diskretisierung ermöglicht es exakte Übereinstimmungen zwischen Behandlungs- und Kontrollgruppenbeobachtungen hinsichtlich ihrer klassierten Ausprägungen zu finden. Sowohl Behandlungs- als auch Kontrollbeobachtungen die mindestents einen exakten Match haben, werden Teil des gematchten Datensatzes. In matchit() wird Coarsened Exact Matching mit method = \"cem\" durchgeführt. Über das Argument cutpoints geben wir an, dass hours in 6 Klassen und age in 4 Klassen eingeteilt werden soll.7 Mit k1k = TRUE erfolgt Eins-zu-eins-Matching: Bei mehreren exakten Matches wird die Beobachtung mit der geringsten Mahalanobis-Distanz (für die unklassierten Matching-Variablen) gewählt.\n7 Diese Werte wurden ad-hoc gewählt da sie zu einem guten Ergebnis führen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMit Coarsened Exact Matching erhalten wir einen Datensatz mit 82 Beobachtungen und guter Balance.\n\n6.2.3 Matching mit der Mahalanobis-Distanz\nDie Euklidische Distanz misst den direkten Abstand zwischen zwei Punkten und ist nicht invariant gegenüber Transformationen, insbesondere bei unterschiedlichen Skalierungen und bei Korrelation der Matching-Variablen. Die Mahalanobis-Distanz hingegen ist ein standardisiertes Distanzmaß, das unter Berücksichtigung der Varianz-Kovarianz-Struktur der Daten angibt, wie viele Standardabweichungen zwei Datenpunkte voneinander entfernt sind. Die Mahalanobis-Distanz ist invariant gegenüber linearen Transformationen (Skalierung, Translation und Rotation) der Daten und bietet ein genaueres Maß für die Unähnlichkeit zweier Beobachtungen hinsichtlich ihrer Ausprägungen der Matching-Variablen.\nBetrachte die Datenpunkte \\(P_1=(X_1,Y_1)'\\) und \\(P_2=(X_2,Y_2)'\\) für die Matching-Variablen \\(X\\) und \\(Y\\). Die Mahalanobis-Distanz zwischen \\(P_1\\) und \\(P_2\\) ist definiert als \\[\\begin{align*}\n  d_M(P_1,\\,P_2) = \\sqrt{(P_1 - P_2)'\\boldsymbol{S}^{-1} (P_1 - P_2)},\n\\end{align*}\\] wobei \\(\\boldsymbol{S}\\) die Varianz-Kovarianz-Matrix von \\(X\\) und \\(Y\\) ist. Die Mahalanobis-Distanz \\(d_M(P_1,\\,P_2)\\) ist also die Euklidische Distanz zwischen den standardisierten Datenpunkten.\nIn empirischen Anwendungen ersetzen wir die (unbekannten) Komponenten der Varianz-Kovarianz-Matrix durch Stichprobenvarianten. Dies ergibt die Formel\n\\[\\begin{align*}\n  \\widehat{d}_M(P_1,\\,P_2) = \\sqrt{\n  \\begin{pmatrix}\n    X_1 - X_2\\\\\n    Y_1 - Y_2\n  \\end{pmatrix}'\n  \\begin{pmatrix}\n    \\widehat{\\text{Var}}(X^2) & \\widehat{\\text{Cov}}(X, Y) \\\\\n     \\widehat{\\text{Cov}}(X, Y) & \\widehat{\\text{Var}}(Y^2)\n  \\end{pmatrix}^{-1}\n    \\begin{pmatrix}\n    X_1 - X_2\\\\\n    Y_1 - Y_2\n  \\end{pmatrix}\n}.\n\\end{align*}\\]\nDie nachstehende interaktive Grafik zeigt Beobachtungen zweier Matching-Variablen, die aus einer bivariaten Normalverteilung mit positiver Korrelation generiert wurden. Diese bivariate Verteilung ist identisch für Beobachtungen aus der Kontrollgruppe (rot) und Beobachtungen aus der Behandlungsgruppe (blau). Für die ausgewählte Beobachtung aus der Behandlungsgruppe (schwarzer Rand) werden potentielle Matches in der Kontrollgruppe innerhalb der vorgegebenen Mahalanobis-Distanz in Cyan kenntlich gemacht. Beachte, dass die Mahalanobis-Distanz Varianzen und Kovarianzen der Daten berücksichtigt, sodass die gematchten Beobachtungen in einem elliptischen Bereich um die betrachtete behandelte Beobachtung liegen. Eine Euklidische Distanz hingegen (gestrichelte Linie) ignoriert die Skalierung der Daten.\n\n\n\n\n\n\n\n\n\n\nFür Eins-zu-Eins-Matching im Website-Beispiel anhand der Mahalanobis-Distanz mit matchit() setzen wir distance = \"mahalanobis\" und wählen method = \"nearest\". Mit diesen Parametern wird jeder Behandlung aus der Behandlungsgruppe die gemäß \\(d_M\\) am ehesten vergleichbarste Beobachtung aus der Kontrollgruppe zugewiesen, wobei keine mehrfachen Matches zulässig sind.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie Ergebnisse zeigen, dass für sämtliche \\(149\\) Beobachtungen aus der Behandlungsgruppe ein individueller Match in der Kontrollgruppe gefunden werden konnte. Es werden lediglich \\(2\\) Beobachtungen der \\(151\\) Beobachtungen in der Kontrollgruppe nicht gematcht.\nEntsprechend zeigt die Balance-Table eine ähnliche Diskrepanz beider Gruppen hinsichtlich der Matching-Variablen an.\nMahalanobis-Distanz mit Caliper .25 für Propensity Scores basierend auf logistischer Regression\nFür eine strengeres Matching-Kriterium kann ein Caliper, d.h. eine maximal zulässige Distanz, herangezogen werden. Die Mahalanobis-Distanz hat jedoch keine einheitliche Skala: Ob eine Distanz als groß oder klein betrachten werden kann, hängt von der Anzahl der Matching-Variablen und dem Überlappungsgrad zwischen den Gruppen ab. Daher wird die Beschränkung durch einen Caliper nicht auf \\(\\widehat{d}_M\\) sondern auf Propensity Scores angewendet.\nIm nächsten Code-Beispiel spezifizieren wir mit distance = \"glm\", dass Propensity Scores gemäß der Vorschrift in formula geschätzt werden. Mit mahvars = ~ age + male + hours legen wir die Matching-Variablen für die Berechnung von \\(\\widehat{d}_M\\) fest. caliper = .25 legt fest, dass lediglich Beobachtungen der Kontrollgruppe bei einer absoluten Differenz der Propensity Scores von höchstens \\(0.25\\) Standardabweichungen als Match für eine Beobachtung in der Behandlungsgruppe qualifiziert sind.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie Balance-Table zeigt einen deutlichen Effekt der Beschränkung qualifizierter Beobachtungen durch caliper = .25: Aufgrund der oberen Grenze für die Propensity-Score-Differenz von XX wird für lediglich \\(104\\) Beobachtungen aus der Behandlungsgruppe ein individueller Match in der Kontrollgruppe gefunden.8 Weiterhin finden wir eine verbesserte Balance für den gematchten Datensatz.\n8 Die durch caliper implizierte Obergrenze ergibt sich als .25 * sd(fitted(darkmode_ps_logit))).\n6.2.4 Propensity Score Matching\nEine gängige Variante ist Matching ausschließlich anhand von Propensity Scores innerhalb eines Calipers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLaut Balance-Table führt Eins-zu-Eins-Matching basierend auf Propensity Scores zu einem Datensatz mit \\(104\\) gematchten Beobachtungen in der Behandlungsgruppe. Hinsichtlich der standardisierten Mittelwertdifferenz (Diff.Adj) erzielt diese Methode die beste Balance unter den betrachteten Ansätzen.\nVergleich der Balance verschiedener Verfahren mit Love-Plot\nStandardisierte Mittelwertdifferenzen für verschiedene Matching-Verfahren können grafisch mit einem Love-Plot (Love 2004) veranschaulicht werden. Hierzu nutzen wir cobalt::love.plot() und übergeben die mit matchit() generierten Objekte im Argument weights.\n\n\n\n\n\n\n\n\nDie Grafik zeigt, dass Coarsened Exact Matching (CEM) unter allen betrachteten Verfahren die Stichprobe mit der besten Balance ergibt. Diesen gematchten Datensatz erhalten wir mit MatchIt::match.data().\n\n\n\n\n\n\n\n\ndarkmode_matched enthält Gewichte (weights) für die jeweilige Gruppe zu denen gemachte Beobachtungen gehören (subclass). Dies ist relevant, falls Beobachtungen mehrfach gematcht werden. Wegen Eins-zu-eins-Matching ohne Zurücklegen gibt es in unserem Beispiel XX Beobachtungspaare und sämtliche Gewichte sind 1. Die Berücksichtigung der Gewicht in den nachfolgenden Aufrufen von Schätzfunktionen (bspw.lm()) ist daher nicht nötig und erfolgt lediglich zur Illustration der grundsätzlichen Vorgehensweise.\nEine Wiederholung der grafischen Analyse in Kapitel 6.1 zeigt eine deutlich verbesserte Vergleichbarkeit hinsichtlich der Verteilung der Matching-Variablen in darkmode_matched.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWir beobachten eine bessere Balance bei age und hours. Inbesondere ist male für Kontroll- und Behandlungsgruppe ausgeglichen.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matching</span>"
    ]
  },
  {
    "objectID": "Matching.html#sec-regadj",
    "href": "Matching.html#sec-regadj",
    "title": "\n6  Matching\n",
    "section": "\n6.3 Schätzung und Inferenz für den Behandlungseffekt nach Matching",
    "text": "6.3 Schätzung und Inferenz für den Behandlungseffekt nach Matching\nWir schätzen nun den Behandlungseffekt von dark_mode auf read_time für die mit CEM und Propensity Score Matching ermittelten Datensätzen und vergleichen die Ergebniss anschließend mit einer Regressionsschätzung ohne Matching.\nWir kombinieren die Matching-Verfahren mit linearer Regression, d.h. wir Schätzen den Behandlungseffekt anhand es gematchten Datensatzes als Mittelwertdifferenz nach zusätzlicher Kontrolle für die Matching-Variablen. Diese Kombination von Matching mit Regression wird in der Literatur als Regression Adjustment bezeichnet und ist insbesondere hilfreich, wenn Backdoors mit Matching geschlossen werden sollen, der kausale Effekt jedoch nur unter Verwendung einer nicht-trivialen Regressionsfunktion ermittelt werden kann. Zum Beispiel kann bei einer kontinuierlichen Behandlungsvariable und einem nicht-linearen Zusammenhang mit \\(Y\\) der kausale Effekt nicht durch einen bloßen Mittelwertvergleich erfasst werden, sondern erfordert eine adäquate Modellierung dieses Zusammenhangs in der Regressionsfunktion. Die zusätzliche Kontrolle für Matching-Variablen kann die Varianz der Schätzung verringern und das Risiko einer verzerrten Schätzung abmildern, falls nach Matching noch Unterschiede in der Balance von Behandlungs- und Kontrollgruppe vorliegen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeachte, dass für die gematchten Datensätze jeweils ein durchschnittlicher Behandlungseffekt für die Beobachtungen mit erfolgter Behandlung ermittelt wird: In sämtlichen oben gezeigten Matchibg-Verfahren werden mit estimand = \"ATT\" vergleichbarere Kontrollbeobachtungen für die behandelten Beobachtungen ermittelt. Wir schätzen den Effekt der Behandlung, indem wir die Ergebnisse von behandelten Personen mit denen von gematchten Personen vergleichen, die keine Behandlung erhalten haben. Diese Vergleichsgruppe dient als Ersatz für den hypothetischen Zustand der Behandlungsgruppe, wenn keine Behandlung erfolgt wäre. Dies entspricht der Definition eines ATT — ein average treatment effect on the treated.\n\n6.3.1 Cluster-robuste Standardfehler\nFür Matching-Verfahren sind die mit summary() berechneten Standardfehler (und damit auch Konfidenzintervalle, t-Statistiken und p-Werte) für den Behandlungseffekt grundsätzlich ungültig. Je nach Matching-Verfahren liegen unterschiedliche Quellen von Schätzunsicherheit vor, die bei der Berechnung von Standardfehlern zusätzlich zu der “üblichen” Stichproben-Variabilität berücksichtig werden müssen. Gründe hierfür sind der Matching-Prozess ansich oder weitere Unsicherheit durch die Schätzung zusätzlicher Parameter, etwa bei der Berechnung von Propensity Scores mit logistischer Regression. Eine weiterere Ursache zusätzlicher Variation durch den Matching-Prozess, die wir bisher nicht näher betrachtet haben ensteht durch Zurücklegen, d.h. wenn Beobachtungen mehrfach gematcht werden können. Auch dieser Faktor wird in der von summary() verwendeten Formel für den Standardfehler des Effekt-Schätzers nicht berücksichtigt.\nDie Standardfehlerberechnung für Matching-Schätzer von Behandlungseffekten ist ein Gegenstand aktueller methodischer Forschung. P. C. Austin und Small (2014) und Abadie und Spiess (2022) belegen die Gültigkeit von cluster-robusten Standardfehlern mit Clustering auf Ebene der Beobachtungsgruppen (subclass im output von match.data()) bei Matching ohne Zurücklegen. Für Matching anhand von Propensity Scores (auch mit Zurücklegen) zeigt Imbens (2016), dass ignorieren der zusätzlichen Unsicherheit durch die Schätzung der Propensity Scores zu konservativer Inferenz für den ATE anhand eines cluster-robusten Standardfehlerschätzers führt, jedoch ungültige Inferenz für die Schätzung des ATT bedeuten kann. Ähnlich zu P. C. Austin und Small (2014) deuten die Ergebnisse der Simulationsstudie von Bodory u. a. (2020) jedoch auf grundsätzlich bessere Eigenschaften der Schätzung hin, wenn die Standardfehler nicht für die Schätzung der Propensity Scores adjustiert werden.\nWeiterhin ist die Kontrolle für Kovariablen mit Erklärungskraft für die Outcome-Variable und für die Matching-Variablen mit Regression Adjustment (vgl. Kapitel 6.3) für die Schätzung des Behandlungseffekts nach Matching eine etablierte Strategie, vgl. Hill und Reiter (2006) und Abadie und Spiess (2022). So können die Varianz der Schätzung und das Risiko einer Verzerrung der Standardfehler aufgrund verbleibender Imbalance von Behandlungs- und Kontrollgruppe nach Matching verringert werden.\nZur Demonstration von (cluster)-robuster Inferenz und für eine tabellarische Zusammenfassung der Ergebnisse nutzen wir die Pakete marginaleffects und modelsummary. Mit marginaleffects::avg_comparisons() können p-Werte und Kofindenzintervalle unter Berücksichtigung von robuster Standardfehlern und der Gewichte aus dem Matching-Verfahren berechnet werden.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWir fassen die Ergebnisse mit modelsummary::modelsummary() tabellarisch zusammen.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matching</span>"
    ]
  },
  {
    "objectID": "Matching.html#sec-bootmatching",
    "href": "Matching.html#sec-bootmatching",
    "title": "\n6  Matching\n",
    "section": "\n6.4  Bootstrap-Schätzung kausaler Effekte bei Matching",
    "text": "6.4  Bootstrap-Schätzung kausaler Effekte bei Matching\nEin Bootstrap-Verfahren generiert mit Resampling (wiederholtes Ziehen mit Zurücklegen) aus dem Original-Datensatz (viele) künstliche Datensätze, für die der Schätzer (d.h. das gesamte Verfahren inkl. Matching!) jeweils berechnet wird. Die Verteilung der so gewonnenen Bootstrap-Schätzwerte approximiert die wahre, unbekannte Stichprobenverteilung des Schätzers des Behandlungseffekts. Mit dieser simulierten Verteilung können wir Inferenz betreiben: Wir können einen Bootstrap-Punktschätzer des Behandlungseffekts (Stichprobenmittel der Bootstrap-Schätzungen) sowie Standardfehler (Standardabweichung der der Bootstrap-Schätzungen) und p-Werte berechnen.\nDer Bootstrap kann hilfreich sein, wenn unklar ist, wie Standardfehler für die Unsicherheit des Matching-Prozesses zu adjustieren sind, um gültige Inferenzsstatistiken zu erhalten. Abadie und Imbens (2008) zeigen analytisch, dass der Standard-Bootstrap die Stichprobenverteilung für Schätzer kausaler Effekte anhand von gematchten Datensätzen (d.h. bei Zuordnung/Selektion von Beobachtungen mit Matching) nicht korrekt abbilden kann. Grundsätzlich problematisch hierbei ist, wenn der Bootstrap eine verzerrte Schätzung produziert und/oder zu kleine Standardfehler liefert. Abadie und Imbens (2008) belegen die Tendenz des Bootstraps zu konservative (d.h. zu große) Standardfehler zu produzieren. Simulationsnachweise (Bodory u. a. 2020; Hill und Reiter 2006; P. C. Austin und Small 2014; P. C. Austin und Stuart 2017) finden, dass Bootstrap-Standardfehler u.a. bei Propensity Score Matching mit Zurücklegen leicht konservativ sind somit das gewünschte nominale Signifikanzniveau eines Bootstrap-Hypothesentests nicht überschritten wird, weshalb der Standard-Bootstrap trotz seiner Schwächen in der empirischen Forschung oft angewendet wird.\nWir betrachen als nächstes einen Bootstrap-Algorithmus für Inferenz bezüglich eines kausalen Effekts nach Matching und demonstrieren die Schätzung anhand unseres Website-Beispiels für den ATT nach Propensity-Score-Matching.\n\n\n\n\n\n\nAlgorithmus: Bootstrap-Schätzer für Matching mit Regression Adjustment\n\n\n\n\nGeneriere eine Bootstrap-Stichprobe durch \\(N\\) Züge mit Zurücklegen aus der \\(N\\)-elementigen originalen Stichprobe.\nWende das Matching-Verfahren für die Bootstrap-Stichprobe an. Schätze den Behandlungseffekt \\(\\beta\\) anhand der gematchten Stichprobe mit Regression. Speichere den Punktschätzer des Behandlungseffekts \\(\\widehat{\\beta}_b^*\\).\nFürhre die Schritte 1 und 2 für \\(b=1,\\dots,B\\) aus, wobei \\(B\\) eine hinreichend große Anzahl von Bootstrap-Replikationen ist.\nBerechne den Bootstrap-Schätzer des Behandlungseffekts \\(\\overline{\\beta}^* = \\frac{1}{B}\\sum_{b=1} \\widehat{\\beta}_b^*\\) und den Standardfehler \\(\\text{SE}(\\overline{\\beta}^*) = \\sqrt{\\frac{1}{B-1}\\sum_{b=1}^B(\\widehat{\\beta}_b^*-\\overline{\\beta}^*)^2}\\).\n\n\n\nWir Implementieren nun einen Bootstrap-Schätzer des ATT im Website-Beispiel für Propensity-Score-Matching. Hierzu definieren wir eine R-Funktion boot_fun() für die Schritte 1 und 2 im obigen Algorithmus.\n\n\n\n\n\n\n\n\nWir berechnen nun eine Bootstrap-Schätzung des ATT von dark_mode auf readingtime nach Propensity Score Matching mit einem caliper von 0.25 sowie den zugehörigen Standardfehler und ein 95%-KI mit der zuvor definierten Funktion boot_fun.\n\n\n\n\n\n\n\n\nDen Bootstrap-Schätzer des ATT sowie den Bootstrap-Standardfehler berechnen wir mit mean() und sd() anhand der \\(B=199\\) Bootstrap-Replikationen in boot_out$t.\nBeachte, dass der Bootstrap-Schätzer des Behandlungseffekts nicht unmittelbar von boot() ausgegeben wird: Der Eintrag original ist die Schätzung anhand der ursprünglichen gesamten Stichprobe und bias ist die Differenz zwischen dieser Schätzung und dem Mittelwert der Bootstrap-Schätzungen.\n\n\n\n\n\n\n\n\nWir können prüfen, dass die Berechnung des Standardfehlers dem Stichprobenstandardabweichung der Bootstrap-Schätzungen entspricht.\n\n\n\n\n\n\n\n\nEin 95%-Konfidenzintervall für den kausalen Effekt erhalten wir mit boot::boot.ci().9\n9 type = \"bca\" (bias-corrected accelerated) ist eine gängige Implementierung für die Berechnung des Bootstrap-Konfidenz-Intervalls.\n\n\n\n\n\n\n\nBeachte, dass der Bootstrap-Standardfehler sowie das Bootstrap-Konfidenzintervall nahe der mit avg_comarisons berechneten Werte für sum_PSC sind.\nBootstrap-Standardfehler für IPW-Schätzer des ATE berechnen\nDie Bootstrap-Funktion boot_fun kann leicht für eine Schätzung des Standardfehlers für den IPW-Schätzer des ATE aus Kapitel 6.1.3 angepasst werden. Statt einer Matching-Prozedur berechnen wir hierzu für \\(B\\) Bootstrap-Stichproben den Schätzer \\(\\widehat{\\tau}^\\text{IPW}\\) mit Trimming der Propensity Scores.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn diesem Fall ist der Bootstrap-Standardfehler von ca. XXX gut mit dem anhand von summary(model_ipw) berechneten Standardfehler vergleichbar. Ein 95%-Konfidenzintervall für den ATE erhalten wir wie zuvor mit boot.ci().",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matching</span>"
    ]
  },
  {
    "objectID": "Matching.html#regression-matching-und-doubly-robust-schätzung",
    "href": "Matching.html#regression-matching-und-doubly-robust-schätzung",
    "title": "\n6  Matching\n",
    "section": "\n6.5  Regression, Matching und Doubly-Robust-Schätzung",
    "text": "6.5  Regression, Matching und Doubly-Robust-Schätzung\nAls Doubly-Robust-Schätzer bezeichnet man Methoden, die trotz Fehlspezifikationen im Matching-Verfahren oder bei falscher Form der Regressionsfunktion für die Outcome-Variable eine robuste Schätzung des kausalen Effekts ermöglichen.10 Unter der Voraussetzung, dass die verwendeten Matching-Variablen sämtliche Backdoors schließen, ist mit Doubly-Robust-Schätzung eine konsistente Schätzung des Behandlungseffekts unter abgeschwächten Annahmen gewährleistet. Dies macht Doubly-Robust-Schätzer besonders nützlich in empirischen Anwendungen, wenn Modellspezifikationen herausfordernd sind.\n10 Bspw. kann eine falsche funktionale Form bei logistischer Regression eine verzerrte Schätzung von Propensity Scores und damit eine unzureichende Balance bedeuten.Der von Wooldridge (2010) vorgeschlagene Doubly-Robust-Schätzer (Inverse Probability Weighted Regression Adjustment, IPWRA) für den ATE erreicht seine vorteilhaften Eigenschaften durch eine geschickte Kombination von IPW und Regression Adjustment.\n\n\n\n\n\n\nAlgorithmus: IPWRA-Schätzer des ATE\n\n\n\nVgl. Wooldridge (2010).\n\nBerechne IPW anhand von Propensity Score mit logistischer Regression unter Verwendung der Matching-Variablen.\n\nRegression Adjustment:\n\nSchätze lediglich für die Behandlungsgruppe eine mit den IPW gewichtete Regressionspezifikation der Outcome-Variable mit Kontrolle für die Matching-Variablen.\nWiederhole Schritt A für die Kontrollgruppe.\n\n\nBerechne Vorhersagen der Outcome-Variable für die angepassten Modelle aus 2 (a) und 2 (b) anhand des gesamten Datensatzes.\nSchätze den ATE als Mittelwert-Differenz der Vorhersagen aus Schritt 3.\n\n\n\nWir implementieren den Doubly-Robust-Schätzer des ATE von Wooldridge (2010) für das Website-Beispiel in der Funktion IPWRA() under Adaption des Schemas von IPW_boot().\n\n\n\n\n\n\n\n\nSchätzung des ATE mit IPWRA():\n\n\n\n\n\n\n\n\nWie bei \\(\\widehat{\\tau}^\\text{IPW}\\) gibt es keine formale Darstellung des Standardfehlers für den IPWRA-Schätzer, sodass auch hier der Bootstrap für Inferenz genutzt werden muss.\n\n\n\n\n\n\n\n\nWir berechnen die interessierenden Statistiken analog zu der Vorgehensweise in Kapitel 6.4.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie Bootstrap-Schätzung des ATE mit IPWRA ist mit den Ergebnissen für die Bootstrap-Variante von \\(\\widehat{\\tau}^\\text{IPW}\\) vergleichbar, hat allerdings einen etwas kleineren Standardfehler.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matching</span>"
    ]
  },
  {
    "objectID": "R_Einfuehrung.html",
    "href": "R_Einfuehrung.html",
    "title": "\n2  Statistische Programmierung mit R\n",
    "section": "",
    "text": "2.1 tidyverse\ntidyverse ist eine Sammlung von R-Paketen, die für Datenmanipulation, -analyse und -visualisierung entwickelt wurden, und einen konsistenten, benutzerfreundlichen und funktionalen Programmierstil fördern. Im Fokus der bereitgestellten Funktionen sind saubere, strukturierte und reproduzierbare Data-Science-Workflows.\nWir laden zunächst die Paketsammlung tidyverse in R. Für die Reproduktion mit dem R GUI oder mit RStudio muss das Paket vorab mit install.packages() installiert werden. In den interaktiven R-Konsolen in diesem Kapitel (und im Rest des Buchs) sind die benötigten R-Pakete bereits installiert und geladen, sofern nicht anders beschrieben.\n# Paket tidyverse installieren\n# install.packages(\"tidyverse\")\n\n# Paket 'tidyverse' laden\nlibrary(tidyverse)\nFür das Verständnis von Code-Chunks ist es hilfreich, Zwischenergebnisse explizit zu evaluieren und in der Konsole auszugeben. Hierfür umschließen wir häufig Code-Zeilen mit runden Klammern. Der nächste Chunk illustriert dies für die Variable x.",
    "crumbs": [
      "Grundlagen",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistische Programmierung mit R</span>"
    ]
  },
  {
    "objectID": "R_Einfuehrung.html#tidyverse",
    "href": "R_Einfuehrung.html#tidyverse",
    "title": "\n2  Statistische Programmierung mit R\n",
    "section": "",
    "text": "2.1.1 Lange, weite und “tidy” Datenformate\nWir betrachten den in Tabelle 2.1 dargestellten Datensatz Klausurergebnisse.\n\n\n\n\n\n\n\n\nName\nMikro\nMakro\nMathe\n\n\n\nTim\nNA\n1.3\n3\n\n\nLena\n1\n3\nNA\n\n\nRicarda\n2\n1.7\n1.3\n\n\nSimon\n2.3\n3.3\nNA\n\n\n\n\n\n\n\nTabelle 2.1: Datensatz Klausurergebnisse\n\n\n\nDer Datensatz ist noch nicht in der R-Arbeitsumgebung verfügbar. Mit der Funktion tribble() können wir Tabelle 2.1 händisch als R-Objekt der Klasse tibble definieren\n\n\n\n\n\n\nklausurergebnisse enhält die Klausurnoten der vier Studierenden (Boebachtungen) spaltenweise pro Modul, d.h. die Spaltennamen Mikro, Makro und Mathe sind Ausprägungen der Variable Modul. Der Datensatz liegt also nicht im s.g. Tidy-Format vor.\n\n\n\n\n\n\nTidy-Format\n\n\n\nTidy-Format: Jede Spalte ist eine Variable, jede Reihe ist eine Beobachtung und jede Zelle enthält einen einen Wert. Datensätze im Tidy-Format sind häufig lang: Die Zeilendimension ist größer als die Spaltendimension.\n\n\nDas Tidy-Format ist hilfreich für statistische Analysen mit tidyverse-Funktionen wie bspw. ggplot(). Wir nutzen die Funktion tidyr::pivot_longer(), um klausurergebnisse ein (langes) Tidy-Format zu transformieren.\n\n\n\n\n\n\nBeachte, dass die Spalte Name die Zugehörigkeit der Ausprägungen (Note) jeder Variable (Modul) zu einer Beobachtung identifiziert. Mit dieser Information können wir den langen Datensatz wieder in das ursprüngliche (weite) Format zurückführen. Wir nutzen hierzu tidyr::pivot_wider().\n\n\n\n\n\n\nWenn die Zuweisung von Zwischenergebnissen in Variablen nicht benötigt wird, kann eine Verkettung von Funktionsaufrufen die Verständlichkeit des Codes verbessern. Hierzu wird der Pipe-Operator %&gt;% genutzt. Wir wiederholen die Transformationen mit den tidyr::pivot_*-Funktion bei Verwendung von %&gt;%.\n\n\n\n\n\n\nEin Beispiel für den Nachteil des weiten Formats im Umgang mit tidyverse-Paketen ist die Funktion tidyr::drop_na(). Diese entfernt sämtliche Zeilen eines Datensatzes, die NA-Einträge (d.h. fehlende Werte) aufweisen. Beachte, dass diese Operation im ursprünglichen weiten Format zum Entfernen ganzer Beobachtungen aus wide führt.\n\n\n\n\n\n\nIm Tidy-Format long hingegen bleiben die übrigen Informationen betroffener Beobachtungen erhalten.\n\n\n\n\n\n\n\n2.1.2 Pinguine und Pipes\nIn diesem Abschnitt zeigen wir die Verwendung häufig verwendeter dplyr-Funktionen (s.g. Verben) für die Transformation von Datensätzen: mutate(), select(), filter(),summarise() und arrange().\nFür die Illustration verwenden wir den Datensatz penguins aus dem R-Paket palmerpenguins. Dieser Datensatz wurde im Zeitraum 2007 bis 2009 von Dr. Kristen Gorman im Rahmen des Palmer Station Long Term Ecological Research Program zusammengetragen und enthält Größenmessungen für drei Pinguinarten, die auf den Inseln des Palmer-Archipels in der Antarktis beobachtet wurden.\n\n# Paket 'palmerpenguins' installieren\n# install.packages(\"palmerpenguins\")\n\n# Paket 'palmerpenguins' laden\nlibrary(palmerpenguins)\n\nMit data() wird der Datensatz in der Arbeitsumgebung verfügbar gemacht. Wir nutzen glimpse(), um einen Überblick zu erhalten.\n\n\n\n\n\n\n\n2.1.2.1 dplyr::mutate()\n\nMit mutate() können bestehende Variablen überschrieben oder neue Variablen als Funktion bestehender Variablen definiert werden. mutate() operiert in der Spaltendimension des Datensatz.\nWir definieren eine neue Variable body_mass_kg als Transformation body_mass_g/1000.\n\n\n\n\n\n\nMit across() kann die dieselbe Operation auf mehrere Variablen angewendet werden.\nIm nachstehenden Beispiel ändern wir den typ (type) der Variablen species, island, sex und year zu character.\n\n\n\n\n\n\ntransmute() ist eine Variante von mutate(), die lediglich die transformierten Variablen beibehält.\n\n\n\n\n\n\n\n2.1.2.2 dplyr::select()\n\nMit select() werden Variablen aus dem Datensatz ausgewählt. Dies geschieht entweder über den Variablennamen…\n\n\n\n\n\n\n… oder über eine Indexmenge.3\n3 Hilfreich: dplyr::pull() selektiert eine Variable und wandelt diese in einen Vektor um.\n\n\n\n\n\nVariablen können anhand eines Muster im Namen selektiert werden. Die Selektion von ends_with(\"mm\") bezieht nur Variablen mit Endung mm im Namen ein:\n\n\n\n\n\n\nMit where() können wir Variablen aufgrund bestimmter Eigenschaften ihrer Ausprägungen selektieren.\n\n\n\n\n\n\n\n2.1.2.3 dplyr::filter()\n\nDas Verb filter() filtert den Datensatz in der Zeilendimension. So können Beobachtungen ausgewält werden, deren Merkmalsausprägungen bestimmte Kriterien erfüllen. Hierzu muss filter() ein logischer (logical) Ausdruck übergeben werden. Häufig erfolgt dies über Vergleichsoperatoren.\n\n\n\n\n\n\n\n\n\n\n\n\nOft ist es praktisch, mehrere Kriterien zu kombinieren.\n\n\n\n\n\n\nAnalog: komma-getrennte Kriterien werden intern über den Und-Operator (&) verknüpft.\n\n\n\n\n\n\nÄhnlich wie bei select() verwenden wir häufig nützliche Funktionen, welche die Interpretation des Codes erleichtern. dplyr::between() erlaubt filtern innerhalb eines Intervals.\n\n\n\n\n\n\nMit diesen Verben sind wir bereits in der Lage, den Datensatz gemäß folgender Vorschrift zu bereinigen:\n\nEntfernen der Maßeinheiten aus den Variablennamen\nEntfernen von Pinguinen mit fehlenden Werten (NA)\nEntfernen von Pinguinen mit einem Gewicht oberhalb des 95%-Stichprobenquantils\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDurch die Verkettung mit %&gt;% können wir sämtliche Schritte für die Bereinigung ohne das Abspeichern von Zwischenergebnissen durchführen.\n\n\n\n\n\n\n\n2.1.2.4 dplyr::summarise()\n\nDas Verb summarise() fasst Variablen über Beobachtungen hinweg zusammen. Der nachstehende Code-Chunk erzeugt eine Tabelle mit Stichprobenmittelwert und -standardabweichung von flipper_length_mm.4 Um zu vermeiden, dass die Auswertung aufgrund fehlender Werte (NA) in flipper_length_mm scheitert, lassen wir NAs mit na.rm = TRUE bei der Berechnung unberücksichtigt (wir verwenden weiterhin den unbereinigten Datensatz penguins).\n4 dplyr::summarise() darf nicht mit base::summary() verwechselt werden!\n\n\n\n\n\nVarianten von summarise() können über mehrere Variablen angewendet werden. Wir verwenden across() und where(), um lediglich numerische Variablen mit den in der liste definierten Funktionen zusammenzufassen. Beachte, dass \\(x) mean(x) eine anonyme Funktion definiert.\n\n\n\n\n\n\n\n2.1.2.5 dplyr::arrange()\n\nMit arrange() können Datensätze in Abhängigkeit der beobachteten Ausprägungen von Variablen sortiert werden.\n\n\n\n\n\n\nDie Funktion dplyr::desc() kehrt die Reihenfolge zu einer absteigenden Sortierung um.\n\n\n\n\n\n\nKomplexe Sortier-Muster werden durch Übergabe von Variablennamen in der gewünschten Reihenfolge erreicht.\n\n\n\n\n\n\n\n2.1.2.6 Operationen mit gruppierten Datensätzen\nFür manche Transformationen ist eine Gruppierung der Daten hilfreich. Wir illustrieren nachfolgend die unterschiedlichen Verhaltensweisen ausgewählter Verben durch Vergleiche von gruppierten und nicht-gruppierten Anwendungen.\n\n\n\n\n\n\nspecies hat drei Ausprägungen. Entsprechend ist penguins_grouped nun in drei Gruppen eingeteilt.\nBei gruppierten Datensätzen fasst summarise() die Variablen pro Guppe zusammen.\n\n\n\n\n\n\n\n\n\n\n\n\nmutate() definiert bzw. transformiert für jede Gruppe separat. Im dies zu veranschaulichen, ziehen wir eine Zufallsstichprobe von 10 Pinguinen aus der Datensatz.\n\n\n\n\n\n\n\n\n\n\n\n\nFür den ungruppierten Datensatz berechnet mutate() das Stichprobenmittel von bill_length_mm über alle zehn Datenpunkte und weißt diesen Wert jeweils in der Variable mean zu.\n\n\n\n\n\n\nBei gruppierten Daten berechnet mutate() das Stichprobenmittel pro Gruppe und weist die Mittelwerte entsprechend zu.\n\n2.1.3 Eine explorative Analyse mit ggplot2\nDer bereinigte Datensatz penguins_cleaned eignet sich gut für eine graphische Auswertung mit dem R-Paket ggplot2, welches Bestandteil des tidyverse ist. Nachfolgend untersuchen wir Zusammenhänge zwischen den Körpermaßen der Pinguine.\nWir erstellen zunächst einen einfachen Punkteplot des Gewichts (body_mass) und der Schnabeltiefe (bill_depth).\n\n\n\n\n\n\nDie Grafik zeigt einen positiven Zusammenhang zwischen dem Gewicht und der Schnabeltiefe. Als nächstes passen wir den Code so an, dass die Datenpunkte entsprechend der Art (species) eingefärbt sind.\n\n\n\n\n\n\nOffenbar gibt es deutliche Unterschiede in der (gemeinsamen) Verteilung von Gewicht und Schnabeltiefe zwischen den verschiedenen Arten.\nUm den Zusammenhang zwischen Gewicht und Schnabeltiefe zu untersuchen, schätzen wir lineare Regressionen \\[body\\_mass = \\beta_0 + \\beta_1 bill\\_depth + u\\] separat für jede der drei Pinguinarten mit der KQ-Methode. Anschließend zeichnen wir die geschätzten Regressionsgeraden ein.\n\n\n\n\n\n\nDie Schätzungen bekräftigen die Vermutung, dass der lineare Zusammenhang zwischen Gewicht und Schnabeltiefe sich nicht zwischen den verschiedenen Pinguinarten unterscheidet: Pinguine der Art Gentoo sind im Mittel schwerer als Pinguine der übrigen Arten, haben jedoch eine geringere Schnabeltiefe.\nDer nachfolgende Code fügt der Grafik eine Regressionsline über alle Arten hinzu. Wir setzen hierbei das Argment inherit_aes = FALSE und legen damit fest, dass die Regression für body_mass und bill_depth ohne Differenzierung per species durchgeführt wird.\n\n\n\n\n\n\nOffenbar ist die vorherige Analyse per Spezies sinnvoller: Die Regression über alle Arten suggeriert einen negativen Zusammenhang zwischen Gewicht und Schnabeltiefe.\nFacetting mit facet_wrap() erlaubt eine Untersuchung des Zusammenhangs je Insel (island), auf der die Messung erfolgt ist.\n\n\n\n\n\n\nWir sehen, dass es hinsichtlich des Zusammenhangs von Gewicht und Schnabeltiefe keine wesentlichen Diskrepanzen zwischen den drei Inseln gibt. Darüber hinaus lässt sich anhand der Facetten leicht erkennen, wie die drei Arten über die Inseln verteilt sind.",
    "crumbs": [
      "Grundlagen",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistische Programmierung mit R</span>"
    ]
  },
  {
    "objectID": "RR.html",
    "href": "RR.html",
    "title": "\n3  Reproduzierbarkeit\n",
    "section": "",
    "text": "3.1 RMarkdown\nReproduzierbare Forschung meint Prozesse, bei denen Forschende ihre Analysen und Ergebnisse so dokumentieren, dass sie von anderen exakt nachvollzogen und durch Wiederholung der dokumentierten Vorgehensweise reproduziert werden können. Die Anforderungen an solche Prozesse können im Wesentlichen anhand von drei Kriterien zusammengefasst werden:\nRMarkdown ist ein Dokumentationsformat im R-Ökosystem, das es Nutzern ermöglicht die oben genannten Anforderungen an reprozierbare Forschung zu erfüllen. Es kombiniert die Funktionalitäten von Markdown, einer schlanken Markup-Sprache für Textformatierung, mit der Möglichkeit, direkt in R geschriebenen Code auszuführen und die Ergebnisse, einschließlich Tabellen und Grafiken in ein Ziel-Dokument einzubinden.",
    "crumbs": [
      "Grundlagen",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproduzierbarkeit</span>"
    ]
  },
  {
    "objectID": "RR.html#rmarkdown",
    "href": "RR.html#rmarkdown",
    "title": "\n3  Reproduzierbarkeit\n",
    "section": "",
    "text": "Transparenz: Der gesamte Analyseprozess sollte für Dritte einsehbar und transparent sein. Die Nachvollziehbarkeit des Prozesses sollte durch adäquate Dokumentation sämtlicher Schritte gewährleistet sein.\nAutomatisierung: Analysen sollten möglichst automatisiert durchgeführt werden, um das Risiko menschlicher Fehler (zum Beispiel bei der Datenverarbeitung oder bei Berechnungen) gering zu halten.\nTeilbarkeit: Die verwendeten Daten, der Code und die Ergebnisse sollten leicht geteilt und überprüft werden können, idealerweise anhand eines (Datei)Formats, dass geringe technische Anforderungen hinsichtlich der Reproduzierbarkeit der Inhalte voraussetzt.\n\n\n\n3.1.1 Struktur\nEin typisches Rmarkdown-Dokument (eine .rmd-Datei) besteht aus drei Hauptkomponenten:\n\n\nYAML-Header: Dieser Bereich am Anfang des Dokuments wird durch --- eingegrenzt und enthält Metadaten wie den Titel, Autor, Datum und das gewünschte Ausgabeformat (z. B. PDF, HTML, Word). Den Argumenten können, je nach Definition, Objekte des Typs numeric, character oder function übergeben werden. Der nachfolgende Chunk definiert ein Dokument mit dem Titel, Autor und Datumsangebe sowie eine Ausgabe als HTML-Dokument.\n\n---\ntitle: \"Reproduzierbare Forschung mit R\"\nauthor: \"Martin Arnold\"\ndate: \"24. April 2023\"\noutput: html_document\n---\n\n\n\nCode-Chunks und Inline-Code: Code-Chunks enthalten ausführbaren R-Code (oder auch Code in anderen unterstützten Sprachen wie Python, SQL usw.), der während der Erstellung des Output-Dokument ausgeführt wird. Die Ergebnisse des Codes (z.B. Outputs von Schätzfunktionen wie lm(), Grafiken oder Tabellen) werden, gemeinsam mit dem Code, direkt im Dokument angezeigt. Ein einfacher Code-Chunk ist folgendermaßen aufgebaut:\n```{r}\n# Ein einfacher Code-Chunk\n1 + 1\n```\nCode-Chunks können mit Optionen versehen werden, die das Verhalten von Knitr beim Auswerten des Codes festlegen. Das Schema hierfür ist ```{r, option = wert}, wobei mehrere Optionen komma-getrennt gesetzt werden können. Einige nützliche Optionen3 (und ihre Standardwerte) sind:\n\n\neval (TRUE): Gibt an, ob der Code-Chunk ausgeführt werden soll\n\necho (TRUE): Gibt an, ob (oder welche Zeilen des) Quellcodes eines Chunks ausgegeben werden sollen\n\ncache (FALSE): Gibt an, ob die Ergebnisse des Code-Chunks zwischengespeichert werden sollen\n\nwarning (TRUE): Gibt an, ob Warnungen ausgegeben werden sollen\n\nmessage (TRUE): Gibt an, ob Hinweise ausgegeben bleiben sollen\n\ninclude (TRUE): Gibt an, ob der Code-Chunk im Ausgabedokument enthalten sein soll\n\nInline-Code kann im Fließtext verwendet werden, um die Ergebnisse einzeiligen Codes (oft kleine Rechnungen oder der Inhalt eines Objekts) direkt im Text (s.u.) auszugeben. Inline-Code-Ausdrücke können wie folgt ausgewertet werden:\n\n\n`r 1+1`\n\n\n\nMarkdown-formatierter Text: Dies sind Abschnitte mit Fließtext, der in der Markdown-Syntax formatiert wird. Hierbei können Textformatierungen wie Überschriften, Listen, Links, Bilder aus externen Dateien und weitere Elemente durch kurze Befehle erstellt oder eingebunden werden. Zur Erläuterung statistischer Methoden können \\(\\LaTeX\\)-Formeln in Text-Abschnitten gesetzt werden.\n3 Ausführliche Erläuterungen und weitere Optionen können hier eingesehen werden.\n3.1.2 Datei erstellen und kompilieren\nUm eine .Rmd-Datei in das gewünschte Output-Format umzuwandeln (zu kompilieren), sind mehrere Software-Pakete und Schritte erforderlich:\n\nRStudio-Installation: Obwohl RMarkdown auch über die Befehlszeile (R-Konsole / R-GUI) ausgeführt werden kann, ist RStudio die empfohlene Entwicklungsumgebung, da es eine benutzerfreundliche Schnittstelle bietet, mit der .Rmd-Dateien leicht erstellt, bearbeitet und kompiliert werden können. Beachte: Für die Verwendung von RStudion muss die R-Konsole installiert sein.\n\nrmarkdown-Paket: Um .Rmd-Dateien verarbeiten zu können, muss das R-Paket rmarkdown installiert sein. Dies erfolgt über die R-Konsole oder in RStudio mit folgendem Befehl:\n\n# RMarkdown installieren\ninstall.packages(\"rmarkdown\")\n\n\n\nFormat-abhängige Pakete: Um HTML-Output zu erzeugen, sind zusätzlichen Pakete erforderlich, da die erforderlichen Abhängigkeiten mit dem rmarkdown-Paket installiert werden. Für PDF-Output-Formate muss eine \\(\\LaTeX\\)-Distribution verfügbar sein.4 \\(\\LaTeX\\) wird verwendet, um das Dokument in ein professionell formatiertes PDF zu konvertieren. Für Anfänger empfiehlt sich die Nutzung von TinyTeX, eine kompakte Distribution, die direkt über die R-Konsole installiert werden kann:\n\n# tinytex-LaTeX-Distribution installieren\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\n\n\n4 LaTeX-Formeln in HTML-Outputs können ohne eine \\(\\LaTeX\\)-Installation erzeugt werden. Hierbei werden Formeln im Web-Browser mit MathJax dargestellt.Sind diese Komponenten der vorhanden, ist Rmarkdown in der Lage den Kompilierungsprozess durchzuführen. Der grobe Ablauf ist in Abbildung 3.1 dargestellt und erfolgt so:\n\nDie von R-Code abhängigen Komponenten des .Rmd-Dokuments werden ausgeführt. Sämtliche Strukturelemente und ihre Ergebnisse werden durch das Tool knitr in ein Markdown-Dokument (Dateiendung .md) übersetzt.5\nDas in Schritt 1 erzeugte Markdown-Dokument wird dem Tool pandoc übergeben. Pandoc ist ein Open-Source-Tool zur Dokumentenkonvertierung, das Texte in zahlreiche verschiedene Formate transformiert, darunter HTML, PDF, Word, und viele weitere. Die Formatierung und Bereitstellung des Outputs erfolgt gemäß der im yaml-Header festgelegten Format-Funktion (output: ...).\n\n5 Dieser Prozess wird auch als knitting (engl. stricken) bezeichnet: sämtliche Komponenten werden nach Ausführung des R-Codes zusammengestrickt und in ein .md-Format überführt.\n\n\n\n\nAbbildung 3.1: Workflow beim Kompilieren von Rmd-Dokumenten (Quelle: Xie, Dervieux, und Riederer 2020)\n\n\nUm eine neue .rmd-Datei in RStudio zu erstellen und zu kompilieren, nutzen wir die Menü-Schaltflächen Datei \\(\\rightarrow\\) Neue Datei und wählen R markdown im Kontext-Menü aus. In dem sich anschließend öffnenden Dialog sind bereits Standard-Einstellungen für ein einfaches HTML-Dokument gesetzt.6 Nach Bestätigung der Konfiguration mit “OK” wird ein .Rmd-Dokument mit Beispiel-Code geöffnet. Abbildung 3.2 zeigt diesen Ablauf.\n6 Die hier erfolgte Konfiguration setzt lediglich rudimentäre Optionen im yaml-Header, die später angepasst werden können.\n\n\n\n\nAbbildung 3.2: RStudio: .Rmd-Datei erstellen\n\n\nDas neue .Rmd-Dokument kann nun bearbeitet werden. Beachte, dass Code-Chunks (grau hinterlegt) direkt im Dokument ausgeführt werden können und das Ergebnis im Editor angezeigt wird. Dies erfolgt durch Klick auf den “Play”-Button im jeweiligen Chunk.7 Abbildung 3.3 zeigt diesen Vorgang für zwei Chunks in standardmäßig bereitgestellten Minimalbeispiel. Die so ggf. erzeugten R-Objekte werden in der Arbeitsumgebung von RStudio zugeordnet.\n7 Eine Keyboard-Shurtcut zum Ausführungen von Chunks ist CMD + Enter in MacOS und STRG + Enter bei Windows.\n\n\n\n\nAbbildung 3.3: RStudio: Code-Chunks in .Rmd-Datei ausführen\n\n\n.Rmd-Dateien sind also eine Alternative zur Programmierung von R-Code in einem R-Skript (einer .R-Datei), die dem Nutzer mehr Komfort und die Möglichkeit zu Literate Programming bietet. Literate Programming meint die Praxis, Quellcode und erklärenden Text so zu kombinieren, dass der Code von Menschen besser verstanden werden kann. Der Fokus liegt dabei darauf, den Programmablauf verständlich zu dokumentieren, indem Code und dessen Beschreibung in einem einzigen Dokument integriert werden. So werden nicht nur die technischen Aspekte der Analyse dargestellt, sondern auch der gedankliche Prozess hinter der Analyse klar und nachvollziehbar.\nNach erstmaligem abspeichern der .Rmd-Datei (hier unter dem namen diamanten.rmd) kann das Dokument kompiliert werden. Hierzu klicken wir in der Aktionsleiste unterhalb des Dokumenten-Tabs auf den Button “Knit”, siehe Abbildung 3.4.8\n8 Eine Keyboard-Shurtcut zum Kompilieren der .Rmd-Datei ist CMD + Shift + k in MacOS und STRG + Shift + k bei Windows.\n\n\n\n\nAbbildung 3.4: RStudio: .Rmd-Datei kompilieren\n\n\nRMarkdown beginnt nun mit der Kompilierung des Dokuments, wobei der Ablauf in Abbildung 3.1 für das entsprechende Output-Format ausgeführt wird. Der Fortschritt der Kompilierung wird im Tab “Render” angezeigt und informiert zunächst über die Ausführung der Code-Chunks und zeigt anschließend den Log für die Umwandlung in das Output-Format mit pandoc, siehe Abbildung 3.4.\n\n\n\n\n\nAbbildung 3.5: RStudio: Fortschritt der Kompilierung im Tab ‘Render’\n\n\nWenn dieser Prozess fehlerfrei abschließt, wird im Verzeichnis von diamanten.rmd eine gleichnamige Datei mit der Endung .html erstellt. Die erzeugte HTML-Datei wird automatisch im Vorschaufenster von RStudio angezeigt, kann jedoch auch in jedem Webbrowser dargestellt und damit zur Reproduktion der Analyse an andere weitergegeben werden.\n\n3.1.3 Ein Beispiel\nDas folgene Minimalbeispiel zeigt den Aufbau einer .Rmd-Datei für eine kurze Analyse des Datensatzes ggplot2::diamonds unter Verwendung der oben erläuterten Definitionen und Strukturelemente:\n\nIm yaml-Header definieren wir den Output als HTML-Dokument mit dem Titel “Diamanten” sowie einer Datumsangabe.\nDie Datei enthält zwei Code-Chunks, die den Datensatz ggplot2::diamonds einlesen, unter Verwendung von dplyr-Funktionen transformieren und eine kleine graphische Auswertung mit ggplot2 erstellen.\nDie Schritte der Analyse werden im Fließtext zwischen den Code-Chunks erläutert. Hierbei kommt sowohl Markdown-Syntax zur Formatierung zum Einsatz als auch Inline-Code um Informationen mit R-Code zu berechnen und unmittelbar in die Erläuterungen einzubinden. Dank dieser Vorgehensweise passen sich die Erklärungen der Analyse automatisch an, wenn die verwendeten Variablen in den Code-Chunks geändert werden.\nDer letzte Textblock verwendet \\(\\LaTeX\\)-Code, um die zuvor mit R-Code durchgeführte Berechnung zu erläutern.\n\ndiamanten.Rmd:\n\n\n\n---\ntitle: \"Diamanten\"\ndate: 2024-01-05\noutput: html_document\n---\n\nWir lesen zunächst den Datensatz ein und filtern Diamanten mit einem Gewicht\nvon weniger als 2.5 Karat.\n\n```{r}\nlibrary(tidyverse)\ndata(\"diamonds\")\nkleiner &lt;- diamonds %&gt;%\n            filter(carat &lt; 2.5)\n```\n\nWir haben Beobachtungen für `r nrow(diamonds)` Diamanten. \n\nLediglich `r nrow(diamonds) - nrow(kleiner)` sind **schwerer** \nals 2.5 *Karat*. \n\nDer nachfolgende Code zeigt, wie die Häufigeitsverteilung von `carat` mit \nR geplottet werden kann.\n\n```{r, echo = FALSE}\nkleiner %&gt;% \n  ggplot(aes(carat)) + \n  geom_freqpoly(binwidth = 0.01)\n```\n\nWie schwer ist der *schwerste* \"kleine\" Diamant?\n\n```{r}\nkleiner$carat %&gt;% max()\n```\n\nDie Formel für diese Berechnung ist $$\\max(\\textup{carat}).$$\n\n\nEine .rmd-Datei mit dem obigen Code kann hier herunterlagen werden. Alternativ kann in RStudio ein neues Dokument erstellt werden (siehe Kapitel 3.1.2) und der Inhalt kopiert und eingefügt werden.\n\n\n\n\n\nAbbildung 3.6: RStudio: diamonds.Rmd kompilieren\n\n\n\n\n\n\nNach der Kompilierung des Dokuments mit knitr (siehe Abbildung 3.6) erhalten wir das in Abbildung 3.7 gezeigte HTML-Dokument. Das Resultat ist eine Webseite, die hier eingesehen werden kann.\n\n\n\n\n\nAbbildung 3.7: diamonds.html\n\n\nSofern eine \\(\\LaTeX\\)-Distribution installiert ist, kann der Output ohne weiteren Aufwand als PDF-Datei generiert werden. Hierzu setzen wir über das Kontext-Menü neben dem “Knit”-Button im Dokumenten-Tab die Option Knit to PDF, siehe Abbildung 3.8. RMarkdown beginnt dann unmittelbar mit der Kompilierung des Dokuments und erzeugt anschließend mit pandoc und dem \\(\\LaTeX\\)-Compiler die PDF-Datei diamonds.pdf im entsprechenden Arbeitsverzeichnis.\n\n\n\n\n\nAbbildung 3.8: diamanten.pdf\n\n\nAbbildung 3.9 zeigt das finale PDF-Dokument diamanten.pdf.\n\n\n\n\n\nAbbildung 3.9: diamanten.pdf\n\n\n\n3.1.4 Tabellen\nIn RMarkdown können einfache Tabellen durch Markup-Syntax erstellt werden. Hierbei werden die Zeichen “-”, “|” und “:” kombiniert um die Dimensionen und die Formatierung der Tabelle zu definieren: Jede Spalte wird durch senkrechte Striche getrennt. Die Kopfzeile der Tabelle wird mit Bindestrichen unterhalb definiert. Um die Ausrichtung der Inhalte in den Spalten festzulegen, verwendet man unterhalb der Kopfzeile Doppelpunkte in den Spaltendefinitionen.\nDer nachfolgende Markdown-Code-Chunk definiert eine Tabelle mit vier Zeilen und vier Spalten, deren Inhalte unterschiedliche Ausrichtungen haben. Tabelle 3.1 ist das Ergebnis.\n\n| rechts | links | standard | zentriert |\n|-------:|:------|----------|:---------:|\n|   12   |   12  |   12     |   12      | \n|  123   |  123  |  123     |  123      | \n|    1   |    1  |    1     |    1      |\n\n\n\n\n\n\nrechts\nlinks\nstandard\nzentriert\n\n\n\n12\n12\n12\n12\n\n\n123\n123\n123\n123\n\n\n1\n1\n1\n1\n\n\n\n\n\nTabelle 3.1: Eine einfache mit markdown erzeugte Tabelle\n\n\n\nOft ist es wünschenswert, die Ergebnisse statistischer Analysen in einer Tabelle darzustellen. Das R-Paket knitr bietet mit knitr::kable() eine rudimentäre Funktion, um den Inhalt eines “rechteckigen” R-Objekts9 für diverse Output-Formate zu formatieren.\n9 Hierzu zählen typische Formate wie Vektoren, Matrizen, data.frames und tibbles.10 Bei Kompilierung zu PDF (Kapitel 3.1.2) erzeugt kable() automatisch eine entsprechende \\(\\LaTeX\\)-Tabelle.Im nächsten Code-Chunk definieren wir eine Matrix m mit den numerischen Werten aus Tabelle 3.1 und nutzen kable() für eine Transformation in ein HTML-kompatibles tabellarisches Format.10 Tabelle 3.2 zeigt das Ergebnis.\n```{r}\nlibrary(tidyverse)\n\n# matrix erzeugen\nm  &lt;- matrix( \n  c(12, 123, 1), \n  nrow = 3, \n  ncol = 4, \n  byrow = TRUE\n)\n\n# Spaltennamen festlegen \ncolnames(m) &lt;- c(\"rechts\", \"links\", \n                 \"standard\", \"zentriert\")\n\n# Tabelle Erzeugen\nknitr::kable(\n  m, \n  align = c(\"r\", \"l\", \"l\", \"c\"), \n  format = \"html\"\n)\n```\n\n\n\n\n\n\nrechts\nlinks\nstandard\nzentriert\n\n\n\n12\n123\n1\n12\n\n\n123\n1\n12\n123\n\n\n1\n12\n123\n1\n\n\n\n\n\n\nTabelle 3.2: Mit knitr::kable() erzeugte tabellarische Darstellung der Matrix m\n\n\n\nStatistische Analysen profitieren erheblich von einer klaren und strukturierten Aufbereitung der Ergebnisse in Tabellen. Es gibt zahlreiche R-Pakete, die aus Modell-Objekten, wie sie beispielsweise von der Funktion lm ausgegeben werden, informative tabellarische Darstellungen der Regressionsergebnissen zu erzeugen.\nEin modernes und vielseitiges Paket für diesen Zweck ist modelsummary. Mit modelsummary können wir die Ergebnisse direkt in gut formatierte Tabellen exportieren, die in verschiedenen Formaten wie HTML, \\(\\LaTeX\\) oder Word ausgegeben werden können. Darüber hinaus bietet das Paket viele Optionen zur Anpassung des Outputs, etwa das Hinzufügen von Konfidenzintervallen, statistischen Tests oder zusätzlichen Statistiken.\nZur illustration von modelsummary() schätzen wir nachfolgend das einfache Regressionsmodell\n\\[\\begin{align*}\n  \\textup{price} = \\beta_0 + \\beta_1 \\textup{carat} + u\n\\end{align*}\\]\nanhand des Datenusatzes ggplot2::diamonds, um den Einfluss des Gewichts von Diamanten auf ihren Verkaufspreis zu untersuchen.\n\n# `diamonds` zuweisen\ndf &lt;- ggplot2::diamonds\n\n# Regression durchfühen\n(\n  s &lt;- summary(\n    lm(formula = price ~ carat, data = df)\n  )\n)\n\n\nCall:\nlm(formula = price ~ carat, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18585.3   -804.8    -18.9    537.4  12731.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2256.36      13.06  -172.8   &lt;2e-16 ***\ncarat        7756.43      14.07   551.4   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1549 on 53938 degrees of freedom\nMultiple R-squared:  0.8493,    Adjusted R-squared:  0.8493 \nF-statistic: 3.041e+05 on 1 and 53938 DF,  p-value: &lt; 2.2e-16\n\n\nFür eine professionelle tabellarische Darstellung der Regressionsergebnisse übergeben wir das Objekt s in einer bennanten Liste an modelsummary(). Mit star = T erhalten wir Signifikanzzeichen an den geschätzten Koeffizienten. output = \"gt\" legt fest, dass das Paket gt für die Formatierung der Tabelle verwendet werden soll.11 Tabelle 3.3 zeigt die tabellierten Regressionsergebnisse.\n11 Mit dieser Konfiguration werden die meisten Tabellen in diesem Buch erzeugt.\nlibrary(modelsummary)\nlibrary(gt)\n\n# Regressionsoutput mit modelsummary darstellen\nmodelsummary(\n  models = list(\"Abh. Variable: price\" = s), \n  stars = T, \n  output = \"gt\"\n)\n\n\n\n\n\n\n\n\nAbh. Variable: price\n\n\n\n(Intercept)\n-2256.361***\n\n\n\n(13.055)\n\n\ncarat\n7756.426***\n\n\n\n(14.067)\n\n\nNum.Obs.\n53940\n\n\nR2\n0.849\n\n\nR2 Adj.\n0.849\n\n\nRMSE\n1548.53\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\nTabelle 3.3: Mit modelsummary und gt erstellte tabellarische Zusammenfassung einer Regression\n\n\n\n\n3.1.5 Mathematische Formeln\nIn RMarkdown können mathematische Formeln sowohl inline (innerhalb einer Textzeile) als auch als Blockgleichungen gesetzt werden. Inline-Gleichungen werden direkt im Fließtext platziert. Wir verwenden einzelne Dollarzeichen $…$, um den mathematischen Ausdruck zu markieren.\n\n\n\n\n\n\nBeispiel: Inline-Gleichung\nDie Gleichung $y = \\beta_0 + \\beta_1x$ beschreibt eine lineare Funktion.\nIn mit RMardkown kompilierten Dokumenten wird dieser Satz dargestellt als:\nDie Gleichung \\(y = \\beta_0 + \\beta_1x\\) beschreibt eine lineare Funktion.\n\n\n\nBlockgleichungen stehen separat vom Text, zentriert und hervorgehoben. Sie werden mit doppelten Dollarzeichen eingefasst:\n$$ ... $$\n\n\n\n\n\n\nBeispiel: Block-Gleichung\nDer Ausdruck\n$$y = \\beta_0 + \\beta_1 x$$\nerscheint im kompilierten Dokument als\n\\[y = \\beta_0 + \\beta_1x.\\]\n\n\n\nMit Gleichungs-Umgebungen wie \\begin{align}…\\end{align} können mehrzeilige Gleichungen dargestellt werden. Hierbei wird dient & als Anker-Punkt für die Ausrichtung der Gleichungen. Mit \\\\ wird eine neue Zeile begonnen.\n\n\n\n\n\n\nBeispiel: Normale Dichtefunktion\nDer Ausdruck\n\\begin{align*}\n  f(x) &= \\frac{1}{\\sigma \\sqrt{2\\pi}} \n  \\exp\\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right) \\\\\n       &= \\frac{1}{\\sigma \\sqrt{2\\pi}} \n  \\exp\\left( -\\frac{x^2 - 2x\\mu + \\mu^2}{2\\sigma^2} \\right)\n\\end{align*}\nerscheint im kompilierten Dokument als\n\\[\\begin{align*}\n  f(x) &= \\frac{1}{\\sigma \\sqrt{2\\pi}}\n  \\exp\\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right) \\\\\n       &= \\frac{1}{\\sigma \\sqrt{2\\pi}}\n  \\exp\\left( -\\frac{x^2 - 2x\\mu + \\mu^2}{2\\sigma^2} \\right).\n\\end{align*}\\]",
    "crumbs": [
      "Grundlagen",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproduzierbarkeit</span>"
    ]
  },
  {
    "objectID": "RR.html#shiny",
    "href": "RR.html#shiny",
    "title": "\n3  Reproduzierbarkeit\n",
    "section": "\n3.2 Shiny",
    "text": "3.2 Shiny\nShiny ist ein R-Paket, das die Erstellung interaktiver Webanwendungen (Apps) und Dashboards direkt aus R heraus ermöglicht. Mit Shiny können Nutzer dynamische, datengetriebene Visualisierungen und interaktive Elemente wie Slider, Dropdown-Menüs und Schaltflächen in ihren Anwendungen einbinden, ohne hierzu üblicherweise erforderliche Web-Standards wie HTML, CSS und JavaScript sowie die server-seitige Ausführung von R-Programmcode zu beherrschen.12 Die Anwendungen laufen entweder lokal auf dem eigenen Rechner oder können über einen Webserver bereitgestellt werden (Hosting), sodass sie über das Internet zugänglich sind.13\n12 Kenntnisse in der Programmierung von Websiten sind jedoch hilfreich für das Fine-Tuning von Shiny-Applikationen.13 Für Online-Hosting fallen in der Regel Gebühren an. Kleine Applikationen mit beschränkter Laufzeit können über RStudio auf shinyapps.io kostenfrei gehostet werden.Shiny-Apps eignen sich besonders gut für die Bereitstellung von Data-Science-Tools: Nutzer einer Shiny-Anwendung können die Parameter der Analyse ändern und sehen unmittelbar, wie sich diese Änderungen auf die Ergebnisse auswirken, was die Nachvollziehbarkeit erhöht. Da die zugrunde liegenden R-Codes in die Anwendung eingebettet sind, bleibt der gesamte Analyseprozess offen zugänglich, und andere Forschende können die Anwendung nutzen, um die Ergebnisse auf ihre eigenen Daten anzuwenden oder die Schritte zu reproduzieren. Diese “interaktive Transparenz” fördert also die Reproduzierbarkeit datengetriebener Analysen, indem sie nicht nur den Code, sondern den gesamten Analyseablauf zugänglicher macht.\nShiny ermöglicht zudem eine effektive Kommunikation von Forschungsergebnissen, indem komplexe Analysen in interaktive, leicht verständliche Webanwendungen überführt werden. Dies erleichtert die Vermittlung von Ergebnissen erheblich, insbesondere für Nicht-Experten, und trägt dazu bei, dass Forschungserkenntnisse nicht nur reproduzierbar, sondern auch zugänglicher und verständlicher gemacht werden.\n\n3.2.1 Grundlagen\nEin Shiny-Projekt kann ähnlich wie ein RMarkdown-Projekt direkt in RStudio entwickelt werden. Dazu wählen wir im Menü für eine neue Datei: File \\(\\rightarrow\\) New File \\(\\rightarrow\\) Shiny Web App…. Im folgenden Dialogfeld müssen lediglich der Name der Applikation sowie optional das Arbeitsverzeichnis festgelegt werden. Standardmäßig kann der gesamte Code der Applikation in einer einzigen Datei namens app.R organisiert werden. Diese Datei wird nach einem Klick auf “Create” erstellt, siehe Abbildung 3.10.\n\n\n\n\n\nAbbildung 3.10: RStudio: Shiny-App erstellen\n\n\nDer nächste Code-Chunk zeigt den grundsätzlichen Aufbau einer Shiny-App. Dieser besteht aus drei Komponenten:\n\nBenutzer-Interface: In ui &lt;- fluidPage() definieren wir das Layout und die Darstellung der App. Der Funktion fluidPage() können Funktions-Objekte übergeben werden, die verschiedene Benutzeroberflächenelemente wie Schaltflächen, Schieberegler, Eingabefelder und auch Ausgabeelemente (z.B. eine Grafik) definiert und ihre Anordnung strukturiert.\nServer-Logik: In server &lt;- function(input, output, session) {...} wird die Funktionsweise der App mit R-Code programmiert. Hier legen wir fest, wie Reaktionenen auf Benutzereingaben verarbeitet, Berechnungen durchgeführt und die Ergebnisse als Ausgaben zurück an die Benutzeroberfläche übergeben werden.\nApp-Objekt: shinyApp(ui = ui, server = server) startet den Server und verknüpft das Benutzer-Interface ui mit der Server-Logik server. Die App wird anschließend über eine (lokale) IP-Adresse zur Ausführung bereitstellt und kann in einem Web-Browser aufgerufen werden.14\n\n14 Diese Komponente der App muss in der Regel nicht weiter modifiziert werden.Der nächste Code-Chunk zeigt diesen grundsätzlichen Aufbau.\n\nlibrary(shiny)\n\n# Benutzer-Interface definieren\nui &lt;- fluidPage(\n  # Input-Elemente & Output-Elemente\n)\n\n# Logik für Shiny-Server festlegen\nserver &lt;- function(input, output, session) {\n  # Server-Code\n}\n\n# Shiny-App erzeugen und starten\nshinyApp(ui = ui, server = server)\n\n\n3.2.2 Die Standard-App\nDie Standard-Shiny-App in der Datei app.R erzeugt eine interaktive Grafik, die die Verteilung der Wartezeiten bis zum nächsten Ausbruch des Geysirs Old Faithful im Yellowstone-Nationalpark in den USA visualisiert.15 In der Shiny-App kann der Benutzer über einen Slider die Anzahl der Klassen des Histogramms variieren. Die App ist so programmiert, dass sich die Grafik automatisch für die gewählte Anzahl an Intervallen aktualisiert.\n15 Der Datensatz ist in der Basis-Installation von R unter datasets::faithful verfügbar.\n# This is a Shiny web application. You can run the application by clicking\n# the 'Run App' button above.\n#\n# Find out more about building applications with Shiny here:\n#\n#    https://shiny.posit.co/\n#\n\nlibrary(shiny)\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n\n    # Application title\n    titlePanel(\"Old Faithful Geyser Data\"),\n\n    # Sidebar with a slider input for number of bins \n    sidebarLayout(\n        sidebarPanel(\n            sliderInput(\"bins\",\n                        \"Number of bins:\",\n                        min = 1,\n                        max = 50,\n                        value = 30)\n        ),\n\n        # Show a plot of the generated distribution\n        mainPanel(\n           plotOutput(\"distPlot\")\n        )\n    )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n\n    output$distPlot &lt;- renderPlot({\n        # generate bins based on input$bins from ui.R\n        x    &lt;- faithful[, 2]\n        bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n\n        # draw the histogram with the specified number of bins\n        hist(x, breaks = bins, col = 'darkgray', border = 'white',\n             xlab = 'Waiting time to next eruption (in mins)',\n             main = 'Histogram of waiting times')\n    })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\nUm die App auszuführen, genügt ein Klick auf “Run App” oben im Dokumenten-Tab, siehe Abbildung 3.11.\n\n\n\n\n\nAbbildung 3.11: RStudio: Shiny-App erstellen\n\n\napp.R wird nun ausgeführt, indem ein lokaler Shiny-Server gestartet wird. Die App wird anschließend entweder im integrierten Vorschaufenster von RStudio oder in einem externen Browser automatisch geöffnet. Zuerst wird die in ui definierte Benutzeroberfläche geladen und angezeigt, sodass interaktive Elemente wie Schieberegler oder Schaltflächen verfügbar werden. Gleichzeitig wird die in der server-Funktion definierte Logik aktiv, in der festgelegt ist, wie Benutzereingaben über das interaktive Elemente verarbeitet und die Ausgaben in Echtzeit aktualisiert werden.16 Das nachfolgende Widget zeigt das Resultat für die Standard-App.\n16 Je nach Komplexität der Berechnung kann es hierbei zu Verzögerungen bei der erstmaligen Anzeige oder Aktualisierungen kommen.#| standalone: true\n#| viewerHeight: 650\n#\n# This is a Shiny web application. You can run the application by clicking\n# the 'Run App' button above.\n#\n# Find out more about building applications with Shiny here:\n#\n#    https://shiny.posit.co/\n#\n\nlibrary(shiny)\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n\n    # Application title\n    titlePanel(\"Old Faithful Geyser Data\"),\n\n    # Sidebar with a slider input for number of bins\n    sidebarLayout(\n        sidebarPanel(\n            sliderInput(\"bins\",\n                        \"Number of bins:\",\n                        min = 1,\n                        max = 50,\n                        value = 30)\n        ),\n\n        # Show a plot of the generated distribution\n        mainPanel(\n           plotOutput(\"distPlot\")\n        )\n    )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n\n    output$distPlot &lt;- renderPlot({\n        # generate bins based on input$bins from ui.R\n        x    &lt;- faithful[, 2]\n        bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n\n        # draw the histogram with the specified number of bins\n        hist(x, breaks = bins, col = 'darkgray', border = 'white',\n             xlab = 'Waiting time to next eruption (in mins)',\n             main = 'Histogram of waiting times')\n    })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n3.2.3 Beispiel: Normalverteilung\nIn diesem Beispiel zeigen wir, wie eine statische R-Grafik in mit Shiny in eine interaktive Grafik umgewandelt werden kann. Als Fallbeispiel betrachten wir die Veranschaulichung des Effekts des Verteilungsparameters Standardabweichung auf die Form der Dichtefunktion einer standardnormalverteilten Zufallsvariable. Ähnlich wie in der Standard-Shiny-App liegt der Fokus hierbei darauf das Ergebnis einer statistischen Berechnung (Dichtefunktion) und der Variation von Parametern der Berechnnug (Variation der Standardabweichung) durch grafische interaktive Darstellung in leicht-verständlicher Art zu kommunizieren.\nEin guter Ansatz für die Konzeption einer Shiny-App ist es, zunächst eine repräsentative Statische Version des Codes zu definieren. Wir nutzen ggplot2 um die für eine Sequenz x von 1000 Werten im Bereich -4 bis 4 berechneten Werte der Dichtefunktion einer Standardnormalverteilung als Glockenkurve zu visualisieren.\n\nlibrary(ggplot2)\n\nx &lt;- seq(\n  from = - 4, \n  to = 4,\n  length.out = 1000\n  )\n\ny &lt;- dnorm(x, sd = 1)\ndf &lt;- tibble::tibble(x = x, y = y)\n    \nggplot(\n  data = df, \n  mapping =  aes(x = x, y = y)\n) +\ngeom_line(color = \"steelblue\", lwd = 1) +\nlabs(x = \"X\", y = \"Dichte\") +\nlims(x = c(-10, 10)) +\ntheme_minimal()\n\n\n\n\n\n\n\nZunächst definieren wir das Benutzer-Interface für die Shiny-App. Wir verwenden shiny::fluidPage(), um das Layout der App zu gestalten und fügen wir mit shiny::sliderInput() einen Slider hinzu, der erlaubt die Standardabweichung der Normalverteilung zwischen 0.1 und 3 anzupassen. Der Slider wird mit einer Schrittweite von 0.1 konfiguriert. Zusätzlich legen wir mit shiny::plotOutput() fest, dass die resultierende Grafik im Output-Panel der Benutzeroberfläche angezeigt wird.\nAnschließend richten wir die Server-Logik ein. Mit shiny::renderPlot() erreichen wir, dass die Grafik jedes Mal neu gezeichnet wird, wenn der Benutzer den Wert der Standardabweichung über den Slider variiert. Der aktuelle Wert des Schiebereglers wird durch die Variable input$sd an zwei Stellen des Codes für die statische Grafik integriert:\n\nIn der Definition der Sequenz x. Wir modifizieren die Argumente from und to so, dass Start- und Endwerte in x entsprechend der gewählten Standardabweichung den relevanten Bereich der X-Achse abdecken.\nBei der Berechnung der Dichte mit dnorm(). So stellen wir sicher, dass die Grafik die neu berechnete Normalverteilung mit der jeweils gewählten Standardabweichung in anzeigt.\n\nBeachte, dass die X-Achse auf einen festen Bereich von -10 bis 10 begrenzt ist, damit die Änderungen in der “Breite” der Dichtefunktion erkennbar sind. Andernfalls skaliert ggplot2 die Achse automatisch, um für den gewählten Wert der Standardabweichung die bestmögliche Darstellung zu gewährleisten.17\n17 “Bestmöglich” meint hier eine “hinreichend große” Skalierung der Achse. Dies ist hinderlich, wenn wir den Effekt der Skalierung der Verteilung beobachten wollen.Sobald wir die App starten und der Schieberegler verwenden, wird die Grafik dynamisch aktualisiert: Kleine Werte für die Standardabweichung führen zu einer stärkeren Zentrierung der typischen Glockenform der Verteilung um den Erwartungswert 0.\n\nlibrary(shiny)\nlibrary(ggplot2)\n\n# Benutzer-Interface definieren\nui &lt;- fluidPage(\n  titlePanel(\"Normalverteilung\"),\n      sliderInput(\n        inputId = \"sd\",\n        label = \"Standardabweichung:\",\n        min = 0.1,\n        max = 3,\n        value = 1,\n        step = 0.1,\n        width = \"100%\"\n    ),\n    mainPanel(width = 12,\n      plotOutput(outputId = \"distPlot\", height = 400)\n    )\n)\n\n# Shiny-Server konfigurieren\nserver &lt;- function(input, output, session) {\n  output$distPlot &lt;- renderPlot({\n    x &lt;- seq(\n      from = - 3.33 * input$sd,\n      to = 3.33 * input$sd,\n      length.out = 1000\n    )\n    y &lt;- dnorm(x, sd = input$sd)\n    data &lt;- data.frame(x = x, y = y)\n    \n    ggplot(data, aes(x = x, y = y)) +\n      geom_line(color = \"steelblue\", lwd = 1) +\n      labs(x = \"X\", y = \"Dichte\") +\n      lims(x = c(-10, 10)) +\n      theme_minimal()\n  })\n}\n\n# Shiny-App erzeugen, Server starten\nshinyApp(ui = ui, server = server)\n\n#| standalone: true\n#| viewerHeight: 600\nlibrary(shiny)\nlibrary(ggplot2)\n\n# Define your Shiny UI here\nui &lt;- fluidPage(\n  titlePanel(\"Normalverteilung\"),\n      sliderInput(\n        inputId = \"sd\",\n        label = \"Standardabweichung:\",\n        min = 0.1,\n        max = 3,\n        value = 1,\n        step = 0.1,\n        width = \"100%\"\n    ),\n    mainPanel(width = 12,\n      plotOutput(outputId = \"distPlot\", height = 400)\n    )\n)\n\n# Define your Shiny server logic here\nserver &lt;- function(input, output, session) {\n  output$distPlot &lt;- renderPlot({\n    # Generate a sequence of x values\n    x &lt;- seq(\n      from = - 4 * input$sd,\n      to = 4 * input$sd,\n      length.out = 1000\n    )\n    # Compute the normal distribution density\n    y &lt;- dnorm(x, sd = input$sd)\n    # Create a data frame\n    data &lt;- data.frame(x = x, y = y)\n\n    # Plot using ggplot2\n    ggplot(data, aes(x = x, y = y)) +\n      geom_line(color = \"steelblue\", lwd = 1) +\n      labs(x = \"X\", y = \"Dichte\") +\n      lims(x = c(-10, 10)) +\n      theme_minimal()\n  })\n}\n\n# Create and launch the Shiny app\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "Grundlagen",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproduzierbarkeit</span>"
    ]
  },
  {
    "objectID": "RR.html#zusammenfassung",
    "href": "RR.html#zusammenfassung",
    "title": "\n3  Reproduzierbarkeit\n",
    "section": "\n3.3 Zusammenfassung",
    "text": "3.3 Zusammenfassung\nIn diesem Kapitel haben wir die Bedeutung der Reproduzierbarkeit datengetriebener Analysen diskutiert und gezeigt, wie diverse Pakete im R-Ökosystem verwendet werden können, um reproduzierbare Analysen zu erstellen. Zu den wichtigsten Werkzeugen gehört RMarkdown, das die Erstellung von Berichten ermöglicht, in denen mathematische Erläuterungen, Programm-Code, code-basierte Outputs und Text nahtlos integriert sind. RMarkdown ermöglicht es, sämtliche Schritte einer Analyse transparent zu dokumentieren und die Ergebnisse in verschiedene Formaten zu exportieren und zu teilen.\nShiny ist ein weiteres leistungsstarkes Werkzeug zur Förderung von Reproduzierbarkeit und insbesondere für die Kommunikation von Forschungsergebnissen. Das Paket ermöglicht Forschenden, Studienergebnisse in interaktive Webanwendungen darzustellen und statistische Methoden in Dashboards nutzbar zu machen. Auch für Nicht-Experten können Shiny-Anwendungen dynamische Visualisierungen und benutzerfreundliche Oberflächen bieten, die komplexe Analysen zugänglicher machen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAllaire, JJ, Yihui Xie, Christophe Dervieux, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, u. a. 2024. rmarkdown: Dynamic Documents for R. https://github.com/rstudio/rmarkdown.\n\n\nChang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke, Yihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, und Barbara Borges. 2024. shiny: Web Application Framework for R. https://CRAN.R-project.org/package=shiny.\n\n\nWickham, H., und an O’Reilly Media Company Safari. 2021. Mastering Shiny. O’Reilly Media, Incorporated. https://books.google.de/books?id=ha1CzgEACAAJ.\n\n\nXie, Yihui, J. J. Allaire, und Garrett Grolemund. 2023. R Markdown: The Definitive Guide: The Definitive Guide. Chapman; Hall/CRC. https://doi.org/10.1201/9781138359444.\n\n\nXie, Yihui, Christophe Dervieux, und Emily Riederer. 2020. R Markdown Cookbook. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown-cookbook.",
    "crumbs": [
      "Grundlagen",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproduzierbarkeit</span>"
    ]
  },
  {
    "objectID": "Reg.html",
    "href": "Reg.html",
    "title": "4  Regression",
    "section": "",
    "text": "4.1 Regression schließt Backdoors: Frish-Waugh-Lovell-Theorem\nDas Frisch-Waugh-Lovell-Theorem (FWL) besagt, dass in einer multiplen linearen Regression die geschätzten Koeffizienten für eine Teilmenge der Regressoren numerisch identisch zu Koeffizientenschätzungen aus folgenden Schritten sind\nIn einem multiplen Modell mit zwei Regressoren \\(X_1,\\ X_2\\), \\[\\begin{align}\n  Y = \\beta_0 + \\beta_1 X + \\beta_2 X_2 + \\epsilon \\label{eq:fwlfullreg}\n\\end{align}\\] kann der Effekt von \\(X_1\\) auf \\(Y\\) also mit der Regression \\[\\begin{align*}\n  \\widehat{u}_{Y,X_2} = \\beta_1 \\widehat{u}_{X_1,X_2} + e \\label{eq:fwl2reg}\n\\end{align*}\\] geschätzt werden, wobei \\(\\widehat{u}_{Y,X_2}\\) und \\(\\widehat{u}_{X_1,X_2}\\) die Residuen der Regression von \\(Y\\) auf \\(X_2\\) und von \\(X_1\\) auf \\(X_2\\) sind.\nFWL ermöglicht daher eine Vereinfachung der Schätzung komplexer Modelle durch die Zerlegung der Schätzung in Teilschritte.\nFür das Verständnis der Schätzung kausaler Effekte mit linearer Regression ist FWL hilfreich, denn es zeigt, wie sowohl die Variation in der Outcome-Variable (\\(\\widehat{u}_{Y,X_2}\\)) als auch die Variation in der Behandlungsvariable (\\(\\widehat{u}_{X_1,X_2}\\)), die jeweils nicht durch Kovariablen (\\(X_2\\)) verursacht wird, mit multipler Regression isoliert werden kann, sodass Backdoors geschlossen werden.\nWir illustrieren dieses Konzept anhand einer multiplen Regression für die Schnabeltiefe (body_mass) von Pinguinen aus dem Datensatz palmerpenguins::penguins,\n\\[\\begin{align}\n  \\textup{body\\_mass} = \\beta_0 + \\beta_1\\cdot\\textup{bill\\_length} + \\beta_2\\cdot \\textup{flipper\\_length} + \\epsilon,\\label{eq:billdepthmodel}\n\\end{align}\\] unter der Annahme, dass \\(\\beta_1\\) der interessierende Effekt ist: Die erwartete Änderung des Gewichts eines Pinguins (in Gramm) für eine Änderung der Schnabel-Länge um 1mm.\nVor der Schätzung von Modell \\(\\eqref{eq:billdepthmodel}\\) lesen wir den Datensatz ein und erstellen eine bereinigte Variante penguins_cleaned, analog zur Vorgehensweise in Kapitel 2.1.2.\nWir schätzen nun Modell \\(\\eqref{eq:billdepthmodel}\\) mit lm() und erhalten eine Zusammenfassung der geschätzen Koeffizienten mit broom::tidy().\nDas Ergebnis der Schätzung ist \\(\\widehat{\\beta}_1\\approx3.80\\). Der nächste Code-Block berechnet die Residuen aus den Regressionen \\[\\begin{align*}\n  \\textup{body\\_mass} =&\\, \\alpha_0 + \\alpha_1 \\textup{flipper\\_length} + u_{\\textup{body\\_mass},\\,\\textup{flipper\\_length}},\\\\\n  \\textup{bill\\_length} =&\\, \\delta_0 + \\delta_1 \\textup{flipper\\_length} + u_{\\textup{bill\\_length},\\,\\textup{flipper\\_length}},\n\\end{align*}\\]\nund speichert diese in body_mass_res und bill_length_res.\nFür den zweiten Schritt regressieren wir body_mass_res auf bill_length_res.\nDer geschätzte Koeffizient aus der Regression der Residuen stimmt mit dem geschätzten Koeffizienten von bill_length aus der großen Regression \\(\\eqref{eq:billdepthmodel}\\) überein.\nWir können den Effekt der Kontrolle für flipper_length visualisieren. Wir plotten hierzu:\nWir erweitern p um die ursprünglichen Datenpunkte und die zugehörige Regressionslinie.\nDer grafische Vergleich beider Vorgehensweisen zeigt den Effekt der Kontrolle für flipper_length: Die geschätzte (schwarze) Regressionslinie für die bereinigten Daten hat eine deutlich geringere Steigung als die anhand der ursprünglichen Daten geschätzte (lilane) Linie. Der Effekt von bill_length auf body_mass wird mit der einfachen Regression lm(body_mass ~ bill_length) vermutlich überschätzt, weil es andere Faktoren (wie flipper_length gibt, die mit bill_length und body_mass korrelieren. Kontrollieren für flipper_length in der multiplen Regression lm(body_mass ~ bill_length + flipper_length) schließt die Backdoor durch flipper_length. Die Konsequenz ist eine deutlich geringere Steigung der lilanen Regressionslinie.",
    "crumbs": [
      "Grundlagen",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "Reg.html#regression-schließt-backdoors-frish-waugh-lovell-theorem",
    "href": "Reg.html#regression-schließt-backdoors-frish-waugh-lovell-theorem",
    "title": "4  Regression",
    "section": "",
    "text": "Rechne die Effekte der übrigen Variablen auf (a) die Outcome-Variable und (b) die Teilmenge der erklärenden Variablen mit Regression heraus und\nregressiere anschließend die Residuen von Schritt (a) auf die Residuen aus Schritt (b).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie originalen Datenpunkte für bill_length und body_mass1 gemeinsam mit der geschätzten Regressionslinie für das Modell \\[ \\textup{body\\_mass} = \\beta_0 + \\beta_1\\textup{bill\\_length} + u \\] (keine Kontrolle für flipper_length!)2.\nDie um flipper_length bereinigten Datenpunkte und die zugehörige geschätzte Regressionslinie.\n\n1 Für eine bessere Lesbarkeit der Grafik zentrieren wir beide Variablen um den jeweiligen Stichprobenmittelwert.2 Der R-Befehl für diese Regression ist lm(I(body_mass - mean(body_mass)) ~ I(bill_length - mean(bill_length)) - 1, data = penguins_cleaned).",
    "crumbs": [
      "Grundlagen",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "Reg.html#sec-bov",
    "href": "Reg.html#sec-bov",
    "title": "4  Regression",
    "section": "\n4.2 Binäre Outcome-Variable",
    "text": "4.2 Binäre Outcome-Variable\nEine binäre Variable, auch als dichotome Variable oder Indikator-Variable bezeichnet, ist eine Variable, die nur zwei Ausprägungen annehmen kann. Diese beiden Ausprägungen werden typischerweise durch die Werte 0 und 1 repräsentiert und dienen dazu, zwei verschiedene Zustände oder Kategorien zu unterscheiden. Formal kann eine binäre Variable \\(B\\) wie folgt definiert werden:\n\\[\\begin{align}\n  B = \\begin{cases}\n  1, & \\text{Eigenschaft trifft zu,} \\\\\n  0, & \\text{Eigenschaft trifft nicht zu.}\n  \\end{cases}\n\\end{align}\\]\nEine in späteren Kapiteln dieses Companions als verwendeter binärer Regressor ist der Indikator für die Zuordnung von Beobachtungen in Behandlungs- oder Kontrollgruppe (1 = Behandlungsgruppe, 0 = Kontrollgruppe).\nFür viele ökonomische Forschungsfragen ist es hilfreich, eine binäre Outcome-Variable mit Regression zu modellieren. Hierzu gibt es verschiedene Ansätze, die wir nachfolgend zusammenfassen und ihre Anwendung mit R zeigen.\n\n4.2.1 Das lineare Wahrscheinlichkeitsmodell\nDas lineare Regressionsmodell\n\\[Y = \\beta_0 + \\beta_1 X_{1} + \\beta_2 X_{2} + \\dots + \\beta_k X_{k} + u\\] mit einer binären abhängigen Variablen \\(Y_i\\in\\{0,1\\}\\) wird als lineares Wahrscheinlichkeitsmodell bezeichnet. Wie üblich modellieren wir den Erwartungswert der abhängigen Variable gegeben der Regressoren \\(X_1,\\dots,X_k\\) als lineare Funktion,\n\\[E(Y\\vert X_1,X_2,\\dots,X_k) = P(Y=1\\vert X_1, X_2,\\dots, X_3).\\] Da \\(Y\\) eine binäre Variable ist, gilt hier\n\\[ P(Y = 1 \\vert X_1, X_2, \\dots, X_k) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k.\\]\nDas lineare Wahrscheinlichkeitsmodell beschreibt also die Wahrscheinlichkeit, dass \\(Y=1\\) als lineare Funktion der Regressoren: \\(\\beta_j\\) misst die Änderung in der Wahrscheinlichkeit für das Ereignis \\(Y_i=1\\), unter der Bedingung, dass die anderen \\(k-1\\) Regressoren konstant gehalten werden. Genau wie bei multipler Regression mit einer kontinuierlichen abhängigen Variablen können die \\(\\beta_j\\) mit der KQ-Methode geschätzt werden.\nAufgrund der Beschränktheit der \\(Y_i\\) auf \\(\\{0,1\\}\\) sind die \\(u_i\\) heteroskedastisch. Folglich sollten Inferenzstatistiken mit robusten Standardfehlern berechnet werden. Weiterhin ist zu beachten, dass \\(R^2\\) in den meisten Anwendungen von linearen Wahrscheinlichkeitsmodellen keine hilfreiche Interpretation hat, da das geschätzte Modell die Daten nicht perfekt erklären kann, wenn die abhängige Variable binär, aber die Regressoren kontinuierlich verteilt sind.\nDas lineare Wahrscheinlichkeitsmodell hat einen wesentlichen Nachteil: Das Modell nimmt an, dass die bedingte Wahrscheinlichkeitsfunktion linear ist und \\(P(Y=1\\vert X_1,\\dots,X_k)\\) nicht auf das für Wahrscheinlichkeiten definierte Intervall \\([0,1]\\) beschränkt ist. Ein angepasstes Modell hat dann für Regressorwerte, die zu Vorhersagen von \\(Y\\) jenseits von \\([0,1]\\) führen keine sinnvolle Interpretation.\nDieser Umstand verlangt nach Regressionsansätzen, die \\(P(Y=1)\\) durch eine auf \\([0,1]\\) beschränkte (nicht-lineare) Funktion der Regressoren modellieren. Häufig verwendete Methoden sind Probit- und Logit-Regression.\nEin lineares Wahrscheinlichkeitsmodell kann mit lm() geschätzt werden, wobei die abhängige Variable den Typ numeric oder integer haben muss.\n\n4.2.2 Probit-Regression\nBei der Probit-Regression wird die Standardnormalverteilungsfunktion \\(\\Phi(\\cdot)\\) verwendet, um die Regressionsfunktion einer binären abhängigen Variable zu modellieren. Wir nehmen an, dass \\[\\begin{align}\n  E(Y\\vert X) = P(Y=1\\vert X) = \\Phi(\\beta_0 + \\beta_1 X), \\label{eq:probitmodel}\n\\end{align}\\] sodass der mit der Verknüfungsfunktion (Link) \\[\\textup{probit}(\\cdot) = \\Phi^{-1}(\\cdot)\\] transformierte Erwartungswert \\(P(Y=1\\vert X)\\) dem linearen Prädiktor \\(z=\\beta_0 + \\beta_1 X\\) entspricht, \\[\\begin{align*}\n  \\Phi^{-1}\\big[P(Y=1\\vert X)\\big] = \\beta_0 + \\beta_1 X.\n\\end{align*}\\]\n\\(z=\\beta_0 + \\beta_1 X\\) in \\(\\eqref{eq:probitmodel}\\) modelliert hier also ein Quantil der Standardnormalverteilung, \\[\\begin{align*}\n\\Phi(z) = P(Z \\leq z) \\ , \\ Z \\sim \\mathcal{N}(0,1).\n\\end{align*}\\] Der Koeffizient \\(\\beta_1\\) in \\(\\eqref{eq:probitmodel}\\) misst die Änderung in \\(z\\), die mit einer Änderung von \\(X\\) um eine Einheit verbunden ist. Obwohl der Effekt einer Änderung in \\(X\\) auf \\(z\\) linear ist, ist der Zusammenhang zwischen \\(z\\) und \\(\\textup{E}(Y\\vert X) = P(Y=1\\vert X)\\) nicht linear, denn \\(\\Phi(\\cdot)\\) ist eine nicht-lineare Funktion von \\(X\\).\n\n\n\n\n\n\n\n\nAufgrund der Nicht-Linearität der Link-Funktion hat der Koeffizient von \\(X\\) keine einfache Interpretation hinsichtlich des Effekts auf \\(P(Y=1\\vert X)\\). Die Änderung in der Wahrscheinlichkeit, dass \\(Y=1\\) ist, durch eine Änderung in \\(X\\) (partieller Effekt von \\(X\\)) kann berechnet werden als:\n\\[\\begin{align*}\n  \\frac{\\partial\\textup{E}(Y\\vert X)}{\\partial X} = \\frac{\\partial\\textup{P}(Y=1\\vert X)}{\\partial X} = \\frac{\\partial\\Phi(\\beta_0 + \\beta_1 X)}{\\partial X} = \\phi(\\beta_0 + \\beta_1 X) \\beta_1,\n\\end{align*}\\] wobei \\(\\phi(\\cdot)\\) die Dichtefunktion der Standardnormalverteilung ist. In empirischen Anwendungen wird der partielle Effekt häufig als Differenz in geschätzten Wahrscheinlichkeiten angegeben:\n\nBerechne die geschätzte Wahrscheinlichkeit, dass \\(Y=1\\) für einen Bezugswert \\(X\\).\nBerechne die geschätzte Wahrscheinlichkeit, dass \\(Y=1\\) für \\(X + \\Delta X\\).\nBerechne die Differenz zwischen der geschätzten Wahrscheinlichkeiten.\n\nWie im linearen Wahrscheinlichkeitsmodell kann das Modell \\(\\eqref{eq:probitmodel}\\) auf eine Probit-Regression mit \\(k+1\\) Regressoren \\(\\boldsymbol{X} := (1, X_1, \\dots, X_k)\\) verallgemeinert werden, um das Risiko einer Verzerrung durch ausgelassene Variablen zu mindern. Die Schritte 1 bis 3 für die Berechnung des partiellen Effekts einer Änderung in \\(X_j\\) erfolgen dann unter der Annahme, dass die übrigen \\(k-1\\) Regressoren konstant gehalten werden, wobei der partielle Effekt von den jeweiligen Regressorwerten abhängt. Im nächsten Abschnitt erläutern wir die Schätzung von Probit-Modellen mit \\(k+1\\) Regressoren.\n\n4.2.2.1 Schätzung\nDie Likelihood-Funktion für Probit-Regression ist\n\\[\\begin{align*}\n  L(\\boldsymbol{\\beta}) = \\prod_{i=1}^n \\Phi(\\mathbf{X}_i^\\top \\boldsymbol{\\beta})^{y_i} \\left[1 - \\Phi(\\mathbf{X}_i^\\top \\boldsymbol{\\beta})\\right]^{1-y_i}\n\\end{align*}\\]\nHierbei ist \\(\\Phi(\\mathbf{X}_i^\\top \\boldsymbol{\\beta})\\) die Wahrscheinlichkeit, dass \\(Y_i = 1\\) ist und \\(1 - \\Phi(\\mathbf{X}_i^\\top \\boldsymbol{\\beta})\\) ist die Wahrscheinlichkeit, dass \\(Y_i = 0\\) ist.\nDie Log-Likelihood-Funktion ergibt sich als\n\\[\\begin{align*}\n  \\mathcal{L}(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left[ y_i \\log \\Phi(\\mathbf{X}_i^\\top \\boldsymbol{\\beta}) + (1 - y_i) \\log \\left(1 - \\Phi(\\mathbf{X}_i^\\top \\boldsymbol{\\beta})\\right) \\right]\n\\end{align*}\\]\nUm den Maximum-Likelihood-Schätzer \\(\\widehat{\\boldsymbol{\\beta}}\\) zu finden, muss die Log-Likelihood-Funktion \\(\\mathcal{L}(\\boldsymbol{\\beta})\\) maximiert werden. In der Praxis erfolgt dies häufig durch numerische Optimierung, da die Log-Likelihood-Funktion der Probit-Regression im Allgemeinen keine geschlossene Form hat damit eine analytische Lösung nicht möglich ist.\nFür eine Anwendung in R und einen Vergleich mit dem linearen Wahrscheinlichkeitsmodell erzeugen wir einen Beispieldatensatz simdata für einen datenerzeugenden Prozess (DGP)3 mit einem normalverteilten Regressor \\(X\\sim N(5,2^2)\\) und \\[\\begin{align*}\n  P(Y=1\\vert X) = \\Phi(z), \\quad z = -4 + 0.7 X.\n\\end{align*}\\]\n3 Siehe Kapitel 5 für Erläuterungen von Simulationsmethoden in R.\n\n\n\n\n\n\n\nDas lineare Wahrscheinlichkeitsmodell schätzen wir wie gewohnt mit lm() und berechnen robuste Standardfehler mit lmtest::coeftest().\n\n\n\n\n\n\n\n\nBeachte, dass lediglich die Vorzeichen der geschätzten Koeffizienten mit denen der wahren Werten übereinstimmmen. Da das lineare Modell fehlspezifiziert ist, sind die KQ-Schätzer der Koeffizienten hier inkonsistent.\nEin Probit-Modell kann mit stats::glm() geschätzt werden. Hierbei ist formula die Formel für den linearen Prädiktor \\(z\\) und family eine Link-Funktion für den Zusammenhang von \\(z\\) und \\(P(Y\\vert X)\\). Mit family = binomial(link = \"probit\") wählen wir die Link-Funktion probit.\n\n\n\n\n\n\n\n\nDie geschätzten Koeffizienten in der Probit-Regression liegen, wie erwartet, nahe bei Parametern des DGPs.\nUm beide Schätzungen gemeinsam zu plotten, erzeugen wir mit predict() Vorhersagen für eine Menge von Werten X im Intervall \\([0,11]\\). Beachte, dass bei Vorhersagen für das Probit-Modell die gewünschte Transformation der vorherzusagenden Variable über type gewählt werden muss.4 Der Standardfall ist type = \"link\", d.h. wir erhalten Vorhersagen für den linearen Prädiktor: \\(\\widehat{z} = \\widehat\\beta_0 + \\widehat{\\beta}_1X\\). Mit type = \"response\" werden diese Werte mit der Link-Funktion, hier \\(\\Phi(\\cdot)\\) zu vorhergesagten Wahrscheinlichkeiten \\(\\widehat{P}(Y=1\\vert X = x)\\) transformiert.\n4 Dies gilt für jedes Modell mit einer anderen Link-Funktion als \\(f(x) = x\\) ist (lineare Regression).\n\n\n\n\n\n\n\nDer nachfolgende Code-Chunk erstellt einen Punkte-Plot mit Jitter-Effekt (position_jitter), wobei die Beobachtungen zufällig leicht vertikal verschoben werden, um Überlappungen zu vermeiden. Zusätzlich zeichnen wir die geschätzten Regressionslinien für mod_lp (LPM) und mod_probit (Probit) sowie die tatsächliche Wahrscheinlichkeitsfunktion \\(P(Y=1\\vert X)\\) ein.\n\n\n\n\n\n\n\n\nDie Grafik zeigt, dass beide Modelle den positiven Zusammenhang für \\(X\\) und \\(P(Y=1\\vert X)\\) erfassen. Das lineare Wahrscheinlichkeitsmodell approximiert die tatsächliche nicht-lineare Wahrscheinlichkeitsfunktion nur schlecht und liefert insbesondere in den Rändern der Verteilung von \\(X\\) (kleine und große Werte) unzulässige Vorhersagen außerhalb des Intervalls \\([0,1]\\). Die Probit-Spezifikation hingegen erfasst den nicht-linearen Verlauf der tatsächlichen Wahrscheinlichkeitsfunktion gut.\n\n4.2.3 Logistische Regression\nBei logistischer Regression wird die logistische Funktion \\(\\Lambda(\\cdot)\\) \\[\\begin{align}\n\\Lambda(z) = \\frac{1}{1 + \\exp(-z)},\n\\end{align}\\] als Link-Funktion genutzt, um die Wahrscheinlichkeitsfunktion von \\(Y\\) gegeben \\(X\\) zu modellieren. Ähnlich wie im Probit-Modell nehmen wir hier an, dass \\[\\begin{align}\nE(Y\\vert X) = P(Y=1\\vert X) = \\Lambda(\\beta_0 + \\beta_1 X). \\label{eq:logitmodel}\n\\end{align}\\] In diesem Modell ist die Link-Funktion \\(\\Lambda^{-1}(\\cdot)\\). Für \\(z=\\beta_0 + \\beta_1 X\\) ist \\(\\Lambda^{-1}(t)\\) der sogenannte logit: Dass logarithmierte Verhältnis von \\(p := P(Y=1\\vert X)\\) und \\(1 - p = P(Y=0\\vert X)\\), \\[\\begin{align*}\n  \\textup{logit(p)} = \\log\\bigg(\\frac{p}{1-p}\\bigg) = \\beta_0 + \\beta_1 X.\n\\end{align*}\\] Der Koeffizient \\(\\beta_1\\) misst also die Veränderung des Logits pro Einheit Änderung im Regressor \\(X\\).\nÄhnlich wie bei Probit-Regression ist der Einfluss von \\(X\\) auf den Logit linear, jedoch besteht auch hier eine nicht-lineare Beziehung zwischen dem linearen Prädiktor und der Wahrscheinlichkeit \\(P(Y=1\\vert X)\\), denn \\(\\Lambda(\\cdot)\\) ist eine nicht-lineare Funktion mit dem Wertebereich \\([0,1]\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie nachstehende interaktive Grafik zeigt, wie die Wahscheinlichkeitsfunktion der latenten Variable \\(z=\\beta_0 + \\beta_1 X\\) von den Parametern \\(\\beta_0\\) und \\(\\beta_1\\) jeweils für Logit- und Probit-Regression beeinflusst wird.\n\n\nAufgrund der Nicht-Linearität von \\(\\Lambda(z)\\) kann der Koeffizient \\(\\beta_1\\) wie im Probit-Modell nicht direkt als Effekt auf die Wahrscheinlichkeit \\(P(Y=1\\vert X)\\) interpretiert werden.\nUm den partiellen Effekt einer Änderung in \\(X\\) am Punkt \\(X\\) auf \\(P(Y=1\\vert X)\\) zu ermitteln, berechnen wir die Ableitung des bedingten Erwartungswerts:\n\\[\\begin{align*}\n\\frac{\\partial\\textup{E}(Y\\vert X)}{\\partial X} = \\frac{\\partial\\textup{P}(Y=1\\vert X)}{\\partial X} = \\frac{\\partial\\Lambda(\\beta_0 + \\beta_1 X)}{\\partial X} = \\lambda(\\beta_0 + \\beta_1 X) \\beta_1,\n\\end{align*}\\] wobei \\(\\lambda(\\cdot)\\), ähnlich wie die Dichtefunktion der Normalverteilung im Probit-Modell, die Dichtefunktion der logistischen Verteilung darstellt. Diese ist gegeben durch \\[\\begin{align*}\n\\lambda(z) = \\Lambda(z) \\cdot (1 - \\Lambda(z)).\n\\end{align*}\\]\nDie Interpretation des angepassten Modells ist aufgrund der Modellierung des Logits intuitiver als für Probit-Regression. Angenommen eine Bank möchte die Wahrscheinlichkeit modellieren, dass ein Kunde einen Kredit nicht zurückzahlt: \\(P(Y=\\textup{Zahlungsausfall}\\vert X)\\), wobei der Regressor \\(X\\) das Verhältnis von aktuellem Schuldenstand und Monatseinkommen ist. Die Schätzung einer logistischen Regression ergibt, dass ein Anstieg von \\(X =x\\) um 0.1 den Logit für die Wahrscheinlichkeit eines Kreditausfalls \\(p(x)\\) um 0.4 erhöht: \\[\\begin{align*}\n  \\textup{logit} \\big[P(Y=\\textup{Zahlungsausfall}\\vert X = x + 0.1)\\big] = \\textup{logit}[p(x)] + 0.4.\n\\end{align*}\\]\nDas Modell besagt dann, dass die Wahrscheinlichkeit der Zahlungsunfähigkeit etwa um den Faktor 1.5 ansteigt, denn \\[\\begin{align*}\n  \\exp\\big(\\textup{logit}[p(x)] + 0.4\\big) = \\frac{p(x)}{1-p(x)} \\cdot \\exp(0.4)\n\\end{align*}\\] mit \\[\\begin{align*}\n  \\exp(0.4) \\approx 1.50.\n\\end{align*}\\]\n\n4.2.4 Schätzung\nÄhnlich wie bei Probit-Regressionen können Logit-Modelle mit Maximum-Likelihood geschätzt werden. Die Likelihood-Funktion für Logit-Regression lautet\n\\[\\begin{align}\n  L(\\boldsymbol{\\beta}) &= \\prod_{i=1}^n \\left(\\frac{1}{1 + \\exp(-\\mathbf{X}_i^\\top \\boldsymbol{\\beta})}\\right)^{y_i} \\left(1 - \\frac{1}{1 + \\exp(-\\mathbf{X}_i^\\top \\boldsymbol{\\beta})}\\right)^{1-y_i},\n\\end{align}\\]\nmit \\[\\frac{1}{1 + \\exp(-\\mathbf{X}_i^\\top \\boldsymbol{\\beta})}\\] der Wahrscheinlichkeit, dass \\(Y_i = 1\\). Für das Ereignis \\(Y_i = 0\\) ist die Wahrscheinlichkeit entsprechend \\[1 - \\frac{1}{1 + \\exp(-\\mathbf{X}_i^\\top \\boldsymbol{\\beta})}.\\]\nDie Log-Likelihood-Funktion lautet\n\\[\\begin{align*}\n  \\mathcal{L}(\\boldsymbol{\\beta}) &= \\sum_{i=1}^n \\left[ y_i \\log \\left(\\frac{1}{1 + \\exp(-\\mathbf{X}_i^\\top \\boldsymbol{\\beta})}\\right) + (1 - y_i) \\log \\left(1 - \\frac{1}{1 + \\exp(-\\mathbf{X}_i^\\top \\boldsymbol{\\beta})}\\right) \\right]\n\\end{align*}\\]\nund erlaubt die Schätzung von \\(\\boldsymbol{\\beta}\\) durch Maximierung mit numerischen Methoden.\nFür Anwendungen mit R nutzen wir stats::glm() und wählen die logistische Link-Funktion mit family = binomial(link = \"logit\"). Für die simulierten Daten simdata aus Kapitel 4.2.2:\n\n\n\n\n\n\n\n\nWir erweitern nun pred um die anhand von predict() für mod_logit geschätzten Wahrscheinlichkeiten von \\(Y=1\\vert X\\) für die Regressorwerte in X.\n\n\n\n\n\n\n\n\nAnhand der Vorhersagen in pred können wir die im Objekt p gespeicherte Grafik um die geschätzte Regressionsfunktion für das Logit-Modell erweitern.\n\n\n\n\n\n\n\n\nDie Logit-Regression kann den wahren Zusammenhang gut abbilden und ist praktisch nicht von der Probit-Schätzung unterscheidbar.\n\n4.2.5 Modellgüte\nÄhnlich wie bei linearer Regression kann die Anpassung von generalisierten linearen Modellen an die Daten mit Anpassungsmaßen verglichen werden. In generalisierten linearen Modellen für binäre Outcome-Variablen kann hierzu die Deviance verwendet werden. Für eine Schätzung anhand der Beobachtungen \\(i=1,\\dots,n\\) berechnet man die Deviance \\(D\\) als \\[\\begin{align*}\n  D = -2 \\sum_{i=1}^{n} \\left[ y_i \\cdot \\log\\left(\\widehat{P}_i\\right) + (1 - y_i) \\cdot \\log\\left(1 - \\widehat{P}_i\\right) \\right].\n\\end{align*}\\] \\(D\\) quantifiziert die Abweichung eines geschätzten Modells von einem perfekt passenden Modell.5 Eine geringere Deviance deutet darauf hin, dass das Modell die Daten besser erklärt. Eine R-Funktion zur Anwendung für Objekte des Typs glm ist deviance().\n5 Im Allgemeinen vergleicht Deviance die Log-Likelihood des geschätzten Modells mit der Log-Likelihood des perfekten Modells.\n\n\n\n\n\n\n\nDie Deviance für das Probit-Modell ist tatsächlich etwas geringer als für das Logit-Modell.\nÄhnlich wie \\(R^2\\) für lineare Regression misst die Deviance \\(D\\) lediglich die Anpassung des Modells an die beobachteten Daten. Daher eignet sich \\(D\\) nur bedingt als Maß zur Einschätzung der Tauglichkeit eines Modells für Out-of-sample-Vorhersagen. Je nach Ziel der Modellierung kann es hilfreich sein, eine robuste Einschätzung der Vorhersage-Güte unter Berücksichtigung der Modellkomplexität vorzunehmen. Hierfür werden oft Informationskriterien verwendet. Das Akaike-Informationskriterium (AIC) \\[\\begin{align*}\n  \\text{AIC} = 2k - 2 \\log(\\widehat{\\mathcal{L}})\n\\end{align*}\\] ist ein Maß zur Bewertung der Güte eines Modells unter Berücksichtigung der Modellkomplexität. Das AIC wägt die Anpassung des Modells an die Daten (gemessen durch die maximierte Likelihood-Funktion \\(\\widehat{\\mathcal{L}}\\)6) und die Komplexität (gemessen als Funktion der Regressoranzahl \\(k\\)) gegeneinander ab. Hierbei werden Modelle mit weniger Parametern bevorzugt, wenn sie die Daten ähnlich gut erklären, sodass eine Überanpassung und damit eine schlechte Vorhersage-Fähigkeit vermieden werden. In R können wir das AIC für Modell-Objekte mit AIC() berechnen.\n6 \\(\\widehat{\\mathcal{L}}\\) meint die Likelihood-Funktion für die geschätzen Koeffizienten.\n\n\n\n\n\n\n\nAuch das AIC zeigt an, dass die Schätzung des Probit-Modells in mod_probit besser geeignet für die Modiellierung von simdata ist als das Logit-Modell mod_logit.\n\n4.2.6 Beispiel: Klassifikation von Palmer-Piniguinen\nAnhand der in diesem Kapitel betrachteten Methoden können wir Modelle zur Klassifikation von Pinguinen in palmerpenguins::penguins hinsichtlich ihres Geschlechts konstruieren. Hierfür nutzen wir den bereinigten Datensatz penguins_cleaned und transformieren zunächst die abhängige Variable sex in ein numerisches Format.\n\n\n\n\n\n\n\n\nWir schätznen nachfolgend ein lineares Wahrscheinlichkeitsmodell penguins_lp, ein Probit-Modell penguins_probit sowie ein Logit-Modell penguins_logit die jeweils sex anhand der simplen Spezikation des linearen Prädiktors \\[z = \\beta_0 + \\beta_1 \\textup{bill\\_depth}\\] erklären.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMit modelsummary::modelsummary() können wir die wichtigsten Ergebnisse der Regressionen in einer pubplikationsfähigen Tabelle darstellen. Hierzu übergeben wir dem Argument models eine (benannte) Liste der geschätzten Modelle. Bei Angabe von vcov = \"HC1\" werden robuste Standardfehler ausgegeben.\n\n\n\n\n\n\n\n\nDie geschätzten negativen und signifikanten Koeffizienten von \\(\\textup{bill\\_depth}\\) erfassen den Zusammenhang, dass Pinguine mit größeren Schnäbeln tendenziell männlich sind: Je größer \\(\\textup{bill\\_depth}\\), desto geringer die geschätzte Wahrscheinlichkeit, dass ein Pinguin weiblich ist. Ein Vergleich anhand des AIC zeigt, dass das Probit-Modell penguins_probit am besten geeignet ist.\nAnalog zur Vorgehensweise in Kapitel 4.2.3 können wir die geschätzten Modelle vergleichen, in dem wir geschätzte Wahrscheinlichkeiten \\(P(Y=\\textup{weiblich}\\vert\\textup{bill\\_depth})\\) für eine Menge repräsentativer Werte des Regressors bill_depth berechnen und gemeinsam mit den Beobachtungen plotten.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie geschätzten Regressionsfunktionen unterscheiden sich für den Bereich beobachteter Werte von \\(bill\\_depth\\) nur wenig. Die Grafik zeigt, dass die Schätzungen mit Probit- und Logit-Ansatz nur wenig Nicht-Linearität aufweisen, sodass eine (leichter interpretierbare) Modellierung mit dem linearen Wahrscheinlichkeitsmodell penguins_lp hier zulässig scheint.\nDie Klassifizierung der Pinguine im Datensatz hinsichtlich ihres Geschlechts anhand vorhergesagter Wahrscheinlichkeiten \\(\\widehat{P}_i\\) kann durch Abgleich mit einem Grenzwert erfolgen. Ein Beispiel ist \\[\\begin{align}\n  \\widehat{Y}_i = \\begin{cases}\n    \\textup{weiblich}, & \\widehat{P}_i \\geq .5,\\\\\n    \\textup{männlich}, & \\textup{sonst.}\n  \\end{cases}\\label{eq:pengclass}\n\\end{align}\\]\nWir können diesen Ansatz in R implementieren, indem wir zunächst die für den Datensatz angepassten Wahrscheinlichkeiten fitted aus den Modell-Objekten auslesen.\n\n\n\n\n\n\n\n\nFür die Klassifikation der Pinguine anhand der jeweiligen geschätzten Wahrscheinlichkeiten wenden wir die Regel \\(\\eqref{eq:pengclass}\\) mit across() spaltenweise für die Modelle an und erzeugen neue Spalten mit vorhersagen des Geschlechts. Mit .names = \"{.col}_pred\" erhalten die neuen Spalten das Suffix _pred.\n\n\n\n\n\n\n\n\nMit summarise() berechnen wir nun den Anteil korrekter Vorhersagen.\n\n\n\n\n\n\n\n\nIn diesem Beispiel unterscheiden sich die Vorhersagen der drei Modelle für den Grenzwert 0.5 nicht.",
    "crumbs": [
      "Grundlagen",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "Reg.html#sec-poissonreg",
    "href": "Reg.html#sec-poissonreg",
    "title": "4  Regression",
    "section": "\n4.3 Modellierung von Zählvariablen",
    "text": "4.3 Modellierung von Zählvariablen\nEine weitere Klasse von \\(Y\\) für die sich der Regressionsansatz von einfacher linearer Regression unterscheidet, sind Zählvariablen: Variablen, die diskrete, nicht-negative Werte annehmen. Wenn die abhängige Variable \\(Y\\) Ereignisse in einem bestimmten Zeitraum misst (wie z.B. die Anzahl der Verkehrsunfälle in Essen innerhalb eines Monats) und weitere Bedingungen erfüllt sind, folgt \\(Y\\) einer Poisson-Verteilung und kann mit Poisson-Regression modelliert werden. Wir erläutern nachfolgend die wesentlichen Komponenten von Poisson-Regression und diskutieren ein R-Beispiel mit fiktiven Daten.\n\n4.3.1 Poisson-Verteilung\nDie Zufallsvariable \\(Y\\) folgt einer Poisson-Verteilung mit Parameter \\(\\lambda\\), wenn:\n\nEreignisse unabhängig voneinander auftreten: Das Auftreten eines Ereignisses beeinflusst nicht die Wahrscheinlichkeit, dass ein weiteres Ereignis auftritt.\nEreignisse einzeln auftreten: Die Wahrscheinlichkeit, dass in einem sehr kleinen Intervall (im Sinne von \\([a,b]\\) für \\(a \\to b\\)) mehr als ein Ereignis auftritt, ist vernachlässigbar.\ndie Rate der Ereignisse konstant ist: Die durchschnittliche Anzahl der Ereignisse pro Zeiteinheit oder pro Raumeinheit \\(\\lambda\\) ist konstant.\n\nDie Wahrscheinlichkeitsfunktion von \\(Y\\) ist\n\\[\\begin{align}\nP(Y = y) = \\frac{\\lambda^y e^{-\\lambda}}{y!} \\quad \\text{für} \\quad y = 0, 1, 2, \\ldots,\\label{eq:poissonpmf}\n\\end{align}\\] wobei \\(\\lambda\\) sowohl der Erwartungswert als auch die Varianz der Verteilung ist, \\[\\textup{E}(Y) = \\textup{Var}(Y) = \\lambda.\\]\nDie nächste Grafik zeigt die diskrete (Wahrscheinlichkeitsmasse-)Funktion \\(\\eqref{eq:poissonpmf}\\) für eine Poisson-verteilte Zufallsvariable \\(Y\\) mit \\(\\lambda = 5\\) für den Wertebereich von 0 bis 15.\n\n\n\n\n\n\n\n\n\n4.3.2 Poisson-Regression\nWie für die bisher betrachteten Regressionsansätzen modelliert Poisson-Regression den Erwartungswert (und damit gleichzeitig die Varianz) \\(\\lambda\\) der abhängigen Variable \\(Y\\) als eine Funktion der Regressoren \\(\\mathbf{X} = (X_1, X_2, \\ldots, X_k)\\). Unter Annahme einer Poisson-Verteilung kann Poisson-Regression als generalisiertes lineares Modell verstanden werden, wobei eine logarithmische Verknüpfungsfunktion für den linearen Prädiktor \\(\\mathbf{X}_i^\\top \\boldsymbol{\\beta}\\) verwendet wird:\n\\[\\begin{align*}\n\\log(\\lambda_i) &= \\mathbf{X}_i^\\top \\boldsymbol{\\beta},\n\\end{align*}\\] sodass \\[\\begin{align*}\n\\lambda_i &= \\exp(\\mathbf{X}_i^\\top \\boldsymbol{\\beta}),\n\\end{align*}\\] wobei\n\n\n\\(\\lambda_i = \\textup{E}(Y_i\\vert \\mathbf{X}_i) = \\textup{Var}(Y_i\\vert \\mathbf{X}_i)\\) für Beobachtung \\(i\\),\n\n\\(\\mathbf{X}_i\\) der Vektor der unabhängigen Variablen für Beobachtung \\(i\\) ist, und\n\n\\(\\boldsymbol{\\beta}\\) der Vektor der Regressionskoeffizienten ist.\n\n4.3.3 Schätzung\nDie Likelihood-Funktion für \\(n\\) Beobachtungen ist\n\\[\\begin{align}\nL(\\boldsymbol{\\beta}) = \\prod_{i=1}^n \\frac{\\lambda_i^{y_i} e^{-\\lambda_i}}{y_i!}.\n\\end{align}\\]\nDie Log-Likelihood-Funktion ist daher\n\\[\\begin{align}\n\\mathcal{L}(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left( y_i \\log(\\lambda_i) - \\lambda_i - \\log(y_i!). \\right)\n\\end{align}\\]\nDa \\(\\lambda_i = \\exp(\\mathbf{X}_i^\\top \\boldsymbol{\\beta})\\), wird die Log-Likelihood-Funktion zu\n\\[\\begin{align}\n\\mathcal{L}(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left( y_i (\\mathbf{X}_i^\\top \\boldsymbol{\\beta}) - \\exp(\\mathbf{X}_i^\\top \\boldsymbol{\\beta}) - \\log(y_i!) \\right)\n\\end{align}\\]\nDen ML-Schätzer \\(\\widehat{\\boldsymbol{\\beta}}\\) erhalten wir durch Maximierung der Log-Likelihoodfunktion \\(\\mathcal{L}(\\boldsymbol{\\beta})\\). Die R-Implementierung von Poisson-Regression in stats::glm() wird mit family = poisson(link = \"log\") aufgerufen.\n\n4.3.4 Interpretation der Koeffizienten\nDie Koeffizienten \\(\\boldsymbol{\\beta}\\) in der Poisson-Regression haben eine log-lineare Beziehung zur Zählvariable. Für einen bestimmten Koeffizienten \\(\\beta_j\\) ist die Interpretation wiefolgt:\nEine Änderung der unabhängigen Variable \\(X_j\\) um eine Einheit führt zu einer Änderung des Logarithmus des Erwartungswertes von \\(Y\\) um \\(\\beta_j\\). Der Erwartungswert \\(\\lambda\\) ändert sich also multiplikativ um den Faktor \\(\\exp(\\beta_j)\\).\nAls Beispiel betrachten wir den linearen Prädiktor\n\\[\\begin{align*}\n  \\log(\\lambda) = \\beta_0 + \\beta_1 X.\n\\end{align*}\\]\nFür eine Änderung von \\(X\\) um \\(\\Delta X\\) erhalten wir\n\\[\\begin{align*}\n   \\lambda =&\\, \\exp(\\beta_0 + \\beta_1 X) \\\\\n    \\lambda + \\delta\\lambda =&\\, \\exp(\\beta_0 + \\beta_1 (X + \\Delta X)) \\\\\n    =&\\, \\exp(\\beta_0 + \\beta_1 X) \\cdot \\exp(\\beta\\Delta X) \\\\\n    =&\\, \\lambda \\cdot \\exp(\\beta\\Delta X)\n\\end{align*}\\]\nAngenommen \\(X\\) ist die Anzahl durchgeführter Werbekampagnen und die abhängige Zählvariable \\(Y\\) misst die Anzahl der Verkäufe des beworbenen Produkts pro Monat.\nWenn \\(\\beta_1 = 0.5\\), bedeutet dies, dass jede zusätzliche Werbekampagne (\\(\\Delta X = 1\\)) die erwartete Anzahl der Verkäufe pro Monat um einen Faktor von \\(\\exp(0.5) \\approx 1.65\\) erhöht:\nDas heißt, die Rate der Verkäufe steigt um 65% für jede zusätzliche Werbekampagne.\nZur Veranschaulichung der Schätzung einer Poisson-Regression für dieses Beispiel erzeugen wir Poisson-verteilte Daten für \\[\\begin{align*}\n  \\log(\\lambda) = 2 + 0.4 \\cdot X\n\\end{align*}\\] und ziehen \\(X\\) gleichverteilt aus \\(\\{1,2,\\dots,8\\}\\).\n\n\n\n\n\n\n\n\nDie Züge aus der abhängigen Variablen visualieren wir mit einem Häufigkeits-Histogramm.\n\n\n\n\n\n\n\n\nWir schätzen das Modell und extrahieren die robuste Zusammenfassung mit broom::tidy().\n\n\n\n\n\n\n\n\nDie ML-Schätzung des Poisson-Modells liefert für die verwendete Stichprobe von 500 Beobachtungen Koeffizienten-Schätzungen nahe der wahren Parameter. Zur Veranschaulichung des geschätzten Modells überlagern wir die simulierten Datenpunkte aus dat mit der anhand von mod_poisson geschätzten Anzahl der Verkäufe je Anzahl an Werbekampagnen.",
    "crumbs": [
      "Grundlagen",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "Reg.html#zusammenfassung",
    "href": "Reg.html#zusammenfassung",
    "title": "4  Regression",
    "section": "\n4.4 Zusammenfassung",
    "text": "4.4 Zusammenfassung\nIn diesem Kapitel haben wir erweiterte Konzepte der Regressionsanalyse diskutiert. Anhand des Frisch-Waugh-Lovell-Theorems wurde die Wirkungsweise der Kontrolle von Kovariablen mit multipler Regression auf den interessierenden Koeffizienten (Effekt) einer Variable veranschaulicht. Weiterhin haben wir gängige Typen generalisierter linearer Modelle für binäre und Poisson-verteilte Outcome-Variablen eingeführt. Anhand relevanter Beispiele wurde gezeigt, wie diese generalisierten Regressionsmodelle in R spezifiziert, geschätzt und der resultierende Output mit Paketen wie broom, ggplot2 und modelsummary interpretiert werden kann.",
    "crumbs": [
      "Grundlagen",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "Simulation.html",
    "href": "Simulation.html",
    "title": "5  Simulation",
    "section": "",
    "text": "5.1 Simulation der Stichprobenverteilung eines Schätzers\nZur Illustration der Simulation von Stichprobenverteilungen eines Schätzers betrachten wir zunächst das kanonische Beispiel der Schätzung des Erwartungswerts \\(\\mu_X\\) einer normalverteilen Zufallsvariable \\(X\\) anhand einer \\(n\\)-elementigen Stichprobe, \\[\\begin{align}\n  X_i \\sim\\,u.i.v. N(\\mu_X,\\ \\sigma_X^2), \\quad i=1,\\dots,n.\\label{eq:distx}\n\\end{align}\\] Die Vorschrift \\(\\eqref{eq:distx}\\) ist der datenerzeugende Prozess (DGP)1. Das arithmetische Mittel der \\(X_i\\), \\[\\begin{align*}\n  \\overline{X} := \\sum_{i=1}^n X_i,\n\\end{align*}\\] ist ein konsistenter Schätzer für \\(\\mu_X\\) und hat (aufgrund der Normalverteilung der \\(X_i\\)) die formal herleitbare Eigenschaft, dass die Stichprobenverteilung normal ist2, \\[\\begin{align*}\n  \\overline{X} \\sim N(\\mu_X, \\sigma_X^2 / n), \\qquad \\overline{X}\\xrightarrow{p}\\mu_X.\n\\end{align*}\\]\nWir können die Eigenschaften von \\(\\overline{X}\\) mit wenig Aufwand in R prüfen indem wir die Stichprobenentnahme durch wiederholtes Ziehen einer (großen) Anzahl von \\(N\\) Stichproben aus der Populationsverteilung von \\(X\\) gemäß der Vorschrift \\(\\eqref{eq:distx}\\) simulieren und jeweils \\(\\overline{X}\\) für diese simulierten Stichproben berechnen. Wir simulieren so Züge aus der Stichprobenverteilung von \\(\\overline{X}\\), deren Eigenschaften – in Abhängigkeit von \\(\\mu_X\\), \\(\\sigma_X^2\\) und \\(n\\) – wir anhand der Realisierungen von \\(\\overline{X}\\) ableiten können. In den nachfolgenden Code-Beispielen wählen wir \\[\\begin{align*}\n  \\mu_X = 50,\\quad \\sigma_X^2=12^2,\\quad n = 50.\n\\end{align*}\\]\nEine einfache Zufallsstichprobe von \\(X\\) erzeugen wir mit rnorm() und berechnen das arithmetische Mittel mit mean().\nDieses einfache Simulationsexperiment mit \\(N=1\\) ergibt zwar eine Schätzung des Erwartungswerts von \\(X\\) nahe des wahren Werts \\(\\mu_X = 50\\), ist jedoch wenig informativ über die Qualität des Schätzers, d.h. die Eigenschaften der Stichprobenverteilung von \\(\\overline{X}\\). Diese Realisation könnte zufällig nahe bei \\(\\mu_X = 50\\) liegen, obwohl die Stichprobenverteilung \\(\\overline{X}\\) einen von \\(50\\) verschiedenen Erwartungswert hat und/oder eine große Varianz aufweist.\nUm die Stichprobenverteilung zu simulieren, wiederholen wir das Simulationsexperiment \\(N=1000\\) mal: Wir erzeugen \\(N=1000\\) Realisierungen aus der Stichprobenverteilung von \\(\\overline{X}\\) für die gewählten Parameter. Hierfür verwenden wir Iteration in R: Eine for()-Schleife wiederholt das Simulationsexperiment über 1:N Iterationen und speichert das Ergebnis des j-ten Experiments in einem zuvor definierten numerischen Vektor sim_est.3\nDie zusammenfassenden Statistiken sind Schätzungen der Verteilungseigenschaften von \\(\\overline{X}\\). Da wir die Anzahl der Replikation \\(N\\) selber wählen, können wir die wahren Parameter theoretisch “beliebig” genau approximieren: Lediglich die Rechenleistung des Computers limitiert die Güte dieser Approximation. In vielen Anwendungen liefert eine vierstellige Zahl an Simulationsdurchläufen gute Approximationen. Tatsächlich liegen die simulierten Parameter der Stichprobenverteilung von \\(\\overline{X}\\) nahe bei den wahren Werten.\nFür einen grafischen Vergleich der simulierten Stichprobenverteilung mit der theoretischen \\(N(50, 12^2/50)\\)-Verteilung plotten wir die theoretische Dichte sowie ein Dichte-Histogramm der simulierten Züge von \\(\\overline{X}\\) in sim_est.\nDie Abbildung zeigt, dass die simulierte Stichprobenverteilung (graues Dichte-Histogramm) die Dichte der theoretischen Verteilung (gestrichelte Linie) gut approximiert.",
    "crumbs": [
      "Grundlagen",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "Simulation.html#sec-simspv",
    "href": "Simulation.html#sec-simspv",
    "title": "5  Simulation",
    "section": "",
    "text": "1 Englische Abkürzung für data generating process.2 D.h. hier ist \\(\\overline{X}\\sim N(50, 2.88)\\).\n\n\n\n\n\n\n\n\n\n3 Iteration mit Funktionen aus dem Paket purrr (siehe tab) ist eine Alternative zum “klassischen” Ansatz mit einer for()-Schleife.\n\nIteration mit for()-Schleife\nIteration mit purrr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.1.1 Simulation mit nicht-normalen Daten\nSimulationen sind insbesondere dann hilfreich, wenn die Stichproben-Eigenschaften eines Schätzers unbekannt sind. Im nachfolgenden Beispiel wiederholen wir die Simulation für eine Log-normal-verteilte Zufallsvariable4 \\(X\\) mit Erwartungswert \\(\\mu_X = 50\\) und plotten die geschätzten Dichtefunktionen der Stichprobenverteilungen von \\(\\overline{X}\\) für einen Vergleich.\n4 Ist \\(X\\) log-normal-verteilt mit Erwartungswert \\(\\mu_X\\) und Varianz \\(\\sigma_X^2\\), dann ist \\(\\log X\\) normal-verteilt mit \\(\\mu_{\\log X} = \\log \\mu_X - \\sigma_X^2/2\\) und \\(\\sigma^2_{\\log X} = \\mu_X^2 [\\exp(\\sigma_X^2)-1]\\).\n\n\n\n\n\nDie funktionale Form der Stichprobenverteilung von \\(\\overline{X}\\) für eine log-normalverteilte Population ist unbekannt. Unsere Simulation erlaubt jedoch Rückschlüsse auf Eigenschaften dieser Verteilung: Die Verteilung ist rechtsschief und weißt eine höhere Varianz auf als die Stichprobenverteilung von \\(\\overline{X}\\) für die normal-verteilte Population.\n\n5.1.2 Regression: Verzerrung durch Confounder\nDer DGP in Abbildung 5.1 zeigt eine typische Situation, in der wir den kausalen Effekt von \\(X\\) auf \\(Y\\) nicht ohne Weiteres mit Regression identifizieren können: Die Variablen \\(U\\) und \\(e\\) sind unbeobachtbar. \\(e\\) ist ein unsystematischer Fehler, der lediglich \\(Y\\) beeinflusst. Die Variable \\(U\\) hingegen beeinflusst sowohl \\(X\\) als auch \\(Y\\) und ist damit eine Backdoor-Variable: \\(U\\) ist ein Counfounder, sodass der kausale Effekt von \\(X\\) auf \\(Y\\) in einer Regression, die nicht für den Effekt von \\(U\\) kontrolliert, verzerrt geschätzt wird. Wie stark die Verzerrung ist, hängt von den Eigenschaften des DGP ab. Simulation kann hilfreich sein, um die Verzerrung eines Schätzers in Abhängigkeit der Parameter des DGP zu untersuchen.\n\n\n\n\n\nCONFSIM\nX\nXY\nYX-&gt;Y\nbeta = 2U\nUU-&gt;X\nbeta = 1U-&gt;Y\nbeta = -1e\nee-&gt;Y\n\n\n\n\nAbbildung 5.1: DGP mit unbeobachtbarem Confounder U\n\n\n\n\nFür die Simulation von Stichproben gemäß Abbildung 5.1 nutzen wir den linearen Prozess\n\\[\\begin{align*}\n  X =& \\, a + \\alpha_1 U,\\\\\n  Y =& \\, \\beta_1 X + \\beta_2 U + e,\n\\end{align*}\\]\nmit \\(a \\sim \\mathcal{U}[0,1]\\), \\(U\\sim N(0, 2^2)\\) und \\(e\\sim N(0, 10)\\) und wählen die Parameter \\(\\alpha_1 = 1\\), \\(\\beta_1 = 2\\) und \\(\\beta_2 = -1\\).5 Der nächste Code-Block implementiert den DGP und zeigt die KQ-Schätzung des fehlspezifizierten Modells\n5 \\(\\mathcal{U}[a,b]\\) meint die stetige Gleichverteilung auf dem Intervall \\([a,b]\\)\\[\\begin{align*}\n  Y = \\beta_1 X + \\epsilon\n\\end{align*}\\]\nanhand einer simulierten Stichprobe mit einem Umfang von 100 Beobachtungen.\n\n\n\n\n\n\nAnalog zu Kapitel 5.1 ist diese Simulation mit nur einer Iteration wenig aussagekräftig für die Verzerrung der KQ-Schätzers. Für eine ausführlichere Analyse implementieren wir zunächst beide Schritte der Simulation (Stichprobe generieren, Modell schätzen) in einer Funktion sim_function_conf(), der wir sämtliche Parameter sowie das zu schätzende Regressionsmodell als Fomel übergeben können. Diese Funktion gibt die aus dem lm-Objekt ausgelesene KQ-Schätzung von \\(\\beta_1\\) zurück. Wir testen die Funktion sim_function_conf() durch Verwendung des selben Seeds.\n\n\n\n\n\n\nDer mit sim_function_conf() simulierte Zug aus der Stichprobenverteilung stimmt mit dem Ergebnis des vorherigen Zufallsexperiments überein. Wir können nun die Simulation durch iterative Berechnung von sim_function_conf() über die Indexmenge 1:N durchführen. Wie zuvor visualisieren wir die Simulationsergebnisse mit einem Plot der geschätzten Dichtefunktion von \\(\\widehat{\\beta}_1\\).\n\n\n\n\n\n\nDer grafische Vergleich der simulierten Stichprobenverteilung von \\(\\widehat{\\beta}_1\\) mit dem wahren Koeffizienten \\(\\beta_1 = 2\\) (gestrichelte rote Linie) zeigt, dass \\(\\widehat{\\beta}_1\\) aufgrund des ausgelassenen Confounders \\(U\\) für die hier gewählte Spezifikation des DGP tatsächlich ein verzerrter Schätzer für den kausalen Effekt \\(\\beta_1\\) ist: Die geschätzte Dichtefunktion ist ein Indikator dafür, dass die Verteilungsfunktion von \\(\\widehat{\\beta}_1\\) einen Großteil der Wahrscheinlichkeitsmasse im Wertebereich oberhalb von \\(\\beta_1 = 2\\) hat, sodass wir den kausalen Effekt im Mittel überschätzen.\nWir können die Simulationsergebnisse in sim_est nutzen, um die Verzerrung6 von \\(\\widehat\\beta_1\\) zu schätzen.\n6 Die Verzerrung (engl. bias) eines Schätzers \\(\\widehat{\\beta}_1\\) für \\(\\beta_1\\) ist \\(\\textup{bias}(\\widehat{\\beta}_1) = \\textup{E}(\\widehat{\\beta}_1 - \\beta_1)\\).\n\n\n\n\n\nDiese Berechnung anhand von Zügen aus der Stichprobenverteilung von \\(\\widehat{\\beta}_1\\) zeigt eine deutliche positive Verzerrung des KQ-Schätzers von \\(\\beta_1\\) aufgrund des Confounders \\(U\\) von etwa 0.94.\nAnhand von sim_function_conf() können wir die Identifizierbarkeit des kausalen Effekts \\(\\beta_1\\) bei Kontrolle für \\(U\\) im Modell\n\\[\\begin{align*}\n  Y = \\beta_0 + \\beta_1 X + \\beta_2 U + \\varepsilon\n\\end{align*}\\]\nüberprüfen, in dem wir die Simulation unter Angabe dieses Modells über das Argument formula durchführen.\n\n\n\n\n\n\nDie geschätzte Stichprobenverteilung von \\(\\widehat{\\beta}_1\\) bei Kontrolle für den Confounder \\(U\\) zeigt keine Anzeichen für eine Verzerrung des Schätzers für den kausalen Effekt von \\(X\\) auf \\(Y\\). Die Monte-Carlo-Schätzung der Verzerrung von \\(\\widehat{\\beta}_1\\) unterstützt das Ergebnis der grafischen Analyse.\n\n\n\n\n\n\n\n5.1.3 Regression: Verzerrung durch Collider\nDer Graph in Abbildung 5.2 zeigt einen DGP, bei dem die Variable \\(C\\) ein Collider auf dem Pfad \\(X\\rightarrow C \\leftarrow Y\\) ist. Für die Identifikation des Effekts von \\(X\\) auf \\(Y\\) ist \\(C\\) damit eine irrelavante Variable, für die nicht kontrolliert werden darf: Statistische Verfahren, die den Effekt von \\(X\\) auf \\(Y\\) unter Kontrolle des Effekts von \\(C\\) auf \\(Y\\) schätzen sind verzerrt, weil ein Backdoor-Pfad durch \\(C\\) besteht.\n\n\n\n\n\nCOLSIM\nX\nXY\nYX-&gt;Y\nbeta_1 = 2C\nCX-&gt;C\nbeta = .25Y-&gt;C\nbeta = .25e\nee-&gt;Y\n\n\n\n\nAbbildung 5.2: DGP mit Collider-Variable C\n\n\n\n\nÄhnlich wie in Kapitel 5.1.2 können wir die Konsequezen der Kontrolle für eine Collider-Variable in einem linearen Regressionsmodell anhand einer Simulationsstudie untersuchen. Wir implementieren hierzu den DGP\n\\[\\begin{align*}\n  X \\sim& \\, N(0,1),\\\\\n  Y =& \\, \\beta_1 X + e,\\\\\n  C =& \\, N(0,1) + .25X + .25Y,\n\\end{align*}\\] wobei \\(e \\sim N(0,3^2)\\) erneut ein unsystematischer Fehlterterm für \\(Y\\) ist. Wie im DGP aus Kapitel 5.1.2 wählen wir \\(\\beta_1 = 2\\) für den wahren kausalen Effekt von \\(X\\) auf \\(Y\\).\n\n\n\n\n\n\nIn der nachfolgenden Simulation iterieren wir sim_function_col() für zwei verschiedene Modelle und speichern Ergebnisse in einer tibble: Die erste Spalte von sim_est_col enthält simulierte KQ-Schätzungen von \\(\\widehat{\\beta}_1\\) in einem einfachen Regressionsmodell, dass nicht für den Collider \\(C\\) kontrolliert (formula = \"Y ~ X\"). Die zweite Spalte hingegen sammelt Schätzungen für ein multiples Modell mit Kontrolle für \\(C\\) (formula = \"Y ~ X + C\").\n\n\n\n\n\n\nFür die Auswertung der Simulationsergebnisse hinsichtlich der Konsequenzen der Kontrolle für den Collider \\(C\\) plotten wir die geschätzen Dichtefunktionen für die Stichprobenverteilung von \\(\\widehat{\\beta}_1\\) aus beiden Monte-Carlo-Experimenten gemeinsam und vergleich mit dem wahren Wert \\(\\beta_1 = 2\\).\n\n\n\n\n\n\nDie Abbildung zeigt deutlich die (negative) Verzerrung des KQ-Schätzers für den interessierenden kausalen Effekt im Modell mit Kontrolle für den Collider \\(C\\). Die Simulationsergebnisse für eine einfache Regression von \\(Y\\) auf \\(X\\) hingegen stimmen mit dem theretischen Resultat überein, dass der KQ-Schätzer für \\(\\beta_1\\) in diesem Modell erwartungstreu ist.\nDurch Zusammenfassung der Simulationsergebnisse mit dplyr::summarize() können wir den Collider-Bias im Modell mit Kontrolle für \\(C\\) schätzen und die Unverzerrtheit von \\(\\widehat{\\beta}_1\\) in der einfachen Regression von \\(Y\\) auf \\(X\\) prüfen.",
    "crumbs": [
      "Grundlagen",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "Simulation.html#regression-durchschnittlicher-behandlungseffekt",
    "href": "Simulation.html#regression-durchschnittlicher-behandlungseffekt",
    "title": "5  Simulation",
    "section": "\n5.2 Regression: Durchschnittlicher Behandlungseffekt",
    "text": "5.2 Regression: Durchschnittlicher Behandlungseffekt\nWir können Simulation einsetzen, um die Güte der Schätzung von Behandlungseffekten in linearen Regressionsmodellen zu untersuchen. Hierzu implementieren wir einen DGP gemäß Abbildung 5.3.\n\n\n\n\n\nHETEFFSIM\nB\nBY\nYB-&gt;Y\nβ_1 heterogenX\nXX-&gt;Y\nβ_2 = 2e\nee-&gt;Y\n\n\n\n\nAbbildung 5.3: DGP mit heterogenen Behandlungseffekten\n\n\n\n\nFür die Komponenten wählen wir: \\[\\begin{align}\n  \\begin{split}\n    \\text{B} &\\sim \\text{Bernoulli}(0.5) \\\\\n    X &\\sim N(0, 2) \\\\\n    \\beta_1 &\\sim \\mathcal{U}(0, 3) \\\\\n    e &\\sim N(0, 2) \\\\\n    \\\\\n    Y &= 1 + \\beta_1 \\cdot \\text{B} + 2 \\cdot X + e\n  \\end{split}\\label{eq:dgpatesim}\n\\end{align}\\]\nIn diesem linearen DGP bestimmt \\(B\\) über die Zuteilung der Behandlung. \\(X\\) ist eine Determinante der Outcome-Variable \\(Y\\) ohne Backdoor-Pfad. Der Behandlungseffekt \\(\\beta_1\\) ist heterogen, da \\(\\beta_1\\) aus einer kontinuierlichen Gleichverteilung auf dem Intervall \\([0, 3]\\) gezogen wird. Der durchschnittliche Behandlungseffekt (ATE) entspricht hier dem Erwartungswert von \\(\\beta_1\\), was zu einem ATE von 1.5 führt, da\n\\[\\begin{align*}\n  \\textup{ATE} =&\\, \\textup{E}(\\beta_1)\\\\\n  =&\\, \\frac{1}{2}(a + b)\\\\\n  =&\\, \\frac{1}{2}(0 + 3) = 1.5.\n\\end{align*}\\]\nWir verfizieren nachfolgend mit Monte-Carlo-Simulation für Stichproben von DGP \\(\\eqref{eq:dgpatesim}\\) mit \\(n=150\\), dass\n\nder KQ-Schätzer für \\(\\beta_1\\) im Modell \\(Y=\\beta_0 + \\beta_1 B + \\epsilon\\) ein erwartungstreuer Schätzer für den ATE ist und\nkontrollieren für \\(X\\) hier die Präzision der Schätzung des ATE verbessert.\n\nIm nachfolgenden Code-Block erzeugen wir die Simulationsergebnisse in jeder Iteration durch Berechnung der interessierenen Schätzer auf derselben simulierten Stichprobe und geben die Ergebnisse als tibble() mit einer Reihe und zwei benannten Einträgen zurück. Die Funktion map_dfr() fasst die Ergebnisse nach der Iteration über 1:N als tibble() zusammen. Wir nutzen hier einen Ansatz, bei dem die Schritte für die Simulation dem Argument .f über den Syntax ~ { ... } innerhalb einer anonymen Funktion ohne Argumente.7\n7 Anonyme Funktionen in R (auch Lambda-Funktionen genannt) sind Funktionen, die ohne Namen definiert werden und in der Regel sofort angwendet werden. ~ ist eine Kurznotation für anonyme Funktionen innerhalb von purrr-Funktionen.\n\n\n\n\n\nFür die grafische Auswertung transformieren wir die Simulationsergebnisse in sim_res_ATE mit pivot_longer() in ein langes Format.\n\n\n\n\n\n\nDie grafische Analyse der Stichprobenverteilungen bestätigt, dass der KQ-Schätzer für \\(\\beta_1\\) sowohl im Modell mit Kontrolle für \\(X\\) als auch im Modell ohne \\(X\\) erwartungstreu für den ATE von 1.5 ist. Da \\(X\\) gemäß DGP \\(\\eqref{eq:dgpatesim}\\) einen größeren Teil der Variation in \\(Y\\) verursacht, verbesserte Kontrolle für \\(X\\) hier die Präzision der Schätzung des ATE mit KQ.\nDie Variabilität der Ansätze können wir durch Zusammenfassung der Spalten in sim_res_ATE mit summarize() und across() quantifizieren, wobei wir sd als zusammenfassende Statistik nutzen und .names = \"sd_{.col}\" ein entsprechendes Präfix für die Variablennamen einführt.",
    "crumbs": [
      "Grundlagen",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "Simulation.html#verletzung-von-modellannahmen-heteroskedastizität",
    "href": "Simulation.html#verletzung-von-modellannahmen-heteroskedastizität",
    "title": "5  Simulation",
    "section": "\n5.3 Verletzung von Modellannahmen: Heteroskedastizität",
    "text": "5.3 Verletzung von Modellannahmen: Heteroskedastizität\nUnter der Annahme von Homoskedastizität (konstante Varianz) der Fehlerterme des Modells ist die KQ-Schätzung von Behandlungseffekten anhand linearer Regression effizient, d.h. der KQ-Schätzer hat die geringste Varianz unter allen unverzerrten Schätzern des interessierenden Koeffizienten.8 Hängt die Varianz der Fehlerterme von den Regressoren ab, liegt Heteroskedastizität vor: Die Fehlerterm-Varianz unterscheided sich zwischen den Beobachtungen, in Abhängigkeit der Ausprägungen der Regressoren. Unter Heteroskedastie ist der KQ-Schätzer nicht länger der effizienteste Schätzer: In Verfahren wie der gewichteten KQ-Methode (WKQ)9 haben Beobachtungen mit geringer Fehlerterm-Varianz einen größeren Einfluss als Beobachtungen mit großer Varianz, sodass der WKQ-Schätzer präziser als der KQ-Schätzer sein kann.\n8 Die ist die Kern-Aussage des Gauss-Markov-Theorems.9 Engl. weighted least squares.Weiterhin sind bei heteroskedastischen Fehlertermen die (aus historischen Gründen) standardmäßig von summary() berechneten Standardfehler ungültig, und können die Unsicherheit von KQ unterschätzen. Dies ist problematisch, da unter Verwendung von ungültigen Standardfehlern berechnete Inferenzsstatistiken ebenfalls ungültig sind. Als Konsequenz erhöht sich die Gefahr falscher Schlussfolgerungen hinsichtlich des zu schätzenden Behandlungseffekts anhand von Test-Statistiken und Konfidenzintervallen.\nWir zeigen nachfolgend, wie die Konsequenzen von Heteroskedastizität für den KQ-Schätzer mit Simulation untersucht werden können. Hierfür betrachten wir den DGP\n\\[\\begin{align}\n  \\begin{split}\n    X \\sim&\\, \\chi^2_{50}\\\\\n    e \\sim&\\, N(0, \\sigma_e^2(X))\\\\\n    Y =&\\, \\beta_1 X + e,\n  \\end{split}\\label{eq:simhetdgp}\n\\end{align}\\]\nwobei die Fehlerterm-Varianz \\(\\sigma_e^2(X)\\) eine Funktion von \\(X\\) ist, \\[\\begin{align*}\n  \\sigma_e^2(X) = \\textup{Var}(e\\vert X) = \\lvert X-50 \\rvert^{p}.\n\\end{align*}\\] Der Parameter \\(p\\) bestimmt die funktionale Form dieser bedingten Varianz.\nDer nächste Code-Chunk plottet die Funktion \\(f(x) = \\lvert X-50 \\rvert^{p}\\) für verschiedene \\(p\\) über den Wertebereich von \\(X\\). Hierfür berechnen wir \\(f(x)\\) für ein mit expand_grid() erstelltes Raster von Werten für \\(X\\) und \\(p\\).\n\n\n\n\n\n\nFür die Simulation heteroskedastischer Daten mit \\(\\eqref{eq:simhetdgp}\\) verwenden wir \\(p = 1.5\\) und \\(\\beta_1 = 0\\) (kein Effekt). Die nachfolgende Grafik zeigt eine simulierte Stichprobe mit 150 Beobachtungen.\n\n\n\n\n\n\nIn den Simulationsdurchläufen schätzen wir jeweils \\(\\beta_1\\) im Modell \\(Y=\\beta_1X+\\epsilon\\) mit KQ, berechnen den nur bei Homoskedastie gültigen Standardfehler und eine bei Heteroskedastie robuste Schätzung. Für lezteres verwenden wir sandwich::vcovHC() und mit type = \"HC1\".10 Wir fassen diese Schritte in der Funktion sim_function_het() zusammen.\n10 Der Typ HC1, auch bekannt als Huber-White-Schätzer, ist ein häufig genutzter Standardfehler und wird in späteren Kapiteln dieses Companions häufig angewendet.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie quantitative Auswertung der Ergebnisse deuten darauf hin, dass der nur bei Homoskedastie gültige Standardfehler die Standardabweichung von \\(\\widehat{\\beta}_1\\) deutlich unterschätzt. Der anhand der Simulationsergebnisse geschätzte Erwartungswert des robusten Standardfehlers hingegen liegt nahe bei der Standardabweichung der simulierten Stichprobenverteilung von \\(\\widehat{\\beta}_1\\). Die Tendenz des nur bei Homoskedastie gültigen Standardfehlers, kleinere Schätzungen der Unsicherheit von \\(\\widehat{\\beta}_1\\) zu liefern, können wir anhand der geschätzten Dichtefunktionen für se und se_rob grafisch zeigen.\n\n\n\n\n\n\nAls Konsequenz falscher Standardfehler kann die nominale \\(\\alpha\\)-Fehlerrate von Signifikanztests verletzt sein: Der Hypothesentest lehnt dann eine korrekte Nullhypothese mit einer geringeren oder höheren Fehlerwahrscheinlichkeit (das nominale Signifikanzniveau \\(\\alpha\\)) als gewünscht ab.\nInwiefern die in der Simulation gewählte Form von Heteroskedastizität die \\(\\alpha\\)-Fehlerrate eines t-Tests beeinflusst, können wir anhand der Simulationsergebnisse schätzen: Wir berechnen die Ablehnrate für t-Tests der (wahren) Null-Hypothese \\(H_0:\\beta_1=0\\) zum 5%-Niveau jeweils für robuste und nicht-robuste Standardfehler und vergleichen. Wir vergleichen die so berechneten Test-Statistiken mit dem entsprechenden kritischen Wert der t-Verteilung mit \\(n-1\\) Freiheitsgraden.\n\n\n\n\n\n\nDie Simulationsergebnisse deuten darauf hin, dass ein Fehler erster Art für den Signifikanztest ohne korrigierte Standardfehler oberhalb des 5%-Niveaus liegt. Für die robuste Teststatistik hingegen liegt die Schätzung des \\(\\alpha\\)-Fehlers nahe 5%.",
    "crumbs": [
      "Grundlagen",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "Simulation.html#der-bootstrap",
    "href": "Simulation.html#der-bootstrap",
    "title": "5  Simulation",
    "section": "\n5.4 Der Bootstrap",
    "text": "5.4 Der Bootstrap\nIn den zuvor diskutierten Simulationsstudien haben wir iterativ Stichproben basierend auf einem bekannten DGP generiert, um die Eigenschaften der Stichprobenverteilung eines Schätzers (für bestimmte Parameterwerte) zu untersuchen. Der Bootstrap (Efron 1979) ist eine Methode zur Approximation der unbekannten Stichprobenverteilung eines Schätzers, bei der zufällige Stichproben mit Zurücklegen aus einer beobachteten Stichprobe gezogen werden.11 Bootstrapping approximiert die Verteilung eines Schätzers aus beobachteten Datenpunkten und ohne Kenntnis des wahren DGP. Der Bootstrap ist daher eine Simulationsmethode, die in empirischen Anwendungen für die Bestimmung der Unsicherheit (Standardfehler) eines Schätzers genutzt werden kann. Bootstrapping ist hilfreich für Hypothesentests, wenn die beobachtete Stichprobe klein oder untypisch ist, was dazu führen kann, dass Standard-Inferenz für Behandlungseffekte nicht zuverlässig anwendbar oder ungültig ist. Insbesondere wenn Modellannahmen verletzt sind wird bootstrapping häufig in empirischen Studien eingesetzt, um Konfidenzintervalle, Standardfehler oder p-Werte zu berechnen.\n11 Diese Vorgehensweise wird auch als resampling bezeichnet.Die nachfolgenden Key Facts fassen die wesentlichen Eigenschaften des Bootstraps und die grundsätzliche Vorgehensweise zusammen.\n\n\n\n\n\n\nKey Facts zum Bootstrap\n\n\n\nDer Bootstrap ist eine statistische Methode, die durch wiederholtes Ziehen von Stichproben mit Zurücklegen aus einer beobachteten Stichprobe die Stichprobenverteilung eines Schätzers approximieren kann. Bootstrapping ermöglicht die Berechnung gültiger Inferenzstatistiken, ohne starke Annahmen über den zugrundeliegenden unbekannten DGP zu treffen.\nVorgehensweise\n\nStichproben mit Zurücklegen ziehen: Ziehe wiederholt (\\(B\\) mal) Stichproben der Größe \\(N\\) (Größe der ursprünglichen Stichprobe) mit Zurücklegen aus der originalen Stichprobe.\nParameterschätzung für jede Bootstrap-Stichprobe: Wende die Schätzfunktion \\(\\widehat{\\beta}\\) auf jede der \\(B\\) Bootstrap-Stichproben an und erhalte \\(B\\) Bootstrap-Schätzwerte \\(\\widehat{\\beta}^1,\\dots,\\widehat{\\beta}^B\\) des interessierenden Koeffizienten \\(\\beta\\).12\n\nVerteilungsparameter Schätzen: Anhand der \\(B\\) Bootstrap-Schätzungen aus Schritt 2 können die Eigenschaften der Verteilung des Schätzers \\(\\widehat{\\beta}\\) (oder für Funktionen von \\(\\widehat{\\beta}\\)) geschätzt werden:\n\nDas arithmetische Mittel der \\(B\\) Bootstrap-Schätzungen ist ein konsistenter Schätzer des Erwartungswerts von \\(\\widehat{\\beta}\\)\n\nDer Standardfehler des Schätzers \\(\\widehat{\\beta}\\) kann durch die Standardabweichung der \\(B\\) Bootstrap-Schätzungen approximiert werden\nEin \\(1-\\alpha\\%\\)-Bootstrap-Konfidenzintervall für \\(\\beta_1\\) kann anhand des \\(\\alpha/2\\%\\)- und des \\(1-\\alpha/2\\%\\)-Perzentils der Bootstrap-Verteilung approximiert werden\n\n\n\n\nBootstrapping wird häufig genutzt, um die Unsicherheit eines Schätzers zu ermitteln. Dies ist insbesondere dann hilfreich, wenn keine analytische Darstellung für die Verteilung eines Schätzers existiert oder “starke” Verteilungsannahmen (z.B. Normalverteilung) unplausibel sind.\nEin Bootstrap-Schätzer für einen unbekannten Parameter13 ist im Allgemeinen nicht erwartungstreu für den interessierenden Effekt, selbst wenn ein erwartungstreuer Schätzer verwendet wird. Bootstrap-Schätzer sind jedoch in vielen Fällen unter schwachen Annahmen konsistent für den wahren Parameter.\n\n\n\n13 Das arithmetische Mittel von \\(B\\) Bootstrap-Schätzwerten ist ein einfacher Bootstrap-Schätzer.12 Häufig wird für \\(B\\) eine ungerade Anzahl (z.B. 199) gewählt. Dies ermöglicht ein genaue Bestimmung des Medians der Bootstrap-Verteilung, was die Berechnung von Konfidenzintervallen erleichtert.Der nächste Abschnitt diskutiert Bootstrapping der Stichprobenverteilung von \\(\\overline{X}\\) anhand des Beispiels aus Kapitel 5.1.\n\n5.4.1 Interaktive Visualisierung des Bootstraps\nDie nachfolgende interaktive Visualisierung illustriert die Vorgehensweise des oben beschriebenen Bootstrap-Algorithmus für die Schätzung eines \\(95\\%\\)-Konfidenzintervalls für den Erwartungswert einer normalen Verteilung basierend auf einer Stichprobe, wenn das arithmetische Mittel als Schätzer verwendet wird. Diese Pupulationsverteilung ist \\(N(\\mu = 50, \\sigma^2 = 12^2)\\). In jeder Iteration des Bootstraps wird eine Bootstrap-Stichprobe mit Zurücklegen aus der originalen Stichprobe gezogen. Für jede Bootstrap-Stichprobe berechnen wir das Arithmetische Mittel (grüne Punkte). Für hinreichend viele Bootstrap-Schätzungen erhalten wir eine repräsentative Bootstrap-Verteilung unseres Schätzers.14 Basierend auf dieser Verteilung berechnen wir eine Bootstrap-Schätzung des Erwartungswerts \\(\\mu\\) (schwarzer Punkt) und ein \\(95\\%\\)-Konfidenzintervall (schwarze Linie).\n14 Die Grüne Linie zeigt eine Kerndichte-Schätzung der Bootstrap-Verteilung. Die Bandweite dieser Schätzung kann in der Applikation variiert werden.In der interaktiven Grafik können wir insbesondere folgende Eigenschaften des Bootstrap-Verfahrens überprüfen:\n\nDie Populationsverteilung ist normal. Daher ist auch die Stichproben-Verteilung des arithmetischen Mittels \\(\\overline{X}\\) für jede Stichprobengröße normal. Die Illustration zeigt, dass die Bootstrap-Verteilung diese Normalverteilung des Schätzers (die typische Glockenform) tatsächlich gut approximieren kann, wenn \\(B\\) hinreichend groß gewählt wird.\nDer Bootstrap-Schätzer von \\(\\mu\\) ist nicht erwartungstreu für \\(\\mu\\), da wir mit Zurückziehen aus einer Stichprobe ziehen und so die zentrale Tendenz in den beobachteten Daten reproduzieren. Diese Verzerrung verringert sich mit der Größe der originalen Stichprobe.\nGrößere Stichproben verringern die Unsicherheit des interessierenden Schätzers \\(\\overline{X}\\): Die Varianz der Stichprobenverteilung von \\(\\overline{X}\\) nimmt mit der Stichprobengröße ab. Da der Bootstrap die Verteilung von \\(\\overline{X}\\) approximiert, spiegelt sich diese Eigenschaft auch in der Bootstrap-Verteilung.\n\n\n\n\n5.4.2 Bootstrap mit R durchführen\nMit dem R-Paket boot kann Bootstrapping komfortabel für eine selbst definierte Schätzfunktion durchgeführt werden. Diese Funktion (boot_fun im nachfolgenden Code-Beispiel) muss den Schätzer unter Angabe einer Indexmenge für die beobachteten Daten berechnen und zurückgeben.15\n15 boot::boot() zieht in jeder Bootstrap-Iteration mit Zurücklegen aus der Indexmenge der originalen Stichprobe, um die Bootstrap-Stichprobe festzulegen.Der nächste Code-Chunk zeigt, wie boot::boot() angewendet werden kann, um den Bootstrap von Efron (1979) für \\(\\overline{X}\\) anhand einer Stichprobe mit \\(n=250\\) aus der in Kapitel 5.1 betrachteten \\(N(\\mu = 50, \\sigma^2 = 12^2)\\)-Populationsverteilung für \\(B=999\\) Iterationen zu berechnen.\n\n\n\n\n\n\nDer Output in boot_est beinhaltet die Schätzung von \\(\\mu\\) mit \\(\\overline{X}\\) anhand der originalen Stichprobe (original), eine Schätzung der Verzerrung von \\(\\overline{X}\\) (bias) sowie den Bootstrap-Standardfehler (std. error). Wir können die Berechnung dieser Maße mit der originalen Stichprobe sowie den Bootstrap-Berechnungen von \\(\\overline{X}\\) (boot_est$t) nachvollziehen.\n\n\n\n\n\n\nDie Bootstrap-Verteilung von \\(\\overline{X}\\) ist recht gut mit der tatsächlichen \\(N(\\mu=50,\\sigma^2 = 12^2/250)\\)-Stichprobenverteilung vergleichbar.\n\n\n\n\n\n\nMit boot::boot.ci() erhalten wir ein symmetrisches 95%-Bootstrap-Konfidenzintervall für \\(\\mu\\), \\[\\begin{align}\n  95\\%\\textup{-KI}_\\textup{boot} = \\big[2\\cdot \\overline{X}_o - q_{.975,\\,\\textup{boot}}, \\ 2\\cdot \\overline{X}_o - q_{.025,\\,\\textup{boot}}\\big].\\label{eq:95bootci}\n\\end{align}\\]\n\n\n\n\n\n\nFür die Berechnung per Hand mit der Formel \\(\\eqref{eq:95bootci}\\) bestimmen wir die Quantile der Bootstrap-Verteilung mit der Funktion quantile().\n\n\n\n\n\n\nAnhand dieses Konfidenzintervalls können wir Hypothesen zum 5%-Niveau testen.\n\n\n\n\n\n\nWir können die (wahre) Nullhypothese \\(H_0: \\mu = 50\\) also zum 5%-Niveau nicht ablehnen.",
    "crumbs": [
      "Grundlagen",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "Simulation.html#zusammenfassung",
    "href": "Simulation.html#zusammenfassung",
    "title": "5  Simulation",
    "section": "\n5.5 Zusammenfassung",
    "text": "5.5 Zusammenfassung\nIn diesem Kapitel haben wir Monte-Carlo-Simulationen als Hilfsmittel zur Analyse der Eigenschaften von Koeffizientenschätzern in ökonometrischen Modellen betrachtet. In Simulationen können die Parameter eines DGP vollständig kontrolliert und insbesondere variiert werden, um deren Auswirkungen auf das Verhalten eines Schätzers systematisch zu untersuchen: Simulation erlaubt es die Verteilung eines Schätzers durch iterative Berechnung für simulierte Datensätze näherungsweise zu bestimmen. Solche Approximationen sind hilfreich, da analytische Lösungen für die Verteilungseigenschaften von Schätzfunktionen oft nicht verfügbar sind. In methodischer Forschung erlauben Simulations-Studien es insbesondere, verschiedene anwendbare Schätzer gemäß ihrer Güte bei Verletzungen von Modellannahmen zu bewerten und so Empfehlungen hinsichtlich der Verlässlichkeit in empirischen Anwendungen zu treffen.\nDer Bootstrap ist eine Simulationsmethode zur Aschätzung der Unsicherheit eines Schätzers im empirischen Anwenungen ohne starke Verteilungsannahmen zu treffen. Bootstrapping ist besonders nützlich, wenn es plausible ist, dass herkömmliche Inferenz unzuverlässig ist oder versagt, weil die Approximationen mit einer asymptotischen Verteilung aufgrund zu kleiner Stichprobengröße schlecht ist, oder gar die asymptotische Verteilung unbekannt ist. Der Bootstrap wird in empirischen Studien häufig zur Berechnung verlässlicher Konfidenzintervalle für kausale Effekte angewendet.\nDie gezeigten Code-Beispiele illustrieren die Anwendung dieser Techniken in R mit Paketen für Iteration (purrr) und Resampling (boot).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEfron, B. 1979. „Bootstrap Methods: Another Look at the Jackknife“. The Annals of Statistics 7 (1). https://doi.org/10.1214/aos/1176344552.",
    "crumbs": [
      "Grundlagen",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "FixedEffects.html",
    "href": "FixedEffects.html",
    "title": "7  Panel-Daten",
    "section": "",
    "text": "7.1 Pooled Regression und unbeobachtbare Heterogenität\nEin Panel-Datensatz enthält Beobachtungen von \\(n\\) Einheiten für (bis zu) \\(T\\) Zeitpunkte, wobei \\(t=1,\\dots,T\\). Betrachte das Panel-Modell\nwobei \\(U_i\\) unbeobachtete und \\(X_i\\) beobachtete, zeitlich-invariante Heterogenitäten zwischen den Beobachtungseinheiten \\(i=1,\\dots,n\\) sind. Wie zuvor ist \\(B_{it}\\) die Behandlungsvariable und \\(\\beta_1\\) der (für alle Beobachtungseinheiten identische) interessierende kausale Effekt einer Veränderung von \\(B_{it}\\) auf \\(Y_{it}\\).\nAngenommen wir beobachten \\(Y_{it}\\) und \\(B_{it}\\) für \\(T=1\\), also für eine Periode. Bei Korrelation zwischen den unbeobachtbaren zeit-invarianten Effekten \\(U_i\\) und der Behandlungsvariable \\(B_{it}\\) kann der kausale Effekt \\(\\beta_1\\) nicht identifiziert werden. Diese Situation ist in Abbildung 7.1 dargestellt.\nFE_dag_single_period\nU\nUiX\nXiU-&gt;X\nB\nBt=1U-&gt;B\nY\nYt=1U-&gt;Y\nX-&gt;B\nX-&gt;Y\nB-&gt;Y\n\n\n\n\nAbbildung 7.1: Backdoors durch beobachtete und unbeobachtete Variablen\nAbbildung 7.1 zeigt Backdoors durch die \\(U_i\\), die wir mit einer “naiven” KQ-Schätzung der fehlspezifizierten Regression \\[\\begin{align}\n  Y_{it} = \\beta_0 + \\beta_1 B_{it} + \\beta_2 X_i + \\varepsilon_{it},\\quad t=1,\\label{eq:femodelfail}\n\\end{align}\\] mit \\(\\varepsilon_{it} = U_i + \\epsilon_{it}\\) nicht schließen können.1\nWir betrachten nun eine Generalisierung von Abbildung 7.1 für \\(T=2\\) Perioden, dargestellt in Abbildung 7.2.\nFE_dag2\nU\nUiX\nXiU-&gt;X\nB1\nBt=1U-&gt;B1\nY1\nYt=1U-&gt;Y1\nB2\nBt=2U-&gt;B2\nY2\nYt=2U-&gt;Y2\nX-&gt;B1\nX-&gt;Y1\nX-&gt;B2\nX-&gt;Y2\nB1-&gt;Y1\nB1-&gt;B2\nB2-&gt;Y2\n\n\n\n\nAbbildung 7.2: Panel-Design mit Backdoors durch beobachtete und unbeobachtete zeit-invariante Variablen\nDer in Abbildung 7.2 gezeigte Zusammenhang führt (idealerweise) zwar zu einer Verdoppelung des Beobachtungsumfangs, jedoch besteht weiterhin das in Abbildung 7.1 gezeigte Endogenitätsproblem, falls die Regression \\(\\eqref{eq:femodelfail}\\) nun anhand einer Zusammenlegung (Pooling) aller Beobachtungen für \\(t\\in\\{1,2\\}\\) geschätzt wird:2 Die unbeobachteten zeit-invarianten Einflüsse \\(U_i\\) verursachen auch für Periode \\(t=2\\) Backdoor-Pfade.\nIn diesem Kapitel betrachten wir Panel-Verfahren, welche den kausalen Effekt in Abbildung 7.2 und Verallgemeinerungen hiervon schätzen können. Bevor wir diese Methoden betrachten, veranschaulichen wir die verzerrte Schätzung eines Behandlungseffekts mit Pooled Regression in Modell \\(\\eqref{eq:femodelfail}\\) bei unbeobachtbaren Heterogenitäten für einen simulierten Datensatz paneldata.csv. Der Datensatz enthält Beobachtungen von \\(n=12\\) Einheiten zu \\(T=8\\) Perioden. Alle Einheiten weisen unbeobachtbare zeit-invariante Heterogenitäten auf, die mit \\(B_{it}\\) korellieren. Der wahre Behandlungseffekt beträgt \\(\\beta_1 = -1\\).3\nWir lesen zunächst den Datensatz ein und selektieren die benötigten Variablen.\n# Datensatz 'paneldata.csv' einlesen\npaneldata &lt;- read_csv(\"datasets/paneldata.csv\") %&gt;% \n  select(X, Y, ID, time, col)\n# Panel-Dimensionen bestimmen\npaneldata %&gt;%\n  summarise(\n    N = unique(ID) %&gt;% length(),\n    T = unique(time) %&gt;% length()\n  )\n\n# A tibble: 1 × 2\n      N     T\n  &lt;int&gt; &lt;int&gt;\n1    12     8\nMit der Funktion plm::is.pbalanced() überprüfen wir, ob im Panel-Datensatz für alle beobachteten Einheiten die gleiche Anzahl an Beobachtungsperioden vorliegt (balanced panel).4\nlibrary(plm)\n\n# Datensatz balanced?\nis.pbalanced(\n  x = paneldata, \n  index = c(\"ID\", \"time\")\n)\n\n[1] TRUE\nZunächst schätzen wir eine Pooled Regression für die ersten beiden Zeitperioden, basierend auf einem entsprechend gefilterten Datensatz.5\n# Subsetting der Daten für 2 Perioden (t = {1, 2})\npaneldata_T2 &lt;- paneldata %&gt;% \n  filter(\n    dplyr::between(time, 1, 2)\n  )\nFür die Schätzung von \\(\\eqref{eq:femodelfail}\\) nutzen wir fixest::feols().\nlibrary(fixest)\n\n# Naive KQ-Schätzung für t = {1, 2}\npanel_KQ &lt;- feols(\n  fml = Y ~ X, \n  data = paneldata_T2\n)\n\n# Statistische Zusammenfassung\nsummary(panel_KQ)\n\nOLS estimation, Dep. Var.: Y\nObservations: 24\nStandard-errors: IID \n            Estimate Std. Error  t value   Pr(&gt;|t|)    \n(Intercept) -2.50547   1.481150 -1.69157 1.0485e-01    \nX            3.80692   0.194202 19.60295 2.0252e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 3.28785   Adj. R2: 0.943388\nDie Schätzung von \\(\\beta_1\\) ist 3.81 und weist auf eine deutliche Verzerrung hin. Wir illustrieren die Problematik in Abbildung 7.3, in dem wir die für die Regression verwendeten Daten (Kreise) sowie die Beobachtungen späterer Perioden (Kreuze) nach Gruppenzugehörigkeit einfärben und die Schätzung der Pooled Regression abtragen.\nlibrary(cowplot)\n\n# Plot: Naiver KQ-Schätzer für t = 1, 2\nggplot(\n  mapping = aes(x = X, y = Y)\n) +\n  geom_point(\n    data = paneldata %&gt;% \n      filter(time &gt; 2),\n    mapping = aes(color = col),\n    pch = 3,\n    show.legend = F\n  ) +\n  geom_point(\n    data = paneldata %&gt;% \n      filter(time %in% 1:2),\n    mapping = aes(color = col),\n    show.legend = F\n  ) +\n  # Naive KQ-Schätzung für t = 1, 2\n  geom_smooth(\n    data = paneldata %&gt;% \n      filter(time %in% 1),\n    method = \"lm\", \n    se = F,\n    col = \"black\"\n  ) +\n  scale_color_identity() +\n  theme_cowplot()\n\n\n\n\n\n\nAbbildung 7.3: paneldaten.csv – Pooled Regression für t = 1, 2\nAbbildung 7.3 zeigt einen negativen Verlauf des Zusammenhangs zwischen X und Y anhand der Variation der Beobachtungen innerhalb der farblich gekennzeichneten Gruppen. Dieser negative Zusammenhang kann aufgrund der Endogenität von \\(X\\) nicht erfasst werden.\nEine Erweiturung der Regression auf sämtliche Perioden (Pooling aller \\(n\\times T = 12 \\times 8 = 96\\) Beobachtungen) erhöht lediglich die Präzision der Schätzung (geringerer Standardfehler von \\(\\widehat{\\beta}_1\\)), nicht aber die Endogenität, vgl. Abbildung 7.4.\n# Naive KQ-Schätzung für t = 1,...,8\npanel_KQ &lt;- feols(\n  fml = Y ~ X, \n  data = paneldata \n)\n\n# Statistische Zusammenfassung\nsummary(panel_KQ)\n\nOLS estimation, Dep. Var.: Y\nObservations: 96\nStandard-errors: IID \n            Estimate Std. Error  t value  Pr(&gt;|t|)    \n(Intercept) -1.45318   1.008859 -1.44042   0.15307    \nX            3.76470   0.134364 28.01868 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 4.63461   Adj. R2: 0.891928\n# Plot: Naiver KQ-Schätzer für t = 1,...,8\nggplot(\n  data = paneldata,\n  mapping = aes(x = X, y = Y)\n) +\n  geom_point(\n    mapping = aes(color = col),\n    show.legend = F\n  ) +\n  # Pooled Schätzung\n  geom_smooth(\n    data = paneldata,\n    method = \"lm\", \n    se = F,\n    col = \"black\"\n  ) +\n  scale_color_identity() +\n  theme_cowplot()\n\n\n\n\n\n\nAbbildung 7.4: paneldaten.csv – Pooled Regression für t = 1, …, 8",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Panel-Daten</span>"
    ]
  },
  {
    "objectID": "FixedEffects.html#sec-panel-uh",
    "href": "FixedEffects.html#sec-panel-uh",
    "title": "7  Panel-Daten",
    "section": "",
    "text": "\\[\\begin{align}\n  Y_{it} = \\beta_0 + \\beta_1 B_{it} + \\beta_2 X_i + \\beta_3 U_i + \\epsilon_{it},\\label{eq:unobshetmodel}\n\\end{align}\\]\n\n\n\n\n1 Wegen \\(E(\\varepsilon_{it}\\vert B_{it})\\neq 0\\) ist der KQ-Schätzer von \\(\\beta_1\\) nicht erwartungstreu und inkonsistent.\n\n\n2 Pooled Regression kann auch berechnet werden, wenn nicht für alle \\(n\\) Einheiten jeweils \\(T\\) Beobachtungen vorliegen (unbalanced Panel).3 Der (R code für den) DGP ist diesem StackExchange-Post entnommen.\n\n\n\n\n4 Fehlende Beobachtungen sind typischerweise mit einem NA-Wert gekennzeichnet. Der Differenz-Schätzer kann auch berechnet werden, wenn der Datensatz nicht ausgeglichen (unbalanced) ist.\n\n5 Wir verweisen nachfolgend explizit auf Funktionen aus dplyr, falls Funktionen aus plm identische Namen haben.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Panel-Daten</span>"
    ]
  },
  {
    "objectID": "FixedEffects.html#regression-in-differenzen",
    "href": "FixedEffects.html#regression-in-differenzen",
    "title": "7  Panel-Daten",
    "section": "\n7.2 Regression in Differenzen",
    "text": "7.2 Regression in Differenzen\nWir betrachten erneut den in Abbildung 7.2 dargestellten DGP für \\(T=2\\) Zeitperioden. In dieser Situation können Backdoors durch die \\(U_i\\) anhand einer simplen Transformation von Modell \\(\\eqref{eq:femodel}\\) geschlossen werden: Regression der Zeit-Differenzen zwischen den Perioden \\(t=2\\) und \\(t=1\\), \\[\\begin{align}\n  \\Delta Y_{it} = \\beta_1 \\Delta B_{it} + e_{it}, \\qquad i=1,\\dots,n,\\qquad t=1,2 \\label{eq:femodeldiff},\n\\end{align}\\] wobei \\(\\Delta Y_{it} := Y_{i2} - Y_{i1}\\) und \\(\\Delta e_{it} := \\epsilon_{i2} - \\epsilon_{i1}\\) für \\(t=2\\). Beachte, dass \\(\\Delta U_i=\\Delta X_i=0\\). Differenzieren der Komponenten führt zu einem Modell, in dem weder für (beobachtbare) \\(X_i\\) noch für (unbeobachtbare) \\(U_i\\) kontrolliert werden muss, damit \\(\\beta_1\\) identifiziert werden kann.6 Der Behandlungseffekt \\(\\beta_1\\) kann mit KQ geschätzt werden.7\n6 Ein Nachteil der Differenzbildung ist also, dass wir die Koeffizienten der beobachtbaren, zeitlich konstanten Regressoren nicht schätzen können.7 Der Differenzen-Schätzer ist erwartungstreu und konsistent, wenn \\(E(\\epsilon_{is}\\vert B_{it})=0\\) für \\(s\\geq t\\).Zur Transformation der Regressoren in fixest::feols() verwenden wir den Operator d(). Dieser benötigt die im Argument panel.id als Formel spezifizierten Identifikationsvariablen für Einheiten (ID) und Zeitpunkte (time).\n\n# Panel-Schätzer: KQ-Regression in Differenzen\npanel_diff &lt;- feols(\n  fml = d(Y) ~ d(X) - 1, \n  data = paneldata %&gt;% \n      filter(\n        dplyr::between(time, 1, 2)\n      ),\n  panel.id = ~ ID + time\n)\n\n# Statistische Zusammenfassung\nsummary(panel_diff)\n\nOLS estimation, Dep. Var.: d(Y, 1)\nObservations: 12\nStandard-errors: Clustered (ID) \n        Estimate Std. Error  t value  Pr(&gt;|t|)    \nd(X, 1) -1.03205   0.326838 -3.15767 0.0091166 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.907432   Adj. R2: 0.509923\n\n\nDie Schätzung anhand der Regression in Differenzen liegt nahe beim wahren Behandlungseffekt \\(\\beta_1 = -1\\). Abbildung Abbildung 7.5 zeigt die ersten Differenzen der Daten und den mit KQ geschätzten Zusammenhang.\n\n# Transformation zu Differenzen\npaneldata_diff &lt;- paneldata %&gt;% \n  mutate(\n    DeltaX = X - dplyr::lag(X),\n    DeltaY = Y - dplyr::lag(Y)\n  ) %&gt;%\n  drop_na()\n\n# Plot: KQ-Schätzer für Differenzen  \n  ggplot(\n    mapping = aes(x = DeltaX, y = DeltaY)\n  ) + \n    geom_point(\n      data = paneldata_diff %&gt;% \n        filter(time &gt; 2),\n      mapping = aes(color = col),\n      pch = 3,\n      show.legend = F\n    ) +\n    geom_point(\n      data = paneldata_diff %&gt;% \n        filter(time == 2),\n      mapping = aes(color = col),\n      show.legend = F\n    ) +\n  geom_smooth(\n    data = paneldata_diff %&gt;% \n      filter(time == 2),\n    method = \"lm\", \n    se = F,\n    color = \"black\"\n  ) + \n  scale_color_identity() +\n  theme_cowplot()\n\n\n\n\n\n\nAbbildung 7.5: paneldaten.csv – Regression in Differenzen für t = 2\n\n\n\n\nWie bei Pooling können wir den Differenzen-Schätzer für den gesamten Datensatz berechnen.\n\n# Panel-Schätzer: KQ-Regression in Differenzen\npanel_diff &lt;- feols(\n  fml = d(Y) ~ d(X) - 1, \n  data = paneldata,\n  panel.id = ~ ID + time\n)\n\n# Statistische Zusammenfassung\nsummary(panel_diff)\n\nOLS estimation, Dep. Var.: d(Y, 1)\nObservations: 84\nStandard-errors: Clustered (ID) \n         Estimate Std. Error  t value   Pr(&gt;|t|)    \nd(X, 1) -0.982042   0.135217 -7.26269 1.6178e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 1.12641   Adj. R2: 0.571235\n\n\nBeachte, dass der Standardfehler der Schätzers etwas größer ist als für den KQ-Schätzer in der Pooled Regression. Gründe hierfür sind der Verlust von \\(12\\) Beobachtungen bei der Bildung der \\(T-1 = 7\\) Differenzen und die standardmäßige Verwenudung von cluster-robusten Standardfehlern, siehe auch Kapitel 7.5.\n\n# Plot: KQ-Schätzer in Differenzen (alle t)\nggplot(\n  data = paneldata_diff,\n  mapping = aes(x = DeltaX, y = DeltaY)\n) + \n  geom_point(\n    color = \"gray\",\n    show.legend = F\n  ) +\n  geom_point(\n    mapping = aes(color = col),\n    show.legend = F\n  ) +\n  geom_smooth(\n    method = \"lm\", \n    se = F,\n    color = \"black\"\n  ) + \n  scale_color_identity() +\n  theme_cowplot()\n\n\n\n\n\n\nAbbildung 7.6: paneldaten.csv – Regression in Differenzen für t = 2, …, 8",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Panel-Daten</span>"
    ]
  },
  {
    "objectID": "FixedEffects.html#fixed-effects-regression",
    "href": "FixedEffects.html#fixed-effects-regression",
    "title": "7  Panel-Daten",
    "section": "\n7.3 Fixed-Effects-Regression",
    "text": "7.3 Fixed-Effects-Regression\nDie KQ-Schätzung der Regression in Differenzen hat den Nachteil, dass die Koeffizienten von einheiten-spezifischen Variablen nicht geschätzt werden können. Weiterhin impliziert die Differenzbildung einen Verlust des Beobachtungsumfangs bei der Schätzung des kausalen Effekts.8 Für den Datensatz paneldata.csv verlieren wir \\(1/8\\) der Stichprobe. Abhängig von der empirischen Fragestellung und der Datenverfügbarkeit (Verhältnis von \\(T\\) und \\(n\\)) kann Fixed-Effects-Regression eine nützliche Alternative zu Regression in Differenzen sein.\n8 Eine Reduktion des Beobachtungsumfangs erhöht die Varianz der Schätzung. Für \\(T=2\\) ist der Differenzen-Schätzer äquivalent zu den Schätzern im Fixed-Effects-Modell, ist jedoch ineffizient für \\(T&gt;2\\).9 Beachte, dass eine bessere Anpassung an die Daten bei der Modellierung von paneldata.csv mit einheiten-spezifischen Achsenabschnitten anhand von Abbildung 7.4 plausibel scheint.Wir betrachten erneut Modell \\(\\eqref{eq:unobshetmodel}\\) und definieren \\[\\begin{align*}\n  \\alpha_i = \\beta_0 + \\beta_3 U_i.\n\\end{align*}\\] Nach einsetzen in \\(\\eqref{eq:unobshetmodel}\\) erhalten wir das Modell \\[\\begin{align}\n  Y_{it} = \\alpha_i + \\beta_1 B_{it} + \\beta_2 X_i + \\epsilon_{it} \\label{eq:femodel},\n\\end{align}\\] mit einheiten-spezifischen Konstanten (“feste Effekte”) \\(\\alpha_i\\) für \\(i=1,\\dots,n\\), die als individuelle Achsenabschnitte nterpretieren werden können. Das Modell \\(\\eqref{eq:femodel}\\) wird daher auch als Fixed-Effects-Modell bezeichnet.9\n\n7.3.1 Within- und LSDV-Schätzer\nFür die Vermeidung von Backdoors durch die \\(\\alpha_i\\) subtrahieren wir die einheiten-spezifischen Mittelwerte von den Komponenten (Within-Transformation)10, \\[\\begin{align}\n  Y_{it} - \\overline{Y}_i =&\\, (\\alpha_i - \\overline{\\alpha}_i) + \\beta_1 (B_{it} - \\overline{B}_i) + \\beta_2 (X_i - \\overline{X}_i) + (\\epsilon_{it} - \\overline{\\epsilon}_i)\\notag\\\\\n  \\tilde Y_{it} =&\\, \\beta_1 \\tilde B_{it} + \\tilde\\epsilon_{it}.\\label{eq:fewithin}\n\\end{align}\\] Der Within-Schätzer von \\(\\beta_1\\) ist der KQ-Schätzer in \\(\\eqref{eq:fewithin}\\). Dieser Schätzer nutzt die Variabilität innerhalb der Beobachtungseinheiten über die Zeit, um die Koeffizienten der unabhängigen Variablen zu schätzen. Ähnlich wie für den Differenzen-Schätzer eliminieren die Mittelwert-Differenzen \\(\\alpha_i - \\overline{\\alpha}_i=0\\) und \\(X_i - \\overline{X}_i=0\\) den Einfluss zeit-invarianter Variablen.\n10 Die Durchschnitte werden hierbei also über die Zeitperioden berechnet.11 Für \\(n-1\\) Einheiten ist der individuelle Achsenabschnitt damit \\(\\beta_0 + \\gamma_i\\) und für eine Einheit \\(\\beta_0\\). Diese Einheit (hier \\(i=1\\)) wird auch als Referenzkategorie bezeichnet. Alternativ kann das Modell mit \\(n\\) Dummies und ohne die Konstante \\(\\beta_0\\) geschrieben werden.12 KQ ist hier erwartungstreu und konsistent, sofern \\(E(\\epsilon_{is}\\vert B_{it})=0\\) für alle \\(s\\) und \\(t\\).Das Modell \\(\\eqref{eq:femodel}\\) kann weiterhin als eine Regression mit \\(n-1\\) Dummy-Variablen und einer Konstante geschrieben werden, \\[\\begin{align}\n  Y_{it} = \\beta_0 + \\beta_2 B_{it} + \\beta_2 X_i  + \\gamma_2 D^{(2)}_i + \\gamma_3 D^{(3)}_i + \\cdots + \\gamma_n D^{(n)}_i + \\epsilon_{it} \\label{eq:drmodel}.\n\\end{align}\\] Die Darstellung \\(\\eqref{eq:drmodel}\\) hat \\(n\\) verschiedene Achsenabschnitte – einen für jede Beobachtungseinheit – und kann ebenfalls mit KQ geschätzt werden.11 Der KQ-Schätzer ergibt für die Modelle \\(\\eqref{eq:femodel}\\) und \\(\\eqref{eq:drmodel}\\) numerisch äquivalente Schätzungen von \\(\\beta_1\\), wenn \\(X_i\\) in der Dummy-Regression ausgelassen wird.12 Die Schätzung von \\(\\eqref{eq:drmodel}\\) mit KQ wird in der Literatur auch als Least Squares Dummy Variables (LSDV) Regression bezeichnet.\nBeachte, dass die Schätzung der Koeffizienten beobachtbarer zeitlich konstanter Regressoren wie \\(X_{i}\\) lediglich in Modell \\(\\eqref{eq:drmodel}\\) möglich ist.\nFixed-Effects-Regressionen können mit fixest::feols() geschätzt werden.13 Je nach Spezifikation des Formula-Arguments (fml) wird ein effizienter Algorithmus für die entsprechende Transformation von \\(\\eqref{eq:femodel}\\) angwandt. Für paneldata.csv erhalten wir mit fml = Y ~ X | ID per Referenz des Indikators ID eine Variante des Within-Schätzers.\n13 Eine Alternative ist plm::plm() mit dem Argument method = \"within\".\n# Fixed-Effects-Schätzung\npanel_FE &lt;- feols(\n  fml = Y ~ X | ID,  \n  data = paneldata\n)\n\n# Statistische Zusammenfassung\nsummary(panel_FE)\n\nOLS estimation, Dep. Var.: Y\nObservations: 96\nFixed-effects: ID: 12\nStandard-errors: Clustered (ID) \n  Estimate Std. Error  t value   Pr(&gt;|t|)    \nX -1.04507    0.10473 -9.97865 7.5526e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.746006     Adj. R2: 0.996829\n                 Within R2: 0.602694\n\n\nDie Zusammenfassung der Schätzung zeigt einen signifikanten Koeffizienten, der mit einer Schätzung von -1.05 nahe beim wahren Wert von \\(\\beta_1 = -1\\) liegt. Die geschätzten einheiten-spezifischen Effekte können mitfixest::fixef()\\ ausgelesen werden.\n\n# Geschätzte Fixed Effects (Einheiten) auslesen\nfixest::fixef(panel_FE)\n\n$ID\n        1         2         3         4         5         6         7         8 \n 3.814550  7.149097 12.716507 15.319130 23.458442 28.239634 33.659734 35.729516 \n        9        10        11        12 \n41.701817 47.911623 55.044052 59.445967 \n\nattr(,\"class\")\n[1] \"fixest.fixef\" \"list\"        \nattr(,\"exponential\")\n[1] FALSE\n\n\nMit fml = Y ~ X + factor(ID) erfolgt eine Schätzung der Dummy-Regression \\(\\eqref{eq:drmodel}\\) mit \\(n-1=11\\) Dummies. Der Referenzeinheit ist ID == 1. Wir sehen, dass der geschätzte Koeffizient von \\(X\\) mit dem Ergebnis des Within-Schätzers übereinstimmt.\n\n# LSDV-Schätzung\npanel_LSDV &lt;- feols(\n  fml = Y ~ X + factor(ID),  \n  data = paneldata\n)\n\n# Statistische Zusammenfassung\nsummary(panel_LSDV)\n\nOLS estimation, Dep. Var.: Y\nObservations: 96\nStandard-errors: IID \n             Estimate Std. Error   t value   Pr(&gt;|t|)    \n(Intercept)   3.81455   0.298907  12.76164  &lt; 2.2e-16 ***\nX            -1.04507   0.093136 -11.22082  &lt; 2.2e-16 ***\nfactor(ID)2   3.33455   0.427925   7.79236 1.6832e-11 ***\nfactor(ID)3   8.90196   0.446770  19.92516  &lt; 2.2e-16 ***\nfactor(ID)4  11.50458   0.487840  23.58267  &lt; 2.2e-16 ***\nfactor(ID)5  19.64389   0.552272  35.56923  &lt; 2.2e-16 ***\nfactor(ID)6  24.42508   0.601629  40.59826  &lt; 2.2e-16 ***\nfactor(ID)7  29.84518   0.719118  41.50248  &lt; 2.2e-16 ***\nfactor(ID)8  31.91497   0.753382  42.36225  &lt; 2.2e-16 ***\nfactor(ID)9  37.88727   0.833968  45.43012  &lt; 2.2e-16 ***\nfactor(ID)10 44.09707   0.910884  48.41127  &lt; 2.2e-16 ***\nfactor(ID)11 51.22950   1.049050  48.83416  &lt; 2.2e-16 ***\nfactor(ID)12 55.63142   1.128472  49.29801  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.746006   Adj. R2: 0.996829\n\n\n\n7.3.2 Zeit-Fixed-Effects\nNeben zeit-invarianten Heterogenitäten zwischen den Beobachtungseinheiten können beobachtbare und unbeobachtbare Einflüsse vorliegen, die nicht zwischen den Einheiten, jedoch über die Zeit variieren. Ein DGP mit solchen zeitabhängigen Heterogenitäten ist in Abbildung 7.7 für \\(T=2\\) dargestellt.14\n14 Zur Vereinfachung der Interpretierbarkeit vernachlässigt das DAG in Abbildung 7.7 zeitlich konstante Variablen.\n\n\n\n\nFE_dag3\nU1\nUt=1U2\nUt=2U1-&gt;U2\nX1\nXt=1U1-&gt;X1\nB1\nBt=1U1-&gt;B1\nY1\nYt=1U1-&gt;Y1\nX2\nXt=2U2-&gt;X2\nB2\nBt=2U2-&gt;B2\nY2\nYt=2U2-&gt;Y2\nX1-&gt;X2\nX1-&gt;B1\nX1-&gt;Y1\nX2-&gt;B2\nX2-&gt;Y2\nB1-&gt;Y1\nB1-&gt;B2\nB2-&gt;Y2\n\n\n\n\nAbbildung 7.7: Panel-Design mit Backdoors durch beobachtete und unbeobachtete zeitabhängige Variablen\n\n\n\n\nFür beobachtbare zeitabhängige Backdoor-Variablen \\(X_t\\) kann durch Aufnahme dieser in die Regression \\(\\eqref{eq:femodel}\\) kontrolliert werden. Analog zum Fixed-Effects-Ansatz mit einheiten-spezifischen Konstanten können Backdoors durch unbeobachtbare zeitabhängige Einflüsse \\(U_t\\) durch Kontrolle für perioden-spezifische Dummies \\(D_t^{(t)}\\) (Time Fixed Effects) vermieden werden. Das Modell lautet dann\n\\[\\begin{align*}\n  Y_{it} = \\beta_0 + \\beta_1 B_{it} + \\beta_2 X_t + \\lambda_2 D_t^{(2)} + \\cdots + \\lambda_T D_t^{(T)} + \\epsilon_{it}.\n\\end{align*}\\]\nIn empirischen Anwendung ist es häufig plausibel, für zeit- und einheiten-spezifische Effekte zu kontrollieren. Der entsprechende Regressionsansatz wird als Two Way Fixed Effects (TWFE) bezeichnet.\nEin TWFE-Modell für paneldata.csv kann mit feols() leicht unter Angabe der Identifikationsvariable für die Zeitperioden (time) innerhalb des fml-Arguments geschätzt werden.\n\n# TWFE-Schätzung\npanel_TWFE &lt;- feols(\n  fml = Y ~ X | ID + time,  \n  data = paneldata\n)\n\n# Statistische Zusammenfassung\nsummary(panel_TWFE)\n\nOLS estimation, Dep. Var.: Y\nObservations: 96\nFixed-effects: ID: 12,  time: 8\nStandard-errors: Clustered (ID) \n  Estimate Std. Error  t value   Pr(&gt;|t|)    \nX -1.01411   0.096129 -10.5495 4.3209e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.655695     Adj. R2: 0.997325\n                 Within R2: 0.636218\n\n\n\n# Geschätzte Fixed Effects (Einheiten + Zeit) auslesen\nfixest::fixef(panel_TWFE)\n\n$ID\n        1         2         3         4         5         6         7         8 \n 3.608414  6.893444 12.445006 15.020731 23.126151 27.884480 33.255238 35.311437 \n        9        10        11        12 \n41.252680 47.433690 54.515757 58.889276 \n\n$time\n          1           2           3           4           5           6 \n 0.00000000 -0.16836639  0.84624725  0.09109152  0.45843997 -0.38103581 \n          7           8 \n 0.24023573  0.31186366 \n\nattr(,\"class\")\n[1] \"fixest.fixef\" \"list\"        \nattr(,\"references\")\n  ID time \n   0    1 \nattr(,\"exponential\")\n[1] FALSE\n\n\n\n\n\n\n\n\nKey Facts zu Fixed-Effects-Regression\n\n\n\n\nEin Fixed-Effects-Designs betrachten unbeobachtete, mit den erklärenden Variablen korrelierte Heterogenitäten als konstante parameter. Korrigieren für diese Heterogenitäten ist Voraussetzung für eine verzerrungsfreie Schätzung kausaler Effekte.\n\nFixed-Effects-Schätzer schließen Backdoors aufgrund von Heterogenitäten zwischen Beobachtungseinheiten die über die Zeit konstant sind (einheiten-spezifische Effekte) und/oder für Heterogenitäten, die identisch für die Beobachtungseinheiten sind, jedoch über die Zeit variieren (Zeit-Effekte):\n\nKQ nach der Within-Transformation (Within-Schätzer) ist erwartungstreu und konsistent, solange die erklärenden Variablen zeitlich unkorreliert mit den Fehlertermen sind.\nLSDV-Regression ist eine Variante die Backdoors durch unbeobachtbare Heterogenitäten mit Dummy-Variablen schließt. In einer LSDV-Regression können die Koeffizienten zeitlich konstanter Variablen geschätzt werden.\n\n\nStatistische Inferenz für Fixed-Effects-Schätzer erfolgt anhand einer Approximation der asymptotischen Normalverteilung. Da die Fehlerterme heteroskedastisch und/oder über die Zeit korreliert sein können, sollten Cluster-robuste Standardfehler verwendet werden, vgl. Kapitel 7.5.\nFixed-Effects-Modelle können in R mit dem Paketen fixest oder plm berechnet werden.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Panel-Daten</span>"
    ]
  },
  {
    "objectID": "FixedEffects.html#random-effects",
    "href": "FixedEffects.html#random-effects",
    "title": "7  Panel-Daten",
    "section": "\n7.4 Random Effects",
    "text": "7.4 Random Effects\nDer Fixed-Effects-Ansatz behandelt die einheiten-spezifischen Effekte \\(\\alpha_i\\) in Modell \\(\\eqref{eq:femodel}\\) als konstante Parameter, für die korrigiert oder kontrolliert werden muss. Der Random-Effects-Ansatz betrachtet die \\(\\alpha_i\\) als Zufallsvariablen mit einer identischen Verteilung, unter der Annahme, dass die \\(\\alpha_i\\) nicht mit den erklärenden Variablen korellieren.15 Falls diese Annahmen erfüllt sind, ist der Random-Random-Effects-Schätzer effizienter als der Fixed-Effects-Schätzer: Der mittlere quadratische Fehler der Schätzung ist geringer.16\n15 Erwartungstreue und Konsistenz und Effizienz erfordern \\(E(\\alpha_i\\vert B_{it})=0\\) und \\(E(\\epsilon_{it}\\vert\\alpha_i,B_{it})=0\\).16 Die Schätzung erfolgt meist mit der Generalized Least Squares (GLS) oder mit Maximum-Likelihood (ML).Das einfache Random-Effects-Modell notieren wir als \\[\\begin{align*}\n  Y_{it} =&\\, \\beta_0 + \\beta_1 B_{it} + \\varepsilon_{it},\n\\end{align*}\\] wobei sich der Fehlerterm \\(\\varepsilon_{it}\\) aus dem zufälligen individuellen Effekt \\(\\alpha_i\\) und dem unabhängigen Fehler \\(\\epsilon_{it}\\) zusammensetzt, \\[\\begin{align*}\n  \\varepsilon_{it} = \\alpha_i + \\epsilon_{it}.\n\\end{align*}\\]\nFür ein Beispiel simulieren wir Daten gemäß der Vorschrift \\[\\begin{align}\n  Y_{it} = \\alpha_i + \\beta B_{it} + \\epsilon_{it}\\label{eq:resim}\n\\end{align}\\] und wählen \\[\\begin{align*}\n  & \\alpha_i \\overset{u.i.v}{\\sim} N(0,2.5^2), \\\\\n  & \\beta_1 = -1,\\\\\n  & B_{it} \\sim N(0,1),\\\\\n  & \\epsilon_{it} \\overset{u.i.v}{\\sim} N(0,0.75^2).\n\\end{align*}\\] Wie in paneldata.csv erzeugen wir Daten für \\(n=12\\) Individuen, die zu \\(T=8\\) Zeitperioden beobachtet werden. Mit diesen Komponenten wird die Outcome-Variable \\(Y_{it}\\) wie in \\(\\eqref{eq:resim}\\) generiert. Der nachstehende Code erzeugt die Daten als Matrizen B und Y, die anschließend in ein langes Datenformat (tibble) umgewandelt werden.\n\nlibrary(plm)\n\nset.seed(1234)\n\n# Parameter\nn &lt;- 12  # Individuen\nm &lt;- 8   # Perioden\nbeta &lt;- -1 # Behandlungseffekt\nsigma_alpha &lt;- 2.5 # SD für RE\nsigma_epsilon &lt;- 0.75 # SD für Fehler\n\n# Random Effects\nalpha &lt;- rnorm(n, mean = 0, sd = sigma_alpha)\n\n# Matrizen\nB &lt;- matrix(NA, nrow = m, ncol = n)\nY &lt;- matrix(NA, nrow = m, ncol = n)\n\n# Simulation\nfor (i in 1:n) {\n  for (t in 1:m) {\n    B[t, i] &lt;- rnorm(1, mean = 0, sd = 1)\n    epsilon_it &lt;- rnorm(1, mean = 0, sd = sigma_epsilon)\n    Y[t, i] &lt;- alpha[i] + beta * B[t, i] + epsilon_it\n  }\n}\n\n# tibble erzeugen\nRE_paneldata &lt;- tibble(\n  id = factor(rep(1:n, each = m)),\n  time = rep(1:m, times = n),\n  B = as.vector(B),\n  Y = as.vector(Y)\n)\n\nFür die Anpassung des Random-Effect-Schätzers an den simulierten Datensatz RE_paneldata nutzen wir plm::plm() mit model = \"random\". Mit effect = \"individual\" legen wir einheiten-spezifische Random Effects fest.17\n17 Analog zu Fixed-Effects-Modellen können mit effect = \"twoway\" einheiten- und zeit-spezifische zufällige Effekte modelliert werden.\n# Random-Effects-Modell anpassen\npanel_RE &lt;- plm(\n  formula = Y ~ B, \n  data = RE_paneldata, \n  index = c(\"id\", \"time\"),\n  effect = \"individual\",\n  model = \"random\"\n)\n\n# Statistische Zusammenfassung\nsummary(panel_RE)\n\nOneway (individual) effect Random Effect Model \n   (Swamy-Arora's transformation)\n\nCall:\nplm(formula = Y ~ B, data = RE_paneldata, effect = \"individual\", \n    model = \"random\", index = c(\"id\", \"time\"))\n\nBalanced Panel: n = 12, T = 8, N = 96\n\nEffects:\n                 var std.dev share\nidiosyncratic 0.5221  0.7226  0.13\nindividual    3.4801  1.8655  0.87\ntheta: 0.8643\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-2.019439 -0.351116  0.017785  0.420719  2.109010 \n\nCoefficients:\n             Estimate Std. Error  z-value Pr(&gt;|z|)    \n(Intercept) -1.156573   0.552747  -2.0924   0.0364 *  \nB           -0.996175   0.075859 -13.1320   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    143.86\nResidual Sum of Squares: 50.753\nR-Squared:      0.64721\nAdj. R-Squared: 0.64346\nChisq: 172.45 on 1 DF, p-value: &lt; 2.22e-16\n\n\nDie Schätzung zeigt eine gute Anpassung an die Daten und der geschätzte Behandlungseffekt liegt mit -0.996 nahe am wahren Koeffizienten \\(\\beta_1 = -1\\).\nMit plm::ranef() erhalten wir Differenzen der geschätzten einheiten-spezifischen Effekte vom geschätzten Erwartungswert (Intercept).\n\n# Gesch. Random Effects auslesen:\n# Differenzen zur Konstante (Intercept)\nranef(panel_RE)\n\n          1           2           3           4           5           6 \n-1.92217199  1.24364147  3.27470045 -4.12524574  2.00235610  2.71903482 \n          7           8           9          10          11          12 \n-0.17492965 -0.50237866  0.01435925 -1.03252985  0.16092971 -1.65776590 \n\n\nFür die grafische Darstellung der Schätzung berechnen wir zunächst mit fitted() die angepassten Werte. Für ein mit plm() geschätztes Random-Effects-Modell werden die individuellen Effekte von fitted() nicht berücksichtigt und müssen daher manuell hinzugefügt werden. Hierfür lesen wir zunächst den geschätzten Erwartungswert der gemeinsamen Verteilung \\(\\widehat{\\beta}_0\\) aus und addieren anschließend die von ranef() ausgegebenen Differenzen der individuellen Effekte gruppenweise.\n\n# Intercept\nhat_beta_0 &lt;- panel_RE$coefficients[1]\n\n# Gesch. Random Effects jeweils berücksichtigen\nRE_paneldata &lt;- RE_paneldata %&gt;%\n  mutate(\n    fitted_RE = \n      fitted(panel_RE) \n    + hat_beta_0 \n    + rep(ranef(panel_RE), each = m)\n  )\n\n\n# Daten und geschätztes RE-Modell plotten\nggplot(\n  data = RE_paneldata, \n  mapping = \n    aes(\n      x = B, \n      y = Y, \n      col = id\n    )\n) + \n  geom_point(show.legend = F) +\n  geom_line(\n    mapping = aes(y = fitted_RE, group = id), \n    show.legend = F\n  ) +\n  theme_cowplot()\n\n\n\n\n\n\nAbbildung 7.8: RE_paneldata – Random-Effects-Schätzung für simulierte Daten\n\n\n\n\n\n7.4.1 Verzerrung bei Endogenität\nDie Random-Effects-Methode findet bei kausalen Analyse in empirischen Anwendungen selten Anwendung, weil die Annahme von Unkorreliertheit mit den erklärenden Regressoren oft unplausibel ist. Falls diese Annahme (wie in den DAGs Abbildung 7.1 und Abbildung 7.2) verletzt ist, kann Random Effects die ensprechenden Backdoors nicht schließen: Der Random-Effects-Schätzer ist dann verzerrt und inkonsistent, ähnlich wie der naive KQ-Schätzer in einer Pooled Regression. Im nächsten Abschnitt untersuchen wir Konsequenzen dieser Eigenschaft anhand simulierter Daten.\nZur Illustration der Verzerrung des Random-Effects-Schätzers bei Endogenität von erklärenden Variablen verwenden wir erneut den anhand eines Fixed-Effects-DGPs simulierten Datensatz paneldata.csv.\n\n# Random-Effects-Schätzung für `paneldata`\npanel_RE &lt;- plm(\n  formula = Y ~ X, \n  effect = \"individual\",\n  model = \"random\",\n  index = c(\"ID\", \"time\"),  \n  data = paneldata\n)\n\n# Statistische Zusammenfassung\nsummary(panel_RE)\n\nOneway (individual) effect Random Effect Model \n   (Swamy-Arora's transformation)\n\nCall:\nplm(formula = Y ~ X, data = paneldata, effect = \"individual\", \n    model = \"random\", index = c(\"ID\", \"time\"))\n\nBalanced Panel: n = 12, T = 8, N = 96\n\nEffects:\n                 var std.dev share\nidiosyncratic 0.6437  0.8023 0.229\nindividual    2.1732  1.4742 0.771\ntheta: 0.811\n\nResiduals:\n   Min. 1st Qu.  Median 3rd Qu.    Max. \n-6.0355 -1.9888 -0.1651  1.9196  6.4374 \n\nCoefficients:\n            Estimate Std. Error z-value  Pr(&gt;|z|)    \n(Intercept) 18.34586    2.31566  7.9225 2.328e-15 ***\nX            0.77031    0.26346  2.9238  0.003458 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    818.14\nResidual Sum of Squares: 749.94\nR-Squared:      0.083362\nAdj. R-Squared: 0.07361\nChisq: 8.54862 on 1 DF, p-value: 0.0034579\n\n\nOffenbar weicht der Random-Effects-Schätzer des Effects von \\(X\\) auf \\(Y\\) deutlich vom wahren Parameter \\(\\beta_1 = -1\\) ab. Diese Abweichung ist auf die Endogenität von \\(X\\) zurückzuführen.\nDie nächste Grafik vergleicht die Schätzungen des Behandlungseffekts für den Datensatz paneldata.csv mit Pooling (schwarze Linie), Fixed Effects (farbige Linien) und Random Effects (gestrichelte schwarze Linie). Vorab erweitern wir paneldata um die angepassten Werte für die Fixed- und die Random-Effects-Schätzung in panel_FE und panel_RE.18\n18 Für bessere Lesbarkeit erzeugen wir hier mit predict() eine Regressionsgerade, deren Achsenabschnitt dem geschätzten Erwartungswert der Random-Effects-Verteilung entspricht.\npaneldata &lt;- paneldata %&gt;% \n  mutate(\n    # Vorhergesagte Werte für FE-Schätzung\n    yhat_FE = fitted(panel_FE),\n    # Vorhergesagte Werte für RE-Schätzung\n    yhat_RE = predict(panel_RE)\n  )\n\n\nggplot(\n  data = paneldata,\n  mapping = aes(x = X, y = Y)\n) +\n  # Beoachtungen\n  geom_point(\n    mapping = aes(color = col), \n    show.legend = F\n  ) +\n  # Pooling\n  geom_smooth(\n    method = \"lm\", \n    se = F,\n    col = \"black\"\n  ) +\n  # Fixed Effects\n  geom_line(\n    mapping = aes(\n      y = yhat_FE,\n      group = ID,\n      col = col\n    ), \n    show.legend = F\n  ) +\n  # Random Effects\n  geom_line(\n    mapping = aes(y = yhat_RE), \n    lty = \"dashed\", \n    show.legend = F\n  ) +\n  theme_cowplot()\n\n\n\n\n\n\nAbbildung 7.9: paneldaten.csv – Vergleich von Schätzern bei Einheiten mit endogeneen Effekten\n\n\n\n\n\n\n\n\n\n\nKey Facts zu Random-Effects-Regression\n\n\n\n\nRandom-Effects-Ansätze basieren auf der Annahme, dass (unbeobachtbare) Heterogenitäten zufällig sind. Wir nehmen an, dass die unbeobachteten Heterogenitäten nicht mit den erklärenden Variablen korreliert sind. Letzteres führt zu inkonsistenten Schätzern!\nIteratives GLS oder MLE ermöglichen eine effizientere Schätzung des Random-Effects-Modells als Pooling oder Fixed-Effects-Schätzer, da sowohl die Variation innerhalb als auch zwischen den Beoabchtungseinheiten Einheiten genutzt wird.\nIn Random-Effects-Modellen können die Effekte zeitlich konstanter Variablen geschätzt werden, da die einheiten-spezifischen Effekte als zufällig betrachtet werden.\nUnter den skizzierten Annahmen sind Random-Effects-Schätzer asymptotisch normalverteilt. Für Inferenzstatistiken sollten cluster-robuste Standardfehler verwendet werden, vgl. Kapitel 7.5.\nRandom-Effects-Modelle können in R mit den Paketen plm geschätzt werden.\n\n\n\n\nInteraktive Illustration von Panel-Schätzern\nDie nachfolgende interaktive Illustration erlaubt einen Vergleich der in Abbildung 7.9 geplotteten geschätzten Regressionsfunktionen für paneldata.csv mit dem wahren DGP (Show truth) unter Reproduktion der in Abbildung 7.9 gezeigten Komponenten.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Panel-Daten</span>"
    ]
  },
  {
    "objectID": "FixedEffects.html#sec-crse",
    "href": "FixedEffects.html#sec-crse",
    "title": "7  Panel-Daten",
    "section": "\n7.5 Cluster-robuste Standardfehler",
    "text": "7.5 Cluster-robuste Standardfehler\nAufgrund der Zeit-Dimension bei Panel-Datensätzen ist es in vielen empirischen Anwendungen plausibel, dass die Fehlerterme der bisher betrachteten Modelle nicht unabhängig sind: Beobachtungen einer Einheit (auch Cluster genannt), wie z.B. Individuen, Firmen oder geografische Regionen sind oftmals über die Zeit oft korreliert. Solche Abhängigkeit zwischen dem Fehlern führen zu verzerrten Standardfehlern, wenn diese anhand von Formeln berechnet werden, die unter der Annahme von u.i.v. Fehlertermen hergeleitet wurden: Standardfehler für Schätzer, die u.i.v. Fehler annehmen, können die tatsächliche Varianz unterschätzen, was zu falsch-positiven19 Ergebnissen bei Signifikanztests für die Modell-Koeffizienten und damit ungültiger Inferenz hinsichtlich kausaler Effekte führen kann.\n19 Ein falsch-positiver Test zeigt einen “positiven” Zustand (hier ein von null verschiedener Koeffizient) an, obwohl dieser tatsächlich nicht vorliegt.Sogenannte Cluster-robuste Standardfehler korrigieren für zeitliche Korrelation und Heteroskedatizität und ermöglichen konservative Schätzungen hinsichtlich der Variabilität von Koeffizientenschätzern. fixest::feols() berechnet standardmäßig Cluster-robuste Standardfehler, wenn Einheiten- oder Perioden-Variablen innerhalb der Formel (bspw. Y ~ X | ID + time) oder separat im Argument index (etwa panel.id = ~ ID + time) angegeben werden. Für die Identifikation der Cluster wird stets die zuerst genannte Variable (hier ID) herangezogen. Details der zu verwendenen Standardfehler können über das Argument vcov festgelegt werden, siehe ?feols für weitere Hinweise hierzu.\nFür den LSDV-Schätzer in Kapitel 7.3.1 erreichen wir clustering nach ID mit vcov = ~ ID.\n\n# LSDV mit cluster-robusten Standardfehlern\npanel_LSDV_clust &lt;- feols(\n  fml = Y ~ X + factor(ID),  \n  data = paneldata,\n  vcov = ~ ID\n)\n\npanel_LSDV_clust\n\nOLS estimation, Dep. Var.: Y\nObservations: 96\nStandard-errors: Clustered (ID) \n             Estimate Std. Error  t value   Pr(&gt;|t|)    \n(Intercept)   3.81455   0.112794 33.81859 1.8081e-12 ***\nX            -1.04507   0.111454 -9.37663 1.4011e-06 ***\nfactor(ID)2   3.33455   0.178290 18.70292 1.0959e-09 ***\nfactor(ID)3   8.90196   0.235355 37.82357 5.3305e-13 ***\nfactor(ID)4  11.50458   0.332206 34.63086 1.3958e-12 ***\nfactor(ID)5  19.64389   0.454236 43.24597 1.2323e-13 ***\nfactor(ID)6  24.42508   0.536553 45.52218 7.0294e-14 ***\nfactor(ID)7  29.84518   0.714216 41.78733 1.7935e-13 ***\nfactor(ID)8  31.91497   0.763123 41.82152 1.7776e-13 ***\nfactor(ID)9  37.88727   0.874952 43.30213 1.2149e-13 ***\nfactor(ID)10 44.09707   0.978638 45.05965 7.8608e-14 ***\nfactor(ID)11 51.22950   1.159967 44.16462 9.7910e-14 ***\nfactor(ID)12 55.63142   1.262214 44.07448 1.0012e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.746006   Adj. R2: 0.996829\n\n\nBeachte, dass die Standradfehler der meisten Koeffizienten in panel_LSDV_clust etwas größer sind, als für das in Kapitel 7.3.1 geschätzte Modell panel_LSDV.\nFür Schätzungen mitplm::plm() werden von summary() grundsätzlich Standardfehler unter Annahme homoskedastischer u.i.v. Fehler berechnet. Für Cluster-robuste Standardfehler muss ein entsprechender Schätzer im Argument vcov = vcovHC() übergeben werden. Mit cluster = \"group\" wird Korrelation innerhalb der Beobachtungseinheiten über die Zeit berücksichtigt.20 Für die Random-Effects-Schätzung in panel_RE erreichen wir dies wiefolgt.\n20 Clustering für Zeitperioden erfolgt mit group = time.\n# Zusammenfassung RE-Schätzung mit clustered-SEs\nsummary(\n  object = panel_RE, \n  vcov = vcovHC(\n    x = panel_RE, \n    type = \"HC1\", \n    cluster = \"group\"\n  )\n)\n\nOneway (individual) effect Random Effect Model \n   (Swamy-Arora's transformation)\n\nNote: Coefficient variance-covariance matrix supplied: vcovHC(x = panel_RE, type = \"HC1\", cluster = \"group\")\n\nCall:\nplm(formula = Y ~ X, data = paneldata, effect = \"individual\", \n    model = \"random\", index = c(\"ID\", \"time\"))\n\nBalanced Panel: n = 12, T = 8, N = 96\n\nEffects:\n                 var std.dev share\nidiosyncratic 0.6437  0.8023 0.229\nindividual    2.1732  1.4742 0.771\ntheta: 0.811\n\nResiduals:\n   Min. 1st Qu.  Median 3rd Qu.    Max. \n-6.0355 -1.9888 -0.1651  1.9196  6.4374 \n\nCoefficients:\n            Estimate Std. Error z-value Pr(&gt;|z|)    \n(Intercept) 18.34586    4.53330  4.0469 5.19e-05 ***\nX            0.77031    0.46177  1.6682  0.09529 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    818.14\nResidual Sum of Squares: 749.94\nR-Squared:      0.083362\nAdj. R-Squared: 0.07361\nChisq: 2.78274 on 1 DF, p-value: 0.095285",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Panel-Daten</span>"
    ]
  },
  {
    "objectID": "FixedEffects.html#dynamische-modelle",
    "href": "FixedEffects.html#dynamische-modelle",
    "title": "7  Panel-Daten",
    "section": "\n7.6 Dynamische Modelle",
    "text": "7.6 Dynamische Modelle\nViele ökonomische und soziale Prozesse sind autoregressiv: Der Zustand einer Variable in der Vergangenheit beeinflusst ihren aktuellen Zustand. Bei der Modellierung von Outcome-Variablen in Panel-Designs kann es notwendig sein, diese Abhängigkeit zu berücksichtigen, um die Identifizierbarkeit kausaler Effekte zu gewährleisten. Dynamische Panelmodelle verwenden hierzu vergangene Werte (lags) der Outcome-Variable, \\(Y_{it-j}\\) mit \\(j&gt;0\\) als (zusätzliche) Regressoren. Ein einfaches dynamisches Panelmodell ist \\[\\begin{align}\n  Y_{it} = \\rho Y_{it-1} + \\beta_1 B_{it} + \\epsilon_{it}, \\label{eq:dynpanel}\n\\end{align}\\] wobei \\(Y_{it-1}\\) der Wert von \\(Y_{it}\\) in der Periode \\(t-1\\) ist, \\(0\\neq\\lvert\\rho\\rvert&lt;1\\), und \\(\\epsilon_{i,t}\\) ein u.i.v. Fehlerterm ist. In diesem Modell ist \\(Y_{it-1}\\) kausal für \\(Y_{it}\\).\nDie Verwendung dynamischer Modelle hat folgende Motivationen:\n\n\nPräzisere Schätzung: Berücksichtigen von gelaggten abhängigen Variablen ermöglicht die Modellierung einer zeitabhängigen Dynamik von \\(Y_{it}\\). Die Autokorellation und damit die Varianz der Fehlerterme kann reduziert werden, da ein Teil dieser zeitlichen Abhängigkeiten direkt modelliert wird. Dies kann die Präzision der Schätzung des kausalen Effekts \\(\\beta_1\\) verbessern, vgl. Abbildung 7.10.\n\n\n\n\n\n\nLDVOK2\nBt\nBtYt\nYtBt-&gt;Yt\nYtm1\nYt-1Ytm1-&gt;Yt\n\n\n\n\nAbbildung 7.10: Kontrolle für vergangenen Wert verbessert Präzision\n\n\n\n\n\n\nVermeidung von Endogenität: Nichtberücksichtigen von relevanten gelaggten abhängigen Variablen führt zu verzerrten und inkonsistenten Schätzern, vgl. Abbildung 7.11.\n\n\n\n\n\n\nLDVOK1\nBt\nBtYt\nYtBt-&gt;Yt\nV\nVV-&gt;Bt\nYtm1\nYt-1V-&gt;Ytm1\nYtm1-&gt;Yt\n\n\n\n\nAbbildung 7.11: Kontrolle für \\(y_{t-1}\\) schließt Backdoor-Pfad\n\n\n\n\nDie Entscheidung, ob Lags der Outcome-Variable als Regressoren aufgenommen werden, sollte sorgfältig und, falls möglich, unter Berücksichtigung von ökonomischer Theorie hinsichtlich der zeitlichen Dynamik von \\(Y_{it}\\) erfolgen. Beobachtete (Auto)Korrelation von \\(Y_{it}\\) und \\(Y_{it-1}\\) muss nicht ausschließlich durch einen (kausalen) autoregressiven Zusammenhang verursacht werden. Ein DGP bei dem unsere Regression nicht für \\(Y_{it-1}\\) kontrollieren sollte, ist in Abbildung 7.12 gezeigt.\n\n\n\n\n\nLDVnotOK\nBt\nBtYt\nYtBt-&gt;Yt\nV\nVV-&gt;Bt\nYtm1\nYt-1V-&gt;Ytm1\nU\nUU-&gt;Yt\nU-&gt;Ytm1\n\n\n\n\nAbbildung 7.12: Kontrolle für \\(y_{t-1}\\) öffnet Backdoor-Pfad\n\n\n\n\nHier ist \\(Y_{it-1}\\) nicht kausal für \\(Y_{it}\\) und ein Collider. Kontrollieren für \\(Y_{it-1}\\) öffnet also den Backdoor-Pfad\n\\[\\begin{align*}\n  B_{it} \\leftarrow V \\rightarrow Y_{it-1} \\leftarrow U \\rightarrow Y_{it}.\n\\end{align*}\\]\nEine ähnliche Problematik besteht bei der Schätzung dynamischer Panel-Modelle mit mit Fixed- oder Random-Effects-Ansätzen. Wir erläutern dies näher in Kapitel 7.6.1.\n\n7.6.1 Verzerrung in dynamischen Modellen\nEin zentrales Problem bei der Schätzung dynamischer Panelmodelle ist der sogenannte Nickell-Bias. Dieser tritt auf, wenn die Within-Transformation in Anwesenheit von gelagten abhängigen Variablen verwendet wird. Die Mittelwert-Differenzen eliminieren zwar die zeit-invarianten Effekte, führt aber zu Korrelation zwischen den gelagten abhängigen Variablen und den transformierten Fehlertermen: Der Nickell-Bias entsteht bei der Within-Transformation, weil der Regressor (\\(Y_{i,t-1} - \\overline{Y}_{i,-1}\\))21 mit dem transformierten Fehlerterm (\\(\\epsilon_{it} - \\overline{\\epsilon}_i\\)) korreliert ist, was zu einer verzerrten Schätzung führt.\n21 Hier ist \\(\\overline{Y}_{i,-1} = \\frac{1}{T-1}\\sum_{t=2}^T y_{it-1}\\).22 Dies ist problematisch für Mikro-Studien: Die Querschnittsdimension des Panels kann oft “hinreichend” groß gewählt werden. Die Zeit-Dimension ist aus natürlichen Gegebenheiten jedoch oft klein.Nickell (1981) zeigt, dass diese Verzerrung des Within-Schätzers \\(\\widehat{\\rho}^\\textup{Within}\\) nicht verschwindet, wenn die Anzahl der Beobachtungseinheiten (\\(n\\)) divergiert, solange die Zeitdimension (\\(T\\)) endlich ist.22 Der Nickell-Bias ist besonders bei kurzen Panelen (kleines \\(T\\)) problematisch. Es ist \\[\\begin{align*}\n  \\widehat{\\rho}^\\textup{Within} \\approx \\rho - \\frac{1 + \\rho}{T-1},\n\\end{align*}\\] d.h. die Verzerrung beträgt ungefähr \\(-\\frac{1+\\rho}{T-1}\\).\nHinzufügen von \\(Y_{it-1}\\) als Regressor in Fixed- und Random-Effects-Modellen ist jenseits der verzerrten Schätzung von \\(\\rho\\) problematisch, wenn \\(Y_{it-1}\\) mit \\(B_{it}\\) korreliert ist. Kontrollieren für \\(Y_{it-1}\\) öffnet dann Backdoor-Pfade, die wir mit FE- oder RE-Ansätzen schließen wollen. In sämtlichen dynamischen Varianten der Fixed- und Random-Effects-Modelle sind die in diesem Kapitel behandelten Schätzer eines kausalen Effekts \\(\\beta_1 \\neq 0\\) von \\(B_{it}\\) auf \\(Y_{it}\\) daher verzerrt.\nDer Arellano-Bond-Schätzer (Arellano und Bond 1991) ist eine Methode, für diese Form von Endogenität in dynamischen Panel-Modellen korrigiert. Das Verfahren betrachtet die Regression in Differenzen zur Korrektur für Heterogenitäten zwischen den Einheiten und schätzt die Koeffizienten anhand der generalisierten Momentenmethode (GMM). Hierbei werden vergangene Werte von \\(Y_{it}\\) als Instrumente für endogene Differenzen von \\(Y_{it}\\) genutzt. Siehe Wooldridge (2010) für Beispiele.\nAnalog zu Anwendungen mit Querschnittsdaten können die hier betrachteten Panel-Schätzer Endogenitäten aufgrund simultaner Kausalität nicht beheben. Zur Korrektur für simultante Kausalität können Panel-Methoden in Kombination mit einer Schätzstrategie basierend auf Instrument-Variablen hilfreich sein. Wir betrachten solche Schätzer in den empirischen Beispielen in Kapitel Kapitel 8.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Panel-Daten</span>"
    ]
  },
  {
    "objectID": "FixedEffects.html#case-study-einkommen-und-demokratie",
    "href": "FixedEffects.html#case-study-einkommen-und-demokratie",
    "title": "7  Panel-Daten",
    "section": "\n7.7 Case Study: Einkommen und Demokratie",
    "text": "7.7 Case Study: Einkommen und Demokratie\nEine Vielzahl polit-ökonomischer Standardwerke und Studien (bspw. Dahl 1971; Huntington 1991; Rueschemeyer, Stephens, und Stephens 1992) liefert vermeindliche Belege für einen zentralen Grundsatz der Modernisierungstheorie: Ein höheres Pro-Kopf-Einkommen erhöht die Nachfrage der Bevölkerung nach politischer Freiheit und demokratischen Insitutionen. Acemoglu u. a. (2008a) argumentieren, dass der in derartigen länderübergreifenden Analysen mit Pooling häufig als positiv geschätzte Zusammenhang zwischen Einkommen und Demokratisierung nicht kausal interpetiert werden sollte. Ein Grund hierfür ist, dass (unbeobachtbare) ausgelassene länderspezifische Faktoren, die sowohl die ökonomische Entwicklung als auch die Stärke demokratischer Institutionen beeinflussen, wahrscheinlich sind. Um diese mögliche Ursache für Endogenität des Einkommens im Modell \\[\\begin{align}\n  \\text{Demokratisierung}_{it} = \\beta_0 + \\beta_1\\,\\text{PK-Einkommen}_{it-1} + \\epsilon_{it}\n\\end{align}\\] zu adressieren, nutzen Acemoglu u. a. (2008a) Panel-Modelle und Schätzer, die insbesondere für länderspezifische zeit-invariante Einflüsse kontrollieren.\nDas Kernergebnis von Acemoglu u. a. (2008a) ist, dass es keinen kausalen Zusammenhang zwischen dem Einkommen (Wirtschaftswachstum) und der Demokratisierung gibt. Die Autoren zeigen, dass historische und geografische Faktoren, die sowohl das Einkommen als auch die politischen Institutionen beeinflussen, den vermeintlichen Zusammenhang erklären können.\nFür die nachfolgenden Code-Beispiele nutzen wir einen Auszug des Datensatzes aus dem Replikationspaket zu Acemoglu u. a. (2008a), siehe Acemoglu u. a. (2008b).\nWir lesen zunächst den Datensatz acemogluetal2008.csv ein.\n\ndeminc &lt;- read_csv(\"datasets/acemogluetal2008.csv\")\nglimpse(deminc)\n\nRows: 2,321\nColumns: 8\n$ fhpolrigaug  &lt;dbl&gt; NA, NA, NA, NA, 0.50, NA, NA, NA, NA, 1.00, 1.00, 0.12, 0…\n$ lrgdpch      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ country      &lt;chr&gt; \"Andorra\", \"Andorra\", \"Andorra\", \"Andorra\", \"Andorra\", \"A…\n$ year         &lt;dbl&gt; 1950, 1955, 1960, 1965, 1970, 1975, 1980, 1985, 1990, 199…\n$ year_numeric &lt;dbl&gt; 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 31, 32, 33, 3…\n$ sample       &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, …\n$ code         &lt;chr&gt; \"ADO\", \"ADO\", \"ADO\", \"ADO\", \"ADO\", \"ADO\", \"ADO\", \"ADO\", \"…\n$ polity4      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 0.00, 0.00, 0…\n\n\nTabelle 7.1 enthält Beschreibungen der in deminc verfügbaren Variablen.\n\n\n\n\n\n\n\n\nVariable\nBeschreibung\n\n\n\ncountry\nLand\n\n\ncode\nLänder-Code\n\n\nfhpolrigaug\nFreedom House Political Rights Index (FHPRI)\n\n\nlrgdpch\nLog(Reales-Pro-Kopf-Einkommen)\n\n\npolity4\nPolity Composite Democracy Index\n\n\nsample\nZugehörigkeit zur verwendeten Stichprobe (Indikator)\n\n\nyear\nJahr\n\n\nyear_numeric\nID-Variable f. Jahr\n\n\n\n\n\nTabelle 7.1: deminc: Demokratisierung und Einkommen (Acemoglu u. a. 2008a)\n\n\nWie der output von glimpse(deminc) zeigt, enthält der Datensatz in allen Modell-Variablen NA-Einträge und ist damit nicht ‘balanced’, da die entsprechenden Beobachtungen nicht bei der Schätzung berücksichigt werden können.\n\n# Bereinigter Datensatz ist nicht 'balanced'\nis.pbalanced(\n  x = deminc %&gt;% \n    drop_na(), \n  index = c(\"country\", \"year\")   \n)\n\n[1] FALSE\n\n\nWir vernachlässigen zunächst die Panel-Struktur der Daten und regressieren FHPRI auf das Pro-Kopf-Einkommen für die in Acemoglu u. a. (2008a) verwendete Stichprobe mit Beobachtungen in 5-Jahresschritten von 1955 bis 2000 (Beobachtungen mit sample == 1).\n\n# Pooled Regression\ndeminc_pooling1 &lt;- feols(\n  fml = fhpolrigaug ~ lrgdpch,\n  data = deminc %&gt;% \n    filter(sample == 1), \n)\n\nsummary(deminc_pooling1)\n\nOLS estimation, Dep. Var.: fhpolrigaug\nObservations: 960\nStandard-errors: IID \n             Estimate Std. Error  t value  Pr(&gt;|t|)    \n(Intercept) -1.339442   0.068767 -19.4780 &lt; 2.2e-16 ***\nlrgdpch      0.231010   0.008271  27.9287 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.268295   Adj. R2: 0.448221\n\n\nDieser naive Ansatz ingoriert konstante Heterogenitäten zwischen den Ländern und einen (plausiblen) zeitlich verzögerten Einfluss ökonomisch günstiger Bedingung auf die Demokratisierung. Der geschätzte Koeffizient von lrgdpch ist 0.23 (d.h. ein geschätzer positiver Zusammenhang) und signifikant.\nDie Regression von FHPRI auf das Einkommen in \\(t-1\\) (l(lrgdpch)) führt zu ähnlichen Schätzungen der Koeffizienten.23 Dies ist plausibel unter der Hypothese, dass es ausgelassende Faktoren gibt, welche die Demokratisierung und das Pro-Kopf-Einkommen in sämtlichen Perioden beeinflussion.\n23 Die Verwendung von Lags mit l() in fml erfordert die Angabe von panel.id = ~ country + year, damit die Beobachtungen zugeordnet werden können.\n# Pooling mit Einkommem_{t-1}\ndeminc_pooling_lag &lt;- feols(\n  fml = fhpolrigaug ~ l(lrgdpch),\n  panel.id = ~ country + year,\n  data = deminc %&gt;% \n    filter(sample == 1), \n)\n\nsummary(deminc_pooling_lag)\n\nOLS estimation, Dep. Var.: fhpolrigaug\nObservations: 831\nStandard-errors: Clustered (country) \n               Estimate Std. Error  t value  Pr(&gt;|t|)    \n(Intercept)   -1.412158   0.108044 -13.0702 &lt; 2.2e-16 ***\nl(lrgdpch, 1)  0.240773   0.012568  19.1582 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.270376   Adj. R2: 0.456289\n\n\nEin simpler Ansatz zur Kontrolle für fixe länderspezifische Effekte ist die KQ-Schätzung des Regressionsmodell in ersten Differenzen. Analog zu Abbildung 2 in Acemoglu u. a. (2008a) schätzen wir hierfür zunächst ein Modell der Differenzen zwischen den Perioden 1995 und 1970. Zur Schätzung und anschließenden Reproduktion der Grafik (siehe Abbildung 7.13) berechnen wir die Differenzen anhand eines gruppierten Datensatzes.\n\n# Zeit-Differenzen länderweise berechnen\ndeminc_f &lt;- deminc %&gt;%\n  filter(\n    year %in% c(1970, 1995)\n  ) %&gt;%\n  group_by(code) %&gt;%\n  summarise(\n    dlrgdpch = diff(lrgdpch),\n    dfhpolrigaug = diff(fhpolrigaug)\n    ) %&gt;%\n  drop_na()\n\n# Überblick\nglimpse(deminc_f)\n\nRows: 103\nColumns: 3\n$ code         &lt;chr&gt; \"ARG\", \"AUS\", \"AUT\", \"BDI\", \"BEL\", \"BEN\", \"BFA\", \"BGD\", \"…\n$ dlrgdpch     &lt;dbl&gt; 0.102619, 0.408201, 0.632017, -0.201549, 0.543716, -0.026…\n$ dfhpolrigaug &lt;dbl&gt; 0.6666666, 0.0000000, 0.0000000, 0.1666667, 0.0000000, 0.…\n\n\n\n# KQ-Schätzung für Differenzen 1995 - 1970\ndeminc_diff_7095 &lt;- feols(\n  fml = dfhpolrigaug ~ dlrgdpch,\n  data = deminc_f\n) \n\ndeminc_diff_7095 %&gt;% \n  summary()\n\nOLS estimation, Dep. Var.: dfhpolrigaug\nObservations: 103\nStandard-errors: IID \n            Estimate Std. Error  t value  Pr(&gt;|t|)    \n(Intercept) 0.126292   0.040080 3.150983 0.0021415 ** \ndlrgdpch    0.032981   0.063541 0.519045 0.6048649    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.338838   Adj. R2: -0.007214\n\n\nDie Ergebnisse in deminc_diff_7095 passen zu unserer Vermutung hinsichtlich ausgelassener länderspezifischer Heterogenitäten: Der geschätzte Koeffizient von dlrgdpch ist 0.03 und nicht signifikant von 0 verschieden.\n\nggplot(\n  data = deminc_f,\n  mapping = aes(\n    x = dlrgdpch,\n    y = dfhpolrigaug\n  ) \n  ) +\n  # Referenz-Linie (kein Effekt)\n  geom_hline(yintercept = 0, lty = 2) +\n  # Datenpunkte\n  geom_text(\n    mapping = aes(label = code),\n    position = position_jitter(\n      height = .05, \n      seed = 1234\n      )\n    ) +\n  # gesch. lineares Modell einzeichnen\n  geom_smooth(\n    method = \"lm\", \n    se = F) +\n  labs(\n    x = \"Diff. Log(Pro-Kopf-BIP) (1970 - 1995)\",\n    y = \"Diff. Demokratie-Index (1970 - 1995)\"\n  ) +\n  theme_cowplot()\n\n\n\n\n\n\nAbbildung 7.13: deminc – Regression der Zeit-Differenzen zwischen 1995 und 1970\n\n\n\n\nFür die KQ-Schätzung in ersten Differenzen bei Verwendung des gesamten Datensatzes ist das Ergebis noch deutlicher: Die Schätzungen beider Koeffizienten sind klein und insignifikant.\n\n# Panel-Schätzer: KQ-Regression in Differenzen\n# (Alle Perioden)\ndeminc_diff_mod &lt;- feols(\n  fml = d(fhpolrigaug) ~ d(lrgdpch), \n  data = deminc,\n  panel.id = ~ country + year\n)\n\nsummary(deminc_diff_mod)\n\nOLS estimation, Dep. Var.: d(fhpolrigaug, 1)\nObservations: 968\nStandard-errors: Clustered (country) \n               Estimate Std. Error   t value Pr(&gt;|t|) \n(Intercept)    0.005878   0.006013  0.977540  0.32992 \nd(lrgdpch, 1) -0.021539   0.040164 -0.536278  0.59258 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.213629   Adj. R2: -7.634e-4\n\n\nTabelle 2 in Acemoglu u. a. (2008a) vergleicht verschiedene Schätzer des dynamischen Modells\n\\[\\begin{align}\n  \\text{FHPRI}_{it} = \\beta_0 + \\alpha_i + \\lambda_t + \\beta_1 \\text{FHPRI}_{it-1} + \\beta_2 \\text{Einkommen}_{it-1}  + \\varepsilon_{it}\\label{eq:acemmod}\n\\end{align}\\]\nmit Länder- und Zeit-Effekten \\(\\alpha_i + \\lambda_t\\). Der Regressor \\text{FHPRI}_{it-1} soll die “Beständigkeit” der Demokratie zu erfassen und möglicherweise kurz- bis mittelfristige Dynamiken zu berücksichtigen (d.h. die Tendenz des Demokratie-Scores, zu einem Gleichgewichtswert zurückzukehren). Wir betrachten zunächst die Fixed-Effects-Schätzung von \\(\\eqref{eq:acemmod}\\).\n\n# Zeit + country Fixed Effects\ndeminc_FE_mod &lt;- feols(\n  fml = fhpolrigaug ~ \n    l(fhpolrigaug) \n  + l(lrgdpch) \n  | year + country,\n  panel.id = ~ country + year,\n  cluster = ~ country,\n  data = deminc, \n)\n\nsummary(deminc_FE_mod)\n\nOLS estimation, Dep. Var.: fhpolrigaug\nObservations: 988\nFixed-effects: year: 10,  country: 150\nStandard-errors: Clustered (country) \n                  Estimate Std. Error  t value   Pr(&gt;|t|)    \nl(fhpolrigaug, 1) 0.388762   0.048986 7.936204 4.6184e-13 ***\nl(lrgdpch, 1)     0.001137   0.031503 0.036088 9.7126e-01    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.165328     Adj. R2: 0.751359\n                 Within R2: 0.146859\n\n\nÄhnlich wie in der Differenzen-Regression deminc_diff_mod finden wir bei Kontrolle für Länder- und Zeit-Effekte keine Evidenz für einen Zusammenhang des Pro-Kopf-Einkommens (der Vorperiode) und dem Demokratie-Score. Mit fixef(deminc_FE_mod) lesen wir die gschätzten Effekte aus. Aufgrund der Vielzahl an Länder-Effekten empfiehlt sich eine grafischer Zusammenfassung anhand eines Histogramms mit absoluten Häufigkeiten.\n\n# Verteilung der gesch. Länder-Fixed-Effects\ntibble(\n    FE = fixef(deminc_FE_mod)$country, \n    country = names(fixef(deminc_FE_mod)$country)\n    ) %&gt;%\n\nggplot(aes(x = FE)) +\n  geom_histogram(\n    bins = 20,\n    fill = \"steelblue\",\n    color = \"white\"\n  ) +\n  theme_cowplot() +\n  coord_cartesian(expand = F)\n\n\n\n\n\n\nAbbildung 7.14: deminc – Verteilung von Länder-Fixed-Effects\n\n\n\n\nEine Random-Effects-Schätzung mit Länder- und Zeit-Effekten liefert ebenfalls keine Evidenz für einen kausalen Effekt des ökonomischen Wohlstands auf die Demokratisierung.\n\n# Datensatz für plm() formatieren\ndeminc_pdf &lt;- pdata.frame(\n  deminc %&gt;% filter(sample == 1) %&gt;% \n    drop_na(), \n  index = c(\"country\", \"year_numeric\")   \n)\n\n# Random-Effects-Modell\ndeminc_RE_mod &lt;- plm(\n  formula = fhpolrigaug ~ \n    lag(fhpolrigaug, 1) \n  + lag(lrgdpch, 1), \n  effect = \"twoways\",\n  model = \"random\",\n  data = deminc_pdf\n)\n\nsummary(deminc_RE_mod)\n\nTwoways effects Random Effect Model \n   (Swamy-Arora's transformation)\n\nCall:\nplm(formula = fhpolrigaug ~ lag(fhpolrigaug, 1) + lag(lrgdpch, \n    1), data = deminc_pdf, effect = \"twoways\", model = \"random\")\n\nUnbalanced Panel: n = 120, T = 1-8, N = 729\n\nEffects:\n                    var   std.dev share\nidiosyncratic 0.0333234 0.1825470 0.972\nindividual    0.0000000 0.0000000 0.000\ntime          0.0009577 0.0309460 0.028\ntheta:\n           Min.   1st Qu.    Median      Mean   3rd Qu.      Max.\nid    0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\ntime  0.3740041 0.4781926 0.4900268 0.4776474 0.5010907 0.5028656\ntotal 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n\nResiduals:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.88781 -0.09657  0.01010  0.00192  0.08642  0.79340 \n\nCoefficients:\n                     Estimate Std. Error z-value  Pr(&gt;|z|)    \n(Intercept)         -0.433112   0.365330 -1.1855    0.2358    \nlag(fhpolrigaug, 1)  0.697492   0.144153  4.8386 1.308e-06 ***\nlag(lrgdpch, 1)      0.073560   0.050011  1.4709    0.1413    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    98.912\nResidual Sum of Squares: 28.164\nR-Squared:      0.7153\nAdj. R-Squared: 0.71451\nChisq: 68.4006 on 2 DF, p-value: 1.4028e-15\n\n\nUm für Endogenität der Regressoren aufgrund des dynamischen Modells zu korrigieren, verwenden Acemoglu u. a. (2008a) weiterhin den Arellano-Bond-Schätzer. Dieser wird auf die Differenz-Transformation\n\\[\\begin{align}\n  \\Delta\\text{FHPRI}_{it} = \\beta_1 \\Delta\\text{FHPRI}_{it-1} + \\beta_2 \\Delta\\text{Einkommen}_{it-1} + \\Delta\\epsilon_{it}, \\quad t\\geq3,\\label{eq:acemabmod}\n\\end{align}\\]\nvon Modell \\(\\eqref{eq:acemmod}\\) angewendet und nutzt, dass \\(\\text{FHPRI}_{it-j}\\) und \\(\\text{Einkommen}_{it-j}\\) für \\(j\\geq2\\) nicht mit \\(\\Delta\\varepsilon_{it}\\) korelliert sind und damit als Instrumente in einem GMM-Ansatz verwendet werden können.24\n24 Damit die Lags gültige Instrumente sind, darf \\(\\epsilon_{it}\\) nicht seriell korelliert sein.Der Arellano-Bond-Schätzer ist in der Funktion plm::pgmm() implementiert. Wir schätzen Modell \\(\\eqref{eq:acemabmod}\\) mit den zweiten und dritten Lags von \\(\\text{FHPRI}_{it}\\) und \\(\\text{Einkommen}_{it}\\) als Instrumente. Die Spezifikation erfolgt durch den Zusatz | lag(fhpolrigaug, 2:3) + lag(lrgdpch, 2:3) im Argument formula.\n\n# Arellano-Bond-Schätzer\ndeminc_AB_mod &lt;- pgmm(\n  formula = \n    fhpolrigaug ~ \n    lag(fhpolrigaug, 1) \n  + lag(lrgdpch, 1) \n  # Instrumente:\n  | lag(fhpolrigaug, 2:3)\n  + lag(lrgdpch, 2:3),\n    effect = \"twoways\", \n    model = \"twosteps\",\n    data = deminc_pdf\n) \n\nsummary(deminc_AB_mod)\n\nTwoways effects Two-steps model Difference GMM \n\nCall:\npgmm(formula = fhpolrigaug ~ lag(fhpolrigaug, 1) + lag(lrgdpch, \n    1) | lag(fhpolrigaug, 2:3) + lag(lrgdpch, 2:3), data = deminc_pdf, \n    effect = \"twoways\", model = \"twosteps\")\n\nUnbalanced Panel: n = 138, T = 1-9, N = 881\n\nNumber of Observations Used: 598\nResiduals:\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-1.280670 -0.031088  0.000000  0.002335  0.009490  0.993065 \n\nCoefficients:\n                    Estimate Std. Error z-value  Pr(&gt;|z|)    \nlag(fhpolrigaug, 1)  0.50943    0.11696  4.3555 1.327e-05 ***\nlag(lrgdpch, 1)     -0.17873    0.12092 -1.4781    0.1394    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSargan test: chisq(24) = 29.03412 (p-value = 0.21885)\nAutocorrelation test (1): normal = -3.828848 (p-value = 0.00012874)\nAutocorrelation test (2): normal = 0.474211 (p-value = 0.63535)\nWald test for coefficients: chisq(2) = 21.49026 (p-value = 2.155e-05)\nWald test for time dummies: chisq(7) = 18.08866 (p-value = 0.011576)\n\n\nAuch der Arellano-Bond-Schätzer liefert keine Evidenz für die Theorie eines positiven kausalen Effekts des Einkommens fürdie Stärke demokratischer Institutionen: Der interessierende geschätzte Koeffizient ist negativ und insignifikant.\nWeiterhin bekräftigen zusätzliche Inferenzstatistiken im Output von summary(deminc_AB_mod) die Adäquanz des GMM-Verfahrens:\n\nSargan test: Die Nullhypothese des Sargan-Tests25 ist, dass die Instrumente gültig, d.h. unkorreliert mit den Fehlertermen, sind. Diese Hypothese können wir zu keinem in der Praxis relevanten Signifikanzniveau ablehnen.\nAutocorrelation test (1): Test der Nullhypothese, dass die \\(\\Delta\\epsilon_{it}\\) keine AR(1)-Korellationsstruktur aufweisen. Für unkorrelierte \\(\\epsilon_{it}\\) sind die ersten Differenzen \\(\\Delta\\epsilon_{it}\\) AR(1)-korreliert. Tatsächlich können wir diese Nullhypothese ablehnen.\nAutocorrelation test (2): Test der Nullhypothese, dass die \\(\\Delta\\epsilon_{it}\\) keine AR(2)-Korellationsstruktur aufweisen. Dies ist relevant für die Gültigkeit des Arellano-Bond-Schätzers, da AR-Korellation zweiter (oder höherer) Ordnung darauf hinweisen würde, dass die verwendeten Instrumente ungültig sind.\nWald test for coefficients: Wir lehnen die Nullhypothese, dass \\(\\Delta\\text{FHPRI}_{it-1}\\) und \\(\\Delta\\text{Einkommen}_{it-1}\\) keine Erklärtungskraft für \\(\\Delta\\text{FHPRI}_{it}\\) haben, ab. Unter der Alternativ-Hypothese ist mindesten ein Koeffizient von null verschieden. Hierunter fällt das Szenario, dass \\(\\Delta\\text{FHPRI}_{it-1}\\) zeitliche Dynamik in der Outcome-Variable erklärt, jedoch \\(\\Delta\\text{Einkommen}_{it-1}\\) irrelevant ist (kein kausaler Effekt).\nWald test for time dummies: Die Nullhyothese, dass die Zeit-Fixed-Effects irrelevant sind, wird zum 5%-Niveau abgelehnt.\n\n25 Auch Sargan-Hansen-Test genannt (nach Sargan 1958; Hansen 1982).Mit modelsummary::modelsummary() stellen wir die Koeffizientenschätzungen tabellarisch dar. Für eine bessere Lesbarkeit formatieren wir die (andernfalls aus dem jeweiligen formula/fml-Argument übernommenen) Bezeichnungen der Koeffizienten anhand eines bennanten Vektors coef_map.\n\nlibrary(modelsummary)\n\n# Mapping für Koeffizienten festlegen\ncoef_map &lt;- c(\n  \"l(lrgdpch, 1)\" = \"Einkommen_{t-1}\",\n  \"lag(lrgdpch, 1)\" = \"Einkommen_{t-1}\",\n  \"l(fhpolrigaug, 1)\" = \"FHPRI_{t-1}\",\n  \"lag(fhpolrigaug, 1)\" = \"FHPRI_{t-1}\"\n)\n\n# Tabellarische Übersicht erzeugen\nmodelsummary(\n  models = list(\n    \"Pool\" = deminc_pooling_lag,\n    \"FE\" = deminc_FE_mod,\n    \"RE\" = deminc_RE_mod,\n    \"AB\" = deminc_AB_mod\n  ),\n  coef_omit = \"Intercept\",\n  coef_map = coef_map,\n  gof_omit = \"^(?!$).*\", \n  output = \"gt\",\n  stars = TRUE\n) %&gt;%\n  tabopts()\n\n\n\n\n\n\n\n\nPool\nFE\nRE\nAB\n\n\n\nEinkommen_{t-1}\n0.241***\n0.001\n0.074\n-0.179+\n\n\n\n(0.013)\n(0.032)\n(0.050)\n(0.098)\n\n\nFHPRI_{t-1}\n\n0.389***\n0.697***\n0.509***\n\n\n\n\n(0.049)\n(0.144)\n(0.070)\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\nTabelle 7.2: Schätzungen des Effekts von Einkommen auf Demokratisierung (Acemoglu u. a. 2008a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAcemoglu, Daron, Simon Johnson, James A. Robinson, und Pierre Yared. 2008a. „Income and Democracy“. American Economic Review 98 (3): 808–42. https://doi.org/10.1257/aer.98.3.808.\n\n\n———. 2008b. „Replication data for: Income and Democracy“. ICPSR - Interuniversity Consortium for Political; Social Research. https://doi.org/10.3886/E113251V1.\n\n\nArellano, Manuel, und Stephen Bond. 1991. „Some Tests of Specification for Panel Data: Monte Carlo Evidence and an Application to Employment Equations“. The Review of Economic Studies 58 (2): 277. https://doi.org/10.2307/2297968.\n\n\nDahl, Robert Alan. 1971. Polyarchy: Participation and Opposition: Participation and opposition. New Haven: Yale Univ. Press.\n\n\nHansen, Lars Peter. 1982. „Large Sample Properties of Generalized Method of Moments Estimators“. Econometrica 50 (4): 1029. https://doi.org/10.2307/1912775.\n\n\nHuntington, Samuel P. 1991. The Third Wave: Democratization in the Late Twentieth Century: Democratization in the late twentieth century. Norman, OK: Univ. of Oklahoma Press.\n\n\nNickell, Stephen. 1981. „Biases in Dynamic Models with Fixed Effects“. Econometrica 49 (6): 1417. https://doi.org/10.2307/1911408.\n\n\nRueschemeyer, Dietrich, Evelyne H. Stephens, und John D. Stephens. 1992. Capitalist development and democracy. Cambridge: Polity Pr.\n\n\nSargan, J. D. 1958. „The Estimation of Economic Relationships using Instrumental Variables“. Econometrica 26 (3): 393. https://doi.org/10.2307/1907619.\n\n\nWooldridge, Jeffrey. 2010. Econometric Analysis of Cross Section and Panel Data. Second edition. Cambridge, Massachusetts: MIT.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Panel-Daten</span>"
    ]
  },
  {
    "objectID": "IV.html",
    "href": "IV.html",
    "title": "8  IV-Regression",
    "section": "",
    "text": "8.1 Der einfache lineare IV-Schätzer\nIm folgenden nehmen wir an, dass sämtliche Zusammenhänge linear sind. Zunächst betrachten wir das einfache Regressionsmodell\n\\[\\begin{align}\n  Y_i = \\beta_0 + \\beta_1 B_i + u_i \\ \\ , \\ \\ i=1,\\dots,n, \\label{eq:simpleiv}\n\\end{align}\\]\nwobei der Fehlerterm \\(u_i\\) mit dem Regressor \\(B_i\\) korreliert ist (d.h. \\(B\\) ist ein endogener Regressor), sodass der KQ-Schätzer für den kausalen Effekt \\(\\beta_1\\) inkonsistent ist.\nDamit \\(Z\\) ein gültiges Instrument für \\(B\\) in dem in Abbildung 8.1 gezeigten Forschungsdesign ist, müssen die folgenden Bedingungen erfüllt sein:\nSei \\(\\text{Cov}(A,B)\\) die Kovarianz zwischen den Variablen \\(A\\) und \\(B\\). \\(Z\\) muss zwei Bedingungen erfüllen, um ein gültiges Instrument zu sein:\nUnter diesen Annahmen erlaubt das Forschungsdesign die Anwendung des einfachsten IV-Ansatzes, wobei eine endogene Variable \\(B\\) durch eine Instrumentvariable \\(Z\\) instrumentiert wird. Die folgende Umformung zeigt, warum der kausale Effekt von B auf Y anhand der (Ko)Variation in diesen Variablen identifiziert werden kann:\n\\[\\begin{alignat*}{2}\n  \\textup{Cov}(Z,Y) &= \\textup{Cov}(Z,\\beta_0 + \\beta_1 B + u) &\\quad& \\text{(Gl. \\eqref{eq:simpleiv})} \\\\\n  \\\\\n  \\textup{Cov}(Z,Y) &= \\textup{Cov}(Z, \\beta_1 B + u) &\\quad& \\text{($\\beta_0$ konstant)} \\\\\n  \\\\\n  \\textup{Cov}(Z,Y) &= \\beta_1\\textup{Cov}(Z,B) &\\quad& \\text{($\\beta_1$ konstant, $Z$ exogen)} \\\\\n  \\\\\n  \\beta_1 &= \\frac{\\textup{Cov}(Z,Y) }{\\textup{Cov}(Z,B)} &\\quad& \\text{($Z$ relevant)}\n\\end{alignat*}\\]\nEine naheliegende Implementierung gemäß dieses Identifikationsprinzips ist der einfache IV-Schätzer\n\\[\\begin{align}\n  \\widehat{\\beta}_{\\textup{IV}} = \\frac{\\widehat{\\textup{Cov}}(Z,Y) }{\\widehat{\\textup{Cov}}(Z,B)}, \\label{eq:simpleivest}\n\\end{align}\\]\nwobei lediglich die Kovarianzfunktion \\(\\textup{Cov}(\\cdot,\\cdot)\\) durch ihr Stichprobenäquivalent \\(\\widehat{\\textup{Cov}}(\\cdot,\\cdot)\\) ersetzt wird.\nErwartungswert und Konsistenz\nEine hilfreiche Darstellung von \\(\\eqref{eq:simpleivest}\\) ist\n\\[\\begin{align*}\n  \\widehat{\\beta}_\\textup{IV} = \\beta_1 + \\frac{\\sum_{i=1}^n (Z_i-\\overline Z) u_i}{\\sum_{i=1}^n (Z_i - \\overline{Z})B_i}.\n\\end{align*}\\]\nAnhand dieser Form kann der Erwartungswert sowie das Verhalten für große Stichproben untersucht werden. Eine wichtige Eigenschaft ist\n\\[\\begin{align}\n  \\textup{E}\\big(\\widehat{\\beta}_\\textup{IV}\\big) = \\beta_1 + \\underbrace{\\textup{E}\\bigg(\\frac{\\sum_{i=1}^n (Z_i-\\overline Z) u_i}{\\sum_{i=1}^n (Z_i - \\overline{Z})B_i}\\bigg)}_{\\neq0},\\label{eq:ivbiasterm}\n\\end{align}\\]\nsodass \\(\\widehat{\\beta}_\\textup{IV}\\) bei Endogenität von \\(X\\) ein verzerrter Schätzer von \\(\\beta_1\\) ist.1 Glücklicherweise kann man zeigen, dass\n\\[\\begin{align*}\n  \\frac{\\sum_{i=1}^n (Z_i-\\overline Z) u_i}{\\sum_{i=1}^n (Z_i - \\overline{Z})B_i}\\to 0 \\quad \\text{für} \\quad n\\to\\infty\n\\end{align*}\\]\nbei Gültigkeit der IV-Annahmen \\(\\eqref{eq:ivassum1}\\) und \\(\\eqref{eq:ivassum2}\\), d.h. \\(\\widehat{\\beta}_\\textup{IV}\\) ist ein konsistener Schätzer für \\(\\beta_1\\),\n\\[\\begin{align*}\n  \\widehat{\\beta}_\\textup{IV} \\to \\beta_1 \\quad \\text{für} \\quad n\\to\\infty.\n\\end{align*}\\]\nSchwache Instrumente\nBeachte, dass die Annahme der Relevanz des Instruments für die Herleitung des Identifikationsprinzips und des Schätzers \\(\\eqref{eq:simpleivest}\\) von entscheidender Bedeutung ist: Sind \\(Z\\) und \\(B\\) unkorreliert, so ist \\(\\text{Cov}(B,Z) = 0\\) und \\(\\beta_1\\) damit nicht identifizierbar. Weiterhin würde \\(\\widehat{\\text{Cov}}(B,Z)\\to0\\) wenn \\(n\\to\\infty\\), sodass \\(\\widehat{\\beta}_{\\textup{IV}}\\to\\infty\\)!\nIn empirischen Anwendungen ist \\(\\text{Cov}(B,Z) = 0\\) unwahrscheinlich. Die Kovarianz von \\(B\\) und \\(Z\\) kann jedoch gering sein. In diesem Fall bezeichnet man \\(Z\\) als ein schwaches Instrument. Anhand von Gleichung \\(\\eqref{eq:simpleivest}\\) können wir die Konsequenzen der Verwendung eines schachen Instruments ableiten: Ein sehr schwacher linearer Zusammenhang von \\(B\\) und \\(Z\\) — und damit tendenziell \\(\\widehat{\\text{Cov}}(B,Z) \\approx 0\\) — kann trotz Relevanz von \\(Z\\) zu einem Schätzer mit großer Varianz führen.\nDie Relevanz eines Instruments kann mit einem Hypothesentests geprüft werden. Wie die “Stärke” eines Instruments beurteilt werden sollte, ist eine nicht-triviale Frage. Wir betrachten diesen Aspekt näher in Kapitel 8.2.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IV-Regression</span>"
    ]
  },
  {
    "objectID": "IV.html#der-einfache-lineare-iv-schätzer",
    "href": "IV.html#der-einfache-lineare-iv-schätzer",
    "title": "8  IV-Regression",
    "section": "",
    "text": "1. Relevanz des Instruments für die endogene Variable\n\\(B\\) und \\(Z\\) müssen korreliert sein (Pfeil von Z nach B in Abbildung 8.1): \\[\\begin{align}\n    \\text{Cov}(B,Z) \\neq 0 \\label{eq:ivassum1}\n  \\end{align}\\]\n\n\n2. Exogenität des Instruments hinsichtlich der Outcome-Variable\nDas Instrument \\(Z\\) darf nicht mit dem Fehlerterm \\(u\\) in der Modellgleichung \\(\\eqref{eq:simpleiv}\\) korreliert sein (keine Pfade von Z nach Y außer durch B in Abbildung 8.1): \\[\\begin{align}\n    \\text{Cov}(Z,u) = 0 \\label{eq:ivassum2}\n  \\end{align}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 Beachte, dass der endogene Regressor \\(B_i\\) mit \\(u_i\\) korreliert, sodass der Erwartungswert des Bruchs in \\(\\eqref{eq:ivbiasterm}\\) ungleich \\(0\\) ist.\n\n\n\n\n\n\n\n8.1.1 Identifizierbarer Behandlungseffekt\nRegression ermöglicht in der Regel die Schätzung eines durchschnittlichen Behandlungseffekts (ATE) für eine betrachtete Population mit heterogenen Behandlungseffekten (in Modell \\(\\eqref{eq:simpleiv}\\) unterstellen wir den empirisch unwahrscheinlichen Fall eines einheitlichen kausalen Effekts \\(\\beta_1\\)). Das Adjustieren für Endogenität mit IV-Regression hat hier einen Preis: Beachte, dass im IV-Design ein Teil der beobachteten Variation der Behandlungsvariable \\(B\\) (wie in \\(\\eqref{eq:simpleiv}\\) angenommen) endogen ist und wir den Effekt von \\(B\\) auf \\(Y\\) anhand der exogenen Variation eines validen Instruments \\(Z\\) schätzen. Daher können wir grundsätzlich nur einen lokalen durchschnittlichen Behandlungseffekt (LATE) identifizieren: Der LATE ist eine gewichtete Variante des ATE, wobei Beobachtungen, deren Behandlung gut durch \\(Z\\) erklärt wird den größten Einfluss haben.2\n2 Siehe Kapitel 10 und 19 in Huntington-Klein (2021) für nicht-technische Erläuterungen und Beispiel zu LATE.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IV-Regression</span>"
    ]
  },
  {
    "objectID": "IV.html#sec-2SLS",
    "href": "IV.html#sec-2SLS",
    "title": "8  IV-Regression",
    "section": "\n8.2 2SLS-Verfahren",
    "text": "8.2 2SLS-Verfahren\nDer einfache IV-Schätzer \\(\\eqref{eq:simpleivest}\\) ist ein Spezialfall eines allgemeineren Regressionsverfahrens, das in zwei Schritten Durchgeführt wird. Im fall von Modell \\(\\eqref{eq:simpleiv}\\) erfolgt die Schätzung anhand der folgenden einfachen Regressionen:\n\n\n1. Stufe: Regression von \\(B\\) auf \\(Z\\)\nRegressiere die endogene Variable \\(B\\) auf das Instrument \\(Z\\): \\[\\begin{align}\n    B_i = \\alpha_0 + \\alpha_1 Z_i + u_i, \\quad 1,\\dots,n.\n  \\end{align}\\] Berechne die geschätzten Werte \\[\\begin{align}\n    \\widehat{B}_i = \\widehat{\\alpha}_0 + \\widehat{\\alpha}_1 Z_i.\n  \\end{align}\\]\nDie \\(\\widehat{B}_i\\) aus dieser Regression sind “bereinigt”: Die Variation in den \\(\\widehat{B}_i\\) wird lediglich durch die exogene Variation in \\(Z\\) verursacht.\n\n\n2. Stufe: Regression von \\(Y\\) auf \\(\\widehat{B}\\)\nRegressiere die Beobachtungen der abhängigen Variable \\(Y_i\\) auf die geschätzten Werte \\(\\widehat{B}_i\\) aus der 1. Stufe: \\[\\begin{align}\n    Y_i = \\gamma_0 + \\gamma_1 \\widehat{B}_i + e_i.\n  \\end{align}\\] Der geschätzte Koeffizient \\(\\widehat{\\gamma}_1\\) aus dieser Regression ist der Two-Stage-Least-Squares-Schätzer (2SLS-Schätzer) von \\(\\beta_1\\).\n\n\n\n8.2.1 Äquivalenz von 2SLS und einfacher IV-Schätzer\nDer KQ-Schätzer von \\(\\gamma_1\\) in der 2. Stufe ist \\[\\begin{align}\n  \\widehat{\\gamma}_1 = \\frac{\\sum (\\widehat{B}_i - \\overline{\\widehat{B}})(Y_i - \\bar{Y})}{\\sum (\\widehat{B}_i - \\bar{\\widehat{B}})^2} = \\frac{\\widehat{\\text{Cov}}(\\widehat{B}, Y)}{\\widehat{\\textup{Var}}(\\widehat{B}_i)}.\n\\end{align}\\] Da \\(\\widehat{B}_i = \\widehat{\\alpha}_0 + \\widehat{\\alpha}_1 Z_i\\), die angepassten Werte aus der 1. Stufe, eine lineare Funktion von \\(Z_i\\) sind, können wir \\(\\widehat{\\text{Cov}}(\\widehat{B}, Y)\\) wie folgt schreiben: \\[\\begin{align}\n  \\widehat{\\text{Cov}}(\\widehat{B}, Y) = \\widehat{\\alpha}_1 \\widehat{\\text{Cov}}(Z, Y)\n\\end{align}\\] Eine ähnliche Umformung zeigt, dass \\[\\begin{align}\n  \\widehat{\\text{Var}}(\\widehat{B}) = \\widehat{\\alpha}_1^2 \\widehat{\\text{Var}}(Z).\n\\end{align}\\]\nKombinieren wir diese Ergebnisse, erhalten wir folgende Darstellung für den KQ-Schätzer von \\(\\gamma_1\\) in der 2. Stufe: \\[\\begin{align}\n  \\widehat{\\gamma}_1 = \\frac{\\widehat{\\text{Cov}}(\\widehat{B}, Y)}{\\widehat{\\text{Var}}(\\widehat{B})} = \\frac{\\widehat{\\alpha}_1 \\widehat{\\text{Cov}}(Z, Y)}{\\widehat{\\alpha}_1^2 \\widehat{\\text{Var}}(Z)} = \\frac{\\widehat{\\text{Cov}}(Z, Y)}{\\widehat{\\alpha}_1 \\widehat{\\text{Var}}(Z)}\n\\end{align}\\]\nDer KQ-Schätzer von \\(\\widehat{\\alpha}_1\\) in der 1. Stufe ist definiert als \\(\\widehat{\\alpha}_1 = \\frac{\\widehat{\\text{Cov}}(Z, B)}{\\widehat{\\text{Var}}(Z)}\\). Somit gilt \\[\\begin{align}\n  \\widehat{\\gamma}_1 = \\frac{\\widehat{\\text{Cov}}(Z, Y)}{\\frac{\\widehat{\\text{Cov}}(Z, B)}{\\widehat{\\text{Var}}(Z)} \\widehat{\\text{Var}}(Z)} = \\frac{\\widehat{\\text{Cov}}(Z, Y)}{\\widehat{\\text{Cov}}(Z, B)} = \\widehat{\\beta}_{\\textup{IV}}.\\label{eq:equiviv}\n\\end{align}\\]\n\n8.2.2 Wozu 2SLS?\n\n\n\n\n\nIV_DAG\nX\nXB\nBX-&gt;B\nY\nYX-&gt;Y\nZ\nZX-&gt;Z\nU\nUU-&gt;B\nU-&gt;Y\nB-&gt;Y\nZ-&gt;B\n\n\n\n\nAbbildung 8.2: IV-Design mit beobachtbaren Confoundern\n\n\n\n\nIn der Praxis werden IV-Schätzungen mit statistischer Software häufig anhand einer Implementierung des 2SLS-Verfahren durchgeführt. Gründe hierfür sind:\n\n\nBedingte Exogenität. Für den in Abbildung 8.1 dargestellten DGP ist die Exogenität von Z gegeben. In empirischen Anwendungen kann es jedoch schwierig sein, ein Instrument Z zu finden, dass plausibel nur mit dem endogenen Regressor korreliert. Eine schwächere Bedingung als Exogenität ist Exogenität nach Kontrolle für beobachtbare gemeinsame Determinanten (X) von Z und Y.3\nDiese Situation ist in Abbildung 8.2 dargestellt: Aufgrund der Pfeile von X zu Z und Y ist die Bedingung \\(\\eqref{eq:ivassum2}\\) verletzt. Durch Kontrolle für X in beiden Regressionen des 2SLS-Verfahrens können die Pfade durch X geschlossen werden.\n\nMehrere endogene Variablen / Instrumente. Der 2SLS-Ansatz kann leicht für mehrere endogene Variablen (und mehrere Instrumentvariablen) erweitert werden. Für jede der endogenen Variablen erfolgt hierbei eine separate Regression in der ersten Stufe des 2SLS-Verfahrens, wobei jeweils mehrere Instrumente verwendet werden und zusätzlich für beobachtbare Kovariablen X kontrolliert werden kann.\n\nStatistische Inferenz. Die Berechnung von \\(\\widehat{\\beta}_\\textup{IV}\\) erfordert Sorgfalt bei der Konstruktion von Inferenzstatistiken. Die in Kapitel 8.2.1 gezeigte Äquivalenz impliziert, dass wir bei der Schätzung der Variabilität von \\(\\widehat{\\beta}_\\textup{IV}\\) die Unsicherheit aus den beiden KQ-Schätzungen des 2SLS-Verfahrens berücksichtigen müssen. Korrigierte Standardfehlerformeln können vergleichsweise einfach anhand der Regressionsdarstellung hergeleitet werden und sind statistischer Software implementiert (bspw. ivreg in R).\nWeiterhin ermöglicht das Regressions-Framework diagnostische Tests. So kann die Relevanz von Instrumenten anhand von F-Tests für die Koeffizienten in den Regressionen der ersten Stufe des 2SLS-Verfahrens überprüft werden. Sofern mehr Instrumente als endogene Variablen vorliegen, kann die Exogenität der Instrumente getestet werden.\n\n3 Formal: \\(\\textup{Cov}(Z,u\\vert X) = 0\\).\n8.2.3 Interaktive Illustrationen: 2SLS-Verfahren\nDer DGP im nachfolgenden interaktiven Beispiel ist\n\\[\\begin{align}\n  \\begin{split}\n    X_i =&\\, \\gamma \\cdot Z_i + v_i,\\\\\n    Y_i =&\\, \\beta_1 \\cdot X_i + u_i,\n  \\end{split}\n  \\label{eq:OJSIVDGP}\n\\end{align}\\] wobei \\(Z_i \\sim N(0, .25^2)\\). Die Fehlerterme \\(v_i\\) und \\(u_i\\) sind \\(N(0,1)\\)-verteilt und folgen einer gemeinsam Normalverteilung mit Korrelationsparameter \\(\\rho\\), \\[\\begin{align}\n\\begin{pmatrix}\nv_i \\\\ u_i\n\\end{pmatrix}\n\\sim N\n\\bigg[\n\\begin{pmatrix}\n0 \\\\ 0\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & \\rho \\\\\n\\rho & 1\n\\end{pmatrix}\n\\bigg].\n\\end{align}\\]\nWir sind daran interessiert, den Parameter \\(\\beta_1\\) zu schätzen, um den Effekt von \\(X\\) auf \\(Y\\) zu bestimmen, wobei wir \\(n=1000\\) Beobachtungen von \\((X_i, Y_i, Z_i)\\) verwenden.\nBeachte, dass \\(X_i\\) eine Funktion der (exogenen) Variable \\(Z_i\\) und des Fehlerterms \\(v_i\\) ist. Wenn die Fehlerterme \\((v_i, u_i)’\\) im DGP \\(\\eqref{eq:OJSIVDGP}\\) korreliert sind (\\(\\rho\\neq0\\)), dann besteht Korrelation zwischen \\(X_i\\) und \\(u_i\\). Dies impliziert, dass \\(X\\) ein endogener Regressor im “naiven” Regressionsmodell \\[\\begin{align}\nY_i = \\beta_0 + \\beta_1 X_i + e_i, \\quad i = 1,\\dots,N.\n\\end{align}\\] Der KQ-Schätzer von \\(\\beta_1\\) ist dann verzerrt und inkonsistent.\nWenn \\(Z\\) Erklärungskraft für \\(X\\) hat, d.h. \\(\\gamma\\neq0\\) mit \\(|\\gamma|\\) nicht zu klein gewählt ist (\\(Z\\) ist relevant), und \\(Z\\) nur über \\(X\\) auf \\(Y\\) wirkt (\\(Z\\) ist exogen in der Gleichung von \\(Y_i\\)), dann kann der kausale Effekt von \\(X\\) auf \\(Y\\) anhand einer Instrumentvariablen-Regression mit \\(Z\\) konsistent geschätzt werden.\nDie folgende Interaktive Grafik berechnet den 2SLS-Schätzer Analog zu der in Kapitel 8.2 erläuterten Vorgehensweise:\n\n\nSchätze das Regressionsmodell\n\\[\\begin{align}\nX_i = \\alpha_0 + \\alpha_1 \\cdot Z_i + \\epsilon_i\n\\end{align}\\] und berechnen Sie die angepassten Werte \\(\\widehat{X}_i\\). Dieser Schritt isoliert die exogene Variation in \\(X_i\\), die auf \\(Z_i\\) zurückzuführen ist.\n\n\nSchätze den Effekt von \\(X\\) auf \\(Y\\) unter Verwendung von \\(\\widehat{X}_i\\), dem exogenen Teil von \\(X_i\\) aus 1., in der Regression\n\\[\\begin{align}\nY_i = \\delta_0 + \\delta_1 \\cdot \\widehat{X}_i  + \\varepsilon_i.\n\\end{align}\\] Der KQ-Schätzer \\(\\widehat{\\delta}_1\\) von \\(\\delta_1\\) ist der 2SLS-Schätzer von \\(\\beta_1\\).\n\n\nDie Grafik illustriert das Verhalten des IV-Schätzers (lila Grade) im Vergleich zum KQ-Schätzer (gestrichelte Grade) und zum wahren Zusammenhang (grüne Grade) für eine simulierte Stichprobe (gegeben der gewählten Parameter) anhand der geschätzten Regressionslinien. Die voreingestellten Parameter-Kombinationen zeigen\n\ndie Verzerrung von 2SLS, wenn \\(Z\\) ein schwaches Instrument für \\(X\\) ist,\neine Situation in der \\(X\\) exogen ist (KQ ist unverzerrt), was zu vergleichbaren Schätzungen führt (sofern \\(Z\\) kein Schwaches Instrument ist)\neine starke Verzerrung von KQ, wenn \\(X\\) endogen und irrelevant (\\(\\beta_1 = 0\\)) ist.\n\n\n\nAnhand der obigen interaktiven Grafik lässt sich zwar die Verzerrung der Schätzer einschätzen, jedoch nicht die Unsicherheit der Schätzung. Um die gesamte Verteilung der Schätzer in endlichen Stichproben zu illustrieren, zeigt die nachstehende Grafik Kerndichteschätzungen basierend auf \\(N\\) simulierte Stichproben der Größe \\(n\\) für den gewählten DGP.4 Die Applikation gibt weiterhin den R-Code für eine Simulation der Daten gemäß der gewählten Parameter aus.\n4 Beachte, dass die Simulation insbesondere für große \\(N\\) und \\(n\\) einige Sekunden dauern kann (grauer Balken am linken Rand zeigt laufende Berechnung an).Die voreingestellten Parameter-Kombinationen zeigen:\n\nWenn \\(Z\\) ein schwaches Instrument ist, kann der IV-Schätzer auch in großen Stichproben eine hohe Verzerrung haben und eine deutlich größere Varianz als der KQ-Schätzer haben.\nWenn der interessierende Regressor (Behandlungsvariable) \\(X\\) exogen ist, hat der IV-Schätzer eine größere Varianz als der KQ-Schätzer.\nWenn die Endogenität von \\(X\\) “stark” ist (d.h. hohe Korrelation von \\(u\\) und \\(v\\)), kann der IV-Schätzer auch in kleinen Stichproben hilfreich sein sofern das Instrument \\(Z\\) nicht zu schwach ist: IV hat dann eine deutlich geringere Verzerrung als KQ, aber eine größere Varianz.\n\n\n\n\n8.2.4 Schätzung mit R\n\nlibrary(faux) # install.packages(\"faux\")\n\nset.seed(1234)\n\nn &lt;- 1000\n\nerrors &lt;- rnorm_multi(\n    n, mu = c(0, 0), sd = c(1, 1),\n    r = -0.8, varnames = c(\"u\", \"v\"), \n)\n\nZ &lt;- rnorm(n, sd = .25)\nX &lt;- 2 * Z + errors$v\nY &lt;- 0 * X + errors$u\n\nthe_data &lt;- data.frame(X, Y)\n\n\nlibrary(cowplot)\n\nthe_data %&gt;%\n  mutate(Xhat = lm(X ~ Z)$fitted) %&gt;%\n  \n  ggplot(aes(x = X, y = Y)) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\", \n    se  = FALSE, \n    col = \"red\"\n  ) +\n  geom_hline(\n    yintercept = 0, \n    col = \"darkgreen\"\n    ) +\n  geom_smooth(\n    mapping = aes(x = Xhat, y = Y), \n    method = \"lm\", \n    se  = FALSE,\n    col = \"steelblue\"\n    ) +\n  theme_cowplot()\n\n\n\n\n\n\n\n\nlibrary(AER)\n\nKQ_mod_sim &lt;- lm(\n  formula = Y ~ X,\n  data = the_data\n)\n\nsummary(KQ_mod_sim)\n\n\nCall:\nlm(formula = Y ~ X, data = the_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.10480 -0.48829  0.00025  0.49527  2.42887 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.01074    0.02254   0.476    0.634    \nX           -0.64579    0.02025 -31.889   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7128 on 998 degrees of freedom\nMultiple R-squared:  0.5047,    Adjusted R-squared:  0.5042 \nF-statistic:  1017 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\n\nlibrary(ivreg)\n\niv_mod_sim &lt;- ivreg(\n  formula = Y ~ X | Z,\n  data = the_data\n)\n\nsummary(iv_mod_sim)\n\n\nCall:\nivreg(formula = Y ~ X | Z, data = the_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.24501 -0.67660  0.01894  0.69021  3.52318 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.020535   0.031863   0.644    0.519\nX           -0.007139   0.059949  -0.119    0.905\n\nDiagnostic tests:\n                 df1 df2 statistic p-value    \nWeak instruments   1 998     294.5  &lt;2e-16 ***\nWu-Hausman         1 997     415.3  &lt;2e-16 ***\nSargan             0  NA        NA      NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.007 on 998 degrees of freedom\nMultiple R-Squared: 0.0111, Adjusted R-squared: 0.01011 \nWald test: 0.01418 on 1 and 998 DF,  p-value: 0.9052",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IV-Regression</span>"
    ]
  },
  {
    "objectID": "IV.html#beispiel-der-schwache-staat-die-bauern-und-die-mafia",
    "href": "IV.html#beispiel-der-schwache-staat-die-bauern-und-die-mafia",
    "title": "8  IV-Regression",
    "section": "\n8.3 Beispiel: Der schwache Staat, die Bauern und die Mafia",
    "text": "8.3 Beispiel: Der schwache Staat, die Bauern und die Mafia\nAcemoglu, De Feo, und De Luca (2020) untersuchen den Aufstieg und die Auswirkungen der sizilianischen Mafia gegen Ende des 19. Jahrhunderts. Die Studie argumentiert, dass die Verbreitung der Mafia teilweise auf das Aufkommen sozialistischer Bauern-Organisationen (Fasci siciliani) zurückzuführen ist, die in einem Umfeld schwacher staatlicher Präsenz die Rechte der Bauern verteidigten. Grundbesitzer, Verwalter und lokale Politiker wandten sich also an die Mafia, um diese sozialistische “Bedrohung” zu bekämpfen und die Kontrolle über die landwirtschaftlich arbeitende Bevölkerung zu behalten. Die Studie identifiziert einen signifikanten (positiven) kausalen Zusammenhang zwischen dem Aufstieg der Fasci und der Ausbreitung der Mafia in Sizilien.\n\n8.3.1 Identifikationsstrategie\nDie Beziehung zwischen dem Aufkommen der Fasci und der Mafia-Aktivität im einfachen Regressionsmodell \\[\\begin{align}\n  \\textup{Mafia-Aktivität} = \\beta_0 + \\beta_1 \\textup{Fasci siciliani} + \\varepsilon \\label{eq:fascimafiamod1}\n\\end{align}\\] ist wahrscheinlich aufgrund mehrerer Faktoren endogen:\n\nExterne Faktoren, die sowohl die Entstehung der Fasci siciliani als auch der Mafia beeinflussen sind wahrscheinlich. Beispielsweise könnten wirtschaftliche Notlagen und politische Instabilität sowohl die Organisation der Bauern als auch die Etablierung der Mafia begünstigen.\nSimultante Kasalität: Die Präsenz der Fasci könnte die Mafia-Aktivitäten beeinflussen und umgekehrt: Intensive Mafia-Aktivitäten könnten die Notwendigkeit für Bauernorganisationen erhöhen, was wiederum die Mafia stärkt. Außerdem erschwert die zeitliche Nähe zwischen der Entstehung der Fasci und der Verbreitung der Mafia die Identifizierung der Kausalrichtung.\n\nWir können das Modell \\(\\eqref{eq:fascimafiamod1}\\) um (messbare) unter 1. fallende Regressoren erweitern, um einer verzerrten Schätzung aufgrund relevanter ausgelassener Variablen vorzubeugen. Eine Verzerrung aufgrund der in 2. beschriebenen simultanen Kausalität kann jedoch nicht durch multiple Regression vermieden werden. Acemoglu, De Feo, und De Luca (2020) nutzen die folgende Identifikationsstrategie: Eine extreme Dürre im Jahr 1893 (insbesondere Regenausfall im Frühling) hatte einen erheblichen negativen Einfluss auf die landwirtschaftliche Produktion in Sizilien und verursachte große Not unter den Bauern, was die Entstehung der Fasci beförderte. Die Dürre kann als natürliches Experiment betrachtet werden, wobei durch die Variation in der Niederschlagsmenge (das Instrument) die Variation im Aufkommen von Fasci-Organisationen in verschiedenen Gemeinden, die unabhängig von der späteren Mafia-Aktivität ist, isoliert werden kann. Anhand dieser exogenen Variation im Organisationsgrad der Bauern kann der (lokale) kausale Effekt auf die Entwicklung der Mafia identifiziert werden.\n\n\n\n\n\nIV_DAG\nX\nMafia-FaktorenB\nFasciX-&gt;B\nY\nMafia in 1900X-&gt;Y\nX2\nFasci-FaktorenX2-&gt;B\nX3\nGeo-FaktorenX3-&gt;Y\nZ\nDürreX3-&gt;Z\nU\nUU-&gt;B\nU-&gt;Y\nB-&gt;Y\nY-&gt;B\nZ-&gt;X\nZ-&gt;X2\nZ-&gt;B\n\n\n\n\nAbbildung 8.3: Zulässiger DGP im IV-Design von Acemoglu, De Feo, und De Luca (2020)\n\n\n\n\n\n8.3.2 Reproduktion der Kernergebnisse mit R\nWir lesen zunächst den Datensatz ADD_Mafia_municipality.dta5 ein und verschaffen uns einen Überblick.\n5 Der Datensatz stammt aus dem MIT Economics Data Archive und liegt im STATA-Format .dta vor.\nlibrary(haven)\n\n# Datensatz einlesen\nADD_dat &lt;- read_stata(\n  file = \"datasets/ADD_Mafia_municipality.dta\"\n  )\n\nglimpse(ADD_dat)\n\nRows: 333\nColumns: 28\n$ peasants_fasci          &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0…\n$ sp3m1893_n30            &lt;dbl&gt; 0.2025284, 1.0492120, 0.3623592, 0.4212372, 0.…\n$ cl1_stn_sp1893_n30      &lt;dbl&gt; 26, 4, 26, 5, 4, 26, 26, 4, 4, 4, 26, 5, 26, 4…\n$ predr_peas_fasci        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ruralcentre1861         &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1…\n$ Rural_rent              &lt;dbl&gt; 5.099932, 3.597782, 2.558312, 7.590975, 4.1333…\n$ Urban_rent              &lt;dbl&gt; 0.0000000, 1.1374048, 0.3895219, 2.6512258, 0.…\n$ agricola_rel            &lt;dbl&gt; 0.9637574, 0.9701000, 0.9097173, 0.9832645, 1.…\n$ seminatoritot_rel       &lt;dbl&gt; 0.7835485, 0.8199988, 0.5116088, 0.7404211, 0.…\n$ sulfurproduction1868_70 &lt;dbl&gt; 0.0, 153.0, 0.0, 0.0, 0.0, 25.5, 0.0, 0.0, 117…\n$ Citrus_groves           &lt;dbl&gt; 5.555364e-03, 3.233516e-04, 5.223450e-04, 3.03…\n$ Olives_groves           &lt;dbl&gt; 0.0059671006, 0.0158138741, 0.0023271707, 0.04…\n$ Vineyards               &lt;dbl&gt; 0.048398037, 0.024294233, 0.006733574, 0.06361…\n$ Mafia1885               &lt;dbl&gt; 0, 3, 0, 0, 2, 3, 0, 0, 0, 0, 3, 0, 0, 2, 2, 1…\n$ Mafia1900               &lt;dbl&gt; 0, 3, 0, 0, 1, 3, 0, 0, 3, 0, 3, 3, 0, 0, 0, 3…\n$ lnpop1861               &lt;dbl&gt; 7.396335, 10.080754, 7.693937, 8.177516, 7.493…\n$ lnsurface               &lt;dbl&gt; 7.259003, 10.598988, 8.283580, 7.060150, 7.190…\n$ centreheight            &lt;dbl&gt; 554, 588, 292, 420, 720, 440, 740, 646, 625, 6…\n$ maxheight               &lt;dbl&gt; 723, 866, 621, 475, 821, 511, 899, 884, 704, 8…\n$ slope2                  &lt;dbl&gt; 176.0, 343.0, 250.0, 102.5, 235.5, 150.5, 364.…\n$ pa_pdist1856            &lt;dbl&gt; 71.0, 92.0, 74.0, 106.0, 68.0, 111.0, 68.0, 4.…\n$ port2_pdist1856         &lt;dbl&gt; 71, 34, 74, 48, 55, 85, 68, 4, 40, 46, 48, 48,…\n$ roads1799               &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1…\n$ ave_temp                &lt;dbl&gt; 14.60, 14.80, 16.40, 15.60, 14.10, 16.05, 13.9…\n$ sp3m_ave_n30            &lt;dbl&gt; 158.4963, 135.3397, 129.4469, 126.1981, 144.14…\n$ var_sp3m_n30            &lt;dbl&gt; 0.2702444, 0.4196214, 0.1592561, 0.1744232, 0.…\n$ distretto1853           &lt;chr&gt; \"caltanissetta\", \"caltanissetta\", \"caltanisset…\n$ provincia1853           &lt;chr&gt; \"caltanissetta\", \"caltanissetta\", \"caltanisset…\n\n\nDas tibble-Objekt ADD_dat enthält Variablenbschreibungen im Attribut label. Mit Funktionen aus dem tidyverse-Paket purrr können wir diese Einträge auslesen.\n\n# Attribut 'label' für die ersten 5 Variablen anzeigen\nmap_chr(\n  .x = ADD_dat, \n  .f = ~ attr_getter(\"label\")(.)\n) %&gt;%\n  .[1:5]\n\n                                                                   peasants_fasci \n              \"Dummy for the presence of a peasants fasci organization in 1893-4\" \n                                                                     sp3m1893_n30 \n\"Relative rainfall in spring 1893 interpolated from weather stations within 30km\" \n                                                               cl1_stn_sp1893_n30 \n                  \"Closest weather station active in the spring 1893 within 30km\" \n                                                                 predr_peas_fasci \n      \"Dummy for the presence of a peasants fasci organization before March 1893\" \n                                                                  ruralcentre1861 \n                                  \"Dummy for the municipality being an agro-town\" \n\n\nTabelle 8.1 gibt einen Überblick der in ADD_dat verfügbaren Variablen auf Gemeinde-Ebene und deren Definition.\n\n\n\n\n\n\n\n\nVariable\nBeschreibung\n\n\n\npeasants_fasci\nPräsenz Fasci in 1893-Q4 (Dummy)\n\n\nsp3m1893_n30\nDürre-Intensität: Relative Regenmenge im Frühling 1893 (ggü. Mittel v. 1841 bis 1881)\n\n\ncl1_stn_sp1893_n30\nNächste aktive Wetterstation (Frühling 1893)\n\n\npredr_peas_fasci\nPräsenz Fasci vor März 1893 (Dummy)\n\n\nruralcentre1861\nLandwirtschaftliche Gemeinde? (Dummy)\n\n\nRural_rent\nDurchschn. ländliche Pacht / Hektar in 1953\n\n\nUrban_rent\nDurchschn. städtische Pacht / Hektar in 1953\n\n\nagricola_rel\nAnteil bewirtschaftetes Land in 1853\n\n\nseminatoritot_rel\nAnteil Anbaufläche Getreide in 1853\n\n\nsulfurproduction1868_70\nDurschn. Schwefel-Produktion / Jahr für 1868-1870\n\n\nCitrus_groves\nAnteil Anbaufläche Zitrusfrüchte in 1853\n\n\nOlives_groves\nAnteil Anbaufläche Oliven in 1853\n\n\nVineyards\nAnteil Anbaufläche Wein in 1853\n\n\nMafia1885\nIntensität Mafia gem. Damiani (1885), Skala 0-3\n\n\nMafia1900\nIntensität Mafia gem. Cutrera (1900), Skala 0-3\n\n\nlnpop1861\nLog(Einwohner) in 1861\n\n\nlnsurface\nLog(Fläche) in 1853\n\n\ncentreheight\nHöhe des Gemeindezentrums (M.ü. NN)\n\n\nmaxheight\nMax. Höhe (M.ü. NN)\n\n\nslope2\nDurschn. Höhe (M.ü. NN)\n\n\npa_pdist1856\nEntfernung (Postweg) von Palermo in 1856\n\n\nport2_pdist1856\nEntfernung (Postweg) nächer Hafen in 1856\n\n\nroads1799\nDirekter Zugang Postweg in 1799 (Dummy)\n\n\nave_temp\nDurschn. Jahrestemperatur\n\n\nsp3m_ave_n30\nDurschn. Regenmenge im Frühling für 1881-1941\n\n\nvar_sp3m_n30\nVarianz Regenmenge im Frühling für 1881-1941\n\n\ndistretto1853\nDistrikt-Zugehörigkeit in 1853\n\n\nprovincia1853\nProvinz-Zugehörigkeit in 1853\n\n\n\n\n\n\n\nTabelle 8.1: ADD_dat — Charakteristika von sizilianischen Gemeinden im 19. Jahrhundert\n\n\n\nVor der Analyse filtern wir die Daten entsprechen der Vorgehensweise in Acemoglu, De Feo, und De Luca (2020): Es werden lediglich Gemeinden berücksichtigt, die in maximal 30km Entfernung zu einer Wetter-Station liegen (!is.na(sp3m1893_n30)) und für die eine Messung der Mafia-Aktivität gegen Ende des 19. Jahrhunderts (gemessen auf einer Skala von 0 bis 4, vgl. Cutrera (1900)) vorliegt (!is.na(Mafia1900)).\n\n# Datensatz filtern\nADD_dat &lt;- ADD_dat %&gt;%\n  filter(\n    # Nur Gemeinden mit Wetter-Station in 30km Umkreis\n    !is.na(sp3m1893_n30), \n    # Gemeinden mit Mafia-Präsenz Ende des 19. Jdt.\n    !is.na(Mafia1900)\n  )\n\n\n8.3.2.1 First-Stage-Regressionen\nTabelle 3 in Acemoglu, De Feo, und De Luca (2020) präsentiert First-Stage-Regressionen Für den endogenen Regressor \\(\\textup{Fasci siciliani}\\) in \\(\\eqref{eq:fascimafiamod1}\\): Die Auswirkungen der Dürre in 1893 (das Instrument) auf den Organisationsgrad der Bauern (der endogene Regressor). Hierbei werden verschiedene, aus dem DAG in Abbildung 8.3 abgeleitete Spezifikationen betrachtet:\n\n\nPanel A: Keine Fixed Effects\n\nModell ohne Kovariablen\nModell mit Kovariablen für Bildung von Bauernorganisationen\nModell mit Kovariablen für Bildung von Bauernorganisationen und Indikatoren für Mafia-Präsenz\nModell mit Kovariablen für Bildung von Bauernorganisationen, Indikatoren für Mafia-Präsenz und geologische Faktoren\n\n\n\nPanel B: Modelle wie in Panel A und Kontrolle für Provinz-Fixed-Effects\n\nWir schätzten nachfolgend sämtliche Regressionen mit feols und berechnen nach Distrikt-Zugehörigkeit (distretto1853) sowie nächstgelegener Wetterstation (cl1_stn_sp1893_n30) geclusterte Standardfehler.6 Hierzu setzen wir vcov = ~ distretto1853 + cl1_stn_sp1893_n30. Diese Spezifikation berücksichtigt, dass die Niederschlagsmenge in einer Gemeinde sowie viele Kovariablen eine Korrelation mit benachbarten Gemeinden aufweisen können. Cluster-robuste Standardfehler sind inbesondere nötig, weil die Niederschlagsdaten für viele Gemeinden aus benachbarten Wetterstationen interpoliert werden, was zu einer zusätzlichen Korrelation zwischen den Beobachtungen führt. Standardfehler mit Two-way-clustering sind eine gängige Methode zur Berücksichtigung derartiger Korrelation.\n6 Siehe Kapitel Kapitel 7 für Erläuterungen zu Fixed-Effects und cluster-robusten Standardfehlern.\nlibrary(fixest)\n\n# First-Stage-Regression:\n# keine Kontroll-Variablen\nfasci_mod &lt;- feols(\n  peasants_fasci ~ sp3m1893_n30, \n  vcov = ~ distretto1853 + cl1_stn_sp1893_n30,\n  data = ADD_dat,\n  )\n\n# Statistische Zusammenfassung\nsummary(fasci_mod)\n\nOLS estimation, Dep. Var.: peasants_fasci\nObservations: 245\nStandard-errors: Clustered (distretto1853 & cl1_stn_sp1893_n30) \n              Estimate Std. Error  t value   Pr(&gt;|t|)    \n(Intercept)   0.950863   0.098323  9.67077 2.2119e-09 ***\nsp3m1893_n30 -1.003055   0.130752 -7.67142 1.1758e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.380736   Adj. R2: 0.353963\n\n\nDer obige Code-Chunk führt die First-Stage-Regression ohne Kovariablen aus. Wir finden einen signifikanten positiven Effekt der Niederschlagsmenge auf den Organisationsgrad der Bauern.\nAcemoglu, De Feo, und De Luca (2020) verwenden Bootstrap cluster-robuste Standardfehler (Cameron, Gelbach, und Miller 2008, 2011), die aufwendig zu implementieren sind, jedoch zu nahezu identischen Ergebnissen führen. Der nächste Code-Chunk zeigt, wie cluster-robuste Bootstrap-Inferenz für das geschätzte Modell fasci_mod berechnet werden können. Hierfür nutzen wir fwildclusterboot::boottest().7\n7 fwildclusterboot implementiert Varianten des Wild Bootstrap (Liu 1988).\n# Wild-Bootstrap-Standardfehler mit clustering\nlibrary(fwildclusterboot)\n\n# boottest-Objekt erzeugen\nfasci_mod_boot &lt;- boottest(\n  object = fasci_mod, \n  clustid = ~ distretto1853 + cl1_stn_sp1893_n30,\n  param = \"sp3m1893_n30\", # Koeffizient\n  B = 999 # Bootstrap-Züge\n)\n\n# Statistische Zusammenfassung des Bootstraps\nsummary(fasci_mod_boot)\n\nboottest.fixest(object = fasci_mod, param = \"sp3m1893_n30\", B = 999, \n    clustid = ~distretto1853 + cl1_stn_sp1893_n30)\n    \n Hypothesis: 1*sp3m1893_n30 = 0\n Observations: 245\n  Bootstr. Type: rademacher\n Clustering: 2-way\n Confidence Sets: 95%\n Number of Clusters: 23 30 63\n \n\n\n                term estimate statistic p.value conf.low conf.high\n1 1*sp3m1893_n30 = 0   -1.003    -7.659       0   -1.325    -0.675\n\n\nDie statistische Zusammenfassung umfasst die Punktschätzung, die robuste t-Statistik sowie das 95%-Konfidenzintervall für den Koeffizienten von sp3m1893_n30. Mit diesen Komponenten können wir den Bootstrap-Standardfehler beispielsweise durch die folgende Transformation berechnen:\n\n# Standardfehler über t-Statistik und\n# gesch. Koeffizienten berechnen\n1/fasci_mod_boot$t_stat * fasci_mod_boot$point_estimate\n\n[1] 0.1309592\n\n\nBei der Interpretation des geschätzten Koeffizienten (-1) berückstigen wir, dass die abhängige Variable binär ist: Die First-Stage-Regression ist ein lineares Wahrscheinlicheitsmodell. Eine positive Änderung der relativen Regenmenge um eine Einheit (100%) verringert die Wahrscheinlichkeit also um 100%. Diese Interpretation ist jedoch wenig aussagekräftig für einen Einordnung des Effekts der Dürre. Stattdessen betrachten wir die Verteilung relativen Regenmengen und vergleichen den Effekt für eine von starker Dürre betroffenen Gemeinden (25%-Quantil) und einer Gemeinde mit einem relativ geringen Rückgang des Niederschlags (75%-Quantil)\n\n# Relativer Effekt der Dürre im Frühling 1893\n(\n  ADD_dat %&gt;% \n  pull(sp3m1893_n30) %&gt;% \n  quantile(\n    probs = c(.25, .75), \n    na.rm = T\n    )\n) %&gt;% \n  diff() * fasci_mod_boot$point_estimate\n\n       75% \n-0.5174457 \n\n\nFür eine Gemeinde mit 75% des langjährigen mittleren Niederschlags ist die geschätzte Wahrscheinlichkeit der Bildung von Bauern-Organisationen nur etwa halb so groß, wie für eine von starker Dürre betroffene Gemeinde.\nZur Vermeidung von Backdoors durch ausgelassene Variablen betrachten Acemoglu, De Feo, und De Luca (2020) weitere Spezifikationen für die First- und Second-Stage-Regressionen, die zusätzlich für Determinanten der Fasci, der Mafia, sowie für geographische Faktoren kontrollieren. Weiterhin werden diese Regressionen mit Fixed Effekts auf der Provinz-Ebene gemäß der Grenzen von 1853 (provincia1853) wiederholt, um für unbeobachtbare Heteogenitäten zwischen den verschiedenen Provinzen zu kontrollieren. Die nachfolgenden Code-Chunks zeigen die Implementierung dieser Regressionen mit fixtest::feols(). Für eine bessere Übersicht der Ergebnisse verwenden wir in summary() das Argument n = 1, sodass jeweils nur die Schätzung des ersten Koeffizienten (Effekt der relativen Regenmenge) ausgegeben wird.\n\n# First-Stage-Regression:\n# + Fasci-Determinanten\nfasci_mod_F &lt;- feols(\n  peasants_fasci ~ \n    sp3m1893_n30 \n    # Fasci\n  + predr_peas_fasci\n  + ruralcentre1861\n  + Rural_rent\n  + Urban_rent\n  + agricola_rel\n  + seminatoritot_rel,\n  \n  vcov = ~ distretto1853 + cl1_stn_sp1893_n30,\n  data = ADD_dat \n  ) \n\n# Statistische Zusammenfassung\nsummary(fasci_mod_F, n = 1)\n\nOLS estimation, Dep. Var.: peasants_fasci\nObservations: 245\nStandard-errors: Clustered (distretto1853 & cl1_stn_sp1893_n30) \n            Estimate Std. Error t value Pr(&gt;|t|) \n(Intercept) 0.634719   0.426989  1.4865  0.15134 \n... 7 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.363148   Adj. R2: 0.397391\n\n\n\n# First-Stage-Regression:\n# + Fasci-Determinanten\n# + Mafia-Determinanten\nfasci_mod_FM &lt;- feols(\n  fml = peasants_fasci ~ \n    sp3m1893_n30\n    # Fasci\n  + predr_peas_fasci\n  + ruralcentre1861\n  + Rural_rent\n  + Urban_rent\n  + agricola_rel\n  + seminatoritot_rel\n    # Mafia\n  + sulfurproduction1868_70\n  + Citrus_groves\n  + Olives_groves\n  + Vineyards\n  + Mafia1885,\n  \n  vcov = ~ distretto1853 + cl1_stn_sp1893_n30,\n  data = ADD_dat \n) \n\n# Statistische Zusammenfassung\nsummary(fasci_mod_FM, n = 1)\n\nOLS estimation, Dep. Var.: peasants_fasci\nObservations: 245\nStandard-errors: Clustered (distretto1853 & cl1_stn_sp1893_n30) \n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.7415   0.396747 1.86895 0.075002 .  \n... 12 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.353698   Adj. R2: 0.416027\n\n\n\n# First-Stage-Regression:\n# + Fasci-Determinanten\n# + Mafia-Determinanten\n# + geographische Faktoren\nfasci_mod_FMG &lt;- feols(\n  peasants_fasci ~ \n    sp3m1893_n30\n    # Fasci\n  + predr_peas_fasci\n  + ruralcentre1861\n  + Rural_rent\n  + Urban_rent\n  + agricola_rel\n  + seminatoritot_rel\n    # Mafia\n  + sulfurproduction1868_70\n  + Citrus_groves\n  + Olives_groves\n  + Vineyards\n  + Mafia1885\n    # Geographie\n  + lnpop1861\n  + lnsurface\n  + centreheight\n  + maxheight\n  + slope2\n  + pa_pdist1856\n  + port2_pdist1856\n  + roads1799\n  + ave_temp\n  + sp3m_ave_n30\n  + var_sp3m_n30,\n  \n  vcov = ~ distretto1853 + cl1_stn_sp1893_n30,\n  data = ADD_dat \n) \n\n# Statistische Zusammenfassung\nsummary(fasci_mod_FMG, n = 1)\n\nOLS estimation, Dep. Var.: peasants_fasci\nObservations: 245\nStandard-errors: Clustered (distretto1853 & cl1_stn_sp1893_n30) \n            Estimate Std. Error  t value Pr(&gt;|t|) \n(Intercept) 0.016011    1.12144 0.014277  0.98874 \n... 23 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.347776   Adj. R2: 0.407315\n\n\n\n# First Stage\n# + Provinz-Fixed-Effects\nfasci_mod_FE &lt;- feols(\n  peasants_fasci ~ \n    sp3m1893_n30\n  | provincia1853, \n  vcov = ~ distretto1853 + cl1_stn_sp1893_n30,\n  data = ADD_dat,\n  ) \nsummary(fasci_mod_FE, n = 1)\n\nOLS estimation, Dep. Var.: peasants_fasci\nObservations: 245\nFixed-effects: provincia1853: 7\nStandard-errors: Clustered (distretto1853 & cl1_stn_sp1893_n30) \n              Estimate Std. Error  t value   Pr(&gt;|t|)    \nsp3m1893_n30 -0.762897   0.138668 -5.50162 1.5781e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.370663     Adj. R2: 0.372191\n                 Within R2: 0.119989\n\n\n\n# First Stage\n# + Provinz-Fixed-Effects\n# + Fasci-Determinanten\nfasci_mod_FE_F &lt;-feols(\n  fml = peasants_fasci ~ \n    sp3m1893_n30 \n  \n  + predr_peas_fasci\n  + ruralcentre1861\n  + Rural_rent\n  + Urban_rent\n  + agricola_rel\n  + seminatoritot_rel\n  \n  | provincia1853, \n  vcov = ~ distretto1853 + cl1_stn_sp1893_n30,\n  data = ADD_dat \n  ) \n\nsummary(fasci_mod_FE_F, n = 1)\n\nOLS estimation, Dep. Var.: peasants_fasci\nObservations: 245\nFixed-effects: provincia1853: 7\nStandard-errors: Clustered (distretto1853 & cl1_stn_sp1893_n30) \n              Estimate Std. Error  t value   Pr(&gt;|t|)    \nsp3m1893_n30 -0.813957   0.159025 -5.11842 3.9568e-05 ***\n... 6 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.356253     Adj. R2: 0.404994\n                 Within R2: 0.187084\n\n\n\n# First Stage\n# + Provinz-Fixed-Effects\n# + Fasci-Determinanten\n# + Mafia-Determinanten\nfasci_mod_FE_FM &lt;- feols(\n  fml = peasants_fasci ~ \n    sp3m1893_n30 \n  \n  + predr_peas_fasci\n  + ruralcentre1861\n  + Rural_rent\n  + Urban_rent\n  + agricola_rel\n  + seminatoritot_rel\n  \n  + sulfurproduction1868_70\n  + Citrus_groves\n  + Olives_groves\n  + Vineyards\n  + Mafia1885\n  \n  | provincia1853,\n  vcov = ~ distretto1853 + cl1_stn_sp1893_n30,\n  data = ADD_dat \n) \n\nsummary(fasci_mod_FE_FM, n = 1)\n\nOLS estimation, Dep. Var.: peasants_fasci\nObservations: 245\nFixed-effects: provincia1853: 7\nStandard-errors: Clustered (distretto1853 & cl1_stn_sp1893_n30) \n              Estimate Std. Error  t value  Pr(&gt;|t|)    \nsp3m1893_n30 -0.738703   0.165957 -4.45117 0.0002004 ***\n... 11 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.341461     Adj. R2: 0.441285\n                 Within R2: 0.253188\n\n\n\n# First Stage\n# + Provinz-Fixed-Effects\n# + Fasci-Determinanten\n# + Mafia-Determinanten\n# + geografische Faktoren\nfasci_mod_FE_FMG &lt;- feols(\n  peasants_fasci ~ sp3m1893_n30 \n  \n  + predr_peas_fasci\n  + ruralcentre1861\n  + Rural_rent\n  + Urban_rent\n  + agricola_rel\n  + seminatoritot_rel\n  \n  + sulfurproduction1868_70\n  + Citrus_groves\n  + Olives_groves\n  + Vineyards\n  + Mafia1885\n  \n  + lnpop1861\n  + lnsurface\n  + centreheight\n  + maxheight\n  + slope2\n  + pa_pdist1856\n  + port2_pdist1856\n  + roads1799\n  + ave_temp\n  + sp3m_ave_n30\n  + var_sp3m_n30\n  \n  | provincia1853,  \n  vcov = ~ distretto1853 + cl1_stn_sp1893_n30,\n  data = ADD_dat \n) \n\nsummary(fasci_mod_FE_FMG, n = 1)\n\nOLS estimation, Dep. Var.: peasants_fasci\nObservations: 245\nFixed-effects: provincia1853: 7\nStandard-errors: Clustered (distretto1853 & cl1_stn_sp1893_n30) \n              Estimate Std. Error  t value  Pr(&gt;|t|)    \nsp3m1893_n30 -0.718998   0.209917 -3.42515 0.0024214 ** \n... 22 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.338173     Adj. R2: 0.423955\n                 Within R2: 0.267501\n\n\nFür eine tabellarische Übersicht fassen wir die interessierenden Komponenten der Schätzungen mit modelsummary::modelsummary() zusammen. Hierfür sammeln wir zunächst jeweils die Modelle mit und ohne Provinz-Fixed-Effects in benannten Listen, sodass die Ergebnisse später in separaten Panelen dargestellt werden können. Für die Formatierung von Labels für die Fixed Effects und Koeffizienten definieren wir das tribble-Objekt gm und übergeben reguläre Ausdrücke für eine Auswahl an zusammenfassenden Statistiken.\n\nlibrary(modelsummary)\n\n# Modelle in benannten Listen sammeln\npanels &lt;- list(\n  \"Panel A\" = list(\n    \"Keine Controls\" = fasci_mod,\n    \"Fasci\" = fasci_mod_F,\n    \"Fasci + Mafia\" = fasci_mod_FM,\n    \"Fasci + Mafia + Geo\" = fasci_mod_FMG\n  ),\n  \"Panel B (Fixed Effects)\" = list(\n    \"Keine Controls\" = fasci_mod_FE,\n    \"Fasci\" = fasci_mod_FE_F,\n    \"Fasci + Mafia\" = fasci_mod_FE_FM,\n    \"Fasci + Mafia + Geo\" = fasci_mod_FE_FMG\n  )\n)\n\n# Format für FE-label\ngm &lt;- tribble(\n  ~raw, ~clean, ~fmt,\n  \"FE: provincia1853\", \"FE: Provinz\", ~fmt\n)\n\n# Tabellarische Zusammenfassung\nmodelsummary(\n  models = panels,\n  shape = \"rbind\",\n  coef_omit = \"^(?!(sp3m1893\\\\_n30*)$).*\",\n  stars = T,\n  gof_omit = \"^(?!(FE.*)$).*\",\n  coef_map = c(\n    \"sp3m1893_n30\" = \"Rel. Niederschlag 1893\"\n    ),\n  gof_map = gm, \n  notes = c(\n    \"Abhängige Variable: Peasants Fasci\", \n    \"Cluster-robuste Standardfehler: Naheste Wetterstation + Distrikt\"\n  ),\n  output = \"gt\"\n) %&gt;%\n  tabopts()\n\n\n\n\n\n\n\n\nKeine Controls\nFasci\nFasci + Mafia\nFasci + Mafia + Geo\n\n\n\nPanel A\n\n\nRel. Niederschlag 1893\n-1.003***\n-0.942***\n-0.934***\n-0.782***\n\n\n\n(0.131)\n(0.121)\n(0.128)\n(0.153)\n\n\nPanel B (Fixed Effects)\n\n\nRel. Niederschlag 1893\n-0.763***\n-0.814***\n-0.739***\n-0.719**\n\n\n\n(0.139)\n(0.159)\n(0.166)\n(0.210)\n\n\nFE: Provinz\nX\nX\nX\nX\n\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\nAbhängige Variable: Peasants Fasci\n\n\nCluster-robuste Standardfehler: Naheste Wetterstation + Distrikt\n\n\n\n\n\n\n\nTabelle 8.2: ADD_dat – First-Stage-Regressionen aus Acemoglu, De Feo, und De Luca (2020)\n\n\n\nDie Ergebnisse in Tabelle 8.2 bestätigen die Vermutung der Autoren, dass der Effekt des relativen Niederschlags auf den Organisationsgrad der Bauern ohne Kontrolle für die betrachteten Kovariablen tendenziell überschätzt wird. Tatsächlich führt multiple Regression mit Fixed Effects und sämtlichen Kovariablen (letzte Spalte in Panel B) zu einem etwa 30% kleineren geschätzten Effekt als für die einfache Regression in Panel A.\n\n8.3.2.2 Second-Stage-Regressionen\nMit fml = Mafia1900 ~ 1 | peasants_fasci ~ sp3m1893_n30 legen wir fest, dass Mafia1900 auf eine Konstante sowie die angepassten Werte des endogenen Regressors peasants_fasci aus der ersten Stufe (instrumentiert durch sp3m1893_n30) regressiert werden soll. Wie für die Regressionen in Tabelle 8.2 schätzen wir jeweils vier verschiedene Spezfikationen, unterscheiden zwischen Modellen mit und ohne Provinz-Fixed-Effects und berechnen cluster-robuste Standardfehler.\n\n# Second-Stage-Regression:\n# Keine Kontrollvariablen\n# (1)\nfasciIV_mod &lt;- feols(\n  fml = Mafia1900 ~ 1 \n  # IV-Spezifikation\n  | peasants_fasci ~ sp3m1893_n30,\n  vcov = ~ distretto1853 + cl1_stn_sp1893_n30,\n  data = ADD_dat\n)\n\nsummary(fasciIV_mod, n = 1)\n\nTSLS estimation - Dep. Var.: Mafia1900\n                  Endo.    : peasants_fasci\n                  Instr.   : sp3m1893_n30\nSecond stage: Dep. Var.: Mafia1900\nObservations: 245\nStandard-errors: Clustered (distretto1853 & cl1_stn_sp1893_n30) \n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 0.692647   0.203977 3.39571 0.0025978 ** \n... 1 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 1.16456   Adj. R2: -0.027853\nF-test (1st stage), peasants_fasci: stat = 134.7, p &lt; 2.2e-16 , on 1 and 243 DoF.\n                        Wu-Hausman: stat =  35.2, p = 1.013e-8, on 1 and 242 DoF.\n\n\n\n# Second-Stage-Regression:\n# + Fasci-Determinanten\nfasciIV_mod_F &lt;- feols(\n  fml = Mafia1900 ~ \n  # Fasci\n  predr_peas_fasci\n  + ruralcentre1861\n  + Rural_rent\n  + Urban_rent\n  + agricola_rel\n  + seminatoritot_rel\n  # IV-Spezifikation\n  | peasants_fasci ~ sp3m1893_n30,\n  vcov = ~ distretto1853 + cl1_stn_sp1893_n30,\n  data = ADD_dat\n) \n\nsummary(fasciIV_mod_F, n = 1)\n\nTSLS estimation - Dep. Var.: Mafia1900\n                  Endo.    : peasants_fasci\n                  Instr.   : sp3m1893_n30\nSecond stage: Dep. Var.: Mafia1900\nObservations: 245\nStandard-errors: Clustered (distretto1853 & cl1_stn_sp1893_n30) \n             Estimate Std. Error   t value Pr(&gt;|t|) \n(Intercept) -0.253013    1.03236 -0.245083  0.80866 \n... 7 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 1.14179   Adj. R2: -0.01306\nF-test (1st stage), peasants_fasci: stat = 115.1, p &lt; 2.2e-16 , on 1 and 237 DoF.\n                        Wu-Hausman: stat =  39.0, p = 1.931e-9, on 1 and 236 DoF.\n\n\n\n# Second-Stage-Regression:\n# + Fasci-Determinanten\n# + Mafia-Determinanten\nfasciIV_mod_FM &lt;- feols(\n  fml = Mafia1900 ~ \n    # Fasci\n    predr_peas_fasci\n  + ruralcentre1861\n  + Rural_rent\n  + Urban_rent\n  + agricola_rel\n  + seminatoritot_rel\n    # Mafia\n  + sulfurproduction1868_70\n  + Citrus_groves\n  + Olives_groves\n  + Vineyards\n  + Mafia1885\n    # IV-Spezifikation\n  | peasants_fasci ~ sp3m1893_n30,\n  vcov = ~ distretto1853 + cl1_stn_sp1893_n30,\n  data = ADD_dat\n) \n\nsummary(fasciIV_mod_FM, n = 1)\n\nTSLS estimation - Dep. Var.: Mafia1900\n                  Endo.    : peasants_fasci\n                  Instr.   : sp3m1893_n30\nSecond stage: Dep. Var.: Mafia1900\nObservations: 245\nStandard-errors: Clustered (distretto1853 & cl1_stn_sp1893_n30) \n             Estimate Std. Error   t value Pr(&gt;|t|) \n(Intercept) -0.272777    1.03221 -0.264266  0.79403 \n... 12 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 1.0177   Adj. R2: 0.177817\nF-test (1st stage), peasants_fasci: stat = 111.6, p &lt; 2.2e-16 , on 1 and 232 DoF.\n                        Wu-Hausman: stat =  30.9, p = 7.381e-8, on 1 and 231 DoF.\n\n\n\n# Second-Stage-Regression:\n# + Fasci-Determinanten\n# + Mafia-Determinanten\n# + Geographie\nfasciIV_mod_FMG &lt;- feols(\n  fml = Mafia1900 ~ \n    # Fasci\n    predr_peas_fasci\n  + ruralcentre1861\n  + Rural_rent\n  + Urban_rent\n  + agricola_rel\n  + seminatoritot_rel\n    # Mafia\n  + sulfurproduction1868_70\n  + Citrus_groves\n  + Olives_groves\n  + Vineyards\n  + Mafia1885\n    # Geographie\n  + lnpop1861\n  + lnsurface\n  + centreheight\n  + maxheight\n  + slope2\n  + pa_pdist1856\n  + port2_pdist1856\n  + roads1799\n  + ave_temp\n  + sp3m_ave_n30\n  + var_sp3m_n30\n   # IV-Spezifikation\n  | peasants_fasci ~ sp3m1893_n30,\n  vcov = ~ distretto1853 + cl1_stn_sp1893_n30,\n  data = ADD_dat\n) \n\nsummary(fasciIV_mod_FMG, n = 1)\n\nTSLS estimation - Dep. Var.: Mafia1900\n                  Endo.    : peasants_fasci\n                  Instr.   : sp3m1893_n30\nSecond stage: Dep. Var.: Mafia1900\nObservations: 245\nStandard-errors: Clustered (distretto1853 & cl1_stn_sp1893_n30) \n            Estimate Std. Error  t value Pr(&gt;|t|) \n(Intercept) 0.614509    2.70956 0.226793  0.82268 \n... 23 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.896696   Adj. R2: 0.329944\nF-test (1st stage), peasants_fasci: stat = 33.4, p = 2.528e-8, on 1 and 221 DoF.\n                        Wu-Hausman: stat = 12.2, p = 5.753e-4, on 1 and 220 DoF.\n\n\n\n# Second-Stage-Regression:\n# + Provinz-FE\nfasciIV_mod_FE &lt;- feols(\n  fml = Mafia1900 ~ 1 \n  # Fixed Effecs\n  | provincia1853 \n  # IV-Spezifikation\n  | peasants_fasci ~ sp3m1893_n30,\n  vcov = ~ distretto1853 + cl1_stn_sp1893_n30,\n  data = ADD_dat\n) \n\nsummary(fasciIV_mod_FE, n = 1)\n\nTSLS estimation - Dep. Var.: Mafia1900\n                  Endo.    : peasants_fasci\n                  Instr.   : sp3m1893_n30\nSecond stage: Dep. Var.: Mafia1900\nObservations: 245\nFixed-effects: provincia1853: 7\nStandard-errors: Clustered (distretto1853 & cl1_stn_sp1893_n30) \n                   Estimate Std. Error t value  Pr(&gt;|t|)    \nfit_peasants_fasci 0.866875   0.243571 3.55902 0.0017565 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.894467     Adj. R2:  0.378283\n                 Within R2: -0.059948\nF-test (1st stage), peasants_fasci: stat = 32.3    , p = 3.836e-8, on 1 and 237 DoF.\n                        Wu-Hausman: stat =  2.46207, p = 0.117965, on 1 and 236 DoF.\n\n\n\n# Second-Stage-Regression:\n# + Fasci-Determinanten\n# + Provinz-FE\nfasciIV_mod_FE_F &lt;- feols(\n  fml = Mafia1900 ~ \n    # Fasci\n    predr_peas_fasci\n  + ruralcentre1861\n  + Rural_rent\n  + Urban_rent\n  + agricola_rel\n  + seminatoritot_rel\n    # Fixed Effects\n  | provincia1853\n    # IV-Spezifikation\n  | peasants_fasci ~ sp3m1893_n30,\n  vcov = ~ distretto1853 + cl1_stn_sp1893_n30,\n  data = ADD_dat\n) \n\nsummary(fasciIV_mod_FE_F, n = 1)\n\nTSLS estimation - Dep. Var.: Mafia1900\n                  Endo.    : peasants_fasci\n                  Instr.   : sp3m1893_n30\nSecond stage: Dep. Var.: Mafia1900\nObservations: 245\nFixed-effects: provincia1853: 7\nStandard-errors: Clustered (distretto1853 & cl1_stn_sp1893_n30) \n                   Estimate Std. Error t value   Pr(&gt;|t|)    \nfit_peasants_fasci  1.02514   0.256786 3.99219 0.00061474 ***\n... 6 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.878524     Adj. R2:  0.384671\n                 Within R2: -0.0225  \nF-test (1st stage), peasants_fasci: stat = 37.2    , p = 4.42e-9 , on 1 and 231 DoF.\n                        Wu-Hausman: stat =  5.78115, p = 0.016991, on 1 and 230 DoF.\n\n\n\n# Second-Stage-Regression:\n# + Fasci-Determinanten\n# + Mafia-Determinanten\n# + Provinz-FE\nfasciIV_mod_FE_FM &lt;- feols(\n  fml = Mafia1900 ~ \n    # Fasci\n    predr_peas_fasci\n  + ruralcentre1861\n  + Rural_rent\n  + Urban_rent\n  + agricola_rel\n  + seminatoritot_rel\n    # Mafia\n  + sulfurproduction1868_70\n  + Citrus_groves\n  + Olives_groves\n  + Vineyards\n  + Mafia1885\n    # Fixed Effects\n  | provincia1853 \n    # IV-Spezifikation\n  | peasants_fasci ~ sp3m1893_n30,\n  vcov = ~ distretto1853 + cl1_stn_sp1893_n30,\n  data = ADD_dat\n) \n\nsummary(fasciIV_mod_FE_FM, n = 1)\n\nTSLS estimation - Dep. Var.: Mafia1900\n                  Endo.    : peasants_fasci\n                  Instr.   : sp3m1893_n30\nSecond stage: Dep. Var.: Mafia1900\nObservations: 245\nFixed-effects: provincia1853: 7\nStandard-errors: Clustered (distretto1853 & cl1_stn_sp1893_n30) \n                   Estimate Std. Error t value  Pr(&gt;|t|)    \nfit_peasants_fasci  1.33734     0.4272 3.13047 0.0048655 ** \n... 11 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.84824     Adj. R2: 0.41367 \n                Within R2: 0.046778\nF-test (1st stage), peasants_fasci: stat = 31.4    , p = 6.07e-8 , on 1 and 226 DoF.\n                        Wu-Hausman: stat =  6.49318, p = 0.011495, on 1 and 225 DoF.\n\n\n\n# Second-Stage-Regression:\n# + Fasci-Determinanten\n# + Mafia-Determinanten\n# + Geographische Faktoren\n# + Provinz-FE\nfasciIV_mod_FE_FMG &lt;- feols(\n  fml = Mafia1900 ~ \n    # Fasci\n    predr_peas_fasci\n  + ruralcentre1861\n  + Rural_rent\n  + Urban_rent\n  + agricola_rel\n  + seminatoritot_rel\n    # Mafia\n  + sulfurproduction1868_70\n  + Citrus_groves\n  + Olives_groves\n  + Vineyards\n  + Mafia1885\n    # Geo\n  + lnpop1861\n  + lnsurface\n  + centreheight\n  + maxheight\n  + slope2\n  + pa_pdist1856\n  + port2_pdist1856\n  + roads1799\n  + ave_temp\n  + sp3m_ave_n30\n  + var_sp3m_n30\n    # Fixed Effects\n  | provincia1853 \n    # IV-Spezifikation\n  | peasants_fasci ~ sp3m1893_n30,\n  vcov = ~ distretto1853 + cl1_stn_sp1893_n30,\n  data = ADD_dat\n)\n\nsummary(fasciIV_mod_FE_FMG, n = 1)\n\nTSLS estimation - Dep. Var.: Mafia1900\n                  Endo.    : peasants_fasci\n                  Instr.   : sp3m1893_n30\nSecond stage: Dep. Var.: Mafia1900\nObservations: 245\nFixed-effects: provincia1853: 7\nStandard-errors: Clustered (distretto1853 & cl1_stn_sp1893_n30) \n                   Estimate Std. Error t value Pr(&gt;|t|)    \nfit_peasants_fasci  1.56343   0.599753  2.6068 0.016101 *  \n... 22 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.857152     Adj. R2: 0.370653\n                 Within R2: 0.026644\nF-test (1st stage), peasants_fasci: stat = 20.8    , p = 8.72e-6 , on 1 and 215 DoF.\n                        Wu-Hausman: stat =  7.05768, p = 0.008487, on 1 and 214 DoF.\n\n\n\n# Modelle in benannten Listen sammeln\npanels &lt;- list(\n  \"Panel A\" = list(\n    \"Keine Controls\" = fasciIV_mod,\n    \"Fasci\" = fasciIV_mod_F,\n    \"Fasci + Mafia\" = fasciIV_mod_FM,\n    \"Fasci + Mafia + Geo\" = fasciIV_mod_FMG\n  ),\n  \"Panel B (Fixed Effects)\" = list(\n    \"Keine Controls\" = fasciIV_mod_FE,\n    \"Fasci\" = fasciIV_mod_FE_F,\n    \"Fasci + Mafia\" = fasciIV_mod_FE_FM,\n    \"Fasci + Mafia + Geo\" = fasciIV_mod_FE_FMG\n  )\n)\n\n# Tabellarische Zusammenfassung\nmodelsummary(\n  models = panels,\n  shape = \"rbind\",\n  coef_omit = \"^(?!(fit\\\\_peasants\\\\_fasci*)$).*\",\n  stars = T,\n  gof_omit = \"^(?!(FE.*)$).*\",\n  coef_map = c(\n    \"fit_peasants_fasci\" = \"Fasci\"\n    ),\n  gof_map = gm, \n  notes = c(\n    \"Abhängige Variable: Mafia-Präsenz in 1900\", \n    \"Instrument: Rel. Niederschlag 1893\", \n    \"Cluster-robuste Standardfehler: Naheste Wetterstation + Distrikt\"\n  ),\n  output = \"gt\"\n) %&gt;%\n  tabopts()\n\n\n\n\n\n\n\n\nKeine Controls\nFasci\nFasci + Mafia\nFasci + Mafia + Geo\n\n\n\nPanel A\n\n\nFasci\n2.051***\n2.096***\n1.958***\n1.688**\n\n\n\n(0.383)\n(0.371)\n(0.332)\n(0.532)\n\n\nPanel B (Fixed Effects)\n\n\nFasci\n0.867**\n1.025***\n1.337**\n1.563*\n\n\n\n(0.244)\n(0.257)\n(0.427)\n(0.600)\n\n\nFE: Provinz\nX\nX\nX\nX\n\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\nAbhängige Variable: Mafia-Präsenz in 1900\n\n\nInstrument: Rel. Niederschlag 1893\n\n\nCluster-robuste Standardfehler: Naheste Wetterstation + Distrikt\n\n\n\n\n\n\n\nTabelle 8.3: ADD_dat – Second-Stage-Regressionen aus Acemoglu, De Feo, und De Luca (2020)\n\n\n\nDie Ergebnisse für die IV-Schätzung des kausalen Effekts von Bauernorganisationen auf die Präsenz der Mafia zu Beginn des 20. Jahrhundert sind in Tabelle 8.3 dargestellt. Die LATE-Schätzung in Spalte 4 von Panel B hat folgende Interpretation: Die Präsenz der Fasci in einer Gemeinde erhöht die Intensität der Mafia-Aktivität um etwa 1.56 Punkte.8\n8 Nach Cutrera (1900) entspricht eine Erhöhung des Mafia-Index von 1 auf 2 einem Sprung von einer geringen Präsenz der Mafia zu einer signifikanten Präsenz.Wie ist dieser Effekt einzuordnen? Wir berechnen zunächst die Anzahl an Gemeinden, in denen es im Jahr 1900 Fasci gibt.\n\n# Anz. Gemeinden mit Fasci im Jahr 1900\nADD_dat %&gt;% \n  drop_na() %&gt;% \n  filter(\n    peasants_fasci == 1\n  ) %&gt;% \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    84\n\n\nFür diese 84 Gemeinden erwarten wir jeweils einen Effekt von 1.56 Punkten auf die im Jahr 1900 beobachtete gesamte Intensität des Einflusses der Mafia in Sizilien, gemessen als gewichtete Summe.\n\n# Beitrag der Gemeinden mit Fasci im Jahr 1900\n84 * 1.56\n\n[1] 131.04\n\n\nDie gesamte Mafia-Intensität für Sizilien zu Beginn des 20. Jahrhunderts berechnen wir analog.\n\n# Mafia-Intensität in Sizilien im Jahr 1900\nADD_dat %&gt;% \n  drop_na() %&gt;% \n  count(Mafia1900) %&gt;% \n  mutate(Beitrag = Mafia1900 * n) %&gt;%\n  summarise(\n     Intensität = sum(Beitrag)\n  )\n\n# A tibble: 1 × 1\n  Intensität\n       &lt;dbl&gt;\n1        342\n\n\nDer Anteil der Gemeinden mit Bauernorganisationen im Jahr 1900 an dieser Summe erlaubt eine Einordnung des Effekts der Fasci auf die Verbreitung der Mafia.\n\n# Anteil des geschätzten Effekts\n84 * 1.56 / 342\n\n[1] 0.3831579\n\n\nDie Interpretation lautet, dass bis zu 38% der Stärke der Mafia im Jahr 1900 in Sizilien auf ihren Einsatz gegen die Fasci zurückzuführen sein könnte.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IV-Regression</span>"
    ]
  },
  {
    "objectID": "IV.html#case-study-ökonomische-schocks-und-bürgerkriege",
    "href": "IV.html#case-study-ökonomische-schocks-und-bürgerkriege",
    "title": "8  IV-Regression",
    "section": "\n8.4 Case Study: Ökonomische Schocks und Bürgerkriege",
    "text": "8.4 Case Study: Ökonomische Schocks und Bürgerkriege\nEin Kernproblem bei der Schätzung der kausalen Beziehung zwischen wirtschaftlichen Schocks und dem Auftreten von kriegerischen Auseinandersetzungen ist die Endogenität von Variablen, die wirtschaftliche Stabilität messen (vgl. Fearon und Laitin 2003). Beispielsweise kann eine Verschlechterung der wirtschaftlichen Bedingungen eines Landes die Wahrscheinlichkeit eines Konflikts im Inland erhöhen. Gleichermaßen beeinflussen kriegerische Handlung im Inland die wirtschaftliche Situation einer Volkswirtschaft negativ.\n\n\n\n\n\nIV_DAG\nK\nKBIP\nBIPK-&gt;BIP\nKonflikt\nKonfliktK-&gt;Konflikt\nBIP-&gt;Konflikt\nKonflikt-&gt;BIP\n\n\n\n\nAbbildung 8.4: Simultane Kausalität zw. Kriegsrisiko und Wirtschaftslage\n\n\n\n\nIn der “Konfliktgleichung”\n\\[\\begin{align}\n  \\textup{Konflikt-Wsk.} = \\beta_0 + \\beta_1\\, \\Delta\\text{BIP} + \\text{Kontrollv.} + \\epsilon\\label{eq:rainconf1}\n\\end{align}\\]\nliegt dann Endogenität des Regressors für “wirtschaftliche Stabilität”, hier \\(\\Delta\\text{BIP}\\), aufgrund von simultanter Kausalität vor. Die multiple Regression \\(\\eqref{eq:rainconf1}\\) liefert dann eine verzerrte Schätzungen des interessierenden Effekts \\(\\beta_1\\).\nMiguel, Satyanath, und Sergenti (2004) untersuchen die kausale Beziehung zwischen wirtschaftlichen Schocks und dem Auftreten von Bürgerkriegen in 41 afrikanischen Sub-Sahara-Ländern für den Zeitraum von 1981 bis 1999.9 Hierbei adressieren die Autoren das Endogenitätsproblem in einem IV-Ansatz mit Instrumenten, die mit wirtschaftlichen Schocks korreliert sind, aber nicht unmittelbar mit der Wahrscheinlichkeit eines Bürgerkriegs zusammenhängen.\n9 Siehe rainconflict$country %&gt;% unique().Die Identifikationsstrategie basiert auf metereologischen Faktoren: In den betrachteten Agrarökonomien sind Schocks in den Niederschlägen entscheidend für die Entwicklung des Bruttoinlandsprodukts. Ungünstige Wetterbedingungen, die landwirtschaftliche Erträge negativ beeinflussen, haben einen direkten (negativen) Effekt auf die wirtschaftliche Situation. Variation in der Regenmenge ist jedoch keine unmittelbare Determinante der Konfliktwahrscheinlichkeit und folglich exogen in Modellgleichung \\(\\eqref{eq:rainconf1}\\). Maße für die Veränderungen in der Niederschlagsmenge sind somit plausible Instrumente für endogene Variablen, die Veränderungen im Bruttoinlandsprodukt messen.\n\n\n\n\n\nIV_DAG\nC\nCBIP\nBIPC-&gt;BIP\nKonflikt\nKonfliktC-&gt;Konflikt\nNiederschlag\nNiederschlagNiederschlag-&gt;BIP\nBIP-&gt;Konflikt\nKonflikt-&gt;BIP\n\n\n\n\nAbbildung 8.5: IV-Design bei simultaner Kausalität zw. Kriegsrisiko und Wirtschaftslage\n\n\n\n\nAnhand von Länder-Paneldaten mit Informationen über wirtschaftliche Indikatoren, Konfliktereignisse und Niederschläge, isolieren Miguel, Satyanath, und Sergenti (2004) in einem IV-Ansatz die exogene Variation der wirtschaftlichen Entwicklung, d.h. Variation in \\(\\Delta\\text{BIP}\\) die nicht mit dem Fehlerterm in der Konfliktgleichung korreliert ist. Die Ergebnisse ihrer 2SLS-Schätzungen zeigen, dass negative ökonomische Schocks die Wahrscheinlichkeit eines Bürgerkriegs signifikant erhöhen. Miguel, Satyanath, und Sergenti (2004) zeigen weiterhin, dass ihre Ergebnisse robust gegenüber verschiedenen Modell-Spezifikationen unter Berücksichtigung diverser Kontrollvariablen sind.\nWir reproduzieren nachfolgend die zentralen Ergebnisse von Miguel, Satyanath, und Sergenti (2004) anhand eines Auszugs (rainconflict.csv) aus dem der Studie zugrundeliegenden Datensatz. Der vollständige Datensatz ist, nach Registrierung, hier verfügbar.\n\nlibrary(readr)\n\n# Datensatz 'rainconflict' einlesen\nrainconflict &lt;- read_csv2(\n  file = \"datasets/rainconflict.csv\"\n)\n\n\n# Überblick verschaffen\nglimpse(rainconflict)\n\nRows: 743\nColumns: 19\n$ any_prio   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,…\n$ war_prio   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,…\n$ ccode      &lt;dbl&gt; 404, 404, 404, 404, 404, 404, 404, 404, 404, 404, 404, 404,…\n$ country    &lt;chr&gt; \"Guinea-Bissau\", \"Guinea-Bissau\", \"Guinea-Bissau\", \"Guinea-…\n$ gdp_g      &lt;dbl&gt; 0.216560572, 0.104712047, -0.042654045, 0.034653414, 0.0366…\n$ gdp_g_l    &lt;dbl&gt; -0.152877718, 0.216560572, 0.104712047, -0.042654045, 0.034…\n$ GPCP_g     &lt;dbl&gt; 0.170644149, 0.023161817, -0.215036541, 0.098459557, 0.0185…\n$ GPCP_g_l   &lt;dbl&gt; -0.048803251, 0.170644149, 0.023161817, -0.215036541, 0.098…\n$ GPCP_g_fl  &lt;dbl&gt; 0.023161817, -0.215036541, 0.098459557, 0.018551834, 0.0545…\n$ gdp_1979   &lt;dbl&gt; 0.556, 0.556, 0.556, 0.556, 0.556, 0.556, 0.556, 0.556, 0.5…\n$ polity2l   &lt;dbl&gt; -7, -7, -7, -7, -8, -8, -8, -8, -8, -8, -6, -6, -6, -6, 5, …\n$ polity2_IV &lt;dbl&gt; -7, -7, -7, -8, -8, -8, -8, -8, -8, -6, -6, -6, -6, 5, 5, 5…\n$ ethfrac    &lt;dbl&gt; 0.8037570, 0.8037570, 0.8037570, 0.8037570, 0.8037570, 0.80…\n$ relfrac    &lt;dbl&gt; 0.5450, 0.5450, 0.5450, 0.5450, 0.5450, 0.5450, 0.5450, 0.5…\n$ oil        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ lmtnest    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ lpopl1     &lt;dbl&gt; 6.695799, 6.714170, 6.732211, 6.749931, 6.768493, 6.786717,…\n$ tot_100_g  &lt;dbl&gt; 0.3326812088, -0.0604966730, 0.0235692672, 0.6656700373, -0…\n$ year       &lt;dbl&gt; 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990,…\n\n\n\n\n\n\n\n\n\n\nVariable\nBeschreibung\n\n\n\nany_prior\nDummyvariable: Konflikt mit mind. 25 Toten im Jahr t\n\n\nwar_prio\nDummyvariable: Konflikt mit mind. 1000 Toten im Jahr t\n\n\nccode\nLänderkennung (numerisch)\n\n\ncountry\nName des Landes\n\n\ngdp_g\n(BIP_t - BIP_t-1) / BIP_t-1\n\n\ngdp_g_l\n(BIP_t-1 - BIP_t-2) / BIP_t-2\n\n\nGPCP_g\n(Niederschlag_t - Niederschlag_t-1) / Niederschlag_t-1\n\n\nGPCP_g_l\nGPCP_g in t-1\n\n\nGPCP_g_fl\nGPCP_g in t+1\n\n\ngdp_1979\nLog Pro-Kopf-BIP in 1979\n\n\npolity2_IV\nDiff. zw. Demokratie- und Autokrarie-Score im Jahr t (Skala v. -10 bis 10)\n\n\npolity2l\npolity2_IV im Jahr t-1 (Skala v. -10 bis 10)\n\n\nethfrac\nEthno-linguistische Fragmentierung (Anteil)\n\n\nrelfrac\nReligiöse Fragmentierung (Anteil)\n\n\noil\nDummy für Öl-exportierendes Land\n\n\nlmtnest\nLog Anteil Gebirgsregionen an Landesfläche\n\n\nlpopl1\nLog Bevölkerung im Jahr t-1\n\n\ntot_100_g\nHandelsbedingungen (Index, 1995 = 100)\n\n\nyear\nJahr der Beobachtung\n\n\n\n\n\n\n\nTabelle 8.4: rainconflict – Polit-ökonomische Charakteristika afrikanischer Länder v. 1981 bis 1999\n\n\n\n\n# ccode zu Typ 'factor' transformieren\nrainconflict &lt;- rainconflict %&gt;% \n  mutate(\n    ccode = as.factor(ccode)\n  )\n\n\nlibrary(modelsummary)\n\n# Statistische Zusammenfassung\ndatasummary(\n  formula = All( rainconflict %&gt;% select(-ccode, -year) )   \n    ~ (mean + sd) * Arguments(na.rm = TRUE) \n    + N, \n  fmt = 4,\n  data = rainconflict \n) \n\n\n\n \n\n  \n    \n\ntinytable_9tur4t73u2ptbm2uko1h\n\n\n      \n\n \n                mean\n                sd\n                N\n              \n\n\nany_prio  \n                  0.2678 \n                  0.4431\n                  743\n                \n\nwar_prio  \n                  0.1669 \n                  0.3731\n                  743\n                \n\ngdp_g     \n                  -0.0048\n                  0.0707\n                  743\n                \n\ngdp_g_l   \n                  -0.0056\n                  0.0724\n                  743\n                \n\nGPCP_g    \n                  0.0182 \n                  0.2094\n                  743\n                \n\nGPCP_g_l  \n                  0.0113 \n                  0.2067\n                  743\n                \n\nGPCP_g_fl \n                  0.0150 \n                  0.2107\n                  743\n                \n\ngdp_1979  \n                  1.1639 \n                  0.9009\n                  743\n                \n\npolity2l  \n                  -3.6083\n                  5.5543\n                  743\n                \n\npolity2_IV\n                  -3.4035\n                  5.5766\n                  736\n                \n\nethfrac   \n                  0.6546 \n                  0.2374\n                  743\n                \n\nrelfrac   \n                  0.4868 \n                  0.1857\n                  743\n                \n\noil       \n                  0.1184 \n                  0.3233\n                  743\n                \n\nlmtnest   \n                  1.5783 \n                  1.4334\n                  743\n                \n\nlpopl1    \n                  8.7497 \n                  1.2068\n                  743\n                \n\ntot_100_g \n                  -0.0068\n                  0.1552\n                  661\n                \n\n\n\n\n    \n\n\n\nTabelle 8.5: rainconflict – Statistische Zusammenfassung\n\n\n\n\nlibrary(fixest)\n\n# (1)\n(\n  rncnf_mod1 &lt;- feols(\n    fml = gdp_g ~ GPCP_g + GPCP_g_l,\n    data = rainconflict,\n    vcov = ~ ccode\n  )\n)\n\nOLS estimation, Dep. Var.: gdp_g\nObservations: 743\nStandard-errors: Clustered (ccode) \n             Estimate Std. Error  t value  Pr(&gt;|t|)    \n(Intercept) -0.006147   0.002460 -2.49856 0.0166787 *  \nGPCP_g       0.055430   0.016301  3.40029 0.0015378 ** \nGPCP_g_l     0.034058   0.013213  2.57761 0.0137398 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.069815   Adj. R2: 0.02088\n\n\n\n# (2)\nrncnf_mod2 &lt;- feols(\n  fml = gdp_g ~ GPCP_g + GPCP_g_l\n  + gdp_1979\n  + polity2l \n  + ethfrac \n  + relfrac \n  + oil \n  + lmtnest \n  + lpopl1\n  + year:ccode,\n  data = rainconflict,\n  vcov = ~ ccode\n) \n\nrncnf_mod2 %&gt;% \n  print(n = 10)\n\nOLS estimation, Dep. Var.: gdp_g\nObservations: 743\nStandard-errors: Clustered (ccode) \n             Estimate Std. Error   t value  Pr(&gt;|t|)    \n(Intercept) -8.380394   7.836789 -1.069366 0.2913157    \nGPCP_g       0.053402   0.016788  3.180892 0.0028359 ** \nGPCP_g_l     0.031518   0.013739  2.294105 0.0271135 *  \ngdp_1979    -0.568562   0.933728 -0.608916 0.5460230    \npolity2l    -0.000462   0.000697 -0.661827 0.5118772    \nethfrac     -0.290995   5.315419 -0.054745 0.9566138    \nrelfrac      6.344474   6.277193  1.010718 0.3182264    \noil         -0.022946   0.011735 -1.955363 0.0575517 .  \nlmtnest     -0.154883   0.950072 -0.163022 0.8713219    \nlpopl1      -0.103898   0.140671 -0.738588 0.4644693    \n... 41 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.067787   Adj. R2: 0.012893\n\n\n\n# (3)\nrncnf_mod3 &lt;- feols(\n  fml = gdp_g ~ GPCP_g + GPCP_g_l \n  + year:ccode\n  | ccode,\n  data = rainconflict,\n  vcov = ~ ccode\n) \n\nrncnf_mod3 %&gt;% \n  print(n = 2)\n\nOLS estimation, Dep. Var.: gdp_g\nObservations: 743\nFixed-effects: ccode: 41\nStandard-errors: Clustered (ccode) \n         Estimate Std. Error t value  Pr(&gt;|t|)    \nGPCP_g   0.048582   0.016523 2.94024 0.0054275 ** \nGPCP_g_l 0.028004   0.013779 2.03236 0.0487938 *  \n... 41 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.065802     Adj. R2: 0.023299\n                 Within R2: 0.086915\n\n\n\n# (4)\nrncnf_mod4 &lt;- feols(\n  fml = gdp_g ~ GPCP_g + GPCP_g_l + GPCP_g_fl\n  + year:ccode \n  | ccode,\n  data = rainconflict,\n  vcov = ~ ccode\n) \n\nrncnf_mod4 %&gt;% \n  print(n = 3)\n\nOLS estimation, Dep. Var.: gdp_g\nObservations: 743\nFixed-effects: ccode: 41\nStandard-errors: Clustered (ccode) \n          Estimate Std. Error  t value  Pr(&gt;|t|)    \nGPCP_g    0.048944   0.017837 2.743919 0.0090443 ** \nGPCP_g_l  0.028235   0.013783 2.048544 0.0471081 *  \nGPCP_g_fl 0.000617   0.018501 0.033337 0.9735719    \n... 41 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.065801     Adj. R2: 0.021818\n                 Within R2: 0.086917\n\n\n\n# (5)\nrncnf_mod5 &lt;- feols(\n  fml = gdp_g ~ GPCP_g + GPCP_g_l \n  + tot_100_g \n  + year:ccode \n  | ccode,\n  data = rainconflict,\n  vcov = ~ ccode\n) \n\nrncnf_mod5 %&gt;%\n  print(n = 3)\n\nOLS estimation, Dep. Var.: gdp_g\nObservations: 661\nFixed-effects: ccode: 37\nStandard-errors: Clustered (ccode) \n           Estimate Std. Error   t value  Pr(&gt;|t|)    \nGPCP_g     0.053124   0.017432  3.047509 0.0043049 ** \nGPCP_g_l   0.036616   0.014705  2.490060 0.0175258 *  \ntot_100_g -0.002280   0.022286 -0.102318 0.9190722    \n... 37 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.059582     Adj. R2: 0.052542\n                 Within R2: 0.114693\n\n\n\n# Tabellarischer Vergleich mit modelsummary()\n# (Tabelle 3 in Ditella und Schargrodsky, 2004)\nmodelsummary(\n  models = list(\n    \"(1)\" = rncnf_mod1, \n    \"(2)\" = rncnf_mod2, \n    \"(3)\" = rncnf_mod3, \n    \"(4)\" = rncnf_mod4, \n    \"(5)\" = rncnf_mod5\n  ),\n  stars = T, \n  coef_omit = \"^year.*$\",\n  gof_omit = \"^(?!(R2|Num.Obs.|FE.*)$).*\",\n  output = \"gt\"\n) %&gt;%\n  tabopts()\n\n\n\n\n\n\n\n\n(1)\n(2)\n(3)\n(4)\n(5)\n\n\n\n(Intercept)\n-0.006*\n-8.380\n\n\n\n\n\n\n(0.002)\n(7.837)\n\n\n\n\n\nGPCP_g\n0.055**\n0.053**\n0.049**\n0.049**\n0.053**\n\n\n\n(0.016)\n(0.017)\n(0.017)\n(0.018)\n(0.017)\n\n\nGPCP_g_l\n0.034*\n0.032*\n0.028*\n0.028*\n0.037*\n\n\n\n(0.013)\n(0.014)\n(0.014)\n(0.014)\n(0.015)\n\n\ngdp_1979\n\n-0.569\n\n\n\n\n\n\n\n(0.934)\n\n\n\n\n\npolity2l\n\n0.000\n\n\n\n\n\n\n\n(0.001)\n\n\n\n\n\nethfrac\n\n-0.291\n\n\n\n\n\n\n\n(5.315)\n\n\n\n\n\nrelfrac\n\n6.344\n\n\n\n\n\n\n\n(6.277)\n\n\n\n\n\noil\n\n-0.023+\n\n\n\n\n\n\n\n(0.012)\n\n\n\n\n\nlmtnest\n\n-0.155\n\n\n\n\n\n\n\n(0.950)\n\n\n\n\n\nlpopl1\n\n-0.104\n\n\n\n\n\n\n\n(0.141)\n\n\n\n\n\nGPCP_g_fl\n\n\n\n0.001\n\n\n\n\n\n\n\n(0.019)\n\n\n\ntot_100_g\n\n\n\n\n-0.002\n\n\n\n\n\n\n\n(0.022)\n\n\nNum.Obs.\n743\n743\n743\n743\n661\n\n\nR2\n0.024\n0.079\n0.133\n0.133\n0.162\n\n\nFE: ccode\n\n\nX\nX\nX\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\nTabelle 8.6: First-Stage-Regressionen für gdp_g\n\n\n\n\nS1_res &lt;- tibble(\n\n  x = residuals(\n    feols(\n      fml = GPCP_g ~ GPCP_g_l\n      + year:ccode\n      | ccode,\n      data = rainconflict\n    )\n  ),\n  \n  y = residuals(\n    feols(\n      fml = gdp_g ~ GPCP_g_l\n      + year:ccode\n      | ccode,\n      data = rainconflict\n    )\n  )\n  \n)\n\n\nlibrary(ggplot2)\nlibrary(cowplot)\n\nggplot(\n  data = S1_res,\n  mapping = aes(x = x, y = y)) +\n  geom_point(\n    size = .75, \n    alpha = .5\n  ) +\n  geom_smooth(method = \"loess\", span = .5) +\n  coord_cartesian(\n    xlim = c(-.4, .4), \n    ylim = c(-.1, .1)\n  ) +\n  theme_cowplot()\n\n\n\n\n\n\n\n\nfeols(\n  fml = any_prio ~ GPCP_g + GPCP_g_l \n  + year:ccode\n  | ccode,\n  data = rainconflict,\n  vcov = ~ ccode\n) %&gt;% \n  print(n = 2)\n\nOLS estimation, Dep. Var.: any_prio\nObservations: 743\nFixed-effects: ccode: 41\nStandard-errors: Clustered (ccode) \n          Estimate Std. Error   t value Pr(&gt;|t|)    \nGPCP_g   -0.023770   0.041969 -0.566373  0.57430    \nGPCP_g_l -0.121936   0.050328 -2.422836  0.02002 *  \n... 41 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.239168     Adj. R2: 0.671564\n                 Within R2: 0.374052\n\n\n\nfeols(\n  fml = war_prio ~ GPCP_g + GPCP_g_l \n  + year:ccode\n  | ccode,\n  data = rainconflict,\n  vcov = ~ ccode\n) %&gt;% \n  print(n = 2)\n\nOLS estimation, Dep. Var.: war_prio\nObservations: 743\nFixed-effects: ccode: 41\nStandard-errors: Clustered (ccode) \n          Estimate Std. Error  t value Pr(&gt;|t|)    \nGPCP_g   -0.062476   0.029051 -2.15060 0.037605 *  \nGPCP_g_l -0.068689   0.030678 -2.23904 0.030783 *  \n... 41 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.204541     Adj. R2: 0.661198\n                 Within R2: 0.385205\n\n\n\nrf_res &lt;- tibble(\n  \n  x = residuals(\n    feols(\n      fml = GPCP_g_l ~ GPCP_g\n      + year:ccode\n      | ccode,\n      data = rainconflict\n    )\n  ),\n  \n  y = residuals(\n    feols(\n      fml = any_prio ~ GPCP_g\n      + year:ccode\n      | ccode,\n      data = rainconflict\n    )\n  )\n  \n)\n\n\nggplot(\n  data = rf_res , \n  mapping = aes(x = x, y = y)) +\n  geom_point(pch = 19) +\n  geom_smooth(method = \"loess\") +\n  coord_cartesian(\n    xlim = c(-.4, .4), \n    ylim = c(-.2, .4)\n  ) +\n  theme_cowplot()\n\n\n\n\n\n\n\n\n# (1)\nmod_conf_probit &lt;- feglm(\n  fml = any_prio ~\n    gdp_g + \n    gdp_g_l\n  + gdp_1979\n  + polity2l \n  + ethfrac \n  + relfrac \n  + oil \n  + lmtnest \n  + lpopl1 \n  + year,\n  data = rainconflict,\n  vcov = ~ ccode, \n  family = binomial(link = \"probit\")\n) \n\nlibrary(marginaleffects)\n\nmod_conf_probit_avge &lt;- mod_conf_probit %&gt;% \n  avg_slopes() \n\n# (2)\n(\nmod_conf_ols &lt;- feols(\n  fml = any_prio ~\n    gdp_g + \n    gdp_g_l\n  + gdp_1979\n  + polity2l \n  + ethfrac \n  + relfrac \n  + oil \n  + lmtnest \n  + lpopl1 \n  + year,\n  data = rainconflict,\n  vcov = ~ ccode\n) \n)\n\nOLS estimation, Dep. Var.: any_prio\nObservations: 743\nStandard-errors: Clustered (ccode) \n             Estimate Std. Error   t value Pr(&gt;|t|)    \n(Intercept) -5.381928  12.515620 -0.430017 0.669491    \ngdp_g       -0.332716   0.263400 -1.263157 0.213846    \ngdp_g_l     -0.084997   0.240857 -0.352894 0.726021    \ngdp_1979    -0.040662   0.049894 -0.814961 0.419921    \npolity2l     0.000642   0.004515  0.142164 0.887664    \nethfrac      0.230431   0.271367  0.849151 0.400851    \nrelfrac     -0.237764   0.241595 -0.984144 0.330961    \noil          0.045297   0.211526  0.214144 0.831523    \nlmtnest      0.075811   0.039371  1.925548 0.061291 .  \nlpopl1       0.068052   0.051102  1.331675 0.190506    \nyear         0.002483   0.006366  0.390106 0.698528    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.414086   Adj. R2: 0.113662\n\n# (3)\nfeols(\n  fml = any_prio ~ -1\n  +  gdp_g + \n    gdp_g_l\n  + gdp_1979\n  + polity2l \n  + ethfrac \n  + relfrac \n  + oil \n  + lmtnest \n  + lpopl1 \n  + i(ccode, year),\n  data = rainconflict,\n  vcov = ~ ccode\n) %&gt;% \n  summary(n = 10)\n\nOLS estimation, Dep. Var.: any_prio\nObservations: 743\nStandard-errors: Clustered (ccode) \n                 Estimate Std. Error   t value Pr(&gt;|t|)    \ngdp_g           -0.387996   0.208988 -1.856550 0.070751 .  \ngdp_g_l         -0.086461   0.212442 -0.406986 0.686188    \ngdp_1979         2.287334   9.856030  0.232075 0.817663    \npolity2l        -0.001355   0.004148 -0.326768 0.745547    \nethfrac         43.124761  50.926744  0.846800 0.402145    \nrelfrac         39.765573  63.751088  0.623763 0.536324    \noil              0.006172   0.086301  0.071511 0.943347    \nlmtnest         -3.835094   7.462461 -0.513918 0.610137    \nlpopl1           1.211169   0.671842  1.802760 0.078964 .  \nccode::404:year -0.033077   0.019564 -1.690721 0.098669 .  \n... 40 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.294151   Adj. R2: 0.526933\n\n# (4)\nfeols(\n  fml = any_prio ~ -1\n  +  gdp_g \n  +  gdp_g_l\n  + i(ccode, year)\n  | ccode,\n  data = rainconflict,\n  vcov = ~ ccode\n) %&gt;% \n  summary(n = 10)\n\nOLS estimation, Dep. Var.: any_prio\nObservations: 743\nFixed-effects: ccode: 41\nStandard-errors: Clustered (ccode) \n                 Estimate Std. Error    t value  Pr(&gt;|t|)    \ngdp_g           -0.210909   0.156046  -1.351587   0.18410    \ngdp_g_l          0.066801   0.158944   0.420279   0.67653    \nccode::404:year  0.028626   0.001635  17.510274 &lt; 2.2e-16 ***\nccode::420:year -0.015243   0.000745 -20.452968 &lt; 2.2e-16 ***\nccode::432:year  0.007220   0.000373  19.349039 &lt; 2.2e-16 ***\nccode::433:year  0.059746   0.000225 265.584211 &lt; 2.2e-16 ***\nccode::434:year  0.000449   0.000485   0.924683   0.36068    \nccode::435:year  0.000223   0.000544   0.410843   0.68338    \nccode::436:year  0.035483   0.000574  61.802750 &lt; 2.2e-16 ***\nccode::437:year  0.000284   0.001085   0.261893   0.79475    \n... 33 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.239816     Adj. R2: 0.669782\n                 Within R2: 0.370655\n\n#### IV models ####\n\n# (5)\nfeols(\n  fml = any_prio ~\n  + gdp_1979\n  + polity2l \n  + ethfrac \n  + relfrac \n  + oil \n  + lmtnest \n  + lpopl1 \n  + i(ccode, year)\n  | gdp_g + gdp_g_l ~ GPCP_g + GPCP_g_l, \n  data = rainconflict,\n  vcov = ~ ccode\n) %&gt;% \n  summary(n = 10)\n\nTSLS estimation - Dep. Var.: any_prio\n                  Endo.    : gdp_g, gdp_g_l\n                  Instr.   : GPCP_g, GPCP_g_l\nSecond stage: Dep. Var.: any_prio\nObservations: 743\nStandard-errors: Clustered (ccode) \n               Estimate Std. Error   t value Pr(&gt;|t|)    \n(Intercept) -153.157460  60.518562 -2.530752 0.015419 *  \nfit_gdp_g     -1.343217   1.462578 -0.918390 0.363920    \nfit_gdp_g_l   -2.714015   1.036990 -2.617205 0.012453 *  \ngdp_1979       9.446193  11.069500  0.853353 0.398545    \npolity2l      -0.005806   0.004843 -1.198875 0.237631    \nethfrac       83.847478  47.883805  1.751061 0.087601 .  \nrelfrac       95.038889  64.178048  1.480863 0.146478    \noil           -0.058890   0.066111 -0.890779 0.378375    \nlmtnest        2.159746   7.025786  0.307403 0.760132    \nlpopl1        -0.336984   0.868944 -0.387809 0.700213    \n... 41 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.346685   Adj. R2: 0.342806\nF-test (1st stage), gdp_g  : stat = 7.81825, p = 4.389e-4, on 2 and 692 DoF.\nF-test (1st stage), gdp_g_l: stat = 5.56598, p = 0.003999, on 2 and 692 DoF.\n                 Wu-Hausman: stat = 2.56365, p = 0.077756, on 2 and 690 DoF.\n\n# (6)\nfeols(\n  fml = any_prio ~ \n  + i(ccode, year)\n  | ccode\n  | gdp_g + gdp_g_l ~ GPCP_g + GPCP_g_l, \n  data = rainconflict,\n  vcov = ~ ccode\n) %&gt;% \n  summary(n = 10)\n\nTSLS estimation - Dep. Var.: any_prio\n                  Endo.    : gdp_g, gdp_g_l\n                  Instr.   : GPCP_g, GPCP_g_l\nSecond stage: Dep. Var.: any_prio\nObservations: 743\nFixed-effects: ccode: 41\nStandard-errors: Clustered (ccode) \n                 Estimate Std. Error   t value   Pr(&gt;|t|)    \nfit_gdp_g       -1.131763   1.362254 -0.830802 4.1102e-01    \nfit_gdp_g_l     -2.546473   1.070475 -2.378825 2.2230e-02 *  \nccode::404:year  0.006632   0.013806  0.480372 6.3358e-01    \nccode::420:year -0.005215   0.006294 -0.828648 4.1222e-01    \nccode::432:year  0.012689   0.003059  4.148704 1.6945e-04 ***\nccode::433:year  0.063170   0.001807 34.960834  &lt; 2.2e-16 ***\nccode::434:year  0.006202   0.004195  1.478344 1.4715e-01    \nccode::435:year  0.008547   0.004353  1.963488 5.6568e-02 .  \nccode::436:year  0.043387   0.004817  9.007338 3.5998e-11 ***\nccode::437:year  0.017506   0.008435  2.075379 4.4425e-02 *  \n... 33 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.302046     Adj. R2: 0.476172\n                 Within R2: 0.001663\nF-test (1st stage), gdp_g  : stat = 6.67399, p = 0.001345, on 2 and 699 DoF.\nF-test (1st stage), gdp_g_l: stat = 5.70442, p = 0.003488, on 2 and 699 DoF.\n                 Wu-Hausman: stat = 3.14562, p = 0.043689, on 2 and 657 DoF.\n\n# (7)\nfeols(\n  fml = war_prio ~ \n    + i(ccode, year)\n  | ccode\n  | gdp_g + gdp_g_l ~ GPCP_g + GPCP_g_l, \n  data = rainconflict,\n  vcov = ~ ccode\n) %&gt;% \n  summary(n = 10)\n\nTSLS estimation - Dep. Var.: war_prio\n                  Endo.    : gdp_g, gdp_g_l\n                  Instr.   : GPCP_g, GPCP_g_l\nSecond stage: Dep. Var.: war_prio\nObservations: 743\nFixed-effects: ccode: 41\nStandard-errors: Clustered (ccode) \n                 Estimate Std. Error   t value  Pr(&gt;|t|)    \nfit_gdp_g       -1.479970   0.800386 -1.849071  0.071848 .  \nfit_gdp_g_l     -0.768785   0.678533 -1.133010  0.263956    \nccode::404:year -0.001514   0.007526 -0.201188  0.841571    \nccode::420:year  0.007088   0.003431  2.066077  0.045340 *  \nccode::432:year  0.003375   0.001670  2.020730  0.050037 .  \nccode::433:year  0.015992   0.000992 16.119071 &lt; 2.2e-16 ***\nccode::434:year  0.004765   0.002306  2.065964  0.045351 *  \nccode::435:year  0.004698   0.002393  1.963229  0.056599 .  \nccode::436:year  0.005405   0.002624  2.059408  0.046005 *  \nccode::437:year  0.008802   0.004699  1.872980  0.068392 .  \n... 33 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.228949     Adj. R2: 0.575516\n                 Within R2: 0.229726\nF-test (1st stage), gdp_g  : stat = 6.67399, p = 0.001345, on 2 and 699 DoF.\nF-test (1st stage), gdp_g_l: stat = 5.70442, p = 0.003488, on 2 and 699 DoF.\n                 Wu-Hausman: stat = 1.46546, p = 0.231725, on 2 and 657 DoF.\n\nlibrary(modelsummary)\n\nmodelsummary(\n  models = list(\n    mod_conf_probit_avge, \n    mod_conf_ols\n    )\n  )\n\n \n\n  \n    \n\ntinytable_1k91f2rr4kc2az6a073x\n\n\n      \n\n \n                (1)\n                (2)\n              \n\n\nethfrac    \n                  0.220  \n                  0.230    \n                \n\n           \n                  (0.242)\n                  (0.271)  \n                \n\ngdp_1979   \n                  -0.063 \n                  -0.041   \n                \n\n           \n                  (0.058)\n                  (0.050)  \n                \n\ngdp_g      \n                  -0.350 \n                  -0.333   \n                \n\n           \n                  (0.301)\n                  (0.263)  \n                \n\ngdp_g_l    \n                  -0.127 \n                  -0.085   \n                \n\n           \n                  (0.251)\n                  (0.241)  \n                \n\nlmtnest    \n                  0.072  \n                  0.076    \n                \n\n           \n                  (0.036)\n                  (0.039)  \n                \n\nlpopl1     \n                  0.074  \n                  0.068    \n                \n\n           \n                  (0.049)\n                  (0.051)  \n                \n\noil        \n                  0.015  \n                  0.045    \n                \n\n           \n                  (0.210)\n                  (0.212)  \n                \n\npolity2l   \n                  0.001  \n                  0.001    \n                \n\n           \n                  (0.005)\n                  (0.005)  \n                \n\nrelfrac    \n                  -0.271 \n                  -0.238   \n                \n\n           \n                  (0.241)\n                  (0.242)  \n                \n\nyear       \n                  0.003  \n                  0.002    \n                \n\n           \n                  (0.006)\n                  (0.006)  \n                \n\n(Intercept)\n                         \n                  -5.382   \n                \n\n           \n                         \n                  (12.516) \n                \n\nNum.Obs.   \n                  743    \n                  743      \n                \n\nR2         \n                  0.122  \n                  0.126    \n                \n\nR2 Adj.    \n                  0.099  \n                  0.114    \n                \n\nAIC        \n                  779.8  \n                  820.4    \n                \n\nBIC        \n                  830.6  \n                  871.1    \n                \n\nRMSE       \n                  0.42   \n                  0.41     \n                \n\nStd.Errors \n                         \n                  by: ccode\n                \n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAcemoglu, Daron, Giuseppe De Feo, und Giacomo Davide De Luca. 2020. „Weak States: Causes and Consequences of the Sicilian Mafia“. The Review of Economic Studies. https://doi.org/10.1093/restud/rdz009.\n\n\nCameron, A. Colin, Jonah B. Gelbach, und Douglas L. Miller. 2008. „Bootstrap-Based Improvements for Inference with Clustered Errors“. Review of Economics and Statistics 90 (3): 414–27. https://doi.org/10.1162/rest.90.3.414.\n\n\n———. 2011. „Robust Inference With Multiway Clustering“. Journal of Business &amp; Economic Statistics 29 (2): 238–49. https://doi.org/10.1198/jbes.2010.07136.\n\n\nCutrera, Antonino. 1900. La Mafia E I Mafiosi. (Palermo, IT: Reber).\n\n\nFearon, James D., und David D. Laitin. 2003. „Ethnicity, Insurgency, and Civil War.“ American Political Science Review 97 (01): 75–90. https://doi.org/10.1017/s0003055403000534.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. Chapman; Hall/CRC. https://doi.org/10.1201/9781003226055.\n\n\nLiu, Regina Y. 1988. „Bootstrap Procedures under some Non-I.I.D. Models“. The Annals of Statistics 16 (4): 1696–1708. https://doi.org/10.1214/aos/1176351062.\n\n\nMiguel, Edward, Shanker Satyanath, und Ernest Sergenti. 2004. „Economic Shocks and Civil Conflict: An Instrumental Variables Approach“. Journal of Political Economy 112 (4): 725–53. https://doi.org/10.1086/421174.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IV-Regression</span>"
    ]
  },
  {
    "objectID": "DiD.html",
    "href": "DiD.html",
    "title": "\n9  Difference-in-Differences\n",
    "section": "",
    "text": "9.1 Einordnung im Potential Outcomes Framework\nIm Potential Outcomes Framework nehmen wir an, dass jede Einheit \\(i\\) in Abhänigkeit ihres Behandlungsstatus zwei potentielle Ergebnisse hat. Wir unterscheiden zwischen Beobachtungen in Behandlungs- und Kontrollgruppe:\nIn einem DID-Forschungsdesign hängen tatsächliche und potentielle Outcomes von der Zeit \\(t\\) ab: Die Behandlungsgruppe wird zwischen den Zeitpunkten \\(t = 0\\) und \\(t = 1\\) behandelt, während die Kontrollgruppe unbehandelt bleibt. Für die Identifizierung des Behandlungseffekts wird unterstellt, dass \\(Y\\) sich zwischen \\(t=0\\) und \\(t=1\\) in der Behandlungsgruppe ohne eine Behandlung (im Erwartungswert) mit demselben Trend entwickelt hätte, mit dem sich die Kontrollgruppe tatsächlich entwickelt hat (parallele Trends). Die Gültigkeit paralleler Trends ist entscheidend für die Validität der DID-Methode, da so sicherstellt ist, dass die beobachteten Unterschiede in den Ergebnissen auf die Behandlung zurückzuführen sind und nicht auf andere zeitgleich auftretende Faktoren. Der Behandlungseffekt kann dann als eine Differenz von Differenzen geschrieben werden:\n\\[\\begin{align}\n  \\begin{split}\n    \\beta_\\textup{DID} =& \\, \\bigg({\\color{red}\\textup{E}\\big[Y_B(1)\\vert t=1\\big] - \\textup{E}\\big[Y_B(0)\\vert t=0\\big]} \\bigg)\\\\\n  -&\\, \\bigg({\\color{blue}\\textup{E}\\big[Y_K(0)\\vert t=1\\big] -  \\textup{E}\\big[Y_K(0)\\vert t=0\\big]} \\bigg)\n  \\end{split}\\label{eq:DID-ATT1}\n\\end{align}\\]\nDer Effekt \\(\\beta_\\textup{DID}\\) ist ein ATT, der über das Schließen der Backdoor in der Zeit (rote und blaue Differenzen der Erwartungswerte zwischen \\(t=0\\) und \\(t=1\\)) sowie der Backdoor in der Gruppenzugehörigkeit (Differenz der Erwartungswert-Differenzen) identifiziert wird.\nEine Null-Ergänzung von \\(\\eqref{eq:DID-ATT1}\\) mit \\({\\color{blue}\\textup{E}\\big[Y_K(0)\\vert t=1\\big] -  \\textup{E}\\big[Y_K(0)\\vert t=1\\big]}\\) zeigt die Wichtigkeit der Gültigkeit paralleler Trends:\n\\[\\begin{align*}\n    \\beta_\\textup{DID} =& \\, \\bigg({\\color{red}\\textup{E}\\big[Y_B(1)\\vert t=1\\big] - \\textup{E}\\big[Y_B(0)\\vert t=0\\big]} \\bigg)\n  - \\bigg({\\color{blue}\\textup{E}\\big[Y_K(0)\\vert t=1\\big] -  \\textup{E}\\big[Y_K(0)\\vert t=0\\big]} \\bigg)\\\\ + &\\,\n{\\color{blue}\\textup{E}\\big[Y_K(0)\\vert t=1\\big] -  \\textup{E}\\big[Y_K(0)\\vert t=1\\big]}\\\\\n    \\\\\n    =&\\, \\underbrace{{\\color{red}\\textup{E}\\big[Y_B(1)\\vert t=1\\big]} - {\\color{blue}\\textup{E}\\big[Y_B(1)\\vert t=1\\big]}}_{=\\textup{ATT}}\\\\\n    +&\\, \\underbrace{\\bigg({\\color{red}\\textup{E}\\big[Y_B(0)\\vert t=1\\big] - \\textup{E}\\big[Y_B(0)\\vert t=0\\big]} \\bigg) - \\bigg({\\color{blue}\\textup{E}\\big[Y_K(0)\\vert t=1\\big] -  \\textup{E}\\big[Y_K(0)\\vert t=0\\big]} \\bigg)}_{ = \\textup{Verzerrung durch nicht-parallele Trends}}\n\\end{align*}\\]\nDiese Zerlegung zeigt, dass der ATT nur bei parallelen Trends identifziert werden kann, d.h. wir benötigen\n\\[\\begin{align*}\n{\\color{red}\\textup{E}\\big[Y_B(0)\\vert t=1\\big] - \\textup{E}\\big[Y_B(0)\\vert t=0\\big]} = {\\color{blue}\\textup{E}\\big[Y_K(0)\\vert t=1\\big] -  \\textup{E}\\big[Y_K(0)\\vert t=0\\big]}.\n\\end{align*}\\]\nBeachte, dass \\({\\color{red}\\textup{E}\\big[Y_B(0)\\vert t=1\\big]}\\) der Erwartungswert des potentiellen Outcomes einer unbehandelten Behandlungsgruppe in \\(t=1\\) ist. Somit kann die Verzerrung durch nicht-parallele Trends nicht empirisch überprüft werden und muss ausschließlich durch das Forschungsdesign gewährleistet sein. In Anwendungen kann die Plausibilität der Annahme graphisch anhand geschätzter Trends in der Outcome-Variable oder durch Placebo-Tests untersucht werden.\nAnnahmen für DID",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "DiD.html#einordnung-im-potential-outcomes-framework",
    "href": "DiD.html#einordnung-im-potential-outcomes-framework",
    "title": "\n9  Difference-in-Differences\n",
    "section": "",
    "text": "\\(Y_{i,B}(1)\\): \\(Y\\) für Einheit \\(i\\) in der Behandlungsgruppe, wenn diese behandelt wird.\n\n\\(Y_{i,B}(0)\\): \\(Y\\) für Einheit \\(i\\) in der Behandlungsgruppe, wenn diese nicht behandelt wird.\n\n\\(Y_{i,K}(1)\\): \\(Y\\) für Einheit \\(i\\) in der Kontrollgruppe, wenn diese behandelt wird.\n\n\\(Y_{i,K}(0)\\): \\(Y\\) für Einheit \\(i\\) in der Kontrollgruppe, wenn diese nicht behandelt wird.\n\n\n\n\n\n\n\n\n\n\n\nParallele Trends: Die Trends in der Outcome Variable \\(Y\\) in Behandlungs- und Kontrollgruppe würden bis einschließlich \\(t=1\\) parallel verlaufen, wenn es keine Behandlung gäbe. Diese Annahme ist Voraussetzung dafür, dass Veränderungen im Outcome \\(Y\\) für die Behandlungsgruppe, die sich von \\(Y\\) für die Kontrollgruppe unterscheidet, auschließlich dem Effekt der Behandlung zugeschrieben werden kann.\n\nKeine Interferenz und konsistente Behandlung (SUTVA):\n\nKeine Interferenz: Die Behandlung eines Individums hat keinen Einfluss auf das potentielle Outcome anderer Individuen, unabhängig von der Gruppenzugehörigkeit.\nKonsistete Behandlung: Es gibt keine Variation in der Intensität oder Art der Behandlung innerhalb der Behandlungsgruppe.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "DiD.html#schätzung-des-att-mit-did",
    "href": "DiD.html#schätzung-des-att-mit-did",
    "title": "\n9  Difference-in-Differences\n",
    "section": "\n9.2 Schätzung des ATT mit DID",
    "text": "9.2 Schätzung des ATT mit DID\nFür die Schätzung von \\(\\beta_\\text{DID}\\) ersetzen wir die Erwartungswerte in \\(\\eqref{eq:DID-ATT1}\\) durch ihre Stichprobenmomente. Dies liefert den Schätzer\n\\[\\begin{align}\n  \\widehat{\\beta}_\\textup{DID} = \\bigg({\\color{red}\\overline{Y_B(1)\\vert t=1} - \\overline{Y_B(0)\\vert t=0}} \\bigg) -  \\bigg({\\color{blue}\\overline{Y_K(0)\\vert t=1} - \\overline{Y_K(0)\\vert t=0}}\\bigg). \\label{eq:DIDMOMENTS}\n\\end{align}\\]\nDie Implementierung von DID-Schätzern erfolgt meist anhand linearer Regression. Das Modell für zwei Zeitperioden ist\n\\[\\begin{align}\n  Y_{i,\\,t} = \\alpha + \\beta_1 B_i + \\beta_2 Z_t + \\beta_3 (B_i \\times Z_t) + \\epsilon_{i,\\,t}, \\quad t\\in\\{0,1\\}, \\label{eq:DIDREG}\n\\end{align}\\]\nwobei \\(\\beta_3\\) der interessierende Behandlungseffekt ist. Der Regressor \\(B_i \\times Z_t\\) ist die Interaktion zwischen der Behandlungsgruppenzugehörigkeit \\(B_i\\) und einem Indikator für den Zeitpunkt nach der Intervention, \\(Z_i = \\mathbb{I}_{\\{t=1\\}}\\). Beachte, dass wir in Modell \\(\\eqref{eq:DIDREG}\\) für Zeiteffekte und Gruppenzugehörig kontrollieren und damit die sich durch das Forschungsdesign ergebenden Backdoors (vgl. Abbildung 9.1) schließen.\nEs ist \\(\\widehat\\beta_3 = \\widehat{\\beta}_\\text{DID}\\), d.h. der KQ-Schätzer von \\(\\beta_3\\) ist der DID-Schätzer des ATT und numerisch äquivalent zu \\(\\eqref{eq:DIDMOMENTS}\\). Die Berechnung von \\(\\widehat\\beta_\\text{DID}\\) anhand von Modell \\(\\eqref{eq:DIDREG}\\) ist praktisch, da wir so Inferenzstatistiken mit etablierten R-Funktionen wie summary() und lmtest::coeftest() wie gewohnt berechnen können.\nIn empirischen Anwendungen stehen oft Datensätze mit mehreren Gruppen und mehr als zwei Beobachtungsperioden zur Verfügung. Beachte, dass das Modell \\(\\eqref{eq:DIDREG}\\) ein Spezialfall des allgemeinen Forschungsdesigns mit \\(t=1,\\dots,T\\) für \\(T\\geq2\\) Beobachtungsperioden und mehr als zwei Gruppen (mehrere Kontroll- und Behandlungsgruppen) ist. Eine dann häufig genutzte Modellspezifikation für die Schätzung des ATT mit DID ist eine Panel-Regression mit Two-way Fixed Effects,\n\\[\\begin{align}\n  Y_{i,\\,t} =  \\theta_i + \\eta_t + \\beta_\\text{DID}^\\text{TWFE} D_{i,\\, t} + \\epsilon_{i,\\,t}, \\quad t = 1,\\dots,T, \\label{eq:TWFEDIDREG}\n\\end{align}\\]\nwobei \\(\\theta_i\\) und \\(\\eta_t\\) Dummy-Variablen für Gruppen und Zeitperioden sind und \\(D_{i,\\, t}\\) der Behandlungsindikator ist. Dieses lineare Paneldaten-Modell kann komfortabel mit dem R-Paket fixest (s. fixtest::feols()) implementiert werden. Bei mehreren Gruppen sollten cluster-robuste Standardfehler auf Gruppen-Ebene verwendet werden.\nIn Modell \\(\\eqref{eq:TWFEDIDREG}\\) indentifiziert \\(\\beta_\\text{DID}^\\text{TWFE}\\) den ATE, sofern die Annahmen 1 (parallele Trends) und 2 (SUTVA) gelten. Damit die Annahme paralleler Trends gewährleistet ist, dürfen keine heterogenen Behandlungseffekte vorliegen, d.h. die Behandlungseffekte\n\nvariieren nicht zwischen verschiedenen Gruppen\nsind unabhängig vom Zeitpunkt der Behandlung (relevant bei unterschiedlichen Behandlungszeitpunkten)\nentwickeln sich nicht dynamisch über die Zeit1\n\n\n1 Siehe bspw. Goodman-Bacon (2021) für eine detaillierte Diskussion dieser Problematik.2 Die Methoden von Callaway und Sant’Anna (2021) sind im R-Paket did implementiert.Der Umgang mit heterogenen Behandlungseffekten ist Gegenstand der aktuellen ökonometrischen Forschung zu DID-Schätzern. Callaway und Sant’Anna (2021) schlagen eine nicht-parametrische Schätzung von gruppenspezifischen ATE zu veschiedenen Zeitpunkten vor, die zu einem globalen ATT zusammengefasst werden können.2\n\n\n\n\n\n\nKey Facts zum einfachen DID-Schätzer\n\n\n\n\nIm DID-Forschungsdesign kann der ATT durch einen Vergleich von Differenzen in den Ergebnissen vor und nach einer Behandlung zwischen Behandlungs- und Kontrollgruppen identifiziert werden.\nDID benötigt Beobachtungen einer Behandlungs- und einer Kontrollgruppe zu mindestens zwei verschiedenen Zeitpunkten, wobei der Behandlung zwischen diesen Zeitpunkten erfolgt.\nDID ist empfindlich gegenüber Verletzungen der Annahme, dass die zeitlichen Trends in der Outcome-Variable für die Behandlungs- und die Kontrollgruppen vor der Intervention parallel verlaufen.\nDID-Schätzer können in linearen Interaktionsmodellen mit Fixed Effects für Zeitperioden und Gruppenzugehörigkeit implementiert werden. Der interessierenden Effekt sind die Koeffizienten von Interaktionstermem zwischen den Indikatoren für die Nachbehandlungsperioden und für die Zugehörigkeit zu einer Behandlungsgruppe.\nIn R können DID-Modelle mit lm() oder, in Fällen mit mehr als zwei Beobachtungsperioden, mit fixest::feols() geschätzt werden. In Forschungsdesigns mit mehreren Gruppen sollten cluster-robuste Standardfehler verwendet werden.\n\n\n\n\nDie nachfolgende interaktive Grafik illustriert die Schätzung des ATT mit DID sowie die Verletzung der Annahme paralleler Trends anhand simulierte Daten für mehrere Zeitperioden. Der verwendete DID-Schätzer ist der KQ-Schätzer in Modell \\(\\eqref{eq:DIDREG}\\), d.h. wir betrachten ein Forschungsdesign in dem zwei Zeitperioden für die Schätzung verwendet werden, wobei die Behandlung zwischen diesen Perioden erfolgt.\nInteraktive Elemente der Visualisierung\n\nDie Beobachtungen der Individuen zu 6 verschiedenen Zeitpunkten werden als Punkte dargestellt. Die Datenpunkte könn mit Zeige Daten ein- und ausgeblendet werden.\nDie geschätzten Trends beider Gruppen für den gesamten Beobachtungszeitraum und die Gruppenzugehörigkeit können mit Zeige Trends ein- und ausgeblendet werden. Die Auswahl Parallele Trends stellt sicher, dass beide Gruppen (mit Ausnahme des Behandlungseffekts in der Behandlungsgruppe) dem selben zeitlichen Trend folgen. Bei nicht-parallelen Trends folgt die Behandlungsgruppe einem positiven Trend mit größerer positiver Steigung als in der Kontrollgruppe.\nDie Behandlung erfolgt zwischen der mit dem Slider Zeitpunkt ausgewählten und der darauf folgenden Periode. Der tatsächliche Behandlungseffekt kann über den Slider Effekt festgelegt werden.\n\nAnatomie der Schätzung des ATT bei parallelen Trends\n\nWir illustrieren die Schätzen des ATT mit Formel \\(\\eqref{eq:DIDMOMENTS}\\). Kreise zeigen Mittelwerte für Kontroll- und Behandlungsgruppe vor der Intervention. Dreiecke zeigen Mittelwerte nach der Intervention.\nDie gestrichelte rote Linie zeigt den (kontrafaktischen) Verlauf der Behandlungsgruppe ohne Behandlung. Hierbei wird unterstellt, dass sich die Behandlungsgruppe mit demselben Trend wie die Kontrollgruppe entwickelt hätte (blaue Linie).\nDer geschätzte Behandlungseffekt wird als orangene vertikale Linie dargestellt. Dies ist die Differenz zwischen dem tatsächlichen post-Behandlungs-Mittelwert und dem kontrafaktischen Mittelwert der Behandlungsgruppe.\n\nAnatomie der Schätzung des ATT bei nicht-parallelen Trends\n\nFür nicht-parallele Trends zeigt die Grafik den unterstellten kontrafaktischen Trend der Behandlungsgruppe als gestrichelte blaue linie. Der” “tatsächliche” kontrafaktische Verlauf der Behandlungsgruppe wird als gestrichelte rote Linie dargestellt.\n\nAufgrund des steileren (positiven) Trends in der Behandlungsgruppe ergibt sich eine positive Verzerrung von \\(\\color{orange}\\widehat{\\beta}_\\text{DID}\\). Diese Verzerrung wird durch die gestrichelte vertikale schwarze Linie kenntlich gemacht.\n\nFür positive Behandlungseffekte wird der ATT überschätzt: die Verzerrung entspricht der Überlagerung der gestrichelten schwarzen linie mit der orangenen Linie des geschätzten Effekts.\nFür negative Behandlungseffekte wird der ATT unterschätzt: die Verzerrung entspricht der gestrichelten schwarzen Linie oberhalb der orangenen Linie des geschätzten Effekts.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "DiD.html#schätzung-von-did-forschungsdesigns-mit-r",
    "href": "DiD.html#schätzung-von-did-forschungsdesigns-mit-r",
    "title": "\n9  Difference-in-Differences\n",
    "section": "\n9.3 Schätzung von DID-Forschungsdesigns mit R",
    "text": "9.3 Schätzung von DID-Forschungsdesigns mit R\nWir erläutern nachfolgend die Schätzung von DID-Designs mit zwei Zeitperioden mit R und visualisieren die geschätzten Komponenten von \\(\\widehat{\\beta}_\\text{DID}\\) ähnlich wie in der interaktiven Visualisierung. Hierzu erzeugen wir simulierte Daten gemäß der Vorschrift\n\\[\\begin{align*}\n  Y_{i,t} &= 2 + 3 \\cdot Z_t + 5 \\cdot B_i + 4 \\cdot (Z_t \\cdot B_i) + \\epsilon_{i,t}\\\\\n  \\epsilon_{i,t} &\\sim N(0, 1)\\\\\n  Z_t &= \\mathbb{I}_{\\{ t = 1 \\}} \\\\\n  B_i &= \\mathbb{I}_{\\{ i \\in \\textup{Behandlungsgruppe} \\}},\n\\end{align*}\\] wobei wir jeweils \\(100\\) Beobachtungen beider Gruppen zu beiden Zeitpunkten generieren.\n\nlibrary(tibble)\nlibrary(dplyr)\n\n# Seed setzen\nset.seed(1234)\n\n# Anzahl der Beobachtungen (pro Gruppe u. Zeitpunkt)\nn &lt;- 100\n\n# Daten simulieren\ndid_data &lt;- tibble(\n  Z = rep(rep(c(0, 1), each = n), times = 2),\n  B = rep(c(0, 1), each = 2 * n),\n  epsilon = rnorm(4 * n),\n  outcome = 2 + 3 * Z + 5 * B + 4 * Z * B + epsilon\n)\n\n# Überblick\nglimpse(did_data)\n\nRows: 400\nColumns: 4\n$ Z       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ B       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ epsilon &lt;dbl&gt; -1.20706575, 0.27742924, 1.08444118, -2.34569770, 0.42912469, …\n$ outcome &lt;dbl&gt; 0.7929343, 2.2774292, 3.0844412, -0.3456977, 2.4291247, 2.5060…\n\n\nMit lm() implementieren wir ein einfaches Interaktionsmodell und lesen den geschätzten Effekt aus.\n\n# Modell mit Regression schätzen\ndid_model &lt;- lm(\n  formula = outcome ~ Z * B, \n  data = did_data\n)\n\n# Geschätzten ATE auslesen\n(\n  estimated_effect &lt;- coef(did_model)[\"Z:B\"]\n)\n\n     Z:B \n3.639286 \n\n\nDie Schätzung des Behandlungseffekts von \\(3.64\\) liegt nahe beim wahren Effekt von \\(4\\). Eine äquivalente Schätzung können wir mit fixest::feols() erhalten.\n\nlibrary(fixest)\n\n# Interaktionsmodell mit feols() schätzen\nfeols(\n  fml = outcome ~ Z * B,\n  data = did_data\n)\n\nOLS estimation, Dep. Var.: outcome\nObservations: 400\nStandard-errors: IID \n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)  1.84324   0.101233 18.2078 &lt; 2.2e-16 ***\nZ            3.19800   0.143166 22.3378 &lt; 2.2e-16 ***\nB            5.31137   0.143166 37.0994 &lt; 2.2e-16 ***\nZ:B          3.63929   0.202467 17.9747 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 1.00726   Adj. R2: 0.950969\n\n\nFür eine Schätzung mit Two-way-fixed-effects modifizieren wir den Funktionsaufruf von feols()\n\n# Two-way-FE-Regression\nfeols(\n  fml = outcome ~ I(Z * B) | B + Z,\n  data = did_data\n)\n\nOLS estimation, Dep. Var.: outcome\nObservations: 400\nFixed-effects: B: 2,  Z: 2\nStandard-errors: Clustered (B) \n         Estimate Std. Error      t value   Pr(&gt;|t|)    \nI(Z * B)  3.63929    4.2e-15 8.666101e+14 7.3461e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 1.00726     Adj. R2: 0.950969\n                Within R2: 0.449304\n\n\nBeachte, dass im Formel-Argument fml mit I(Z * B) lediglich der Interaktionseffekt als Regressor festgelegt wird. Fixe Effekte für Gruppenzugehörigkeit und Zeitpunkte werden durch den Zusatz | B + Z spezifiziert.3 Diese Reihenfolge führt zur Berechnung von cluster-robusten Standardfehlern auf Gruppen-Ebene (B). Wie erwartet können wir anhand des \\(t\\)-Tests die Nullhypothese \\(H_0:\\,\\beta_\\text{DID} = 0\\) zu jeden relevanten Signifikanzniveau ablehnen.\n3 I(Z * B) statt Z * B stellt sicher, dass perferkte Multikollinearität aufgrund der Fixed-Effekts für B und Z vermieden wird.Für die Visualisierung der Schätzung mit ggplot2::ggplot() berechnen wir zunächst Stichprobenmittelwerte für die Outcome-Variable y beider Gruppen zu beiden Zeitpunkten.\n\n# Stichprobenmittelwerte berechnen\noptions(digits = 4)\n(\n  means &lt;- did_data %&gt;%\n  group_by(Z, B) %&gt;%\n  summarize(\n    mean_outcome = mean(outcome), \n   .groups = 'drop'\n  )\n)\n\n# A tibble: 4 × 3\n      Z     B mean_outcome\n  &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n1     0     0          1.8\n2     0     1          7.2\n3     1     0          5.0\n4     1     1         14. \n\n\nDie Stichprobenmittelwerte in means ermöglichen uns die Schätzung von \\(\\textcolor{red}{E(Y_B(0)|t=2)}\\), das kontrafaktische erwartete Outcome (counterfactual) der Behandlungsgruppe zum Zeitpunkt \\(t=2\\),\n\\[\\begin{align*}\n  \\textcolor{red}{\\overline{Y_B(0)|t=2}} =&\\, \\textcolor{red}{\\overline{Y_B(0)|t=1}}\n  + \\bigg( \\textcolor{blue}{\\overline{Y_K(0)|t=2}} - \\textcolor{blue}{\\overline{Y_K(0)|t=1}} \\bigg)\\\\\n  =&\\, \\textcolor{red}{7.2} + (\\textcolor{blue}{5.0} - \\textcolor{blue}{1.8}) \\\\\n  =&\\, \\textcolor{red}{10.4}.\n\\end{align*}\\]\n\n# Counterfactual für Behandlungsgruppe in t=1\n(\n  counterfactual &lt;- means %&gt;%\n  filter(Z == 0 & B == 1) %&gt;%\n  pull(mean_outcome) +\n  \n  (\n    means %&gt;%\n     filter(Z == 1 & B == 0) %&gt;%\n     pull(mean_outcome) -\n   \n    means %&gt;%\n     filter(Z == 0 & B == 0) %&gt;%\n     pull(mean_outcome)\n   )\n)\n\n[1] 10.35\n\n\nDer geschätzte Behandlungseffekt ist \\[\\begin{align*}\n\\textcolor{orange}{\\widehat{\\beta}_\\text{DID}} =&\\, \\textcolor{red}{\\overline{Y_B(1)\\vert t=2}} - \\textcolor{red}{\\overline{Y_B(0)\\vert t=2}}\\\\\n=&\\,\\textcolor{red}{14} - \\textcolor{red}{10.4}\\\\\n=&\\, \\textcolor{orange}{3.6}.\n\\end{align*}\\]\nWir plotten die Daten mit ggplot2 und zeichnen die Trends sowie den geschätzten Behandlungseffekt ein.\n\n# Simulierte daten plotten\ndid_data %&gt;%\n  ggplot(\n    mapping = aes(\n      x = factor(Z), \n      y = outcome, \n      color = factor(B), \n      group = factor(B))\n    ) +\n  geom_point(\n    position = position_jitter(\n      width = .1, \n      seed = 1234\n      )\n    ) +\n  labs(\n    x = \"Zeitpunkt\",\n    y = \"Outcome Y\",\n    color = \"B\"\n  ) +\n  scale_color_manual(\n    values = c(\"1\" = \"red\", \"0\" = \"blue\")\n  ) +\n  # Trendlinien einzeichnen\n  stat_summary(\n    fun = mean, \n    geom = \"line\", \n    size = 1.5\n    ) +\n  # Counterfactual für B-Gruppe einzeichnen\n  geom_segment(\n    mapping = aes(\n      x = 1, \n      xend = 2, \n      y = means$mean_outcome[\n        means$Z == 0 & means$B == 1\n        ],\n      yend = counterfactual\n    ),\n    linetype = \"dashed\", \n    color = \"red\", \n    size = 1\n    ) +\n  # gesch. Behandlungseffekt einzeichnen\n  geom_segment(\n    mapping = aes(\n      x = 2, \n      xend = 2, \n      y = counterfactual,\n      yend = counterfactual + estimated_effect\n    ),\n    linetype = \"dashed\", \n    color = \"orange\", \n    size = 1) +\n  theme_cowplot() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nAbbildung 9.2: Einfache DID-Schätzung mit R für simulierte Daten",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "DiD.html#case-study-effekt-von-steuererleichterungen-auf-erwerbsbeteilligung",
    "href": "DiD.html#case-study-effekt-von-steuererleichterungen-auf-erwerbsbeteilligung",
    "title": "\n9  Difference-in-Differences\n",
    "section": "\n9.4 Case Study: Effekt von Steuererleichterungen auf Erwerbsbeteilligung",
    "text": "9.4 Case Study: Effekt von Steuererleichterungen auf Erwerbsbeteilligung\nDer Earned Income Tax Credit (EITC) ist ein Steuerguthaben für US-Amerkanische Familien, die unterhalb einer gesetztlich festgelegten Einkoemmnsgrenze liegen. Der genaue Betrag des EITC hängt gestaffelt vom Einkommen ab und, ähnlich zum Kindergeld in Deutschland, steigt mit der Anzahl der zu versorgenden Kinder. Ein wichtiger Unterschied zum Kindergeld ist, dass der EITC nicht beantragt werden muss: Qualifizierten Familien wird der Betrag automatisch durch die Behörden im Jahressteuerausgleich gutgeschrieben. Somit kann Selbstselektion in die Behandlungsgruppe ausgeschlossen werden, da sich die Behandlung ausschließlich durch die im Rahmen der EITC-Ausweitung geänderten Anspruchsgrundlagen ergibt.\nEissa und Liebman (1996) betrachten Veränderungen in der EITC-Gesetzgebung als Intervention, deren Auswirkungen mit sozio-ökonomischen Paneldaten in einem DID-Ansatz untersucht werden können. Die Studie analysiert die Auswirkungen der ersten Ausweitung des EITC im Jahr 1986 auf die Erwerbsbeteiligung und die Löhne von Müttern im erwerbsfähigen Alter. Diese Erweiterung erhöhte die gewährten Steuererleichterungen und die zur Qualifikation für das Programm zu unterschreitende Einkommensgrenze.\nEin zentraler Befund der Studie ist, dass die EITC-Ausweitung von 1986 einen statistisch signifikanten Anstieg der Arbeitsbeteiligung alleinerziehender Frauen von geschätzten 3% bewirkt hat. Eissa und Liebman (1996) finden weiterhin signifikante positive Effekte auf die geleisteten Arbeitsstunden und Evidenz für Einkommensverbesserungen in dieser Gruppe. Die Studienergebnisse sind starke Evidenz, dass Maßnahmen wie der EITC effektiv dazu beitragen können, die Erwerbssituation in der Zielgruppe zu steigern und somit die wirtsschafts- und sozialpolitische Ziele derartiger Programme realisierbar sind.\nIm Jahr 1993 wurde das Programm erneut ausgweitet: Vor 1993 gab es lediglich eine Einkommensstufe für Familien mit Kindern. 1993 wurde eine zusätzliche Stufe für Familien mit zwei oder mehr Kindern eigeführt, die damit einen höheren maximalen Kreditbetrag erhalten konnten als Familien mit nur einem Kind. Dies führte zu einer größeren steuerlichen Entlastung armutsbedrohter Familien.\nAdireksombat (2010) untersucht die Effekte der zweiten EITC-Ausweitung ebenfalls mit einem DID Ansatz und findet Evidenz für einen Anstieg der Arbeitbeteiligung von etwa 5% in der Zielgruppe alleinerziehender Frauen mit mindestens 2 Kindern.\nZur Illustration der empirischen Anwendung von DID mit R untersuchen wir Effekte der EITC-Ausweitung von 1993 nachfolgend anhand eines ähnlichen Datensatzes aus dem CPS wie in der Studie von Adireksombat (2010). Diese Daten umfassen jährliche sozio-ökonomische Merkmale für US-amerikanische Frauen im Zeitraum von 1991 bis 1996 und sind in der Datei eitc_data.csv verfügbar.\nWir lesen den Datensatz zunächst ein.\n\nlibrary(readr)\n\n# EITC-Datensatz einlesen\neitc_data &lt;- read_csv(\"datasets/eitc_data.csv\")\n\nEine Übersicht des Datensatzes eitc_data ist in Tabelle 9.1 dargestellt.\n\n\n\n\n\n\n\n\nVariable\nBeschreibung\n\n\n\nstate\nID-Code Bundesstaat\n\n\nyear\nSteuerjahr\n\n\nurate\nArbeitslosenquote im Bundesstaat (%)\n\n\nchildren\nAnz. Kinder der Frau\n\n\nnonwhite\nDummy für nicht-weiße Frauen\n\n\nfinc\nHaushaltseinkommen im Steuerjahr (US-$)\n\n\nearn\nEinkommen der Frau im Steuerjahr (US-$)\n\n\nage\nAlter\n\n\ned\nAusbildungsniveau der Frau (Jahre)\n\n\nwork\nDummy für Berufstätigkeit\n\n\nunearn\n= Haushaltseinkommen - Einkommen der Frau (Tsd. US-$)\n\n\n\n\n\n\n\nTabelle 9.1: Sozi-ökomonische Variablen zu US-Familien aus dem CPS\n\n\n\nWir erweitern das tibble-Objekt zunächst um eine Dummy-Variable für Mütter (anykids), sowie spezifischere Dummies für Frauen mit einem Kind (onechild) oder mit zwei oder mehr Kindern (twomorekids). Weiterhin erzeugen wir einen Indikator für Beobachtungen nach der EITC-Ausweitung im Jahr 1993 (after1993).\n\n# Dummies für Frauen mit Kindern\n# und Post-Interventionsprediode hinzufügen \neitc_data &lt;- eitc_data %&gt;%\n  mutate(\n    onechild = if_else(children == 1, TRUE, FALSE),\n    twomorekids = if_else(children &gt;= 2, TRUE, FALSE),\n    anykids = if_else(children &gt; 0, TRUE, FALSE),\n    after1993 = if_else(year &gt; 1993, TRUE, FALSE)\n  )\n\nEinen Überblick über den modifizierten Datensatz erhalten wir mit glimpse().\n\n# Modifikationen kontrollieren\nglimpse(eitc_data)\n\nRows: 13,746\nColumns: 15\n$ state       &lt;dbl&gt; 11, 12, 13, 14, 15, 16, 21, 22, 23, 31, 32, 33, 34, 35, 41…\n$ year        &lt;dbl&gt; 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991…\n$ urate       &lt;dbl&gt; 7.6, 7.2, 6.4, 9.1, 8.6, 6.8, 7.3, 6.7, 7.0, 6.4, 6.0, 7.2…\n$ children    &lt;dbl&gt; 0, 1, 2, 0, 3, 1, 0, 0, 1, 2, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0…\n$ nonwhite    &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0…\n$ finc        &lt;dbl&gt; 18714, 4839, 8178, 9370, 14707, 21605, 19147, 64312, 17676…\n$ earn        &lt;dbl&gt; 18714.4, 471.4, 0.0, 0.0, 14706.6, 18854.6, 14141.0, 63802…\n$ age         &lt;dbl&gt; 26, 22, 33, 43, 23, 53, 52, 51, 20, 32, 51, 29, 54, 28, 27…\n$ ed          &lt;dbl&gt; 10, 9, 11, 11, 7, 7, 11, 11, 11, 11, 9, 10, 9, 11, 11, 7, …\n$ work        &lt;dbl&gt; 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1…\n$ unearn      &lt;dbl&gt; 0.0000, 4.3672, 8.1782, 9.3696, 0.0000, 2.7504, 5.0059, 0.…\n$ onechild    &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE…\n$ twomorekids &lt;lgl&gt; FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALS…\n$ anykids     &lt;lgl&gt; FALSE, TRUE, TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, TRUE, …\n$ after1993   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n\n\nDie Plausibilität der Annahme paralleler Trends können wir graphisch anhand einer Gegenüberstellung der Beschäftigungsquote (avg.work = mean(work)) für Frauen mit und ohne Kindern (anykids) über die Zeit (year) einschätzen. Wir gruppieren hierzu den Datensatz entsprechend und fassen die Outcome-Variable (work) gruppenweise zusammen.\n\n# Zeitpunkt-Gruppen-Mittelwerte berechnen\n(\n  eitc_summarised &lt;- eitc_data %&gt;%\n  group_by(year, anykids) %&gt;%\n  summarise(\n    avg.work = mean(work)\n  )\n)\n\n# A tibble: 12 × 3\n# Groups:   year [6]\n    year anykids avg.work\n   &lt;dbl&gt; &lt;lgl&gt;      &lt;dbl&gt;\n 1  1991 FALSE       0.58\n 2  1991 TRUE        0.46\n 3  1992 FALSE       0.57\n 4  1992 TRUE        0.44\n 5  1993 FALSE       0.57\n 6  1993 TRUE        0.44\n 7  1994 FALSE       0.59\n 8  1994 TRUE        0.46\n 9  1995 FALSE       0.57\n10  1995 TRUE        0.51\n11  1996 FALSE       0.55\n12  1996 TRUE        0.50\n\n\n\n# Graphischer Vergleich der Trends\nggplot(data = eitc_summarised) +\n  geom_line(\n    mapping = aes(\n      x = year, \n      y = avg.work, \n      col = anykids\n    )\n  ) +\n  # Indikator für EITC-Erweiterung 1993\n  geom_vline(\n    xintercept = 1993, \n    lty = \"dashed\"\n  ) +\n  scale_x_continuous(\"Jahr\") +\n  scale_y_continuous(\"Beschäftigungsquote\") +\n  scale_color_manual(\n    values = c(\"TRUE\" = \"red\", \"FALSE\" = \"blue\")\n  ) +\n  guides(color = guide_legend(title = \"Kinder?\")) +\n  theme_cowplot()\n\n\n\n\n\n\nAbbildung 9.3: Trends in der Erwerbsbeteiligung US-amerikanischer Frauen im CPS-Datensatz\n\n\n\n\nAbbildung 9.3 zeigt, dass die Beschäftigungsquote für kinderlose Frauen deutlich oberhalb der Quote für Mütter verläuft. Die Trends vor der EITC-Ausweitung im Jahr 1993 sind sehr ähnlich, sodass eine parallele Entwicklung plausibel scheint.\n\n9.4.1 Schätzungen des ATT mit linearen Modellen\nWir berechnen Zunächst den Behandlungseffekt der für Frauen mit Kindern relativ zu kinderlosen Frauen gemäß \\(\\eqref{eq:DIDMOMENTS}\\), wobei wir jeweils sämtliche Perioden vor und nach der Behandlung einbeziehen. Dies führt zu den Ergebnissen in Tabelle 9.2, wobei \\[\\begin{align}\n  \\textcolor{orange}{\\widehat\\beta_\\text{DID}} = (\\textcolor{red}{B} - \\textcolor{red}{A}) - (\\textcolor{blue}{D} - \\textcolor{blue}{C})\n\\end{align}\\] der geschätzte Behandlungseffekt ist.\n\n\n\n\n\n\n\n\n\n\n\nv. EITC-Ausweitung\nn. EITC-Ausweitung\nDifferenz\n\n\n\nKinder\n\\(\\textcolor{red}{A = .446}\\)\n\\(\\textcolor{red}{B = .491}\\)\n\\(\\textcolor{red}{.045}\\)\n\n\nk. Kinder\n\\(\\textcolor{blue}{C = .575}\\)\n\\(\\textcolor{blue}{D = .573}\\)\n\\(\\textcolor{blue}{-.002}\\)\n\n\n\\(\\textcolor{orange}{\\widehat{\\beta}_\\text{DID}}\\)\n\n\n\\(\\textcolor{orange}{.047}\\)\n\n\n\n\n\nTabelle 9.2: eitc_data: Stichprobenmittelwerte für work\n\n\nDie nachfolgenden Code-Chunks zeigen die Schritte zur Berechnung von \\(\\widehat{\\beta}_\\text{DID}\\) mit R.\n\n# A, B, C und D berechnen\n(\n  ABCD &lt;- eitc_data %&gt;%\n    group_by(after1993, anykids) %&gt;%\n    summarise(avg.work = mean(work))\n)\n\n# A tibble: 4 × 3\n# Groups:   after1993 [2]\n  after1993 anykids avg.work\n  &lt;lgl&gt;     &lt;lgl&gt;      &lt;dbl&gt;\n1 FALSE     FALSE       0.58\n2 FALSE     TRUE        0.45\n3 TRUE      FALSE       0.57\n4 TRUE      TRUE        0.49\n\n\n\n# Differenzen bilden\nDminusC &lt;- ABCD %&gt;%\n    filter(anykids == FALSE) %&gt;%\n    pull(avg.work) %&gt;%\n    diff()\n\nBminusA &lt;- ABCD %&gt;%\n    filter(anykids == TRUE) %&gt;%\n    pull(avg.work) %&gt;%\n    diff()\n\n\n# DID-Schätzung: Differenz der Stichprobenmittel-Differenzen\nbeta_DID_means &lt;- BminusA - DminusC\nbeta_DID_means\n\n[1] 0.04687\n\n\nDurch Iteration von summarise() können wir diese Rechenschritte effizienter ausführen.\n\n# Effizienter:\neitc_data  %&gt;% \n    group_by(after1993, anykids) %&gt;% \n    summarise(avg.work = mean(work)) %&gt;%\n    summarise(diff_time = diff(avg.work)) %&gt;%\n    summarise(beta_DID_means = diff(diff_time)) %&gt;%\n    pull(beta_DID_means)\n\n[1] 0.04687\n\n\nWir erhalten also eine positive Schätzung des Behandlungseffekts. Die Interpretation ist, dass die Ausweitung des EITC im Jahr 1993 zu einem Anstieg der Erwerbsbeteiligung in der Gruppe der Frauen mit Kindern von durchschittlich \\(4.69\\%\\) in den Folgeperioden geführt hat.\nFür die Berechnung von Inferenzstatistiken bezüglich \\(\\beta_\\text{DID}\\) schätzen wir ein lineares Interaktionsmodell gemäß \\(\\eqref{eq:DIDREG}\\),\n\\[\\begin{align}\n  \\begin{split}\n    \\text{work}_{i,t} =&\\, \\beta_0 + \\beta_1 \\text{anykids}_{i,t} + \\beta_2 \\text{after1993}_t \\\\\n  +&\\, \\beta_3 (\\text{anykids}_{i,t} \\times \\text{after1993}_t) + \\epsilon_{i,t}.\n  \\end{split}\\label{eq:eitcmod}\n\\end{align}\\]\n\n# Equivalente Schätzung u. Inferenz \n# mit linearem Interaktionsmodell\nDiD_reg &lt;- lm(\n  formula = work ~ anykids * after1993, \n  data = eitc_data\n)\n\nDer geschätzte Koeffizient des Interkationsterms stimmt mit der händisch berechneten Schätzung überein.\n\ntidy(DiD_reg) %&gt;% \n  filter(term == \"anykidsTRUE:after1993TRUE\") %&gt;% \n  pull(estimate)\n\n[1] 0.04687\n\n\nMit coeftest() berechnen wir heteroskedastie-robuste Inferenzstatistiken.\n\n# Robuste Inferenzstatistiken\ncoeftest(\n  x = DiD_reg, \n  vcov. = vcovHC, \n  type = \"HC1\"\n)\n\n\nt test of coefficients:\n\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                0.57546    0.00880   65.38   &lt;2e-16 ***\nanykidsTRUE               -0.12950    0.01165  -11.12   &lt;2e-16 ***\nafter1993TRUE             -0.00207    0.01287   -0.16   0.8720    \nanykidsTRUE:after1993TRUE  0.04687    0.01714    2.73   0.0063 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDer Koeffizient des Interaktionsterms ist zum 1%-Niveau signifikant. fixest::feols() liefert eine identische Schätzung.\n\nlibrary(fixest)\n(\n  DID_twoway &lt;- feols(\n    fml = work ~ anykids * after1993,\n    data = eitc_data, \n    vcov = \"HC1\"\n  )\n)\n\nOLS estimation, Dep. Var.: work\nObservations: 13,746\nStandard-errors: Heteroskedasticity-robust \n                           Estimate Std. Error  t value  Pr(&gt;|t|)    \n(Intercept)                0.575460   0.008802  65.3756 &lt; 2.2e-16 ***\nanykidsTRUE               -0.129498   0.011648 -11.1176 &lt; 2.2e-16 ***\nafter1993TRUE             -0.002074   0.012873  -0.1611 0.8720396    \nanykidsTRUE:after1993TRUE  0.046873   0.017144   2.7342 0.0062619 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.496672   Adj. R2: 0.012384\n\n\nWir erweitern Modell \\(\\eqref{eq:eitcmod}\\) nun um Fixed Effekts für den US-Bundesstaat sowie das Jahr,\n\\[\\begin{align}\n  \\begin{split}\n    \\text{work}_{i,t} =&\\, \\theta_\\text{Staat} + \\eta_t \\\\\n    +&\\, \\beta_1 \\text{anykids}_{i,t} + \\beta_2 (\\text{anykids}_{i,t} \\times \\text{after1993}_t) + \\epsilon_{i,t}.\n    \\end{split}\\label{eq:eitcmodfe}\n\\end{align}\\]\nAnhand der Dummy-Variablen für Bundesstaaten (\\(\\theta_\\text{Staat}\\)) und Jahre (\\(\\eta_t\\)) kontrollieren wir für unbeobachtete zeitinvariante Unterschiede zwischen den Bundesstaaten sowie für allgemeine zeitliche Trends und Schocks, die alle Bundesstaaten in einem bestimmten Jahr betreffen. Dies schließt etwaige Backdoor-Pfade durch den Einfluss spezifischer Eigenschaften der Bundesstaaten (Kultur, Geografie, langfristige politische Einstellungen, etc.) und gemeinsamer zeitlicher Einflüsse.\n\nlibrary(fixest)\n(\n  DID_twoway &lt;- feols(\n    fml = work ~ anykids + I(anykids * after1993) \n    | state + year,\n    data = eitc_data\n  )\n)\n\nOLS estimation, Dep. Var.: work\nObservations: 13,746\nFixed-effects: state: 51,  year: 6\nStandard-errors: Clustered (state) \n                       Estimate Std. Error t value   Pr(&gt;|t|)    \nanykidsTRUE            -0.12391    0.01593  -7.776 3.6952e-10 ***\nI(anykids * after1993)  0.04579    0.01656   2.765 7.9497e-03 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.489063     Adj. R2: 0.038633\n                 Within R2: 0.011101\n\n\nDie Schätzung des ATT bei Kontrolle für Zeit- und Bundesstaat-Effekte in \\(\\eqref{eq:eitcmodfe}\\) unterscheidet sich nur geringfügig gegenüber dem Ergebnis für das Modell \\(\\eqref{eq:eitcmod}\\). Beachte, dass der Behandlungseffekt auch bei geclusterten Standardfehlern auf Bundesstaaten-Ebene (| state + year) signifikant ist.\nEin weiterer Vorteil von DID-Schätzungen mit Regression ist die Möglichkeit zur Kontrolle für individuen-spezifische Kovariablen, um Backdoors aufgrund systematischer Unterschiede zwischen Kontroll- und Behandlungsgruppen zu vermeiden.\nWir erweitern Modell \\(\\eqref{eq:eitcmodfec}\\) um sozio-ökonomische Charakteristika der Frauen: Einen Dummy für nicht-weiße Frauen (nonwhite), quadratische Terme in Alter (age) und Ausbildungsniveau (ed) sowie weitere Einkünfte des Haushalts (unearn),\n\\[\\begin{align}\n  \\begin{split}\n    \\text{work}_{i,t} =&\\, \\theta_\\text{Staat} + \\eta_t \\\\\n    +&\\, \\beta_1 \\text{anykids}_{i,t} + \\beta_2 (\\text{anykids}_{i,t} \\times \\text{after1993}_t) \\\\\n    +&\\, \\beta_3 \\text{unearn} + \\beta_4 \\text{nonwhite} \\\\\n    +&\\, \\beta_5 \\text{age} + \\beta_6 \\text{age}^2 + \\beta_7 \\text{ed} + \\beta_8 \\text{ed}^2 \\\\\n    +&\\, \\epsilon_{i,t}.\n    \\end{split}\\label{eq:eitcmodfec}\n\\end{align}\\]\n\n# Year-FE + State-FE + Kontrollvariablen\n(\n  DID_FE_controls &lt;- feols(\n    fml = work ~ anykids + I(anykids * after1993) \n    \n    + unearn + nonwhite \n    + age + I(age^2)\n    + ed + I(ed^2)\n    \n    | state + year, \n    \n    data = eitc_data\n  )\n)\n\nOLS estimation, Dep. Var.: work\nObservations: 13,746\nFixed-effects: state: 51,  year: 6\nStandard-errors: Clustered (state) \n                        Estimate Std. Error  t value   Pr(&gt;|t|)    \nanykidsTRUE            -0.119221   0.010673 -11.1709 3.4010e-15 ***\nI(anykids * after1993)  0.055755   0.014385   3.8759 3.1021e-04 ***\nunearn                 -0.017695   0.000959 -18.4575  &lt; 2.2e-16 ***\nnonwhite               -0.080399   0.028048  -2.8665 6.0589e-03 ** \nage                     0.026323   0.003664   7.1847 3.0857e-09 ***\nI(age^2)               -0.000316   0.000052  -6.0298 1.9673e-07 ***\ned                     -0.004160   0.006270  -0.6634 5.1013e-01    \nI(ed^2)                 0.001536   0.000535   2.8701 6.0002e-03 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.46896     Adj. R2: 0.115657\n                Within R2: 0.090729\n\n\nDie Schätzung von \\(\\eqref{eq:eitcmodfec}\\) ergibt mit \\(0.0558\\) eine etwas größere Schätzung eines positiven signifikanten Effekt der EITC-Ausweitung auf die Erwerbsbeteiligung von Müttern.\nWie oben erläutert, führte die EITC-Ausweitung von 1993 unter anderem ein Stufensystem für die Höhe des EITC in Ahängigkeit der Kinder-Anzahl ein, sodass unterschiedlich starke Anreize zur Aufnahme einer Beschäftigung für Mütter mit nur einem Kind und mehreren Kindern plausibel sind. Anhand der Dummy-Variablen für (genau) ein Kind (onechild) sowie zwei oder mehr Kinder (twomorekids) können wir eine differenziertere Schätzung des Effekts hinsichtlich des Betreuungsaufwands erhalten. Hierzu modifizieren wir Modell \\(\\eqref{eq:eitcmodfec}\\) entsprechend:\n\\[\\begin{align}\n  \\begin{split}\n    \\text{work}_{i,t} =&\\, \\theta_\\text{Staat} + \\eta_t \\\\\n    +&\\, \\beta_1 \\text{onechild}_{i,t} + \\beta_2 (\\text{onechild}_{i,t} \\times \\text{after1993}_t) \\\\\n    +&\\, \\beta_3 \\text{twomorechild}_{i,t} + \\beta_4 (\\text{twomorechild}_{i,t} \\times \\text{after1993}_t) \\\\\n    +&\\, \\beta_5 \\text{unearn} + \\beta_6 \\text{nonwhite} + \\beta_7 \\text{age} + \\beta_8 \\text{age}^2 + \\beta_9 \\text{ed} + \\beta_{10} \\text{ed}^2 \\\\\n    +&\\, \\epsilon_{i,t}.\n  \\end{split}\\label{eq:eitcmodfecd}\n\\end{align}\\]\n\n# Differenzierung: onechild / twomorekids\n(\n  DID_FE_spec_controls &lt;- feols(\n    fml = work ~ \n      onechild + I(onechild * after1993) \n    + twomorekids + I(twomorekids * after1993) \n    \n    + unearn + nonwhite \n    + age + I(age^2)\n    + ed + I(ed^2)\n    \n    | state + year,\n    \n    data = eitc_data\n  )\n)\n\nOLS estimation, Dep. Var.: work\nObservations: 13,746\nFixed-effects: state: 51,  year: 6\nStandard-errors: Clustered (state) \n                            Estimate Std. Error  t value   Pr(&gt;|t|)    \nonechildTRUE               -0.063581   0.013327  -4.7708 1.6319e-05 ***\nI(onechild * after1993)     0.040539   0.017829   2.2737 2.7311e-02 *  \ntwomorekidsTRUE            -0.161675   0.013127 -12.3166  &lt; 2.2e-16 ***\nI(twomorekids * after1993)  0.064799   0.016411   3.9485 2.4649e-04 ***\nunearn                     -0.017334   0.000992 -17.4790  &lt; 2.2e-16 ***\nnonwhite                   -0.073759   0.027833  -2.6501 1.0745e-02 *  \nage                         0.028990   0.003475   8.3433 4.9204e-11 ***\nI(age^2)                   -0.000357   0.000049  -7.2889 2.1209e-09 ***\ned                         -0.003765   0.006350  -0.5929 5.5595e-01    \nI(ed^2)                     0.001517   0.000552   2.7474 8.3318e-03 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.467886     Adj. R2: 0.119572\n                 Within R2: 0.094887\n\n\nDie interessierenden geschätzten Koeffizienten von I(onechild * after1993) und I(twomorekids * after1993) sind \\(0.0405\\) und \\(0.0648\\). Auch hier sind die Koeffizienten signifikant von null verschieden. Der größere Koeffizient für den Effekt auf Mütter mit zwei oder mehr Kinder liefert Evidenz dafür, dass die Einführung der Zahlstufe für größere Familien im Rahmen der EITC-Ausweitung von 1993 tatsächlich einen etwas stärkeren Anreiz auf die Zielgruppe mit mehreren Kindern hatte.\n\n9.4.2 Probit-Spezifikationen\nIn sämtlichen Modellen in Kapitel 9.4.1 haben wir \\(\\text{work}_{i,t}\\) als kontinulierliche Variable behandelt und den bedingten Erwartungswert als lineare Funktion modelliert. Da \\(\\text{work}_{i,t}\\) eine binäre Variable ist, haben wir damit implizit die bedingte Wahrscheinlichkeit der Erwerbsbeteiligung \\(P(\\text{work}_{i,t} = 1\\vert \\boldsymbol{x}_{i,t})\\) modelliert. Wie in Kapitel 4.2.1 erläutert, kann ein solches lineares Wahrscheinlichkeitsmodell (LPM) einen nicht-linearen Verlauf der Wahrscheinlichkeitsfunktion \\(P(\\text{work}_{i,t} = 1\\vert \\boldsymbol{x}_{i,t})\\) nicht exakt abbilden, wobei tatsächliche Behandlungseffekte unter- oder überschätzt werden können. In manchen Fällen können geschätzte Wahrscheinlichkeiten sogar außerhalb des Intervalls \\([0,1]\\) liegen. Statt eines LPM sollte ein generalisiertes lineares Modell (GLM) verwendet werden.\nWir modellieren nachfolgend den Effekt der EITC-Anpassung auf die Wahrscheinlichkeit einer Erwerbsbeteiligung von Müttern mit Probit-Regression. Anstatt stats::glm() verwenden wir fixest::feglm().4 Analog zu fixest::feols() erlaubt fixest::feglm() die Schätzung von Probit-Regressionen mit Fixed Effekts. Partielle Effekte können mit dem Paket marginaleffects berechnet werden. Wir schätzen zunächst eine Fixed-Effects-Probit-Regression analog zu \\(\\eqref{eq:eitcmodfe}\\), d.h.\n4 Eine weitere Option ist alpaca::feglm().\\[\\begin{align}\n  \\begin{split}\n    \\Phi^{-1}\\bigg[P(\\text{work}_{i,t} = 1\\vert \\boldsymbol{x}_{i,t})\\bigg]\n    =&\\, \\eta_t + \\theta_\\text{Staat} + \\beta_1 \\text{anykids}_{i,t}\\\\\n    +&\\, \\beta_2 (\\text{anykids}_{i,t} \\times \\text{after1993}_{i,t}).\\\\\n  \\end{split}\n\\end{align}\\]\nFür die Berechnung der partiellen Effekte mit marginaleffects::avg_slopes() definieren wir die interargierten Regressoren direkt im Datensatz.\n\n# Interagierte Regressoren definieren\neitc_data &lt;- eitc_data %&gt;% \n  mutate(\n    int_anykids = anykids * after1993,\n    int_onechild = onechild * after1993,\n    int_twomore = twomorekids * after1993\n  )\n\nDie Struktur von feglm() folgt dem selben Schema wie feols(). Das zusätzliche Argument family = binomial(\"probit\") legt die entsprechende Link-Funktion fest.\n\n# Probit-Regression mit TWFE\n(\n  EITC_DID_probit &lt;- feglm(\n    fml = work ~ anykids + int_anykids\n    | state + year, \n    family = binomial(\"probit\"),\n    data = eitc_data\n  )\n)\n\nGLM estimation, family = binomial, Dep. Var.: work\nObservations: 13,746\nFixed-effects: state: 51,  year: 6\nStandard-errors: Clustered (state) \n            Estimate Std. Error z value   Pr(&gt;|z|)    \nanykidsTRUE  -0.3204    0.04212  -7.606 2.8361e-14 ***\nint_anykids   0.1189    0.04276   2.780 5.4308e-03 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nLog-Likelihood: -9,224.4   Adj. Pseudo R2: 0.025402\n           BIC: 19,001.5     Squared Cor.: 0.042557\n\n\nDie Schätzung ergibt einen positiven Effekt für int_anykids. Wie im linearen Modell ist der Koeffizient von int_anykids signifikant. Beachte, dass dieser Koeffizient die geschätzte Änderung der latenten Variable des Probit-Modells misst und nicht direkt als Behandlungseffekt interpretiert werden darf. Stattdessen können wir mit marginaleffects::avg_slopes() den durchschnittlichen partiellen Effekt des Interaktionsterms für Frauen in der Behandlungsgruppe für die Jahre 1994 bis 1996 berechnen. marginaleffects::datagrid() setzt Variablen ohne spezifizierte Werte auf ihren Mittelwert (kontinuierlich) oder Modus (kategorisch). Der Modus von state ist 93 (California).\n\nlibrary(marginaleffects)\n\nEITC_DID_probit %&gt;% \n  avg_slopes(\n    variables = \"int_anykids\", \n    newdata = datagrid(\n      anykids = 1, \n      year = 1994:1996\n    )\n  )\n\n\n        Term          Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 %\n int_anykids mean(1) - mean(0)   0.0456     0.0161 2.83  0.00465 7.7 0.014\n 97.5 %\n 0.0771\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\n\nDer geschätzte durchschnittliche partielle Effekt der EITC-Ausweitung für Mütter im Bundesstaat California für die Jahre 1994 bis 1996 ist eine Erhöhung der Wahrscheinlichkeit der Erwerbsbeteiligung um etwa \\(4.57\\%\\).\nAnalog zu Modell \\(\\eqref{eq:eitcmodfecd}\\) kontrollieren wir in einer weiteren Regression zusätzlich für sozio-ökonomische Charakteristika und differenzieren zwischen dem Effekt für Frauen mit einem Kind und Müttern mit zwei oder mehr Kindern anhand des Modells\n\\[\\begin{align}\n  \\begin{split}\n    \\Phi^{-1}\\bigg[P(\\text{work}_{i,t} = 1)\\bigg] =&\\, \\theta_\\text{Staat} + \\eta_t \\\\\n    +&\\, \\beta_1 \\text{onechild}_{i,t} + \\beta_2 (\\text{onechild}_{i,t} \\times \\text{after1993}_t) \\\\\n    +&\\, \\beta_3 \\text{twomorechild}_{i,t} + \\beta_4 (\\text{twomorechild}_{i,t} \\times \\text{after1993}_t) \\\\\n    +&\\, \\beta_5 \\text{unearn} + \\beta_6 \\text{nonwhite} + \\beta_7 \\text{age} + \\beta_8 \\text{age}^2 + \\beta_9 \\text{ed} + \\beta_{10} \\text{ed}^2 \\\\\n    +&\\, \\epsilon_{i,t}.\n  \\end{split}\n\\end{align}\\]\n\n# Probit-FE-Regression mit Kontrollvariablen\n# und Differenzierung des Effekts\n(\n  EITC_DID_probit &lt;- feglm(\n    fml = work ~ onechild + int_onechild\n    + twomorekids + int_twomore\n    \n    + unearn + nonwhite + age + ed \n    + I(ed^2) + I(age^2)\n    \n    | year + state, \n    family = binomial(\"probit\"),\n    data = eitc_data\n  )\n)\n\nGLM estimation, family = binomial, Dep. Var.: work\nObservations: 13,746\nFixed-effects: year: 6,  state: 51\nStandard-errors: Clustered (year) \n                 Estimate Std. Error  z value   Pr(&gt;|z|)    \nonechildTRUE    -0.180374   0.029751  -6.0628 1.3378e-09 ***\nint_onechild     0.120223   0.041680   2.8844 3.9218e-03 ** \ntwomorekidsTRUE -0.436599   0.022144 -19.7165  &lt; 2.2e-16 ***\nint_twomore      0.183897   0.072453   2.5381 1.1144e-02 *  \nunearn          -0.053551   0.003109 -17.2227  &lt; 2.2e-16 ***\nnonwhite        -0.206060   0.026744  -7.7048 1.3105e-14 ***\nage              0.078925   0.006161  12.8110  &lt; 2.2e-16 ***\ned              -0.009102   0.028450  -0.3199 7.4903e-01    \nI(ed^2)          0.004198   0.001867   2.2479 2.4580e-02 *  \nI(age^2)        -0.000966   0.000082 -11.8142  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nLog-Likelihood: -8,586.0   Adj. Pseudo R2: 0.091602\n           BIC: 17,800.9     Squared Cor.: 0.145969\n\n\nWir können nun zwei partielle Effekte berechnen: 1. für int_onechild und 2. für int_twomore. Hierzu setzten wir jeweils die übrigen Dummy-Variablen mit Bezug zur Anzahl der Kinder auf null. Weiterhin setzen wir nonwhite = 0, d.h. wir betrachten den Effekt für weiße Frauen im Bundesstaat California, wobei die übrigen Regressoren den Wert der jeweiligen Stichprobenmittel haben.\n\n# 1. Durchschn. Partieller Effekt onechild\nEITC_DID_probit %&gt;% \n  avg_slopes(\n    variable = \"int_onechild\",\n    datagrid(\n      onechild = 1, \n      twomorekids = 0, \n      int_twomore = 0,\n      year = 1994:1996,\n      nonwhite = 0\n    )\n  )\n\n\n         Term          Contrast Estimate Std. Error   z Pr(&gt;|z|)    S  2.5 %\n int_onechild mean(1) - mean(0)   0.0411     0.0117 3.5   &lt;0.001 11.1 0.0181\n 97.5 %\n 0.0641\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\n\n\n# 1. Durchschn. Partieller Effekt twomore\nEITC_DID_probit %&gt;% \n  avg_slopes(\n    variable = \"int_twomore\",\n    datagrid(\n      int_onechild = 0, \n      onechild = 0, \n      twomorekids = 1, \n      year = 1994:1996,\n      nonwhite = 0\n    )\n  )\n\n\n        Term          Contrast Estimate Std. Error   z Pr(&gt;|z|)   S  2.5 %\n int_twomore mean(1) - mean(0)   0.0616     0.0228 2.7  0.00687 7.2 0.0169\n 97.5 %\n  0.106\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\n\nÄhnlich wie in Modell \\(\\eqref{eq:eitcmodfecd}\\) erhalten wir signifikante positive Schätzungen. Der durchschnittliche Effekt für int_twomore ist mit einer Erhöhung der Wahrscheinlichkeit zur Aufnahme einer Beschäftigung von etwa \\(6.16\\%\\) etwas höher als die \\(4.11\\%\\) für int_onechild.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "DiD.html#case-study-effekt-von-polizeipräsenz-auf-autodiebstähle",
    "href": "DiD.html#case-study-effekt-von-polizeipräsenz-auf-autodiebstähle",
    "title": "\n9  Difference-in-Differences\n",
    "section": "\n9.5 Case Study: Effekt von Polizeipräsenz auf Autodiebstähle",
    "text": "9.5 Case Study: Effekt von Polizeipräsenz auf Autodiebstähle\nNach einem Terroranschlag auf das größte jüdische Gemeindezentrum in Buenos Aires am 18. Juli 1994 wurden sämtliche jüdische und muslimische Einrichtungen in der Stadt rund um die Uhr von der argentinischen Polizei geschützt. Somit führte dieses Ereignis zu einer geografischen quasi-experimentellen Allokation von Polizeikräften in Gegenden mit entsprechenden Einrichtungen. In einem Forschungsdesign, dass die lokale Straßenkriminalität Kriminalität erklären soll, kann die sich aus der Allokation ergebende Variation in der Polizeipräsenz als exogen angenommen werden. Di Tella und Schargrodsky (2004) nutzen diesen Umstand, um anhand von Polizeistatistiken und Daten zu Autodiebstählen in Buenos Aires vor und nach dem Terroranschlag den Effekt erhöhter Polizeipräsenz auf die Kriminalität in einem Difference-in-Differences-Ansatz zu untersuchen. Die Studienergebnisse deuten darauf hin, dass es einen starken abschreckenden Effekt von Polizeistreifen gibt, der sich nur in einem engen Umkreis um Häuserblocks in denen die Polizeipräsenz erhöht wurde, auswirkt.\nIn diesem Kapitel reproduzieren wir Kernergebnisse der Studie mit R. Die benötigten Daten sind auf der Webseite der American Economic Association verfügbar. Die Daten stammen aus drei nicht zusammenhängenden Stadtvierteln von Buenos Aires die etwa \\(3.2\\%\\) der Stadtfläche ausmachen und \\(6.9\\%\\) der Bevölkerung beherbergen, wobei sich jedem Viertel eine Polizeistation befindet. Bei den untersuchten Stadtvierteln handelt es sich um die Viertel mit der größten Anzahl jüdischer Einrichtungen in der Stadt. Der Großteil der Häuserblocks in diesen Vierteln (insgesamt 876 Blocks) liegen nicht in der Nähe einer geschützten Einrichtung. Blocks die mehr als zwei Blocks von einer geschützten Einrichtung entfernt liegen, bilden die Kontrollgruppe.\nWir verwenden nachfolgend den modifizierten Datensatz polizeipraesenz.RDS.\n\n# Datensatz einlesen\npolizeipraesenz &lt;- readRDS(\"datasets/polizeipraesenz.RDS\")\n\n\n\n\n\nVariable\nBeschreibung\n\n\n\ninstitut\nJüdische Einrichtung im Block\n\n\nbarrio\nStadtviertel\n\n\ndistanz\nEntf. zu Block mit Einrichtung (in Blocks)\n\n\nmonat\nMonat der Beobachtung\n\n\nobserv\nID-Variable für Block\n\n\ntotrob\nDiebstähle pro Monat (normalisiert)\n\n\n\n\n\nTabelle 9.3: polizeipraesenz: Autodiebstähle in Buenos Aires\n\n\n\n9.5.1 Trend-Vergleich\nZur Einschätzung der Annahme paralleler Trends berechnen wir zunächst die Durschnittliche Anzahl an Autodiebstählen Blocks mit bzw. ohne jüdische Institutionen für die Monate April bis Dezember. Beachte, dass die Variable monat zwei Ausprägungen für den Monat des Anschlags aufweist: Juli (1 - 17) für die Juli-Tage vor dem Anschlag und für die verbleibenden Tage Juli (18 - 31).\n\n# Trends berechnen\n(\n  polizeipraesenz_Trends &lt;- polizeipraesenz %&gt;%\n    mutate(institut = as.factor(institut)) %&gt;%\n    group_by(monat, institut) %&gt;%\n    summarise(\n      `Durchschn. Diebstähle` = mean(totrob)\n    )\n)\n\n# A tibble: 20 × 3\n# Groups:   monat [10]\n   monat          institut `Durchschn. Diebstähle`\n   &lt;ord&gt;          &lt;fct&gt;                      &lt;dbl&gt;\n 1 April          0                          0.11 \n 2 April          1                          0.12 \n 3 Mai            0                          0.10 \n 4 Mai            1                          0.088\n 5 Juni           0                          0.076\n 6 Juni           1                          0.13 \n 7 Juli (1 - 17)  0                          0.041\n 8 Juli (1 - 17)  1                          0.020\n 9 Juli (18 - 31) 0                          0.054\n10 Juli (18 - 31) 1                          0.027\n11 August         0                          0.11 \n12 August         1                          0.047\n13 September      0                          0.099\n14 September      1                          0.014\n15 Oktober        0                          0.11 \n16 Oktober        1                          0.061\n17 November       0                          0.10 \n18 November       1                          0.027\n19 Dezember       0                          0.11 \n20 Dezember       1                          0.027\n\n\nDie in polizeipraesenz_Trends erfasste Trendentwicklung für Blocks in der Behandlungsgruppe (institut == 1) und in der Kontrollgruppe (institut == 0) plotten wir mit ggplot2().\n\n# Trends plotten\npolizeipraesenz_Trends %&gt;%\n  ggplot(\n    mapping = aes(\n      x = monat, \n      y = `Durchschn. Diebstähle`, \n      color = institut, \n      group = institut\n    )\n  ) +\n  geom_line() +\n  geom_vline(\n    xintercept = \"Juli (18 - 31)\", \n    lty = \"dashed\") +\n  geom_point() +\n  labs(\n    x = \"Monat\",\n    y = \"Durchschn. Anz. Diebstähle / Monat\"\n  ) +\n  theme_cowplot() +\n  theme(\n    axis.text.x = element_text(\n      angle = 45, \n      hjust = 1\n    ), \n    legend.position = \"top\"\n  )\n\n\n\n\n\n\nAbbildung 9.4: Geschätzte Trends für Autodiebstähle in Buenos Aires im Jahr 1994\n\n\n\n\nAbbildung 9.4 zeigt einen ähnlichen Verlauf der Trends für den Zeitraum unmittelbar vor dem Anschalg am 18. Juli.\nTabelle 2 in Di Tella und Schargrodsky (2004) präsentiert t-Tests für Unterschiede in der Mittleren Anzahl der Diebstähle pro Monat zwischen Blocks mit jüdischen Einrichtungen und verschiedenen distanz-basierten Untergruppen von Blocks ohne eine jüdische Einrichtungen für jede Periode. Zur Reproduktion dieser Ergebnisse erstellen zunächst eine Listen-Spalten für die beobachteten Diebstähle in Abhängigkeit der Distanz zum nächsten Block mit einer jüdischen Einrichtung.\n\n\nd2more Zwei oder mehr Blocks entfernt\n\nd2: Zwei Blocks entfernt\n\nd1: Ein Block entfernt\n\nsame: Jüdische Einrichtung im selben Block\n\n\n(\n  dat_listcol &lt;- polizeipraesenz %&gt;% \n    group_by(observ, monat) %&gt;%\n    transmute(\n      d2more = list(totrob[distanz &gt; 2]),\n      d2 = list(totrob[distanz == 2]),\n      d1 = list(totrob[distanz == 1]),\n      same = list(totrob[institut == 1])\n    )\n)\n\n# A tibble: 8,760 × 6\n# Groups:   observ, monat [8,760]\n   observ monat d2more    d2        d1        same     \n    &lt;dbl&gt; &lt;ord&gt; &lt;list&gt;    &lt;list&gt;    &lt;list&gt;    &lt;list&gt;   \n 1    870 April &lt;dbl [0]&gt; &lt;dbl [0]&gt; &lt;dbl [1]&gt; &lt;dbl [0]&gt;\n 2    851 April &lt;dbl [0]&gt; &lt;dbl [1]&gt; &lt;dbl [0]&gt; &lt;dbl [0]&gt;\n 3    843 April &lt;dbl [0]&gt; &lt;dbl [0]&gt; &lt;dbl [0]&gt; &lt;dbl [1]&gt;\n 4    796 April &lt;dbl [0]&gt; &lt;dbl [0]&gt; &lt;dbl [1]&gt; &lt;dbl [0]&gt;\n 5    790 April &lt;dbl [0]&gt; &lt;dbl [0]&gt; &lt;dbl [0]&gt; &lt;dbl [1]&gt;\n 6    789 April &lt;dbl [0]&gt; &lt;dbl [0]&gt; &lt;dbl [1]&gt; &lt;dbl [0]&gt;\n 7    844 April &lt;dbl [0]&gt; &lt;dbl [0]&gt; &lt;dbl [0]&gt; &lt;dbl [1]&gt;\n 8    858 April &lt;dbl [0]&gt; &lt;dbl [0]&gt; &lt;dbl [1]&gt; &lt;dbl [0]&gt;\n 9    787 April &lt;dbl [0]&gt; &lt;dbl [0]&gt; &lt;dbl [1]&gt; &lt;dbl [0]&gt;\n10    850 April &lt;dbl [0]&gt; &lt;dbl [0]&gt; &lt;dbl [1]&gt; &lt;dbl [0]&gt;\n# ℹ 8,750 more rows\n\n\nMit summarise() können wir Stichprobenmittel und Standardabweichungen der Diebstähle in Blocks dieser Kategorien für alle Perioden berechnen. Anschließend kombinieren wir die Ergebnisse jeweils mit sprintf() und formatieren das Ergebnis mit modelsummary::modelsummary_df().5 Tabelle 9.4 Zeit die Spalten A bis D aus Tabelle 2 in Di Tella und Schargrodsky (2004).\n5 \"%.4f\\n(%.4f)\" gibt das Format des resultierenden character an: Mittelwerte und SDs (in Klammern), gerundet auf vier Nachkommastellen. \\n bewirkt einen Zeilenumbruch.\nlibrary(modelsummary)\n\n# Mittelwerte und Standardabweichungen\nmean_sd &lt;- dat_listcol %&gt;%\n  group_by(monat) %&gt;%\n  summarise(\n    across(\n      d2more:same,\n      .fns = list(\n        mean = ~ mean(unlist(.)), \n        sd = ~ sd(unlist(.))\n      )\n    ),\n    .groups = \"keep\"\n  )\n\n# Desk. Statistiken kombinieren\nformatted_mean_sd &lt;- mean_sd %&gt;%\n  transmute(\n    monat,\n    \"D&gt;2 (A)\" = sprintf(\"%.4f\\n(%.4f)\", d2more_mean, d2more_sd),\n    \"same (B)\" = sprintf(\"%.4f\\n(%.4f)\", same_mean, same_sd),\n    \"D1 (C)\" = sprintf(\"%.4f\\n(%.4f)\", d1_mean, d1_sd),\n    \"D2 (D)\" = sprintf(\"%.4f\\n(%.4f)\", d2_mean, d2_sd)\n  )\n\n# Tabelle mit modelsummary_df() ausgeben\nmodelsummary::datasummary_df(formatted_mean_sd)\n\n\n\n \n\n  \n    \n\ntinytable_wc8umcv2bg4i97l7rga6\n\n\n      \n\nmonat\n                D&gt;2 (A)\n                same (B)\n                D1 (C)\n                D2 (D)\n              \n\n\nApril         \n                  0.0996\n(0.2481)\n                  0.1216\n(0.3614)\n                  0.1211\n(0.2879)\n                  0.1228\n(0.2974)\n                \n\nMai           \n                  0.1084\n(0.2357)\n                  0.0878\n(0.2060)\n                  0.0776\n(0.1817)\n                  0.0973\n(0.2591)\n                \n\nJuni          \n                  0.0785\n(0.1961)\n                  0.1284\n(0.2864)\n                  0.0776\n(0.2151)\n                  0.0697\n(0.1867)\n                \n\nJuli (1 - 17) \n                  0.0393\n(0.1451)\n                  0.0203\n(0.0692)\n                  0.0590\n(0.2101)\n                  0.0310\n(0.1419)\n                \n\nJuli (18 - 31)\n                  0.0393\n(0.1460)\n                  0.0270\n(0.0787)\n                  0.0730\n(0.2177)\n                  0.0686\n(0.2386)\n                \n\nAugust        \n                  0.1184\n(0.2871)\n                  0.0473\n(0.1752)\n                  0.0668\n(0.2197)\n                  0.1272\n(0.3048)\n                \n\nSeptember     \n                  0.1018\n(0.2566)\n                  0.0135\n(0.0573)\n                  0.0901\n(0.2761)\n                  0.0985\n(0.2483)\n                \n\nOktober       \n                  0.1211\n(0.2671)\n                  0.0608\n(0.2157)\n                  0.0978\n(0.2610)\n                  0.0885\n(0.2367)\n                \n\nNovember      \n                  0.0962\n(0.2404)\n                  0.0270\n(0.0787)\n                  0.1102\n(0.2889)\n                  0.1018\n(0.2177)\n                \n\nDezember      \n                  0.1018\n(0.2682)\n                  0.0270\n(0.0787)\n                  0.1165\n(0.2782)\n                  0.1062\n(0.2256)\n                \n\n\n\n\n    \n\n\n\nTabelle 9.4: Deskriptive Statistiken für Diebstähle nach Entfernung zur nächsten jüdischen Einrichtung\n\n\n\nDer nächste Chunk reproduziert die Spalten E bis F von Tabelle 2 in Di Tella und Schargrodsky (2004). Die Einträge sind Mittelwertdifferenzen (Standardfehler in Klammern) zwischen den betrachteten Gruppen von Blocks sowie Ergebnisse für t-Tests (Signifikanz-Sternchen) der Hypothese, dass die jeweilige mittlere Anzahl an Autodiebstählen nicht verschieden ist.\nWir definieren zunächst eine Funktion format_ttest(), die die gewünschen Statistiken aus einem mit t.test() berechneten Objekte ausliest und entsprechend formatiert. Anschließend nutzen wir diese Funktion, um die Daten in dat_listcol entsprechend der Definition in Di Tella und Schargrodsky (2004) für jeden Monat zusammenzufassen. Die Ergebnisse formatieren wir wieder mit datasummary_df().\n\n# Funktion: Ergebnisse von t.test() \n# mit Signifikanzsternchen\nformat_ttest &lt;- function(ttest_result) {\n  estimate &lt;- diff(ttest_result$estimate)\n  stderr &lt;- ttest_result$stderr\n  p_value &lt;- ttest_result$p.value\n  \n  stars &lt;- if (p_value &lt; 0.001) {\n    \"***\"\n  } else if (p_value &lt; 0.01) {\n    \"**\"\n  } else if (p_value &lt; 0.05) {\n    \"*\"\n  } else {\n    \"\"\n  }\n  \n  sprintf(\"%.4f (%.4f)%s\", estimate, stderr, stars)\n}\n\n# Berechnung der t-Tests, Formatierung der Ergebnisse\nresults &lt;- dat_listcol %&gt;%\n  group_by(monat) %&gt;%\n  summarise(\n    \"(E) Diff: B - A\" = format_ttest(t.test(unlist(d2more), unlist(same))),\n    \"(F) Diff: C - A\" = format_ttest(t.test(unlist(d2more), unlist(d1))),\n    \"(G) Diff: D - A\" = format_ttest(t.test(unlist(d2more), unlist(d2)))\n  )\n\n# Formatierung der Tabelle mit modelsummary_df()\nmodelsummary::datasummary_df(results)\n\n\n\n \n\n  \n    \n\ntinytable_ltusa8xn0rz3yptjlp7n\n\n\n      \n\nmonat\n                (E) Diff: B - A\n                (F) Diff: C - A\n                (G) Diff: D - A\n              \n\n\nApril         \n                  0.0221 (0.0606)    \n                  0.0216 (0.0255)  \n                  0.0232 (0.0230) \n                \n\nMai           \n                  -0.0206 (0.0356)   \n                  -0.0308 (0.0181) \n                  -0.0111 (0.0205)\n                \n\nJuni          \n                  0.0498 (0.0480)    \n                  -0.0009 (0.0193) \n                  -0.0088 (0.0155)\n                \n\nJuli (1 - 17) \n                  -0.0190 (0.0133)   \n                  0.0197 (0.0179)  \n                  -0.0083 (0.0116)\n                \n\nJuli (18 - 31)\n                  -0.0122 (0.0146)   \n                  0.0337 (0.0185)  \n                  0.0293 (0.0173) \n                \n\nAugust        \n                  -0.0711 (0.0318)*  \n                  -0.0516 (0.0220)*\n                  0.0088 (0.0244) \n                \n\nSeptember     \n                  -0.0883 (0.0153)***\n                  -0.0117 (0.0249) \n                  -0.0033 (0.0205)\n                \n\nOktober       \n                  -0.0603 (0.0376)   \n                  -0.0233 (0.0241) \n                  -0.0326 (0.0201)\n                \n\nNovember      \n                  -0.0692 (0.0172)***\n                  0.0140 (0.0254)  \n                  0.0055 (0.0184) \n                \n\nDezember      \n                  -0.0747 (0.0181)***\n                  0.0147 (0.0253)  \n                  0.0044 (0.0196) \n                \n\n\n\n\n    \n\n\n\nTabelle 9.5: Mittelwertdifferenzen in Autodiebstählen und t-Tests\n\n\n\nTabelle 9.5 zeigt eine einschlägige Entwicklung der Differenzen über die Zeit: In den Monaten vor dem Anschlag (und der anschließenden Allokation von Polizeipräsenz) bestehen weder zwischen weit entfernten Blocks und solchen mit einer jüdischen Einrichtung (Spalte E) noch zwischen Blocks ohne eine Einrichtung (Spalten F und G) signifikante unterschiede in der Anzahl der Autodiebstähle. Nach der politischen Intervention ergibt sich ein anderes Bild: In Spalte (E) von Tabelle 9.5 finden wir signifikante negative Differenzen. Dies ist Evidenz, das die Kriminalität in besonders gut bewachten Blocks (Spalte B in Tabelle 9.4) in den Folgeperioden des Anschlags geringer war als in Blocks die, mehr als zwei Blocks von einer bewachteten Einrichtung entfernt sind (Spalte A in Tabelle 9.4). Mit Ausnahme einer Signifikanten Differenz im August 1994 finden wir keine Hinweise auf derartige Unterschiede zwischen Blocks in den Kontrollgruppen (Spalten F und G).\n\n9.5.2 Two-Way-Fixed-Effects-Schätzungen\nDi Tella und Schargrodsky (2004) betrachten (Sub-Modelle) der folgenden Regressionsspezifikation für die Schätzung des Effekts von Polizeipräsenz auf die Anzahl der Autodiebstähle.\n\\[\\begin{align}\n  \\begin{split}\n  \\text{totrob}_{i,t} =&\\, \\text{monat}_t + \\text{block}_i \\\\\n  + &\\, \\alpha_0 \\text{same}_{i,t} + \\alpha_1 \\text{oneblock}_{i,t} \\\\\n  + &\\, \\alpha_2 \\text{twoblocks}_{i,t} + \\epsilon_{i,t}\n  \\end{split}\\label{eq:ppbase}\n\\end{align}\\]\nHierbei sind \\(\\text{monat}_t\\) und \\(\\text{block}_i\\) Fixed Effekte für den Monat sowie den Block. Die übrigen Variablen sind für Beobachtungen ab dem Anschlag am 18. Juli 1994 und in Abhäbgigkeit der Distanz zur nächsten jüdischen Einrichtnug definiert:\n\n\\(\\text{same}_{i,t}\\): Dummy-Variable für jüdische Einrichtung im Block und Beobachtung nach dem 17. Juli 1994\n\\(\\text{oneblock}_{i,t}\\): Dummy-Variable für Blocks mit einem Block Entfernung zum nächsten Block mit einer jüdischen Einrichtung und Beobachtung nach dem 17. Juli 1994\n\\(\\text{twoblocks}_{i,t}\\): Dummy-Variable für Blocks mit einer Entfernung von zwei Blocks zum nächsten Block mit einer jüdischen Einrichtung und Beobachtung nach dem 17. Juli 1994\n\nIm nächsten Code-Chunk definieren wir diese Variablen und, gemäß der Vorgehensweise in Di Tella und Schargrodsky (2004), entfernen Beobachtungen für den Zeitraum im Juli nach dem Anschlag (18.07.1994 bis 31.07.1994).\n\n# Variablen definieren und Beobachtungen subsetten\ndat_DID &lt;- polizeipraesenz %&gt;%\n  mutate(\n    same = institut == 1 & monat &gt; \"Juli (1 - 17)\",\n    oneblock = distanz == 1 &  monat &gt; \"Juli (1 - 17)\",\n    twoblocks = distanz == 2 &  monat &gt; \"Juli (1 - 17)\"\n  ) %&gt;% \n  filter(monat != \"Juli (18 - 31)\")\n\nDie Kernergebnisse der Studie werden in Tabelle 3 im Paper präsentiert. Hier werden fünf Regressionen betrachtet:\n\n\nRegression (A)\n\\[\\begin{align}\n    \\begin{split}\n      \\text{totrob}_{i,t}\n        =&\\, \\text{monat}_t + \\text{block}_i \\\\\n       + &\\, \\alpha_0 \\text{same}_{i,t} \\\\\n       + &\\, \\epsilon_{i,t}\n    \\end{split}\\label{eq:ppbaseA}\n\\end{align}\\]\nModell \\(\\eqref{eq:ppbaseA}\\) betrachtet lediglich die geographisch “engste” Definition des Behandlungseffekts: \\(\\alpha_0\\) ist der ATT für Blocks mit einer jüdischen Einrichtung. Die Kontrollgruppe besteht aus sämtlichen Blocks ohne jüdische Einrichtung.\n\n\nRegression (B)\n\\[\\begin{align}\n    \\begin{split}\n      \\text{totrob}_{i,t}\n        =&\\, \\text{monat}_t + \\text{block}_i \\\\\n       + &\\, \\alpha_0 \\text{same}_{i,t} \\\\\n       + &\\, \\alpha_1 \\text{oneblock}_{i,t} \\\\\n       + &\\, \\epsilon_{i,t}\n    \\end{split}\\label{eq:ppbaseB}\n\\end{align}\\]\nModell \\(\\eqref{eq:ppbaseB}\\) erweitert die Behandlungsgruppe um Blocks, die genau einen Block von einem Block mit erhötem Polizeischutz entfernt sind. Die Kontrollgruppe besteht aus Blocks mit einer Entfernung von zwei oder mehr Blocks bis zur nächsten jüdischen Einrichtung.\n\n\nRegression (C)\n\\[\\begin{align}\n    \\begin{split}\n      \\text{totrob}_{i,t}\n        =&\\, \\text{monat}_t + \\text{block}_i \\\\\n       + &\\, \\alpha_0 \\text{same}_{i,t} \\\\\n       + &\\, \\alpha_1 \\text{oneblock}_{i,t} \\\\\n       + &\\, \\alpha_1 \\text{twoblocks}_{i,t} \\\\\n       + &\\, \\epsilon_{i,t}\n    \\end{split}\\label{eq:ppbaseC}\n\\end{align}\\]\nModell \\(\\eqref{eq:ppbaseC}\\) betrachtet zusätzlich den Behandlungseffekt in Blocks mit zwei Blocks entfernung zu einem Block mir erhöhter Polizeipräsenz. Die Kontrollgruppe besteht aus Blocks mit einer Entfernung von mehr als zwei Blocks bis zur nächsten jüdischen Einrichtung.\n\n\nRegression (D) – Querschnitts-Variation\nDa Di Tella und Schargrodsky (2004) eine große Ähnlichkeit hinsichtlich demografischer Merkmale und Autodiebstahlsraten vor der Intervention in Gebieten mit und ohne jüdische Einrichtungen beobachten, betrachten sie auch einen einfachen Querschnittsschätzer. Regression (D) nutzt die Spezifikation \\(\\eqref{eq:ppbaseC}\\), aber berücksichtigt nur Beobachtungen für den Zeitraum nach dem Anschlag (August bis Dezember) und nutzt lediglich Fixed Effekts für die Monate.\n\n\nRegression (E) – Zeitreihen-Variation\nRegression (E) ist eine alternative Spezifikation zu \\(\\eqref{eq:ppbaseC}\\) bei der lediglich die zeitliche Variation in der Behandlungsgruppe (Entfernung zur nächsten Einrichtung \\(\\leq\\) zwei Blocks) genutzt wird. Hierzu werden Fixed Effekts nur für die Blocks, jedoch nicht für die Monate berechnet.\n\n\nWie implementieren nachfolgend die Regressionen (A) bis (E) mit fixest::feols() unter Verwendung heteroskedastie-robuster Standardfehler (vcov = \"HC1\"). Alle Regressionen nutzen den oben definierten Datensatz dat_DID, wobei wir für die Schätzungen in (D) und (E) mit filter() die entsprechenden Subsets auswählen.\n\n# Regression (A)\n(\n  T3A &lt;- feols(\n    fml = totrob ~ \n      same\n    | observ + monat, \n    data = dat_DID, \n    vcov = \"HC1\"\n  )\n)\n\nOLS estimation, Dep. Var.: totrob\nObservations: 7,884\nFixed-effects: observ: 876,  monat: 9\nStandard-errors: Heteroskedasticity-robust \n         Estimate Std. Error t value   Pr(&gt;|t|)    \nsameTRUE -0.07753    0.02244  -3.456 0.00055241 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.216676     Adj. R2: 0.097072\n                 Within R2: 0.001277\n\n# Regression (B)\n(\n  T3B &lt;- feols(\n    fml = totrob ~ \n      same  \n    + oneblock \n    | observ + monat, \n    data = dat_DID, \n    vcov = \"HC1\"\n  )\n)\n\nOLS estimation, Dep. Var.: totrob\nObservations: 7,884\nFixed-effects: observ: 876,  monat: 9\nStandard-errors: Heteroskedasticity-robust \n             Estimate Std. Error t value   Pr(&gt;|t|)    \nsameTRUE     -0.08007    0.02257 -3.5480 0.00039068 ***\noneblockTRUE -0.01326    0.01386 -0.9564 0.33889333    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.216661     Adj. R2: 0.097067\n                 Within R2: 0.001414\n\n# Regression (C)\n(\n  T3C &lt;- feols(\n    fml = totrob ~ \n      same\n    + oneblock \n    + twoblocks \n    | observ + monat, \n    data = dat_DID, \n    vcov = \"HC1\"\n  )\n)\n\nOLS estimation, Dep. Var.: totrob\nObservations: 7,884\nFixed-effects: observ: 876,  monat: 9\nStandard-errors: Heteroskedasticity-robust \n               Estimate Std. Error t value   Pr(&gt;|t|)    \nsameTRUE      -0.080802    0.02295 -3.5216 0.00043173 ***\noneblockTRUE  -0.013988    0.01447 -0.9669 0.33362749    \ntwoblocksTRUE -0.002185    0.01232 -0.1774 0.85920503    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.21666     Adj. R2: 0.096942\n                Within R2: 0.001419\n\n# Regression (D)\n(\n  T3D &lt;- feols(\n    fml = totrob ~ \n      same\n    + oneblock\n    + twoblocks \n    | monat,\n    data = dat_DID %&gt;% \n      filter(\n        monat &gt;= \"August\"\n      ),\n    vcov = \"HC1\"\n  )\n)\n\nOLS estimation, Dep. Var.: totrob\nObservations: 4,380\nFixed-effects: monat: 5\nStandard-errors: Heteroskedasticity-robust \n               Estimate Std. Error t value   Pr(&gt;|t|)    \nsameTRUE      -0.072719    0.01139 -6.3852 1.8897e-10 ***\noneblockTRUE  -0.011581    0.01090 -1.0623 2.8815e-01    \ntwoblocksTRUE -0.003429    0.00925 -0.3707 7.1086e-01    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.256214     Adj. R2: 0.002028\n                 Within R2: 0.00325 \n\n# Regression (E)\n(\n  T3E &lt;- feols(\n    fml = totrob ~\n      same \n    + oneblock \n    + twoblocks \n    | observ,\n    data = dat_DID %&gt;% \n      filter(\n        distanz &lt;= 2\n      ), \n    vcov = \"HC1\"\n  )\n)\n\nOLS estimation, Dep. Var.: totrob\nObservations: 3,816\nFixed-effects: observ: 424\nStandard-errors: Heteroskedasticity-robust \n              Estimate Std. Error t value Pr(&gt;|t|)    \nsameTRUE      -0.05439    0.02201 -2.4713 0.013510 *  \noneblockTRUE   0.01242    0.01260  0.9861 0.324146    \ntwoblocksTRUE  0.02423    0.01011  2.3972 0.016577 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.216568     Adj. R2: 0.093304\n                 Within R2: 0.003304\n\n\nZur Reproduktion von Tabelle 3 in Di Tella und Schargrodsky (2004) sammeln wir die geschätzten Modelle und erzeugen einen tabelarrische Zusammenfassung mit modelsummary::modelsummary(). Über das Argument gof_omit = \"^(?!(R2|Num.Obs.|FE.*)$).*\" wählen wir unter den Goodness-of-Fit-Statistiken \\(R^2\\), die Anzahl der Beobachtungen, sowie Indikatoren für die verwendeten Fixed Effekts mit einem Regular Expression aus.6\n6 Der Ausdruck ^(?!(R2|Num.Obs.|FE.*)$).* matcht jede Zeichenkette, außer sie ist “R2”, “Num.Obs.” oder beginnt mit “FE”. Andere Statistiken als diese Matches werden also in der Tabelle ausgelassen.\n# Tabellarischer Vergleich mit modelsummary()\n# (Tabelle 3 in Ditella und Schargrodsky, 2004)\nmodelsummary(\n  models = list(\n    \"(3A)\" = T3A, \n    \"(3B)\" = T3B, \n    \"(3C)\" = T3C, \n    \"(3D)\" = T3D, \n    \"(3E)\" = T3E\n  ),\n  stars = T,\n  gof_omit = \"^(?!(R2|Num.Obs.|FE.*)$).*\", \n  output = \"gt\"\n)\n\n\n\n\n\n\n\n\n(3A)\n(3B)\n(3C)\n(3D)\n(3E)\n\n\n\nsameTRUE\n-0.078***\n-0.080***\n-0.081***\n-0.073***\n-0.054*\n\n\n\n(0.022)\n(0.023)\n(0.023)\n(0.011)\n(0.022)\n\n\noneblockTRUE\n\n-0.013\n-0.014\n-0.012\n0.012\n\n\n\n\n(0.014)\n(0.014)\n(0.011)\n(0.013)\n\n\ntwoblocksTRUE\n\n\n-0.002\n-0.003\n0.024*\n\n\n\n\n\n(0.012)\n(0.009)\n(0.010)\n\n\nNum.Obs.\n7884\n7884\n7884\n4380\n3816\n\n\nR2\n0.198\n0.198\n0.198\n0.004\n0.195\n\n\nFE: observ\nX\nX\nX\n\nX\n\n\nFE: monat\nX\nX\nX\nX\n\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\nTabelle 9.6: Schätzungen des Effekts von Polizeipräsenz auf Autodiebstähle\n\n\n\nTabelle 9.6 präsentiert die Ergebnisse. Der Koeffizient von same ist jeweils in den Regressionen (3A), (3B) und (3C) negativ und hoch-signifikant. Die Stärke des geschätzten Effekt (und der Standardfehler) unterscheidet sich kaum zwischen den Modellen. Weiterhin sind die Koeffizienten von oneblock und twoblocks jeweils nicht signifikant von null verschieden. Die Interpretation dieser Ergebnisse ist, dass es einen lokal-beschränkten Effekt der erhöhten Polizeipräsenz auf Autodiebstähle in Blocks mit Polizeischutz gab: Die Polizeipräsenz verringerte die durchschnittliche Anzahl der Diebstähle in diesen Blocks um etwa \\(.08\\) Diebstähle pro Monat.\nDie Ergebnisse für die Modelle (3D) und (3E) zeigen ebenfalls signifikante negative Effekte des Polizeischutz anhand von Querschnitts- und Zeitreihenvariation und untermauern damit die Robustheit des gewählten Forschungsdesigns.\nZur besseren Interpretation des geschätzten Effekt in Modell (C) vergleichen wir mit der durchschnittlichen Anzahl der Diebstähle pro Monat in der Kontrollgruppe (Blocks mit einer Entfernung von mehr als zwei Blocks zum nächsten geschützten Block) für den Zeitraum von August 1994 bis Dezember 1994:7\n7 Wir berechnen also \\(\\widehat{\\alpha}_0 / \\overline{\\text{Diebstahlrate}} \\cdot 100\\).\n# Vergl. Effekt mit durchschnittlicher Anz. Diebstähle \n# in der Kontrollgruppe\n( \n  # gesch. Effekt\n  T3C %&gt;% \n    coefficients() %&gt;% \n    .[\"sameTRUE\"] \n) / \n  ( # Durchschnittt\n    polizeipraesenz %&gt;% \n      filter(\n        monat &gt;= \"August\" & monat &lt;= \"Dezember\",\n        distanz &gt; 2\n      ) %&gt;%\n      summarise(\n        m_totrob = mean(totrob)\n      ) %&gt;% \n      pull(m_totrob)\n  ) * 100\n\nsameTRUE \n  -74.92 \n\n\nDie Rechnung zeigt, dass es in dem betrachten Zeitraum durch die Behandlung mit zusätzlicher Polizeipräsenz zu einem Rückgang der Anzahl an Autodiebstählen um durchschnittlich \\(75\\%\\) in Blocks mit jüdischen Einrichtungen kam.\n\n9.5.3 Placebo-Tests\nPlacebo-Tests sind nützlich, um zu überprüfen, ob der geschätzte Zusammenhang zwischen der Erhöhung der Polizeipräsenz und der Verringerung der Kriminalität tatsächlich kausal ist oder ob andere Faktoren eine Rolle spielen. Ein möglicher Einfluss könnte beispielsweise durch zufällige temporäre Schwankungen in den Kriminalitätsraten entstehen. Di Tella und Schargrodsky (2004) wiederholen hierzu die Analyse anhand der Spezifikationen (3A), (3B) und (3C) für fiktive Interventionszeitpunkte vor dem Anschlag. Hierbei werden Ende April (4A), Ende Mai (4B) und Ende Juni (4C) als fiktive Zeitpunkte für die Placebo-Behandlung gewählt. Wenn diese Placebo-Tests ähnliche Ergebnisse (signifikante negative Schätzungen) wie die ursprüngliche Untersuchung liefern, könnte dies darauf hindeuten, dass die in Tabelle 9.6 geschätzten Effekte nicht kausal sind.\nWir replizieren diese Ergebnisse, indem wir die Schätzungen von (3A), (3B) und (3C) mit entsprechender Definition des Behandlungsindikators (group) jeweils für modifizierte Datensätze mit sämtlichen \\(3504\\) Beobachtungen vor dem Anschlag (filter(monat &lt;= \"Juli (1 - 17)\")) wiederholen.\n\n# (4A)\n# Placebo-Schätzung für \ndat_T4A &lt;- polizeipraesenz %&gt;%\n  mutate(\n    group = monat &gt;= \"Mai\" & monat &lt;= \"Juli (1 - 17)\",\n    same = (institut == 1) * group,\n    oneblock = (distanz == 1) * group,\n    twoblocks = (distanz == 2) * group\n  ) %&gt;% \n  filter(monat &lt;= \"Juli (1 - 17)\")\n\n(\n  T4A &lt;- feols(\n    fml = totrob ~ \n      same +\n      oneblock +\n      twoblocks \n    | observ + monat, \n    data = dat_T4A, \n    vcov = \"HC1\"\n  )\n)\n\nOLS estimation, Dep. Var.: totrob\nObservations: 3,504\nFixed-effects: observ: 876,  monat: 4\nStandard-errors: Heteroskedasticity-robust \n          Estimate Std. Error t value Pr(&gt;|t|) \nsame      -0.01864    0.05323 -0.3502  0.72622 \noneblock  -0.02554    0.02519 -1.0138  0.31075 \ntwoblocks -0.03263    0.02264 -1.4410  0.14969 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.18282     Adj. R2: 0.092254\n                Within R2: 0.001211\n\n# (4B)\ndat_T4B &lt;- dat_T4A %&gt;%\n  mutate(\n    group = monat &gt;= \"Juni\" & monat &lt;= \"Juli (1 - 17)\",\n    same = (institut == 1) * group,\n    oneblock = (distanz == 1) * group,\n    twoblocks = (distanz == 2) * group\n  ) \n\n(\n  T4B &lt;- feols(\n    fml = totrob ~ \n      same\n    + oneblock\n    + twoblocks \n    | observ + monat, \n    data = dat_T4B, \n    vcov = \"HC1\"\n  )\n)\n\nOLS estimation, Dep. Var.: totrob\nObservations: 3,504\nFixed-effects: observ: 876,  monat: 4\nStandard-errors: Heteroskedasticity-robust \n          Estimate Std. Error t value Pr(&gt;|t|) \nsame       0.01467    0.04011  0.3658  0.71451 \noneblock   0.01402    0.01976  0.7096  0.47799 \ntwoblocks -0.01466    0.01736 -0.8443  0.39856 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.182863     Adj. R2: 0.091835\n                 Within R2: 7.494e-4\n\n# (4C)\ndat_T4C &lt;- dat_T4A %&gt;%\n  mutate(\n    group = monat %in% c(\"Juli (1 - 17)\"),\n    same = (institut == 1) * group,\n    oneblock = (distanz == 1) * group,\n    twoblocks = (distanz == 2) * group\n  ) \n\n(\n  T4C &lt;- feols(\n    fml = totrob ~ \n      same\n    + oneblock\n    + twoblocks\n    | observ + monat, \n    data = dat_T4C, \n    vcov = \"HC1\"\n  )\n)\n\nOLS estimation, Dep. Var.: totrob\nObservations: 3,504\nFixed-effects: observ: 876,  monat: 4\nStandard-errors: Heteroskedasticity-robust \n           Estimate Std. Error t value Pr(&gt;|t|) \nsame      -0.036111    0.03858 -0.9361  0.34933 \noneblock   0.023105    0.02245  1.0294  0.30340 \ntwoblocks -0.009403    0.01693 -0.5554  0.57870 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.182841     Adj. R2: 0.09205 \n                 Within R2: 9.857e-4\n\n\n\n# Tabellarische Zusamenfassung mit modelsummary()\n# (Tabelle 4 in Ditella und Schargrodsky, 2004)\nmodelsummary(\n  models = list(\n    \"(4A)\" = T4A,\n    \"(4B)\" = T4B,\n    \"(4C)\" = T4C\n  ), \n  stars = T,\n  gof_omit = \"^(?!(R2|Num.Obs.|FE.*)$).*\", \n  notes = \"Fiktive Interventionen: (4A) Ende April. (4B): Ende Mai. (4C): Ende Juni.\",\n  output = \"gt\"\n) \n\n\n\n\n\n\n\n\n(4A)\n(4B)\n(4C)\n\n\n\nsame\n-0.019\n0.015\n-0.036\n\n\n\n(0.053)\n(0.040)\n(0.039)\n\n\noneblock\n-0.026\n0.014\n0.023\n\n\n\n(0.025)\n(0.020)\n(0.022)\n\n\ntwoblocks\n-0.033\n-0.015\n-0.009\n\n\n\n(0.023)\n(0.017)\n(0.017)\n\n\nNum.Obs.\n3504\n3504\n3504\n\n\nR2\n0.321\n0.320\n0.320\n\n\nFE: observ\nX\nX\nX\n\n\nFE: monat\nX\nX\nX\n\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\nFiktive Interventionen: (4A) Ende April. (4B): Ende Mai. (4C): Ende Juni.\n\n\n\n\n\n\n\nTabelle 9.7: Placebo-Tests für fiktive Anschlagszeitpunkte bis Ende Juni\n\n\n\nDie Ergebnisse in Tabelle 9.7 stützen die kausale Interpretation der Regressionen in Tabelle 9.6: Für keinen der fiktiven Zeitpunkte einer Intervention vor dem tatsächlichen Anschlag im Juli 1994 finden wir signifikante Unterschiede in der beobachteten Diebstahlrate zwischen Blocks mit einer jüdischen Einrichtung und solchen ohne.\n\n9.5.4 Weitere Robustness-Checks\nDi Tella und Schargrodsky (2004) betrachten weitere Robustness-Checks. Die Autoren merken zunächst an, dass (positive) Korrelation innerhalb der Block-spezifischen Fehlerterme über die Zeit (und zwischen den Blocks eines Viertels) zu einer Unterschätzung der Unsicherheit der Treatment-Effekt-Schätzer in den DID-Regressionen mit herkömmlichen Standardfehler-Formeln führen kann. In DID-Designs kann dieses Problem durch die Korrelation der Behandlung über die Zeit noch verstärkt werden. Daher werden alternative Spezifikationen betrachtet, um die Robustheit der Interenzstatistiken in Tabelle 9.6 hinsichtlich Korrelation in den Fehlerterme zu überprüfen. Dies geschieht in den Regressionen (T5A) bis (T5C). Weitere Modelle kontrollieren für Stadtviertel-spezifische Effekte (T5D), verwenden eine um Blocks ohne gemeldete Diebstähle reduzierten Datensatz (T5E) und nutzen eine Poisson-Spezifikation. Wir fassen diese Ansätze kurz zusammen:\n\nRegression (T5A): Entfernen der Zeitvariation von \\(\\textit{totrob}_{i,t}\\) innerhalb der Blocks durch Verwendung von Durchschnittswerten für die Monate vor und nach dem Anschlag. Regression dieser Durchschnittswerte auf die Behandlungsvariablen, wie in \\(\\eqref{eq:ppbaseC}\\).\nRegression (T5B): Schätzung der Spezifikation \\(\\eqref{eq:ppbaseC}\\) und Berechnung von Inferenzstatistiken mit cluster-robusten Standardfehlern (Clustering auf Block-Ebene).\nRegression (T5C): Um in Regression \\(\\eqref{eq:ppbaseC}\\) möglicher Korrelation zwischen den Blocks innerhalb eines Standviertel und Korrelation Stadtviertel-spezifischer Schocks über die Zeit zu begegnen, clustern Di Tella und Schargrodsky (2004) Standardfehler auf Stadtviertel-Monats-Ebene.\nRegression (T5D): Diese Regression ersetzt in \\(\\eqref{eq:ppbaseC}\\) die Fixed Effects für die Monate durch Stadtviertel-spezifische Zeit-Effekte (anhand von Indikatorvariablen zur Berechenung der geclusterten Standardfehler in T5C). Wenn keine Stadtviertel-spezifischen Einflüsse vorliegen, sollten die geschätzten Koeffizienten mit denen für das Modell \\(\\eqref{eq:ppbaseC}\\) vergleichbar sein.\nRegresion (T5E): Regression \\(\\eqref{eq:ppbaseC}\\) ohne Blocks in denen keine Diebstähle im Beobachtungszeitraum erfasst wurden. Auslassen dieser Beobachtungen sollte die Signifikanz des ATT-Schätzer nicht beeinflussen, jedoch zu einem stärkeren (negativen) Effekt führen, da die Kontrollgruppe dann ausschließlich aus Blocks mit gemeldeten Auto-Diebstählen besteht.\n\nRegresion (T5F): Die Daten erfassen das Aufkommen von Ereignissen (Diebstähle) innerhalb eines bestimmten Raum-Zeitbezugs (pro Block und Monat) beschreiben. Solche Daten können mit Modellen für Zählvariablen beschrieben werden. Di Tella und Schargrodsky (2004) schätzen eine Poisson-Regression. Hierbei wird der (log) Poisson-Parameter \\(\\lambda_{i,t}\\) (die Inzidenzrate) durch die lineare Funktion in \\(\\eqref{eq:ppbaseC}\\) modelliert, d.h.\n\\[\\begin{align}\n  \\begin{split}\n    \\log(\\lambda_{i,t})\n      =&\\, \\text{monat}_t + \\text{block}_i \\\\\n      + &\\, \\alpha_0 \\text{same}_{i,t} \\\\\n      + &\\, \\alpha_1 \\text{oneblock}_{i,t} \\\\\n      + &\\, \\alpha_1 \\text{twoblocks}_{i,t} \\\\\n      + &\\, \\epsilon_{i,t}.\n    \\end{split}\\label{eq:pppois}\n  \\end{align}\\]\nEine Schätzung des Behandlungseffekts anhand von Poisson-Regression sollte zu ähnlichen Schlussfolgerungen führen, wie die lineare Regression \\(\\eqref{eq:ppbaseC}\\).\n\n\nFür die Reproduktion der Robustness-Checks mit R erweitern wir dat_DID um eine Indikatorvariable für den Zeitraum vor dem Anschlag (pre) und eine kategorische Variable für Stadtviertel-Monat-Effekte mbc.\n\n# Datensatz für weitere Robustness-Checks\ndat_DID_T5 &lt;- dat_DID %&gt;%\n  mutate(\n    # Indikator: vor Anschlag\n    pre = monat &lt; \"Juli (18 - 31)\",\n    # Interaktion: Stadtviertel x Monat\n    mbc = paste0(barrio, \"_\", monat)\n  )\n\nDie Spezifikationen (T5A) bis (T5E) implementieren wir wie zuvor mit fixest::feols() unter Anpassung des Datensatzes, sofern relevant. Für cluster-robuste Standardfehler kann das Argument vcov = ~ cluster gesetzt werden, wobei cluster zu verwendende Ebene ist. Die Poisson-Spezifikation \\(\\eqref{eq:pppois}\\) in (T5F) schätzen wir mit fixest::feglm().8\n8 Der R-Befehl hierfür ist ähnlich wie für die Probit-Regressionen in Kapitel 9.4.2.\n# (5A)\n(\n  T5A &lt;- feols(\n    fml = totrob ~ \n      same\n    + oneblock\n    + twoblocks\n    | observ + monat,\n    data = dat_DID_T5 %&gt;%\n      group_by(observ, pre) %&gt;%\n      mutate(\n        totrob = mean(totrob)\n      )\n  )\n)\n\nOLS estimation, Dep. Var.: totrob\nObservations: 7,884\nFixed-effects: observ: 876,  monat: 9\nStandard-errors: Clustered (observ) \n               Estimate Std. Error t value   Pr(&gt;|t|)    \nsameTRUE      -0.080802    0.02394 -3.3752 0.00077008 ***\noneblockTRUE  -0.013988    0.01509 -0.9272 0.35406278    \ntwoblocksTRUE -0.002185    0.01239 -0.1763 0.86008157    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.07623     Adj. R2: 0.617109\n                Within R2: 0.011346\n\n# (5B)\n(\n  T5B &lt;- feols(\n    fml = totrob ~ \n      same\n    + oneblock\n    + twoblocks\n    | observ + monat,\n    data = dat_DID_T5, \n    vcov = ~ observ\n  )\n)\n\nOLS estimation, Dep. Var.: totrob\nObservations: 7,884\nFixed-effects: observ: 876,  monat: 9\nStandard-errors: Clustered (observ) \n               Estimate Std. Error t value   Pr(&gt;|t|)    \nsameTRUE      -0.080802    0.02394 -3.3752 0.00077008 ***\noneblockTRUE  -0.013988    0.01509 -0.9272 0.35406278    \ntwoblocksTRUE -0.002185    0.01239 -0.1763 0.86008157    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.21666     Adj. R2: 0.096942\n                Within R2: 0.001419\n\n# (5C)\n(\n  T5C &lt;- feols(\n    fml = totrob ~ \n      same\n    + oneblock\n    + twoblocks\n    | observ + monat,\n    data = dat_DID_T5, \n    vcov = ~ mbc\n  )\n)\n\nOLS estimation, Dep. Var.: totrob\nObservations: 7,884\nFixed-effects: observ: 876,  monat: 9\nStandard-errors: Clustered (mbc) \n               Estimate Std. Error t value  Pr(&gt;|t|)    \nsameTRUE      -0.080802    0.02206 -3.6630 0.0011186 ** \noneblockTRUE  -0.013988    0.01631 -0.8576 0.3989448    \ntwoblocksTRUE -0.002185    0.01702 -0.1284 0.8988318    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.21666     Adj. R2: 0.096942\n                Within R2: 0.001419\n\n# (5D)\n(\n  T5D &lt;- feols(\n    fml = totrob ~ \n      same \n    + oneblock \n    + twoblocks \n    | observ + mbc,\n    data = dat_DID_T5,\n    vcov = \"HC1\"\n  )\n)\n\nOLS estimation, Dep. Var.: totrob\nObservations: 7,884\nFixed-effects: observ: 876,  mbc: 27\nStandard-errors: Heteroskedasticity-robust \n               Estimate Std. Error t value  Pr(&gt;|t|)    \nsameTRUE      -0.083449    0.02434  -3.429 0.0006096 ***\noneblockTRUE  -0.016584    0.01571  -1.056 0.2912270    \ntwoblocksTRUE -0.002437    0.01289  -0.189 0.8500732    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.216318     Adj. R2: 0.09747 \n                 Within R2: 0.001412\n\n# (5E)\n(\n  T5E &lt;- feols(\n    fml = totrob ~ \n      same\n    + oneblock\n    + twoblocks\n    | observ + monat,\n    data = dat_DID_T5 %&gt;% \n      group_by(observ) %&gt;% \n      filter(\n        sum(totrob) &gt; 0\n      ),\n    vcov = \"HC1\"\n  )\n)\n\nOLS estimation, Dep. Var.: totrob\nObservations: 5,967\nFixed-effects: observ: 663,  monat: 9\nStandard-errors: Heteroskedasticity-robust \n               Estimate Std. Error t value   Pr(&gt;|t|)    \nsameTRUE      -0.126179    0.03742 -3.3719 0.00075199 ***\noneblockTRUE  -0.017895    0.01942 -0.9213 0.35692825    \ntwoblocksTRUE -0.003944    0.01579 -0.2498 0.80278849    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.248595     Adj. R2: 0.054989\n                 Within R2: 0.002067\n\n# (5F) \n(\n  T5F &lt;- feglm(\n    fml = totrob ~ \n      same\n    + oneblock\n    + twoblocks\n    | observ + monat,\n    data = dat_DID_T5 %&gt;% \n      group_by(observ) %&gt;% \n      filter(\n        sum(totrob) &gt; 0\n      ), \n    family = \"poisson\", \n    vcov = \"HC1\"\n  )\n)\n\nGLM estimation, family = poisson, Dep. Var.: totrob\nObservations: 5,967\nFixed-effects: observ: 663,  monat: 9\nStandard-errors: Heteroskedasticity-robust \n              Estimate Std. Error z value   Pr(&gt;|z|)    \nsameTRUE      -1.21621     0.3525 -3.4502 0.00056013 ***\noneblockTRUE  -0.14272     0.1593 -0.8956 0.37044224    \ntwoblocksTRUE -0.01691     0.1347 -0.1255 0.90009545    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nLog-Likelihood: -1,902.7   Adj. Pseudo R2: -0.193316\n           BIC:  9,665.2     Squared Cor.:  0.172849\n\n\n\n# Tabellarische Zusamenfassung\n# (Tabelle 5 in Di Tella und Schargrodsky, 2004)\nmodelsummary(\n  models = list(\n    \"(T5A)\" = T5A,\n    \"(T5B)\" = T5B,\n    \"(T5C)\" = T5C,\n    \"(T5D)\" = T5D,\n    \"(T5E)\" = T5E,\n    \"(T5F)\" = T5F\n  ), \n  stars = T,\n  gof_omit = \"^(?!(R2|Std.Errors|Num.Obs.|FE.*)$).*\", \n  exponentiate = c(rep(F, 5), T),\n  vcov = list(\"HC1\", ~observ, ~mbc, \"HC1\", \"HC1\", \"HC1\"),  \n  output = \"gt\"\n) \n\n\n\n\n\n\n\n\n(T5A)\n(T5B)\n(T5C)\n(T5D)\n(T5E)\n(T5F)\n\n\n\nsameTRUE\n-0.081***\n-0.081***\n-0.081**\n-0.083***\n-0.126***\n0.296***\n\n\n\n(0.009)\n(0.024)\n(0.022)\n(0.024)\n(0.037)\n(0.104)\n\n\noneblockTRUE\n-0.014*\n-0.014\n-0.014\n-0.017\n-0.018\n0.867\n\n\n\n(0.005)\n(0.015)\n(0.016)\n(0.016)\n(0.019)\n(0.138)\n\n\ntwoblocksTRUE\n-0.002\n-0.002\n-0.002\n-0.002\n-0.004\n0.983\n\n\n\n(0.004)\n(0.012)\n(0.017)\n(0.013)\n(0.016)\n(0.132)\n\n\nNum.Obs.\n7884\n7884\n7884\n7884\n5967\n5967\n\n\nR2\n0.660\n0.198\n0.198\n0.201\n0.162\n0.118\n\n\nStd.Errors\nHC1\nby: observ\nby: mbc\nHC1\nHC1\nHC1\n\n\nFE: observ\nX\nX\nX\nX\nX\nX\n\n\nFE: monat\nX\nX\nX\n\nX\nX\n\n\nFE: mbc\n\n\n\nX\n\n\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\nTabelle 9.8: Weitere Robustness-Checks\n\n\n\nAnhand von Tabelle 9.8 finden wir weitere Evidenz für die Robustheit der DID-Schätzung (T3C) in Tabelle 9.6:\n\nDie Schlussfolgerungen anhand der Inferenzstatistiken sind nicht sensibel gegenüber den unterschiedlichen Spezifikationen für die Berechnung der Standardfehler in (T5A) bis (T5C).\nDie Alternative Spezifikation von Fixed Effects für Kombination von Stadtviertel und Monat (mbc) in (T5D) beeinflusst den Koeffizientenschätzer von same nur marginal. Auch hier ist der geschätzte ATT signifikant.\nWie erwartet führt die Verkleinerung der Kontrollgruppe auf Blocks mit gemeldeten Diebstählen in (T5E) zu einer größeren Schätzung des Effekts. Der Effekt bleibt signifikant.\nIn der Poisson-Regression in (T5F) finden wir ebenfalls einen signifikanten Effekt von same. Beachte das dieser Koeffizient den multiplikativen Einfluss von Polizeipräsenz auf die Inzidenzrate (durchschnittliche Anzahl an Diebstählen pro Monat pro Block) angibt. Die Interpretation des Schätzwerts von etwa \\(0.3\\) bedeutet also eine Reduktion der Inzidenz um eta \\(70\\%\\) in Blocks mit erhöhter Polizeipräsenz gegenüber der Kontrollgruppe (Blocks mit mehr als zwei Blocks Entfernung zur nächsten jüdischen Einrichtung). Diese Schätzung stimmt also gut überein mit unserer Interpretation der Ergebnisse in Tabelle 9.6.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "DiD.html#zusammenfassung",
    "href": "DiD.html#zusammenfassung",
    "title": "\n9  Difference-in-Differences\n",
    "section": "\n9.6 Zusammenfassung",
    "text": "9.6 Zusammenfassung\nDer DID-Schätzer liefert uns eine Schätzung des ATT, indem er die Veränderung der Ergebnisse in der Behandlungsgruppe vor und nach der Intervention mit der entsprechenden Veränderung in der Kontrollgruppe vergleicht. Die Annahme paralleler Trends ist entscheidend: Nur wenn diese Annahme gilt, können wir sicher sein, dass die Differenz in den Differenzen tatsächlich den kausalen Effekt der Behandlung widerspiegelt und nicht durch andere zeitgleiche Faktoren beeinflusst wird.\nZusammenfassend bietet der DID-Ansatz im Potential Outcomes Framework eine robuste Methode zur Schätzung kausaler Effekte, insbesondere wenn randomisierte Experimente nicht durchführbar sind. Durch den Vergleich von Zeitverläufen in Behandlungs- und Kontrollgruppen unter der Annahme paralleler Trends können wir verlässliche Schätzungen des ATT gewinnen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdireksombat, Kampon. 2010. „The Effects of the 1993 Earned Income Tax Credit Expansion on the Labor Supply of Unmarried Women“. Public Finance Review 38 (1): 11–40. https://doi.org/https://doi.org/10.1177/1091142109358626.\n\n\nCallaway, Brantly, und Pedro H. C. Sant’Anna. 2021. „Difference-in-Differences with Multiple Time Periods.“ Journal of Econometrics 225 (2): 200–230. https://doi.org/10.1016/j.jeconom.2020.12.001.\n\n\nDi Tella, Rafael, und Ernesto Schargrodsky. 2004. „Do Police Reduce Crime? Estimates Using the Allocation of Police Forces After a Terrorist Attack“. American Economic Review 94 (1): 115–33. https://doi.org/10.1257/000282804322970733.\n\n\nEissa, N., und J. B. Liebman. 1996. „Labor Supply Response to the Earned Income Tax Credit“. The Quarterly Journal of Economics 111 (2): 605–37. https://doi.org/10.2307/2946689.\n\n\nGoodman-Bacon, Andrew. 2021. „Difference-in-Differences with Variation in Treatment Timing.“ Journal of Econometrics 225 (2): 254–77. https://doi.org/10.1016/j.jeconom.2021.03.014.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "EventStudies.html",
    "href": "EventStudies.html",
    "title": "\n10  Event Studies\n",
    "section": "",
    "text": "10.1 Counterfactuals und Regression\nEvent Studies sind hilfreich, weil sie die Identifizierung kausaler Zusammenhänge in komplexen Situationen mit vielen potentiellen (unbeobachtbaren) Backdoor-Variablen ermöglichen können. Durch die Fokussierung auf einen klar definierten Zeitraum um das Ereignis und die Anwendung zusätzlicher Kontrollmechanismen (Regression) kann der interessierende Effekt oftmals plausibel identifiziert werden.\nFür die Schätzug des Behandlungseffekts ist es nötig, einen plausiblen (kontrafaktischen) Vergleichswert für die nach der Intervention beobachteten Werte der Outcome-Variable zu finden. Hierbei gibt es drei wesentliche Vorgehensweisen:\nIn vielem polit-ökonomischen Anwendungen ist es plausibel, dass eine Intervention einen längerfristigen (konstanten) Effekt hat. In solchen Fällen kann ein Interkationsmodell1 geschätzt werden: \\[\\begin{align}\n  Y_t = \\beta_0 + \\beta_1 t + \\beta_2 \\textup{post}_t + \\beta_3 \\cdot t \\cdot \\textup{post}_t + \\epsilon_t.\\label{eq:eslinint}\n\\end{align}\\] Hier ist \\(t\\) die Zeit-Variable und \\(\\textup{post}_t\\) ein Dummy-Regressor für Perioden nach dem Event. Die Regression \\(\\eqref{eq:eslinint}\\) modelliert also einen linearen Trend in der Outcome-Variable \\(Y\\), der sich vor und nach dem Event unterscheiden kann. Eine Schätzung mit Regressions ist außerdem hilfreich, weil für beobachtbare Determinanten von \\(Y\\), die möglicherweise durch das Event beeinflusst werden, kontrolliert werden kann. Solche “robusten” Spezifikationen erhöhen die plausibilität der Identifikationsstrategie und können eine präzisere Schätzung des Behandlungseffekts gewährleisten.\nModell \\(\\eqref{eq:eslinint}\\) kann leicht auf Panel-Daten erweitert werden, wobei die zusätzliche Information aus der Querschnittsdimension die Schätzung eines zeitlichen Verlaufs des Interventionseffekts ermöglicht. Bei Verfügbarkeit können Beobachtungseinheiten, die nicht von der Intervention betroffen sind als Counterfactuals herangezogen werden. Wir betrachten die Anwendung einer Event-Study-Methodik mit Panel-Daten-Modellen in der Case Study in Kapitel 10.3.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Event Studies</span>"
    ]
  },
  {
    "objectID": "EventStudies.html#counterfactuals-und-regression",
    "href": "EventStudies.html#counterfactuals-und-regression",
    "title": "\n10  Event Studies\n",
    "section": "",
    "text": "Durschnittlichen Entwicklung als Counterfactual: Hierbei werden Mittelwerte der Outcome Variable vor und nach der Intervention vergleichen. Diese Methode ignoriert mögliche Zeiteffekte in der Outcome-Variable und sollte nur für kleine Zeiträume vor und nach der Intervention angewendet werden.\nVorhersage mit Vor-Event-Daten: Modelliere die Outcome-Variable basierend auf Daten vor dem Event und erhalte Vorhersagen für Perioden (unmittelbar) nach dem Event.\nVorhersage mit Nach-Event-Daten: Modelliere die Outcome basierend auf Daten vor dem Event unter Berücksichtigung weiterer Regressoren. Erhalte Vorhersagen für Perioden (unmittelbar) nach dem Event unter Einbezug der nach dem Event beobachteten Regressor-Werte.\n\n\n1 Der Ansatz \\(\\eqref{eq:eslinint}\\) wird auch als segmentierte Regression bezeichnet.\n\n\n\n\n\n\nKey Facts zu Event Studies\n\n\n\n\nEine Event Study misst Auswirkungen eines Ereignisses oder Interventionen auf eine über mehrere Perioden beobachtete Outcome-Variable.\nZiel ist die Schätzung des kausalen Effekt der Intervention durch die Analyse der beobachteten Variation in der Outcome-Variable über die Zeit, meist für Zeitpunkte nahe des Events.\nAnnahme: Das Event ist exogen. In empirischen Studien werden häufig quasi-experimentelle Forschungsdesigns verwendet (Events sind natürliche Schocks).\nRegressionsmodelle können genutzt werden, um den Effekt der Intervention über die Zeit zu schätzen. Hierbei gewährleistet die Kontrolle für potentielle Confounder die Robustheit und Präzision der Schätzung des interessierenden Effekts.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Event Studies</span>"
    ]
  },
  {
    "objectID": "EventStudies.html#beispiel-strukturbruch-in-zeitreihe",
    "href": "EventStudies.html#beispiel-strukturbruch-in-zeitreihe",
    "title": "\n10  Event Studies\n",
    "section": "\n10.2 Beispiel: Strukturbruch in Zeitreihe",
    "text": "10.2 Beispiel: Strukturbruch in Zeitreihe\nZur Illustration der Methodik simulieren wir Zeitreihendaten mit einem durch ein Ereignis in der Mitte des Beobachtungszeitraums ausgelösten Strukturbruch im Erwartungswert (der kausale Effekt des Events). Wir interessieren uns für den Effekt des Events auf die Variablen \\(X_t\\) und \\(Y_t\\).\nDer hierfür verwendete DGP lautet \\[\\begin{align*}\n  X_t = &\\, \\alpha_1 \\textup{post}_t + \\epsilon_t,\\\\\n  \\\\\n  Y_t = &\\, \\beta_1 X_t + \\beta_2 \\textup{post}_t + \\varepsilon_t,\n\\end{align*}\\] für \\(t=1,\\dots,n\\) mit \\(\\epsilon_t,\\ \\varepsilon_t \\sim\\,u.i.v. N(0,1)\\). Der Dummy-Regressor \\(\\textup{post}_t\\) ist definiert als \\[\\begin{align*}\n  \\textup{post}_t =\n  \\begin{cases}\n     1 \\ \\ \\textup{für}  &t &gt; t_\\textup{Event},\\\\\n     0 \\ \\ &\\textup{sonst}\n  \\end{cases}\n\\end{align*}\\] wobei \\(t_\\textup{Event} = \\lfloor n/2 \\rfloor\\). Für den Effekt des Events auf \\(X_t\\) und \\(Y_t\\) wählen wir \\(\\alpha_1 = 2\\) und \\(\\beta_2 = 2\\). Weiterhin ist \\(\\beta_1 = .75\\). Wir generieren beide Zeitreihen für \\(n=200\\) Perioden.\nBeachte das \\(X_t\\) ein Confounder bei der Ermittlung des Effekts auf \\(Y_t\\) ist: Das Event zum Zeitpunkt \\(t=100\\) hat einen kausalen Effekt von \\(\\alpha_1 = \\beta_2 = 2\\) auf beide Variablen. Da \\(X_t\\) einen Einfluss von \\(\\beta_1\\cdot X_t = .75\\cdot X_t\\) auf \\(Y_t\\) hat, müssen wir für \\(X_t\\) kontrollieren, um die back door durch \\(X_t\\) zu schließen. Für \\(X_t\\) hingegen ist der Event-Regressor \\(\\textup{post}_t\\) exogen.\n\nlibrary(ggplot2)\nlibrary(cowplot)\n# Daten simulieren\nset.seed(123)\n\n# Parameter der Simulation\nn &lt;- 200  # Anzahl der Beobachtungen\nt_event &lt;- floor(n/2)  # Zeitpunkt des Ereignisses\neffect_size &lt;- 2  # Größe des Effekts nach dem Ereignis\n\n# Zeitindex\ntime &lt;- 1:n\n\n# X generieren \nx &lt;- ifelse(\n  test = time &lt;= t_event, \n  yes = rnorm(n, mean = 0), \n  no = rnorm(n, mean = effect_size)\n)\n\n# Y generieren\npre_event_mean &lt;- .75 * x\npost_event_mean &lt;- pre_event_mean + effect_size\n\ny &lt;- ifelse(\n  test = time &lt;= t_event, \n  yes = rnorm(n, mean = pre_event_mean), \n  no = rnorm(n, mean = post_event_mean)\n)\n\n# Zusammenführen der Daten\ndata &lt;- tibble(\n  time = time, \n  x = x,\n  y = y, \n  # Event bei t_event\n  post = ifelse(\n    test = time &gt; t_event, \n    yes = TRUE, \n    no = FALSE\n  )\n)\n\n# Überblick\nhead(data)\n\n# A tibble: 6 × 4\n   time       x       y post \n  &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;lgl&gt;\n1     1 -0.560  -0.494  FALSE\n2     2 -0.230  -1.34   FALSE\n3     3  1.56    0.534  FALSE\n4     4  0.0705  0.0240 FALSE\n5     5  0.129   0.768  FALSE\n6     6  1.72   -0.364  FALSE\n\n\n\n# Plotten der Zielvariable und Regressor\nggplot(data, aes(x = time)) +\n  geom_line(aes(y = y, linetype = \"Outcome-Variable\"), size = 1) +\n  geom_vline(xintercept = t_event, linetype = \"dashed\", color = \"red\") +\n  geom_line(aes(y = x, linetype = \"Regressor\")) +\n  labs(x = \"Zeit\", y = \"Wert\", color = \"Legende\") +\n  scale_linetype_discrete(\"\") +\n  theme_cowplot() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n10.2.1 Event-Study-Schätzung: Kausaler Effekt für \\(X_t\\)\n\nWir schätzen zunächst den Effekt des Events auf \\(X_t\\). Die Differenz der Mittelwerte nach und vor des Events ist ein erwartungstreuer und konistenter Schätzer für \\(\\alpha_1\\). Dieser Schätzer ist äquivalent zum KQ-Schätzer von \\(\\alpha_1\\) im Modell \\[\\begin{align}\n  X_t = \\alpha_1 \\textup{post}_t + u.\n\\end{align}\\]\n\n# Effekt-Schätzung für X\n\n# Post-mean vs. Pre-Mean\ndata %&gt;% \n  group_by(post) %&gt;% \n  summarise(x_mean = mean(x)) %&gt;%\n  pull(x_mean) %&gt;%\n  diff()\n\n[1] 1.873371\n\n# ...ist äquivalent zum KQ-Schätzer\nlm(\n  formula = x ~ post,\n  data = data\n) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = x ~ post, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.42968 -0.64741  0.01173  0.69143  2.60768 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.09041    0.09778   0.925    0.356    \npostTRUE     1.87337    0.13829  13.547   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9778 on 198 degrees of freedom\nMultiple R-squared:  0.481, Adjusted R-squared:  0.4784 \nF-statistic: 183.5 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\n\n10.2.2 Event-Study-Schätzung: Kausaler Effekt für \\(Y_t\\)\n\nFür die Schätzung von \\(\\beta_2\\) liefert diese Vorgehensweise einen verzerrten Schätzer des kausalen Effekts des Events auf \\(Y_t\\).\n\n# Verzerrte Effekt-Schätzung für Y\n# Post-mean vs. Pre-Mean\ndata %&gt;% \n  group_by(post) %&gt;% \n  summarise(y_mean = mean(y)) %&gt;%\n  pull(y_mean) %&gt;%\n  diff()\n\n[1] 3.405051\n\n# ...ist äquivalent zum KQ-Schätzer\nlm(\n  formula = y ~ post,\n  data = data\n) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = y ~ post, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9658 -0.8354  0.0923  0.8565  3.5575 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.1737     0.1241    1.40    0.163    \npostTRUE      3.4051     0.1754   19.41   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.241 on 198 degrees of freedom\nMultiple R-squared:  0.6555,    Adjusted R-squared:  0.6537 \nF-statistic: 376.7 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nEine bessere Alternative ist die Schätzung des Effekts anhand der Vorhersage von \\(Y_t\\) durch \\(X_t\\) für Beobachtungen von \\(X_t\\) nach dem Event. Hierfür schätzen wir das Modell \\[\\begin{align}\n  Y_t = \\beta_0 + \\beta_1 X_t + e_t\\label{eq:xpredy}\n\\end{align}\\] mit Beobachtungen vor dem Event und berechnen vorhergesagte Werte \\(\\widehat{Y}_t\\) mit Beobachtungen \\(X_t\\) nach dem Event.\n\n# Modell zur Vorhersage von Y mit X\nmodel_pre_event &lt;- lm(\n  formula = y ~ x, \n  # Beobachtungen vor dem Ereignis\n  data = data %&gt;% \n    filter(!post)\n)\n\n# Vorhersage von Y mit X\ndata &lt;- data %&gt;%\n  mutate(\n    predicted = predict(\n      object = model_pre_event, \n      newdata = data\n    )\n  )\n\nDie nachfolgende Grafik zeigt die mit \\(X_t\\) vorhergesagte Entwicklung von \\(Y_t\\) nach dem Event.\n\n# Vorhersage von Y und X plotten\nggplot(data, aes(x = time)) +\n  geom_line(aes(y = y, color = \"Outcome Y\"), size = 1) +\n  geom_vline(xintercept = t_event, color = \"red\", linetype = \"dashed\") +\n  geom_line(\n    data = data %&gt;% filter(post), \n    mapping = aes(y = predicted, color = \"Counterfactual Y\"),\n    size = 1\n  ) +\n  labs(x = \"Zeit\", y = \"Zielvariable\") +\n  scale_color_manual(\n    \"\",\n    values = c(\n      \"Outcome Y\" = \"black\", \n      \"Counterfactual Y\" = \"steelblue\"\n    )\n  ) +\n  theme_cowplot() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nEin Schätzer des Effekts basierend auf dieser Vorhersage ist die Differenz zwischen dem mittleren beobachteten und vorhersagten Werten von \\(Y_t\\) (counterfactual).\n\n# mean(Y) - mean(gesch. counterfactual)\ndata %&gt;% \n  filter(post) %&gt;% \n  summarise(\n    effect = mean(y) - mean(predicted)\n  )\n\n# A tibble: 1 × 1\n  effect\n   &lt;dbl&gt;\n1   2.39\n\n\nUnter der Annahme, dass der kausale Effekt des Events langfristig (für sämtliche Beobachtungen mit \\(t&gt;t_\\textup{Event}\\)) auf \\(Y_t\\) wirkt, können wir \\(\\beta_2\\) effizienter mit einem Interaktionsmodell schätzen. Hierbei vermeiden wir die Unsicherheit in den vorhergesagten Werten \\(\\widehat{Y}_t\\), die aus der Schätzung des Modells \\(\\eqref{eq:xpredy}\\) resultiert.\n\nlm(\n  formula = y ~ x + post,\n  data = data\n) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = y ~ x + post, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7683 -0.5012  0.1047  0.6146  2.5857 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.10575    0.10044   1.053    0.294    \nx            0.75111    0.07284  10.311   &lt;2e-16 ***\npostTRUE     1.99795    0.19675  10.155   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.002 on 197 degrees of freedom\nMultiple R-squared:  0.7762,    Adjusted R-squared:  0.774 \nF-statistic: 341.7 on 2 and 197 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Event Studies</span>"
    ]
  },
  {
    "objectID": "EventStudies.html#sec-deterrance",
    "href": "EventStudies.html#sec-deterrance",
    "title": "\n10  Event Studies\n",
    "section": "\n10.3 Case Study: Abschreckungseffekte durch Strafgesetzgebung",
    "text": "10.3 Case Study: Abschreckungseffekte durch Strafgesetzgebung\nDavid S. Abrams (2012a) untersucht den Abschreckungseffekt drohender Inhaftierung für Straftaten anhand von Daten zu verschärfter Strafgesetzgebung (“Add-on laws”) in den USA. Solche Gesetze erhöhen in der Regel die Strafen für Delikte, insbesondere wenn nachweißlich Schusswaffen verwendet werden. Die Studie nutzt eine Event-Study-Methodik, um die Auswirkungen dieser Gesetzesänderung auf die Kriminalitätsraten für verschiedene Straftaten zu analysieren. Dabei wird der Zeitpunkt der tatsächlichen Gesetzeseinführung in verschiedenen US-Bundesstaaten herangezogen, um den kausalen Abschreckungseffekt einer drohenden Gefängnisstrafe anhand von Statistiken in den unmittelbaren Folgeperioden zu isolieren.\n\n10.3.1 Identifikationsstrategie\nDer Fokus des Forschungsdesigns liegt auf der Unterscheidung zwischen zwei wesentlichen Wirkungskanälen von verschärften Strafgesetzen auf die Kriminalitätsrate:\n\nAbschreckungseffekt: Die Aussicht auf eine längere Inhaftierung erzielt einen Abschreckungseffekt, der sich ‘positiv’ auf die Kriminalitätsrate auswirkt (im Sinne einer verringerten Kriminalität). Dies ist der vermutete (kausale) Effekt.\nInhaftierungseffekt: Inhaftierte Individuen können keine Straftaten begehen und verringern daher die Kriminalitätsrate. Dieser Effekt ist insbesondere für Straftaten mit großer Rückfälligkeit plausibel (etwa Beschaffungskriminalität). Für längere Haftstrafen erwarten wir daher einen Rückgang dieser Kriminalitätsraten nach der Verschärfung der Gesetze.\n\nDer in 2. beschriebene Effekt der Gesetzesverschärfung kann sich erst nach der im Rahmen der alten Gesetzgebung verhängten durchschnitlichen Mindesthaftstrafe einstellen: Der verringernde Effekt auf die Kriminalitätsrate zeigt sich erst in den Daten, nachdem Straftäter länger als gemäß der alten Gesetzgebung üblich inhaftiert sind. Rückgänge der Kriminalitätrate die in diesem Zeitraum unmittelbar nach der Gesetztesverschärfung gemessen werden, können also auf den Abschreckungseffekt zurückgeführt werden. Anhand von Statistiken zu verhängten Strafen für Überfälle mit Schusswaffen ermittelt David S. Abrams (2012a) ein mittleres Mindest-Strafmaß von drei Jahren.\nDie Exogenität der Verabschiedung von Add-On-Gesetzen wird in David S. Abrams (2012a) wiefolgt begründet: Obwohl die meisten Zusatzgesetze zu Schusswaffen in den 1970er Jahren erlassen wurden und es gemeinsame Trends in den Kriminalitäsraten der Bundesstaaten gibt, sind die genauen Zeitpunkte der Einführung solcher Gesetze aufgrund einer Vielzahl von Eigenheiten des politischen Prozesses in den Bundesstaaten weitgehend zufällig. Die Exogenität ist plausibel, wenn wir (durch Regression) Backdoors aufgrund Bundesstaat-spezifischer (konstanter und zeit-variierender) Faktoren schließen.\n\n10.3.2 Replikation mit R\nDer Datensatz Abrams2012.dta ist ein Auszug der Daten aus dem Replikationspaket (David S. Abrams 2012b), welches hier eingesehen werden kann. Die Daten liegen im STATA-Format .dta vor und können mit haven::read_dta() eingelesen werden.\n\n\n\n\n\n\nDie Beobachtungen (Zeilen) in abrams sind Kriminalstatistiken von staatlichen Rechtsberichterstattungsagenturen, die im Rahmen der Uniform Crime Reports des FBI landesweit für den Zeitraum 1965 bis 2002 zusammengetragen wurden. Eine Erläuterung der verfügbaren (ausschließlich numerischen) Variablen erfolgt in Tabelle 10.1.\n\n\n\n\n\n\n\n\nVariable\nBeschreibung\n\n\n\npcralloffense\nPro-Kopf-Kriminalitätsrate\n\n\npcrrobgun\nPro-Kopf-Rate Raubüberfälle mit Schusswaffen\n\n\npcrasltgun\nPro-Kopf-Rate Angriff mit Schusswaffen\n\n\npcrburgtot\nEinbruch\n\n\npcrmurder\nMord\n\n\nlnpcrrobgun\nLog Pro-Kopf-Rate Raubüberfälle mit Schusswaffen\n\n\nFSTATE\nBundesstaat\n\n\nyear\nJahr\n\n\nymm\nGesetzliche Mindeststrafe in Kraft\n\n\nrelyr\nPerioden rel. zur Einführung von Add-On-Law\n\n\neveraddon\nDummy: Jemals Add-on-Law?\n\n\nyaddon\nDummy: 1 Jahr nach Einführung Add-On-Law\n\n\ntwoyears_add\nDummy: 2 Jahre nach Einführung Add-On-Law\n\n\nthreeyears_add\nDummy: 3 Jahre nach Einführung Add-On-Law\n\n\ndcpoverty\nArmutsquote\n\n\ndcunemp\nArbeitlosenquote\n\n\ndcblack_per\nBevölkerungsstruktur\n\n\ndcp15t17\nAnteil 15 bis 17 Jährige\n\n\ndcp18t24\nAnteil 18 bis 24 Jährige\n\n\ndcp25t34\nAnteil 25 bis 34 Jährige\n\n\ndcpolice_per1\nAnteil Polizeikräfte an Bevölkerung\n\n\ndcprison_per1\nAnteil Inhaftierte an der Bevölkerung\n\n\npost74\nDummy: Beobachtung nach 1974\n\n\nstatepop\nBevölkerung\n\n\n\n\n\n\n\nTabelle 10.1: Abrams2012.dta – Kriminalität in US-Bundesstaaten.\n\n\n\nWir replizieren nachfolgend Kernergebnisse aus David S. Abrams (2012a) hinsichtlich des Abschreckungseffekts von drohenden Gefängnisstrafen auf die Begehung von Raubüberfällen mit Schusswaffen.\nFür einen ersten Eindruck der Entwicklung von Kriminalitätsraten in US-Bundesstaaten, die über den Beobachutungszeitraum Add-On-Gesetze erlassen haben, berechnen wir zunächst mit der Population (statepop) gewichtete Mittelwerte vor und nach der Einführung dieser Gesetzte.2 Hierzu verwenden wir eine Teilmenge des Panel-Datensatzes abrams mit Beobachtungen zu 7 Zeitpunkten vor sowie 6 Zeitpunkten nach dem Inkraftreten der Gesetzesverschärfung.\n2 Die Mittelwerte hier und sämtliche Punktschätzer in den nachfolgenden Regressionen sind bevölkerungsgewichtet, sodass Statistiken von Behörden, die größere Bevölkerungsteile abdecken, einen größeren Einfluss auf die Schätzung haben als Behörden in Regionen mit kleinerer Population.\n# Datensatz für Vergleich mittlerer Kriminalität vorbereiten\nabrams_t1 &lt;- abrams %&gt;% \n  mutate(\n    before = case_when(\n      relyr &gt; 0 ~ \"Danach\",\n      T ~ \"Bevor\"\n    )\n  ) %&gt;% \n  filter(relyr &gt;= -7 & relyr &lt;= 6) %&gt;%\n  rename(\n    `Angriff mit Schusswaffen` = pcrasltgun,\n    `Einbruch` = pcrburgtot,\n    `Raubüberfall mit Schusswaffe` = pcrrobgun,\n    `Mord` = pcrmurder\n  )\n\nFür die tabellarische Zusammenfassung nutzen wir modelsummary::datasummary(). Über das Argument formula definieren wir die zu berechnenden Statistiken diagis::weighted_mean und diagis::weighted_se für vier Kategorien von Delikten. Der Operator * bewirkt eine gruppierte Berechnung dieser Statistiken (gemäß before).\n\nlibrary(modelsummary)\n\n# Tabellarische Zusammenfassung\ndatasummary(\n  formula =\n    `Angriff mit Schusswaffen`\n  + `Einbruch`\n  + `Mord`\n  + `Raubüberfall mit Schusswaffe`\n  \n  ~ (Factor(before) \n     * (\n        (\n          (mean = diagis::weighted_mean) + (SE = diagis::weighted_se) \n        )\n        * Arguments(na.rm = TRUE, w = statepop)\n       )\n     ), \n  \n  data = abrams_t1\n  )\n\n\n\n \n\n  \n    \n\ntinytable_mot3g5pvqakw7gtdn62w\n\n\n      \n\n\n \nBevor\nDanach\n\n\n \n                mean\n                SE\n                mean\n                SE\n              \n\n\n\nAngriff mit Schusswaffen    \n                  108.50 \n                  6.15 \n                  98.13  \n                  5.13 \n                \n\nEinbruch                    \n                  1760.60\n                  44.36\n                  1692.76\n                  96.91\n                \n\nMord                        \n                  15.77  \n                  0.80 \n                  13.21  \n                  1.12 \n                \n\nRaubüberfall mit Schusswaffe\n                  218.59 \n                  15.91\n                  130.72 \n                  7.93 \n                \n\n\n\n\n    \n\n\n\nTabelle 10.2: Mittlere Kriminalitätsraten je 100k Einwohner vor und nach Einführung strafverschärfender Gesetze\n\n\n\nTabelle 10.2 zeigt, dass es in den Zeiträumen nach Gesetzesverschärfungen im Mittel zu einer Verringerung der betrachteten Pro-Kopf-Kriminalitätsraten kommt.3 Da David S. Abrams (2012a) sich auf die Analyse des Effekts auf Delikte unter Gebrauch von Schusswaffen fokussiert, berechnen wir weiterhin Trends in der allgemeinen Pro-Kopf-Kriminalität (pcralloffense) und Raubüberfälle mit Schusswaffen, (pcrrobgun). Wir unterscheiden hierbei zusätzlich zwischen Bundesstaaten, die im Beobachtungszeitraum Add-On-Gesetzte verabschiedet haben (everaddon) und solchen ohne.\n3 Tabelle 10.2 reproduziert Teilergebnisse von Tabelle 1 in David S. Abrams (2012a).\nlibrary(tidyr)\n\n# Populationsgewichtete Trends berechnen\nplot_dat &lt;- abrams %&gt;% \n  select(\n    year, \n    everaddon, \n    pcralloffense, \n    pcrrobgun, \n    statepop\n  ) %&gt;%\n  mutate(\n    # Skalierung für 2. Y-Achse\n    pcrrobgun = pcrrobgun * 33.3\n  ) %&gt;%\n  rename(\n    Schusswaffen = pcrrobgun, \n    Gesamt = pcralloffense,\n    AddOnGesetze = everaddon\n  ) %&gt;%\n  pivot_longer(\n    cols = Schusswaffen:Gesamt, \n    names_to = \"Verbrechen\", \n    values_to = \"Wert\"\n  ) %&gt;%\n  group_by(year, AddOnGesetze, Verbrechen) %&gt;% \n  mutate(\n    AddOnGesetze = as.factor(AddOnGesetze),\n  ) %&gt;%\n  # Gewichtete Mittelwerte\n  summarise(\n    Wert = weighted.mean(\n      Wert, \n      w = statepop, \n      na.rm = T\n    )\n  ) \n\nWir plotten die berechneten Trends in plot_dat mit entsprechender Farbgebung mit ggplot2(). Das Ergebnis (Abbildung 10.1) ist eine Reproduktion von Abbildung 2 in David S. Abrams (2012a).\n\nlibrary(ggplot2)\nlibrary(cowplot)\n\n# Trendverlauf plotten\nggplot(\n  data = plot_dat,\n  mapping = aes(\n    x = year, \n    y = Wert, \n    color = Verbrechen, \n    lty = AddOnGesetze\n  )\n) +\n  geom_line() +\n  scale_y_continuous(\n    name = \"Gesamte Anzeigen / 100k Einw.\",\n    sec.axis = sec_axis(\n      transform = ~ . / 33.3, \n      name = \"Raubüberfälle Schusswaffen / 100k Einw.\"\n    )\n  ) +\n  theme_cowplot() +\n  theme(\n    legend.position = \"bottom\", \n    axis.title.y.left = element_text(color = \"#F8766D\"),\n    axis.title.y.right = element_text(color = \"#00BFC4\")\n  )\n\n\n\n\n\n\nAbbildung 10.1: Populations-gewichtete mittlere Pro-Kopf-Kriminalitätsraten für US-Bundesstaaten (1965–2002)\n\n\n\n\nAbbildung 10.1 zeigt einen starken Anstieg der Kriminalität in den 1960er und 1970er Jahren, insbesondere für Rabüberfälle mit Schusswaffen. Weiterhin sind die Trends in Staaten mit und ohne Zusatzstrafen deutlich korreliert. Die Entwicklung der Pro-Kopf-Kriminalität insgesamt schwächt sich anschließend ab und folgt ab den 1990er Jahren einem rückläufigen Trend. Es ist auffällig, dass sowohl die bewaffneten Raubüberfälle als auch die allgemeine Kriminalität zu Beginn des Beobachtungszeitraums für Staaten mit Add-on-Gesetzen höher sind, sich dies aber im letzten Jahrzehnt ebenfalls umkehrt. Wie in David S. Abrams (2012a) erläutert, könnte dies sowohl auf verschäfte Gesetze als auch auf Variation zwischen den Bundesstaaten zurückzuführen sein. Statistische Verfahren zur Schätzung des Effekts also sollten zwischen diesen Möglichkeiten unterscheiden können, bspw. durch die Kontrolle für Fixed Effects und Zeit-Trends.\n\n10.3.2.1 Basis-Spezifikation\nWir folgen David S. Abrams (2012a) und schätzen zunächst eine Basis-Spezifikation für eine balancierte Teilstichprobe aus abrams. Hierbei werden lediglich Bundesstaaten mit Datenpunkten mindestens 7 Jahre vor und maximal 6 Jahre nach der Einfühung eines Add-On-Gesetzes berücksichtigt. Das Regressionsmodell ist \\[\\begin{align}\n  \\log\\textup{Rob}_{at} = \\beta \\textup{AddOn}^i_{st} + \\lambda_s + \\gamma_t + x_{st} + \\epsilon_{at},\\label{eq:abramsbase}\n\\end{align}\\] wobei \\(\\log\\textup{Rob}_{at}\\) die logarithmierte Pro-Kopf-Rate für Überfälle mit Schusswaffe (lnpcrrobgun). \\(\\textup{AddOn}^i_{st}\\) ist eine Dummy-Variable die 1 ist für Beobachtungen des Bundesstaats \\(s\\) in Jahr \\(t\\), wenn die Einführung des Add-On-Gesetzes \\(i\\) Jahre zurückliegt. Wir folgen David S. Abrams (2012a) und betrachten Modelle mit \\(i\\in\\{1,2,3\\}\\), d.h. wir unterscheiden zwischen Effekten bis zu drei Jahren (durchschnittliche Mindeststrafe nach alter Gesetzgebung) nach Einführung eines Add-On-Gesetzes.\n\\(\\lambda_s\\) und \\(\\gamma_t\\) sind Fixed Effects für Bundesstaaten und Perioden. \\(x_{st}\\) repräsentiert alle Kontrollvariablen auf Bundesstaatenebene. Hierbei werden stets sämtliche Variablen mit einer Bezeichnung beginnend mit dc berücksichtigt, vgl. Tabelle 10.1. Weiterhin wird mit ymm für den Effekt eines aktiven Gesetzes für eine Mindeststrafe kontrolliert.\nEine Erweiterung von \\(\\eqref{eq:abramsbase}\\) kontrolliert für Bundesstaat-spezifische Zeit-Trends4 \\(\\omega_s\\cdot t\\), \\[\\begin{align}\n  \\log\\textup{Rob}_{at} = \\beta \\textup{AddOn}^i_{st} + \\lambda_s + \\gamma_t + \\omega_s\\cdot t + x_{st} + \\epsilon_{at}.\\label{eq:abramstrend}\n\\end{align}\\]\n4 Für die Implementierung in R wählen wir \\(\\omega_s = \\beta_s \\cdot D_s\\), wobei \\(\\beta_s\\) der Trend-Koeffizient für Bundesstaat \\(s\\) und \\(D_s\\) ein Indikator für den Staat \\(s\\) ist.Anhand von \\(\\eqref{eq:abramstrend}\\) kontrollieren wir zusätzlich für über die Zeit variierende Faktoren in \\(\\log\\textup{Rob}_{at}\\) welche die Wahrscheinlichkeit einer Verabschiedung von Add-On in den jeweiligen Bundesstaaten beeinflussen. Diese Spezifikation ist also etwas konservativer als Modell \\(\\eqref{eq:abramsbase}\\), da hier lediglich der Zeitpunkt der Einführung der Gesetze exogen sein muss, siehe die Diskussion in Abschnitt II in David S. Abrams (2012a).\nFür die komfortable Schätzung von Modellen mit Fixed Effects und die Berechnung von cluster-robusten Standardfehlern (Clustering auf Bundesstaaten-Ebene) verwenden wir fixest::feols(), siehe Kapitel 7 für Details zur Schätzung von Panel-Daten-Modellen.\nAufgrund eine Diskontinuität in Daten für die Beobachtungen mehrerer Variablen im Jahr 1974 Schätzen wir \\(\\eqref{eq:abramsbase}\\) und \\(\\eqref{eq:abramstrend}\\) jeweils zusätzlich für Daten nach 1974. Insgesamt schätzen wir also neun Modelle:\n\nSpezifikation \\(\\eqref{eq:abramsbase}\\), jeweils einmal für \\(i\\in\\{1,2,3\\}\\)\n\nSpezifikation \\(\\eqref{eq:abramstrend}\\), jeweils einmal für \\(i\\in\\{1,2,3\\}\\)\n\nSpezifikation \\(\\eqref{eq:abramstrend}\\), jeweils einmal für \\(i\\in\\{1,2,3\\}\\) und für Beobachtungen mit \\(t&gt;1974\\).\n\n\nlibrary(fixest)\n\n# Basis-Spezifikation (1): 1 Jahr\nmod_basis_1J &lt;- feols(\n  fml = lnpcrrobgun ~\n   # Effekt 1 Jahr nach Add-on law              \n   yaddon\n   # Ges. Mindeststrafe?\n   + ymm\n   # Kontrollvariablen\n   + dcpoverty\n   + dcunemp\n   + dcblack_per\n   + dcp15t17\n   + dcp18t24 \n   + dcp25t34\n   + dcpolice_per1\n   + dcprison_per1\n   # Fixed Effects\n   | FSTATE + year,\n   data = abrams %&gt;% \n     filter(relyr &gt;= -7 & relyr &lt;= 6), \n   weights = ~ statepop,\n   vcov = ~ FSTATE\n)\n\nsummary(mod_basis_1J)\n\nOLS estimation, Dep. Var.: lnpcrrobgun\nObservations: 2,975\nWeights: statepop\nFixed-effects: FSTATE: 24,  year: 30\nStandard-errors: Clustered (FSTATE) \n               Estimate Std. Error   t value  Pr(&gt;|t|)    \nyaddon        -0.047700   0.030857 -1.545837 0.1357937    \nymm           -0.119297   0.098727 -1.208357 0.2391873    \ndcpoverty     -0.038550   0.032352 -1.191557 0.2455858    \ndcunemp       -0.031828   0.018550 -1.715780 0.0996445 .  \ndcblack_per   -0.286705   0.080383 -3.566730 0.0016382 ** \ndcp15t17       0.117641   0.180309  0.652440 0.5205854    \ndcp18t24      -0.132965   0.088240 -1.506851 0.1454617    \ndcp25t34       0.104971   0.091479  1.147480 0.2629797    \ndcpolice_per1 -0.003242   0.001738 -1.865732 0.0748846 .  \ndcprison_per1 -0.000759   0.001088 -0.697450 0.4925122    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 480.2     Adj. R2: 0.164734\n              Within R2: 0.014269\n\n\n\n# Basis-Spezifikation (1): 2 Jahre\nmod_basis_2J &lt;- feols(\n  fml = lnpcrrobgun ~\n   # Effekt 2 Jahre nach Add-on law              \n   twoyears_add\n   # Ges. Mindeststrafe?\n   + ymm\n   # Kontrollvariablen\n   + dcpoverty\n   + dcunemp\n   + dcblack_per\n   + dcp15t17\n   + dcp18t24 \n   + dcp25t34\n   + dcpolice_per1\n   + dcprison_per1\n   # Fixed Effects\n   | FSTATE + year,\n   data = abrams %&gt;% \n     filter(relyr &gt;= -7 & relyr &lt;= 6), \n   weights = ~ statepop,\n   vcov = ~ FSTATE\n)\n\nsummary(mod_basis_2J)\n\nOLS estimation, Dep. Var.: lnpcrrobgun\nObservations: 2,975\nWeights: statepop\nFixed-effects: FSTATE: 24,  year: 30\nStandard-errors: Clustered (FSTATE) \n               Estimate Std. Error   t value  Pr(&gt;|t|)    \ntwoyears_add  -0.109812   0.033449 -3.282986 0.0032610 ** \nymm           -0.098116   0.095971 -1.022343 0.3172519    \ndcpoverty     -0.039720   0.031852 -1.247004 0.2249458    \ndcunemp       -0.027034   0.018671 -1.447940 0.1611292    \ndcblack_per   -0.287151   0.077470 -3.706630 0.0011622 ** \ndcp15t17       0.104298   0.177361  0.588054 0.5622266    \ndcp18t24      -0.131530   0.085887 -1.531434 0.1393021    \ndcp25t34       0.107675   0.092675  1.161865 0.2572057    \ndcpolice_per1 -0.003150   0.001688 -1.865982 0.0748482 .  \ndcprison_per1 -0.000812   0.001090 -0.745006 0.4638138    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 480.0     Adj. R2: 0.165212\n              Within R2: 0.014833\n\n\n\n# Basis-Spezifikation (1): 3 Jahre\nmod_basis_3J &lt;- feols(\n  fml = lnpcrrobgun ~\n   # Effekt 2 Jahre nach Add-on law              \n   threeyears_add\n   # Ges. Mindeststrafe?\n   + ymm\n   # Kontrollvariablen\n   + dcpoverty\n   + dcunemp\n   + dcblack_per\n   + dcp15t17\n   + dcp18t24 \n   + dcp25t34\n   + dcpolice_per1\n   + dcprison_per1\n   # Fixed Effects\n   | FSTATE + year,\n   data = abrams %&gt;% \n     filter(relyr &gt;= -7 & relyr &lt;= 6), \n   weights = ~ statepop,\n   vcov = ~ FSTATE\n)\n\nsummary(mod_basis_3J)\n\nOLS estimation, Dep. Var.: lnpcrrobgun\nObservations: 2,975\nWeights: statepop\nFixed-effects: FSTATE: 24,  year: 30\nStandard-errors: Clustered (FSTATE) \n                Estimate Std. Error   t value  Pr(&gt;|t|)    \nthreeyears_add -0.127331   0.040448 -3.148030 0.0045036 ** \nymm            -0.095616   0.095467 -1.001551 0.3269818    \ndcpoverty      -0.045294   0.031315 -1.446383 0.1615611    \ndcunemp        -0.016640   0.018875 -0.881588 0.3871200    \ndcblack_per    -0.285791   0.076026 -3.759110 0.0010212 ** \ndcp15t17        0.090905   0.172412  0.527256 0.6030654    \ndcp18t24       -0.125009   0.083438 -1.498217 0.1476771    \ndcp25t34        0.111274   0.094319  1.179770 0.2501510    \ndcpolice_per1  -0.003071   0.001621 -1.894852 0.0707516 .  \ndcprison_per1  -0.000955   0.001085 -0.879803 0.3880665    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 480.0     Adj. R2: 0.165456\n              Within R2: 0.015121\n\n\nVor der Schätzung der Spezifikationen mit Trends transformieren wir FSTATE zu einer Variable des kategorischen Typs factor, damit Bundesstaat-spezifische Trends mit FSTATE:year über das Argument fml in feols() definiert werden können.\n\n# 'FSTATE' zu factor transformieren\nabrams &lt;- abrams %&gt;%\n  mutate(\n    FSTATE = factor(FSTATE)\n  )\n\n# Spezifikation (2): 1 Jahr\nmod_trends_1J &lt;- feols(\n    fml = lnpcrrobgun ~\n        # Effekt 1 Jahr nach Add-on law              \n        yaddon\n        # Ges. Mindeststrafe?\n      + ymm\n        # Kontrollvariablen\n      + dcpoverty\n      + dcunemp\n      + dcblack_per\n      + dcp15t17\n      + dcp18t24 \n      + dcp25t34\n      + dcpolice_per1\n      + dcprison_per1\n      # Trends: Bundesstaat\n      + FSTATE:year\n      # Fixed Effects\n      | FSTATE + year,\n      data = abrams %&gt;% \n        filter(relyr &gt;= -7 & relyr &lt;= 6), \n      weights = ~ statepop,\n      vcov = ~ FSTATE\n)\n\nsummary(mod_trends_1J, n = 10)\n\nOLS estimation, Dep. Var.: lnpcrrobgun\nObservations: 2,975\nWeights: statepop\nFixed-effects: FSTATE: 24,  year: 30\nStandard-errors: Clustered (FSTATE) \n               Estimate Std. Error   t value   Pr(&gt;|t|)    \nyaddon        -0.022295   0.019188 -1.161916 2.5719e-01    \nymm           -0.127650   0.091537 -1.394509 1.7649e-01    \ndcpoverty      0.021620   0.019026  1.136332 2.6752e-01    \ndcunemp       -0.028313   0.012685 -2.232041 3.5641e-02 *  \ndcblack_per   -0.093720   0.109882 -0.852918 4.0250e-01    \ndcp15t17       0.046466   0.112684  0.412356 6.8390e-01    \ndcp18t24      -0.070860   0.075561 -0.937793 3.5810e-01    \ndcp25t34       0.382635   0.080705  4.741160 8.8739e-05 ***\ndcpolice_per1 -0.001901   0.001714 -1.109019 2.7888e-01    \ndcprison_per1 -0.002372   0.001363 -1.740044 9.5217e-02 .  \n... 24 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 477.7     Adj. R2: 0.166344\n              Within R2: 0.024278\n\n\n\n# Spezifikation (2): 2 Jahre\nmod_trends_2J &lt;- feols(\n    fml = lnpcrrobgun ~\n        # Effekt 1 Jahr nach Add-on law              \n        twoyears_add\n        # Ges. Mindeststrafe?\n      + ymm\n        # Kontrollvariablen\n      + dcpoverty\n      + dcunemp\n      + dcblack_per\n      + dcp15t17\n      + dcp18t24 \n      + dcp25t34\n      + dcpolice_per1\n      + dcprison_per1\n      # Trends: Bundesstaat\n      + FSTATE:year\n      # Fixed Effects\n      | FSTATE + year,\n      data = abrams %&gt;% \n        filter(relyr &gt;= -7 & relyr &lt;= 6), \n      weights = ~ statepop,\n      vcov = ~ FSTATE\n)\n\nsummary(mod_trends_2J, n = 10)\n\nOLS estimation, Dep. Var.: lnpcrrobgun\nObservations: 2,975\nWeights: statepop\nFixed-effects: FSTATE: 24,  year: 30\nStandard-errors: Clustered (FSTATE) \n               Estimate Std. Error   t value   Pr(&gt;|t|)    \ntwoyears_add  -0.078236   0.019048 -4.107316 4.3082e-04 ***\nymm           -0.105409   0.090612 -1.163301 2.5663e-01    \ndcpoverty      0.019495   0.018733  1.040651 3.0885e-01    \ndcunemp       -0.025263   0.013386 -1.887274 7.1808e-02 .  \ndcblack_per   -0.110638   0.104057 -1.063240 2.9871e-01    \ndcp15t17       0.053882   0.113732  0.473759 6.4014e-01    \ndcp18t24      -0.064687   0.074996 -0.862542 3.9729e-01    \ndcp25t34       0.388108   0.081850  4.741677 8.8625e-05 ***\ndcpolice_per1 -0.001887   0.001611 -1.171315 2.5346e-01    \ndcprison_per1 -0.002287   0.001365 -1.675131 1.0745e-01    \n... 24 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 477.7     Adj. R2: 0.166579\n              Within R2: 0.024552\n\n\n\n# Spezifikation (2): 3 Jahre\nmod_trends_3J &lt;- feols(\n    fml = lnpcrrobgun ~\n        # Effekt 1 Jahr nach Add-on law              \n        threeyears_add\n        # Ges. Mindeststrafe?\n      + ymm\n        # Kontrollvariablen\n      + dcpoverty\n      + dcunemp\n      + dcblack_per\n      + dcp15t17\n      + dcp18t24 \n      + dcp25t34\n      + dcpolice_per1\n      + dcprison_per1\n      # Trends: Bundesstaat\n      + FSTATE:year\n      # Fixed Effects\n      | FSTATE + year,\n      data = abrams %&gt;% \n        filter(relyr &gt;= -7 & relyr &lt;= 6), \n      weights = ~ statepop,\n      vcov = ~ FSTATE\n)\n\nsummary(mod_trends_3J, n = 10)\n\nOLS estimation, Dep. Var.: lnpcrrobgun\nObservations: 2,975\nWeights: statepop\nFixed-effects: FSTATE: 24,  year: 30\nStandard-errors: Clustered (FSTATE) \n                Estimate Std. Error   t value   Pr(&gt;|t|)    \nthreeyears_add -0.090290   0.024423 -3.696920 1.1903e-03 ** \nymm            -0.102912   0.090398 -1.138435 2.6666e-01    \ndcpoverty       0.009749   0.016776  0.581161 5.6678e-01    \ndcunemp        -0.016103   0.013425 -1.199424 2.4257e-01    \ndcblack_per    -0.125781   0.093385 -1.346908 1.9113e-01    \ndcp15t17        0.058783   0.107066  0.549033 5.8827e-01    \ndcp18t24       -0.057551   0.074109 -0.776579 4.4532e-01    \ndcp25t34        0.395196   0.082790  4.773470 8.1882e-05 ***\ndcpolice_per1  -0.001767   0.001525 -1.158920 2.5838e-01    \ndcprison_per1  -0.002324   0.001380 -1.683925 1.0572e-01    \n... 24 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 477.6     Adj. R2: 0.166659\n              Within R2: 0.024646\n\n\n\n# Spezifikation (2): 1 Jahr\n# Post 1974\nmod_trends_1J_post &lt;- feols(\n  fml = lnpcrrobgun ~\n        # Effekt 1 Jahr nach Add-on law\n        yaddon\n        # Ges. Mindeststrafe?\n      + ymm\n        # Kontrollvariablen\n      + dcpoverty\n      + dcunemp\n      + dcblack_per\n      + dcp15t17\n      + dcp18t24 \n      + dcp25t34\n      + dcpolice_per1\n      + dcprison_per1\n      # Trends: Bundesstaat\n      + FSTATE:year\n      # Fixed Effects\n      | FSTATE + year,\n      data = abrams %&gt;% \n        filter(\n          relyr &gt;= -7 & relyr &lt;= 6, \n          post74 == 1\n        ), \n      weights = ~ statepop, \n      vcov = ~ FSTATE\n)\n\nsummary(mod_trends_1J_post, n = 10)\n\nOLS estimation, Dep. Var.: lnpcrrobgun\nObservations: 2,234\nWeights: statepop\nFixed-effects: FSTATE: 24,  year: 25\nStandard-errors: Clustered (FSTATE) \n               Estimate Std. Error   t value   Pr(&gt;|t|)    \nyaddon        -0.012520   0.021549 -0.581025 5.6687e-01    \nymm           -0.099381   0.102139 -0.972996 3.4068e-01    \ndcpoverty      0.026379   0.018485  1.427072 1.6700e-01    \ndcunemp       -0.023182   0.019778 -1.172133 2.5314e-01    \ndcblack_per    0.009251   0.141698  0.065288 9.4851e-01    \ndcp15t17       0.695952   0.300633  2.314960 2.9889e-02 *  \ndcp18t24      -0.038216   0.114322 -0.334287 7.4119e-01    \ndcp25t34       0.425447   0.081624  5.212300 2.7592e-05 ***\ndcpolice_per1 -0.001801   0.001954 -0.921672 3.6627e-01    \ndcprison_per1 -0.002203   0.001437 -1.532561 1.3903e-01    \n... 24 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 498.0     Adj. R2: 0.1559  \n              Within R2: 0.020362\n\n\n\n# Spezifikation (2): 2 Jahre\n# Post 1974\nmod_trends_2J_post &lt;- feols(\n  fml = lnpcrrobgun ~\n        # Effekt 1 Jahr nach Add-on law\n        twoyears_add\n        # Ges. Mindeststrafe?\n      + ymm\n        # Kontrollvariablen\n      + dcpoverty\n      + dcunemp\n      + dcblack_per\n      + dcp15t17\n      + dcp18t24 \n      + dcp25t34\n      + dcpolice_per1\n      + dcprison_per1\n      # Trends: Bundesstaat\n      + FSTATE:year\n      # Fixed Effects\n      | FSTATE + year,\n      data = abrams %&gt;% \n        filter(\n          relyr &gt;= -7 & relyr &lt;= 6, \n          post74 == 1\n        ), \n      weights = ~ statepop, \n      vcov = ~ FSTATE\n)\n\nsummary(mod_trends_2J_post, n = 10)\n\nOLS estimation, Dep. Var.: lnpcrrobgun\nObservations: 2,234\nWeights: statepop\nFixed-effects: FSTATE: 24,  year: 25\nStandard-errors: Clustered (FSTATE) \n               Estimate Std. Error   t value   Pr(&gt;|t|)    \ntwoyears_add  -0.057761   0.021975 -2.628446 1.5020e-02 *  \nymm           -0.092985   0.103750 -0.896241 3.7941e-01    \ndcpoverty      0.025632   0.018109  1.415400 1.7035e-01    \ndcunemp       -0.022002   0.020771 -1.059252 3.0048e-01    \ndcblack_per   -0.004794   0.137603 -0.034839 9.7251e-01    \ndcp15t17       0.724109   0.299988  2.413794 2.4150e-02 *  \ndcp18t24      -0.007445   0.118881 -0.062626 9.5061e-01    \ndcp25t34       0.436252   0.086625  5.036094 4.2653e-05 ***\ndcpolice_per1 -0.001837   0.001635 -1.123379 2.7287e-01    \ndcprison_per1 -0.002027   0.001460 -1.388716 1.7822e-01    \n... 24 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 498.0     Adj. R2: 0.156042\n              Within R2: 0.020527\n\n\n\n# Spezifikation (2): 3 Jahre\n# Post 1974\nmod_trends_3J_post &lt;- feols(\n  fml = lnpcrrobgun ~\n        # Effekt 1 Jahr nach Add-on law\n        threeyears_add\n        # Ges. Mindeststrafe?\n      + ymm\n        # Kontrollvariablen\n      + dcpoverty\n      + dcunemp\n      + dcblack_per\n      + dcp15t17\n      + dcp18t24 \n      + dcp25t34\n      + dcpolice_per1\n      + dcprison_per1\n      # Trends: Bundesstaat\n      + FSTATE:year\n      # Fixed Effects\n      | FSTATE + year,\n      data = abrams %&gt;% \n        filter(\n          relyr &gt;= -7 & relyr &lt;= 6, \n          post74 == 1\n        ), \n      weights = ~ statepop, \n      vcov = ~ FSTATE\n)\n\nsummary(mod_trends_3J_post, n = 10)\n\nOLS estimation, Dep. Var.: lnpcrrobgun\nObservations: 2,234\nWeights: statepop\nFixed-effects: FSTATE: 24,  year: 25\nStandard-errors: Clustered (FSTATE) \n                Estimate Std. Error   t value   Pr(&gt;|t|)    \nthreeyears_add -0.054133   0.024312 -2.226590 3.6052e-02 *  \nymm            -0.096420   0.104109 -0.926147 3.6399e-01    \ndcpoverty       0.019571   0.016847  1.161689 2.5728e-01    \ndcunemp        -0.014320   0.018187 -0.787381 4.3910e-01    \ndcblack_per    -0.017284   0.132031 -0.130910 8.9699e-01    \ndcp15t17        0.695999   0.288290  2.414235 2.4127e-02 *  \ndcp18t24       -0.003553   0.118611 -0.029952 9.7636e-01    \ndcp25t34        0.445096   0.089471  4.974763 4.9655e-05 ***\ndcpolice_per1  -0.001663   0.001610 -1.033066 3.1231e-01    \ndcprison_per1  -0.002079   0.001462 -1.422004 1.6845e-01    \n... 24 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 498.0     Adj. R2: 0.15601\n              Within R2: 0.02049\n\n\nWir fassen die Ergebnisse mit tabellarisch mit modelsummary::modelsummary() zusammen. Hierzu sammeln wir die Modell-Objekte gemäß der Struktur der zugrundeliegenden Spezifikation in benannten Listen und erzeugen eine nach dem Zeithorizont nach der Einführung eines Add-On-Gesetzes gruppierte Tabelle. Für eine bessere Übersicht berücksichtigen wir hierbei jeweils nur die Koeffizienten für den Zeitpunkt der Gesetzesverschärfung und die Dummyvariable für gesetzliche Mindeststrafen (ymm). Über das Argument coef_map definieren wir entsprechende Labels für diese geschätzten Koeffizienten.\n\nlibrary(modelsummary)\n\n# Modelle in benannten Listen sammeln\npanels &lt;- list(\n  \"1 Jahr nach Add-On\" = list(\n    \"(1) Basis\" = mod_basis_1J,\n    \"(2) Trends\" = mod_trends_1J,\n    \"(2) Nach 74\" = mod_trends_1J_post\n  ),\n  \"2 Jahre nach Add-On\" = list(\n    \"(1) Basis\" = mod_basis_2J,\n    \"(2) Trends\" = mod_trends_2J,\n    \"(2) Nach 74\" = mod_trends_2J_post\n  ),\n  \"3 Jahre nach Add-On\" = list(\n    \"(1) Basis\" = mod_basis_3J,\n    \"(2) Trends\" = mod_trends_3J,\n    \"(2) Nach 74\" = mod_trends_3J_post\n  )\n)\n\n# Tabellarische Zusammenfassung\nmodelsummary(\n  models = panels,\n  shape = \"rbind\",\n  coef_omit = \"^(?!.*(add|ymm)).*$\",\n  stars = T,\n  gof_omit = \"^(?!(R2*)$).*\",\n  coef_map = c(\n    \"yaddon\" = \"Add-On\",\n    \"twoyears_add\" = \"Add-On\",\n    \"threeyears_add\" = \"Add-On\",\n    \"ymm\" = \"Mindeststrafe\"\n    ),\n  notes = c(\n    \"Abh. Var.: Log Kriminalitätsrate: Raubüberfälle mit Schusswaffen\", \n    \"Cluster-robuste Standardfehler: Bundesstaat\",\n    \"Fixed Effects: Bundesstaat + Jahr\"\n  ),\n  output = \"gt\"\n) %&gt;%\n  tabopts()\n\n\n\n\n\n\n\n\n(1) Basis\n(2) Trends\n(2) Nach 74\n\n\n\n1 Jahr nach Add-On\n\n\nAdd-On\n-0.048\n-0.022\n-0.013\n\n\n\n(0.031)\n(0.019)\n(0.022)\n\n\nMindeststrafe\n-0.119\n-0.128\n-0.099\n\n\n\n(0.099)\n(0.092)\n(0.102)\n\n\nR2\n0.182\n0.190\n0.187\n\n\n2 Jahre nach Add-On\n\n\nAdd-On\n-0.110**\n-0.078***\n-0.058*\n\n\n\n(0.033)\n(0.019)\n(0.022)\n\n\nMindeststrafe\n-0.098\n-0.105\n-0.093\n\n\n\n(0.096)\n(0.091)\n(0.104)\n\n\nR2\n0.183\n0.191\n0.187\n\n\n3 Jahre nach Add-On\n\n\nAdd-On\n-0.127**\n-0.090**\n-0.054*\n\n\n\n(0.040)\n(0.024)\n(0.024)\n\n\nMindeststrafe\n-0.096\n-0.103\n-0.096\n\n\n\n(0.095)\n(0.090)\n(0.104)\n\n\nR2\n0.183\n0.191\n0.187\n\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\nAbh. Var.: Log Kriminalitätsrate: Raubüberfälle mit Schusswaffen\n\n\nCluster-robuste Standardfehler: Bundesstaat\n\n\nFixed Effects: Bundesstaat + Jahr\n\n\n\n\n\n\n\nTabelle 10.3: abrams – Einfluss von Add-on-Gesetzen auf Kriminalitätsrate: Raubüberfälle mit Waffen (David S. Abrams 2012a)\n\n\n\nDie in Tabelle 10.3 zusammengefassten Ergebnisse deuten darauf, dass es im ersten Jahr nach der Einführung eines Add-On-Gesetzes keinen Signifikanten Effekt auf die Kriminalitätsrate für Raubüberfälle mit Schusswaffen gibt, wenngleich der geschätzte Koeffizient in sämtlichen Modellen negativ ist und damit konsistent mit dem vermuteten Abschreckungseffekt ist.In den Folgeperioden finden wir signifikante negative Effekte verschärfter Gesetze auf die Kriminalitätsrate von bis zu 12.7% (Effekt in Modell \\(\\eqref{eq:abramsbase}\\) nach drei Jahren). Die konservativste Spezifikation (Modell \\(\\eqref{eq:abramstrend}\\) mit Beobachtungen nach 1974) liefert Evidenz eines Rückgangs der Kriminalität durch Abschreckungseffekte in Höhe von etwa 5% innerhalb der ersten drei Jahre nach der Gesetzesänderung. Weiterhin ist der geschätzte Effekt von gesetzlichen Vorschriften zu Mindeststrafen zwar durchweg negativ, jedoch insignifikant.\n\n10.3.2.2 Event-Study-Regression\nFür eine detalliertere Einschätzung des zeitlichen Verlaufs des Effekts verschärfender Gesetze betrachtet David S. Abrams (2012a) eine Spezifikation ähnlich zu Jacobson, LaLonde, und Sullivan (1993), \\[\\begin{align}\n  \\log\\textup{Rob}_{at} = \\sum_{i\\in[-6,6]} \\beta_i\\cdot\\textup{AddOn}^i_{st} + \\lambda_s + \\gamma_t + \\omega_s\\cdot t + x_{st} + \\epsilon_{at},\\label{eq:abramsesreg}\n\\end{align}\\] mit perioden-spezifischen Effekten \\(\\beta_i\\) in einem Zeitfenster von sechs Jahren vor und nach Gesetzesänderungen, wobei die \\(\\textup{AddOn}^i_{st}\\) definiert sind wie in \\(\\eqref{eq:abramsbase}\\) Für die Schätzung von Modell \\(\\eqref{eq:abramsesreg}\\) mit R zeigen wir zunächst, wie diese Dummies anhand der Variable relyr effizient erzeugt werden können.\n\n# Ausprägungen von 'relyr'\nabrams %&gt;% \n  pull(relyr) %&gt;% \n  table()\n\n.\n-31 -30 -29 -28 -27 -26 -25 -24 -23 -22 -21 -20 -19 -18 -17 -16 -15 -14 -13 -12 \n 23  23  36  43  43  43  43  43  43  43  43  46  61  81  81  84  84  87  90 119 \n-11 -10  -9  -8  -7  -6  -5  -4  -3  -2  -1   0   1   2   3   4   5   6   7   8 \n160 215 226 233 236 236 236 236 236 236 236 236 236 236 236 236 236 236 213 213 \n  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28 \n200 193 193 193 193 193 193 193 193 190 175 155 155 152 152 149 146 117  76  21 \n 29  30 \n 10   3 \n\n\nHierfür generieren wir mit tidyr::pivot_wider() eine Variante des Datensatzes, abrams_es, in der Beobachtungen von relyr spaltenweise in binärer Kodierung abgetragen sind. Diese Variablen erhalten das Präfix D. Für leichtere Handhabung der Variablennamen innerhalb von feols() wird das negative Vorzeichen wird druch dplyr::rename_with() in einen Underscore umgewandelt.\n\n# Dummy-Variablen für Event-Study-Regression erzeugen\nabrams_es &lt;- abrams %&gt;%\n    pivot_wider(\n        names_from = relyr, \n        values_from = relyr, \n        names_prefix = \"D\", \n        values_fn = list(relyr = ~ 1), \n        values_fill = 0\n    ) %&gt;%\n    rename_with(\n        .fn = ~ gsub(\"-\", \"_\", .), \n        .cols = starts_with(\"D\")\n    )\n\nWir schätzen nun die Regression \\(\\eqref{eq:abramsesreg}\\), sowohl mit als auch ohne Bundesstaat-spezifische Zeit-Trends und fassen die Ergebnisse anschließend mit modelsummary() zusammen.\n\n# Event-Study-Regression ohne Zeit-Trends\nabrams_mod_es &lt;- feols(\n  fml = lnpcrrobgun ~\n      # Rel. Zeitpunkt-Dummies\n      + D_6\n      + D_5\n      + D_4\n      + D_3\n      + D_2\n      + D_1\n      + D0      \n      + D1\n      + D2\n      + D3\n      + D4\n      + D5\n      + D6\n      # Kontrollvariablen\n      + dcpoverty\n      + dcunemp\n      + dcblack_per\n      + dcp15t17\n      + dcp18t24 \n      + dcp25t34\n      + dcpolice_per1\n      + dcprison_per1\n      # Fixed Effects\n      | FSTATE + year,\n       data = abrams_es,\n      weights = ~ statepop, \n      vcov = ~ FSTATE\n)\n\nsummary(abrams_mod_es, n = 13)\n\nOLS estimation, Dep. Var.: lnpcrrobgun\nObservations: 15,516\nWeights: statepop\nFixed-effects: FSTATE: 50,  year: 30\nStandard-errors: Clustered (FSTATE) \n     Estimate Std. Error   t value  Pr(&gt;|t|)    \nD_6  0.253235   0.080335  3.152231 0.0027641 ** \nD_5  0.179108   0.074765  2.395614 0.0204573 *  \nD_4  0.192102   0.068453  2.806330 0.0071697 ** \nD_3  0.194081   0.065261  2.973925 0.0045520 ** \nD_2  0.215373   0.072131  2.985876 0.0044045 ** \nD_1  0.175359   0.086515  2.026920 0.0481297 *  \nD0   0.092910   0.078156  1.188769 0.2402598    \nD1  -0.019041   0.077510 -0.245663 0.8069694    \nD2  -0.134279   0.081576 -1.646061 0.1061512    \nD3  -0.119129   0.095480 -1.247685 0.2180765    \nD4  -0.014835   0.097611 -0.151980 0.8798269    \nD5   0.026076   0.087216  0.298984 0.7662169    \nD6   0.026764   0.065641  0.407732 0.6852466    \n... 8 coefficients remaining\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 508.6     Adj. R2: 0.163961\n              Within R2: 0.008215\n\n\n\n# Event-Study-Regression mit Zeit-Trends\nabrams_mod_es_trends &lt;- feols(\n  fml = lnpcrrobgun ~\n      # Rel. Zeitpunkt-Dummies\n      + D_6\n      + D_5\n      + D_4\n      + D_3\n      + D_2\n      + D_1\n      + D0      \n      + D1\n      + D2\n      + D3\n      + D4\n      + D5\n      + D6\n      # Kontrollvariablen\n      + dcpoverty\n      + dcunemp\n      + dcblack_per\n      + dcp15t17\n      + dcp18t24 \n      + dcp25t34\n      + dcpolice_per1\n      + dcprison_per1\n      # Trends: Bundesstaaten\n      + FSTATE:year\n      # Fixed Effects\n      | FSTATE + year,\n       data = abrams_es,\n      weights = ~ statepop, \n      vcov = ~ FSTATE\n)\n\nsummary(abrams_mod_es_trends, n = 13)\n\nOLS estimation, Dep. Var.: lnpcrrobgun\nObservations: 15,516\nWeights: statepop\nFixed-effects: FSTATE: 50,  year: 30\nStandard-errors: Clustered (FSTATE) \n     Estimate Std. Error   t value   Pr(&gt;|t|)    \nD_6  0.178623   0.061206  2.918420 0.00529961 ** \nD_5  0.117266   0.071136  1.648468 0.10565467    \nD_4  0.140018   0.070565  1.984236 0.05284502 .  \nD_3  0.173958   0.080586  2.158664 0.03580156 *  \nD_2  0.194643   0.054801  3.551842 0.00085685 ***\nD_1  0.168404   0.048365  3.481958 0.00105685 ** \nD0   0.088229   0.058647  1.504399 0.13889772    \nD1  -0.019122   0.060257 -0.317339 0.75233449    \nD2  -0.133242   0.062953 -2.116535 0.03940246 *  \nD3  -0.100628   0.074838 -1.344623 0.18493860    \nD4  -0.048074   0.090217 -0.532869 0.59653275    \nD5  -0.018161   0.066668 -0.272405 0.78645555    \nD6   0.030971   0.045143  0.686059 0.49590930    \n... 57 coefficients remaining\n... 1 variable was removed because of collinearity (FSTATE50:year)\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 505.3     Adj. R2: 0.171869\n              Within R2: 0.020719\n\n\n\n# Modelle in benannten Listen sammeln\nmodelle &lt;- list(\n  \"Ohne Trends\" = abrams_mod_es,\n  \"Zeit-Trends\" = abrams_mod_es_trends\n)\n\n# Tabellarische Zusammenfassung\nmodelsummary(\n  models = modelle,\n  coef_omit = \"^(?!.*(D|D\\\\_)).*$\",\n  stars = T,\n  gof_omit = \"^(?!(R2*)$).*\",\n  notes = c(\n    \"Abh. Var.: Log Kriminalitätsrate: Raubüberfälle mit Schusswaffen\", \n    \"Cluster-robuste Standardfehler: Bundesstaat\",\n    \"Fixed Effects: Bundesstaat + Jahr\"\n  ),\n  output = \"gt\"\n) %&gt;%\n  tabopts()\n\n\n\n\n\n\n\n\nOhne Trends\nZeit-Trends\n\n\n\nD_6\n0.253**\n0.179**\n\n\n\n(0.080)\n(0.061)\n\n\nD_5\n0.179*\n0.117\n\n\n\n(0.075)\n(0.071)\n\n\nD_4\n0.192**\n0.140+\n\n\n\n(0.068)\n(0.071)\n\n\nD_3\n0.194**\n0.174*\n\n\n\n(0.065)\n(0.081)\n\n\nD_2\n0.215**\n0.195***\n\n\n\n(0.072)\n(0.055)\n\n\nD_1\n0.175*\n0.168**\n\n\n\n(0.087)\n(0.048)\n\n\nD0\n0.093\n0.088\n\n\n\n(0.078)\n(0.059)\n\n\nD1\n-0.019\n-0.019\n\n\n\n(0.078)\n(0.060)\n\n\nD2\n-0.134\n-0.133*\n\n\n\n(0.082)\n(0.063)\n\n\nD3\n-0.119\n-0.101\n\n\n\n(0.095)\n(0.075)\n\n\nD4\n-0.015\n-0.048\n\n\n\n(0.098)\n(0.090)\n\n\nD5\n0.026\n-0.018\n\n\n\n(0.087)\n(0.067)\n\n\nD6\n0.027\n0.031\n\n\n\n(0.066)\n(0.045)\n\n\nR2\n0.169\n0.180\n\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\nAbh. Var.: Log Kriminalitätsrate: Raubüberfälle mit Schusswaffen\n\n\nCluster-robuste Standardfehler: Bundesstaat\n\n\nFixed Effects: Bundesstaat + Jahr\n\n\n\n\n\n\n\nTabelle 10.4: KQ-Schätzung der Event-Study-Spezifikation: Koeffizienten der Dummies (David S. Abrams 2012a)\n\n\n\nDie statistische Zusammenfassung beider Event-Study-Spezifikationen in Tabelle 10.4 untermauern die Ergebnisse aus Tabelle 10.3:\n\nDie positiven geschätzten Koeffizienten der Dummies für Zeitpunkte vor der Einführung von Add-On-Gesetzen (\\(i=-6,\\dots,-1\\)) sind signifikant und zeigen einen stabilen Verlauf der Kriminalitätsrate an.\nDie negativen, nicht-signifikant von 0 verschiedenen geschätzten Koeffizienten nach der Einführung von Add-On-Gesetzen (\\(i=0,\\dots,6\\)) sind Evidenz für einen Rückgang der Pro-Kopf-Kriminalitätsrate für Raubüberfälle mit Schusswaffen im Folgezeitraum.\n\nFür eine grafische Veranschaulichung der zeitlichen Entwicklung des geschätzten Effekts plotten wir die Schätzungen aus Tabelle 10.4 zusammen mit einem 1-Standardfehler-Intervall.5 Hierzu lesen wir zunächst die geschätzten Koeffizienten und Standardfehler aus dem Objekten abrams_mod_es und abrams_mod_es_trends aus, berechnen die zugehörigen Konfidenzintervalle und sammeln die Ergebnisse unter Verwendung einer Indikatorvariable für das zugrundeliegende Modell in einem tibble-Objekt\n5 Vgl. Abbildung 4 in David S. Abrams (2012a).\n# Koeffizienten: Modell mit Trend\ncoefficients_1 &lt;- coef(abrams_mod_es_trends)\nstd_errors_1 &lt;- se(abrams_mod_es_trends)\n\n# Koeffizienten: Modell ohne Trend\ncoefficients_2 &lt;- coef(abrams_mod_es)\nstd_errors_2 &lt;- se(abrams_mod_es)\n\n# Subset von i = -6,...,0,...,6 auslesen\nevents &lt;- -6:6\nevent_names &lt;- names(coefficients_1)[1:13]\n\n# Schätzungen je Modell sammeln\nplot_data_1 &lt;- tibble(\n  Modell = \"Mit Trends\",\n  Event = events,\n  Estimate = coefficients_1[event_names],\n  SE = std_errors_1[event_names]\n)\n\nplot_data_2 &lt;- tibble(\n  Modell = \"Ohne Trends\",\n  Event = events,\n  Estimate = coefficients_2[event_names],\n  SE = std_errors_2[event_names]\n)\n\n# Datenpunkte sammeln\nplot_data &lt;- bind_rows(plot_data_1, plot_data_2) %&gt;%\n  # 1-Standardfehler-Intervall\n  mutate(\n    CI_Lower = Estimate - SE,\n    CI_Upper = Estimate + SE\n  )\n\n\n# Event-Study-Plot\nggplot(\n  data = plot_data, \n  mapping = aes(\n    x = Event, \n    y = Estimate, \n    color = Modell, \n    fill = Modell\n  )\n) +\n  # gesch. Effekte\n  geom_point() +\n  geom_line() +\n  # 1-SE-Intervall\n  geom_ribbon(\n    mapping = aes(\n      ymin = CI_Lower, \n      ymax = CI_Upper\n    ),\n    color =\"white\",\n    alpha = 0.2\n  ) +\n  # Hilfslinien\n  geom_hline(\n    yintercept = c(0), \n    linewidth = .2\n  ) +\n  geom_vline(\n    xintercept = 0, \n    linetype = \"dashed\"\n  ) +\n  # Labels + Legende\n  labs(\n    x = \"Zeitpunkt rel. zur Einf. Add-on-Gesetz\",\n    y = \"Log-Rate: Überfälle mit Schusswaffen\") +\n  scale_y_continuous(limits = c(-.4, .4)) +\n  theme_cowplot() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nAbbildung 10.2: Event-Study-Regression – Geschätze Rate: Raubüberfälle rel. zur Einführung von Add-On-Gesetzen\n\n\n\n\nBeachte, dass lediglich die Änderungen der Koeffizienten in Abbildung 10.2 eine sinnvolle Interpretation haben: Die absoluten Werte auf der Y-Achse sind nicht aussagekräftig, da sie aus Regressionen mit mehreren Regressoren mit von Null verschiedenen Mittelwerten stammen.\nEine Auffälligkeit in Abbildung 10.2 ist, dass die mittlere Kriminalitätsrate bereits eine Periode vor dem Zeitpunkt der Gesetzeseinführung eine Trendumkehr aufzuweisen scheint. David S. Abrams (2012a) weißt darauf hin, dass potenzielle Straftäter durch die laufende öffentliche Debatte und Diskussion über das Gesetz erfahren und ihr Verhaltensweisen in Erwartung der Gesetzesänderung anpassen könnten. Die emprische Studienlage hinsichtlich eines solches Verhalten ist jedoch nicht eindeutig, siehe die Diskussion in David S. Abrams (2012a), Abschnitt IV B.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Event Studies</span>"
    ]
  },
  {
    "objectID": "EventStudies.html#zusammenfassung",
    "href": "EventStudies.html#zusammenfassung",
    "title": "\n10  Event Studies\n",
    "section": "\n10.4 Zusammenfassung",
    "text": "10.4 Zusammenfassung\nMit Event Studies können die Auswirkungen spezifischer Ereignisse oder Interventionen auf eine über die Zeit beobachtete Outcome-Variable untersucht werden. Die zugrunde liegende Annahme für die Identifizierbarkeit des kausalen Effekts ist, dass das untersuchte Event als exogener Schock fungiert, der die Outcome-Variable beeinflusst, jedoch unabhängig von anderen unbeobachtbaren Einflussfaktoren ist, sodass die der Front-Door-Pfad isoliert werden kann.\nDer Effekt wird meist in einem zeitlichen Umfeld nahe des Ereignisses analysiert. Zur Bestimmung eines kontrafaktischen Vergleichswerts für die Outcome-Variable nach dem Event kann die Prognose des Outcomes hilfreich sein. Für die präzise Schätzung des Behandlungseffekts werden häufig Regressionsmodelle herangezogen, die es ermöglichen, zeitliche Veränderungen in der Outcome-Variable zu modellieren und zusätzliche Kontrollvariablen zu berücksichtigen. Dadurch wird die Robustheit und Genauigkeit der Ergebnisse erhöht, was insbesondere bei der Analyse langfristiger Interventionseffekte in komplexen ökonomischen und politischen Kontexten von relevant ist.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbrams, David S. 2012a. „Estimating the Deterrent Effect of Incarceration Using Sentencing Enhancements“. American Economic Journal: Applied Economics 4 (4): 32–56. https://doi.org/10.1257/app.4.4.32.\n\n\nAbrams, David S. 2012b. „Replication data for: Estimating the Deterrent Effect of Incarceration Using Sentencing Enhancements“. ICPSR - Interuniversity Consortium for Political; Social Research. https://doi.org/10.3886/E113838V1.\n\n\nJacobson, Louis S., Robert J. LaLonde, und Daniel G. Sullivan. 1993. „Earnings Losses of Displaced Workers“. The American Economic Review 83 (4): 685–709. http://www.jstor.org/stable/2117574.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Event Studies</span>"
    ]
  },
  {
    "objectID": "RDD.html",
    "href": "RDD.html",
    "title": "\n11  Regression Discontiniuty Designs\n",
    "section": "",
    "text": "11.1 Sharp Regression Discontinuity Design\nModell und funktionale Form\nDie korrekte Spezifikation der funktionalen Form für ein RDD ist wichtig, um eine verzerrte Schätzung des Effekts zu vermeiden. Die einfachste Form eines SRDD kann anhand der linearen Regression \\[\\begin{align}\nY_i = \\beta_0 + \\beta_1 B_i + \\beta_2 X_i + u_i\\label{eq-simpleSRDD}\n\\end{align}\\] geschätzt werden, wobei \\(B_i\\) eine Dummy-Variable für das Überschreiten des Schwellenwertes \\(c\\) ist, d.h. \\[\\begin{align*}\n  B_i=\\begin{cases}\n    0 & X_i &lt; c\\\\\n    1 & X_i \\geq c.\n  \\end{cases}\n\\end{align*}\\] Damit ist \\(B_i\\) eine deterministische Funktion der Laufvariable \\(X_i\\) und zeigt die Zugehörigkeit zur Behandlungs- oder Treatmentgruppe an. Der Koeffizient \\(\\beta_1\\) misst den Behandlungseffekt.\nDas Modell \\(\\eqref{eq-simpleSRDD}\\) unterstellt, dass \\(X\\) links- und rechtsseitig von \\(c\\) denselben Effekt auf \\(Y\\) hat. Diese Annahme ist restriktiv. Eine Alternative ist ein lineares Interaktionsmodell \\[\\begin{align}\nY_i = \\beta_0 + \\beta_1 B_i + \\beta_2 (X_i - c) + \\beta_3(X_i - c)\\times B_i + u_i.\\label{eq:linearSRDD}\n\\end{align}\\] Das Modell \\(\\eqref{eq:linearSRDD}\\) kann unterschiedliche lineare Effekte von \\(X\\) auf \\(Y\\) unterhalb (\\(\\beta_2\\)) und oberhalb (\\(\\beta_2 + \\beta_3\\)) von \\(c\\) abbilden. Beachte, dass \\((X_i - c)\\) die um den Schwellenwert zentrierte Laufvariable ist, sodass \\(\\beta_1\\) wie in \\(\\eqref{eq-simpleSRDD}\\) den Unterschied des Effekts von \\(X\\) auf \\(Y\\) für Beoabachtungen am Schwellenwert erfasst.\nUm unterschiedliche nicht-lineare Zusammenhänge von \\(X\\) und \\(Y\\) unterhalb und oberhalb von \\(c\\) abzubilden, können (interargierte) Polynom-Terme in \\(X\\) verwendet werden. Häufig wird eine quadratische Regressionsfunktion genutzt, \\[\\begin{align}\n  Y_i =&\\, \\beta_0 + \\beta_1 B_i + \\beta_2 (X_i - c) + \\beta_3 (X_i - c)^2\\\\\n       &+\\, \\beta_4(X_i - c)\\times B_i + \\beta_5(X_i - c)\\times B_i + u_i.\\label{eq:quadSRDD}\n\\end{align}\\] Gelman und Imbens (2019) zeigen, dass Polynome höherer Ordnung zu verzerrten Schätzern und hoher Varianz führen können.2 Die Authoren empfehlen stattdessen die Schätzung mit lokaler Regression.\nNicht-parametrische Schätzung und Bandweite\nAktuelle Studien nutzen nicht-parametrische Schätzer, die den Behandlungseffekt als Differenz der geschätzten Regressionsfunktionen am Schwellenwert \\(c\\) berechnen. Um auch nicht-lineare Regressionsfunktionen abzubilden zu können, wird häufig lokale Regression verwendet. Dieses Verfahren liefert eine “lokale” Schätzung der Regressionsfunktionen am Schwellenwert, bei der nur Beobachtungen nahe \\(X = c\\) für die Schätzung berücksichtigt werden. Hinreichende Nähe wird hierbei durch eine sogenannte Bandweite \\(h\\) festgelegt, wobei \\[\\begin{align}\n  \\lvert(X_i-c)\\rvert\\leq h \\label{eq:bwc}\n\\end{align}\\] das Kriterium für eine Berücksichtigung von Beobachtung \\(i\\) bei der Schätzung ist.\nUnter Verwendung einer Bandweite \\(h\\) wird der Regressionsansatz \\(\\eqref{eq:linearSRDD}\\) als lokale lineare Regression mit Uniform-Kernelfunktion bezeichnet. Der Uniform-Kernel gibt allen Beobachtungen, innerhalb der Bandweite \\(h\\) dasselbe Gewicht. Ist \\(h\\) so groß, dass der gesamte Datensatz in die Schätzung einbezogen wird, entspricht der lokale lineare Regressions-Schätzer mit Uniform-Kernel dem (globalen) KQ-Schätzer in einem linearen Interaktionsmodell anhand aller Beobachtungen. Neben dem Uniform-Kernel ist der Triangular-Kernel eine in der Praxis häufig genutzte lineare Kernelfunktion. Der nachstehende Code plottet die Uniform- (grün) sowie die Triangular-Kernelfunktion (blau), siehe Abbildung 11.2.\nCodelibrary(ggplot2)\nlibrary(cowplot)\n\n# Kernelfunktionen zeichnen\nggplot() + \n    geom_function(\n      fun = ~ ifelse(\n        test = abs(.) &lt;= 1,\n        yes =  1/2, \n        no = 0\n      ), \n      col = \"green\", \n      n = 1000\n      ) + \n    geom_function(\n      fun = ~ ifelse(\n        test = abs(.) &lt;= 1, \n        yes = 1 - abs(.), \n        no = 0\n      ), \n      col = \"blue\", \n      n = 100\n      ) + \n    scale_x_continuous(\n      name = \"x\", \n      limits = c(-1.5, 1.5), \n      breaks = c(-1, 0, 1)\n    ) +\n    scale_y_continuous(\n      name = \"K(x)\", \n      breaks = c(0, 1), \n      limits = c(0, 1.25)\n    ) +\n    theme_cowplot()\n\n\n\n\n\n\nAbbildung 11.2: Kernelfunktionen auf [-1, 1]\nIn empirischen Studien wird als Basis-Spezifikation oft eine lokale lineare Regression anhand von \\(\\eqref{eq:linearSRDD}\\) mit einer linearen Kernelfunktionen und geringer bandweite \\(h\\) genutzt. Anschließend wird die Robustheit der Ergebnisse anhand flexiblerer Spezifikationen, die Nicht-Linearitäten in der Regressionsfunktion besser abbilden können, geprüft.\nDie nachstehende Visualisierung zeigt die Schätzung des kausalen Effektes der Behandlung \\(B_i\\) anhand lokaler linearer Regression mit einem Uniform-Kernel für wiefolgt simulierte Daten: \\[\\begin{align*}\n  Y_i =&\\, \\beta_1 X_i + \\beta_2 B + \\beta_3 X_i^2 \\times B_i + u_i,\\\\\n  \\\\\n  u_i \\sim&\\, N(0, 0.5), \\quad X_i \\sim U(0, 10), \\quad B = \\mathbb{I}(X_i \\geq c = 5)\\\\\n  \\beta_1 =&\\, .5, \\quad \\beta_2 = 1.5, \\quad \\beta_3 = -0.15\n\\end{align*}\\]\nDiese Vorschrift ist schnell mit R umgesetzt:\nset.seed(1234)\n# Anz.Beobachtungen\nn &lt;- 750\n\n# Parameter definieren\nc &lt;- 5\nbeta_1 &lt;- .5\nbeta_2 &lt;- 1.5\nbeta_3 &lt;- -.15\n\n# Regressionsfunktion definieren\nf &lt;- function(X) {\n  beta_1 * (X - c) + beta_2 * B + beta_3 * B * (X - c)^2\n}\n\n# Daten erzeugen\nX &lt;- runif(n, 0, 11)\nB &lt;- ifelse(X - c &gt;= 0, 1, 0)\nY &lt;- f(X) + rnorm(n, sd = .5)\n\n# Beoabchtungen sammeln\ndat &lt;- data.frame(\n  Y = Y, X = X - c, B = B\n)\nhtml`\n&lt;style&gt;\n.regression {\n  fill: none;\n  stroke: #000;\n  stroke-width: 1.5px;\n}\n.axis line {\n  stroke: #ddd;\n}\n.axis .baseline line {\n  stroke: #555;\n}\n.axis .domain {\n  display: none;\n} \n&lt;/style&gt;\n&lt;link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css\"&gt;\n`\n\n\n\n\n\n\n\nd3 = require(\"d3-array@3\", \"d3-axis@3\", \"d3-regression@1\", \"d3-scale@4\", \"d3-shape@3\", \"d3-selection@3\", \"d3-format\")\n\nmargin = ({left: 55, right: 8, top: 13, bottom: 24});\nbase = Math.min(width, 500);\ninnerWidth = base - margin.left - margin.right;\ninnerHeight = base-100 - margin.top - margin.bottom;\n\n\nviewof bw_daten_LLRU = Inputs.range([.1, 6], {\n  label: \"Bandweite (h)\",\n  step: .1,\n  value: 1.3\n});\n\nxScaleLLRU = d3.scaleLinear()\n   .domain([-6, 6])\n   .range([0, innerWidth]);\n   \nyScaleLLRU = d3.scaleLinear()\n  .domain([-4, 6])\n  .range([innerHeight, 0]);\n\nlineLLRU = d3.line()\n  .x(d =&gt; xScaleLLRU(d[0]))\n  .y(d =&gt; yScaleLLRU(d[1]));\n  \nxAxisLLRU = d3.axisBottom(xScaleLLRU)\n  .tickSize(innerHeight + 10)\n  .tickValues([-6, -4, -2, 0, 2, 4, 6])\n  .tickFormat(d =&gt; d);\n\nyAxisLLRU = d3.axisLeft(yScaleLLRU)\n  .tickSize(innerWidth + 10)\n  .tickFormat(d =&gt; d);\n\nLLRURegression = d3.regressionLinear()\n  .x(d =&gt; d.X)\n  .y(d =&gt; d.Y);\n{\n  const svg = d3.select(DOM.svg(innerWidth + margin.left + margin.right + 20, innerHeight + margin.top + margin.bottom + 20))\n  \n  const g = svg.append(\"g\")\n      .attr(\"transform\", `translate(${margin.left}, ${margin.top})`);\n\n  g.append(\"g\")\n      .attr(\"class\", \"axis\")\n      .call(xAxisLLRU);\n\n  g.append(\"g\")\n    .attr(\"class\", \"axis\")\n    .attr(\"transform\", `translate(${innerWidth})`)\n    .call(yAxisLLRU);\n\n  // Add X axis label:\n  g.append(\"text\")\n    .attr(\"text-anchor\", \"end\")\n    .attr(\"font-size\", 13)\n    .attr(\"x\", innerWidth)\n    .attr(\"y\", innerHeight + margin.top + 25)\n    .text(\"X - c\");\n\n  // Y axis label:\n  g.append(\"text\")\n    .attr(\"text-anchor\", \"end\")\n    .attr(\"transform\", \"rotate(-90)\")\n    .attr(\"font-size\", 13)\n    .attr(\"y\", -margin.left+10)\n    .attr(\"x\", -margin.top+10)\n    .text(\"Y\");\n\n  g.selectAll(\"circle\")\n    .data(transpose(SRDD))\n    .enter().append(\"circle\")\n    .attr(\"r\", 2)\n    .attr(\"cx\", d =&gt; xScaleLLRU(d.X))\n    .attr(\"cy\", d =&gt; yScaleLLRU(d.Y));\n\ng.selectAll(\"circle\")\n .filter( function(d){ return Math.abs(d.X) &lt;= bw_daten_LLRU } )\n .attr(\"fill\", \"orange\")\n .attr(\"stroke\", \"none\");\n \n  &lt;!-- trf --&gt;\nvar  line = d3.line()\n           .x(function(d) { return xScaleLLRU(d.X); }) \n           .y(function(d) { return yScaleLLRU(d.Y_true); }) \n           .curve(d3.curveLinear); \n\n  g.append(\"path\")\n    .datum(transpose(SRDD))\n    .attr(\"d\", line)\n    .attr(\"fill\", \"none\")\n    .attr(\"stroke\", \"red\")\n    .attr(\"stroke-width\", 2);\n\n\nfunction b(d) { return LLRURegression(\n        transpose(SRDD).filter(function(d){ return d.X &lt;= 0 & d.X &gt;= -bw_daten_LLRU })\n    ); }\n\nfunction a(d) { return LLRURegression(\n      transpose(SRDD).filter(function(d){ return d.X &gt; 0 & d.X &lt;= bw_daten_LLRU })\n    ); }\n\n  g.append(\"path\")\n      .attr(\"class\", \"regression\")\n      .datum(b)\n      .attr(\"d\", lineLLRU)\n      .attr(\"stroke-width\", 2)\n      .style(\"stroke\", \"#39FF14\");\n\n  g.append(\"path\")\n      .attr(\"class\", \"regression\")\n      .datum(a)\n      .attr(\"d\", lineLLRU)\n      .attr(\"stroke-width\", 2)\n      .style(\"stroke\", \"#39FF14\");\n  \n  g.append(\"line\")\n  .attr(\"x1\", xScaleLLRU(0))\n  .attr(\"y1\", yScaleLLRU((b().slice(-1))[0][1]))\n  .attr(\"x2\", xScaleLLRU(0))\n  .attr(\"y2\", yScaleLLRU((a().slice(0))[0][1]))\n  .attr(\"stroke\", \"#39FF14\")\n  .attr(\"stroke-width\", 2);\n  \n   g.append(\"text\")\n    .attr(\"x\", d =&gt; xScaleLLRU(-2.75))\n    .attr(\"y\", d =&gt; yScaleLLRU(4.5))\n    .attr(\"dy\", \".35em\")\n    .attr(\"fill\", \"#39FF14\")\n    .text(\n    d3.format(\",.2f\")( (a().slice(0))[0][1] - (b().slice(-1))[0][1] ) \n    );\n  \n  /* dashed line data bw upper */\n  g.append(\"line\")\n  .attr(\"x1\", xScaleLLRU(bw_daten_LLRU))\n  .attr(\"y1\", 0)\n  .attr(\"x2\", xScaleLLRU(bw_daten_LLRU))\n  .attr(\"y2\", innerHeight)\n  .style(\"stroke\", \"blue\")\n  .style(\"stroke-dasharray\", \"4\")\n  .style(\"stroke-width\", \"1\");\n  \n  /* dashed line data bw lower */\n  g.append(\"line\")\n  .attr(\"x1\", xScaleLLRU(-bw_daten_LLRU))\n  .attr(\"y1\", 0)\n  .attr(\"x2\", xScaleLLRU(-bw_daten_LLRU))\n  .attr(\"y2\", innerHeight)\n  .style(\"stroke\", \"blue\")\n  .style(\"stroke-dasharray\", \"4\")\n  .style(\"stroke-width\", \"1\");\n\n  return svg.node();\n}\n\n\n\n\n\n\n\nAbbildung 11.3: Nicht-parametrische Regression auf beiden Seiten des Cut-offs\nDer interssierende Effekt am Schwellenwert \\(c=5\\) beträgt \\(\\beta_2 = 1.5\\). Beachte, dass aufgrund des Terms \\(\\beta_3 X_i^2 \\times B_i\\) ein quadratischer Zusammenhang von \\(Y\\) und \\(X\\) oberhalb von \\(X_i = c\\) vorliegt. Es können folgende Eigenschaften der Schätzung in Abhängigkeit von der Bandweite \\(h\\) beobachtet werden:\nDie Wahl der Bandweite ist also eine wichtige Komponenten der RDD-Schätzung: Kleine Bandweiten erlauben eine Schätzung der Regressionsfunktion nahe des Schwellenwertes mit wenig Verzerrung. Allerdings kann diese Schätzung unpräzise sein, wenn nur wenige Beobachtungen \\(\\eqref{eq:bwc}\\) erfüllen. In der Praxis wird \\(h\\) daher mit einem analytischen Schätzer (vgl. G. Imbens und Kalyanaraman 2012) oder anhand von Cross Validation (bspw. G. W. Imbens und Lemieux 2008) bestimmt. Die später in diesem Kapitel betrachteten R-Pakete halten diese Methoden bereit.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Regression Discontiniuty Designs</span>"
    ]
  },
  {
    "objectID": "RDD.html#sharp-regression-discontinuity-design",
    "href": "RDD.html#sharp-regression-discontinuity-design",
    "title": "\n11  Regression Discontiniuty Designs\n",
    "section": "",
    "text": "2 Ursachen sind Überanpassung an die Daten sowie instabiles Verhalten der Schätzung nahe des Schwellenwertes.\n\n\n\n\n\n\n\n\n\n\n\nFür die voreingestellte Bandweite \\(h = 1.3\\) liefert die lokale lineare Regression eine gute Approximation des Regressionszusammenhangs auf beiden Seiten des Schwellenwertes und die Schätzung des Behandlungseffekts liegt nahe beim wahren Wert \\(\\beta_2 = 1.5\\).\nFür kleinere Bandweiten verringert sich die Datenbasis der Schätzung. Die Varianz der Schätzung nimmt zu und die Approximation der Regressionsfunktion verschlechtert sich. Wir beobachten eine mit \\(h\\to0\\) zunehmende Verzerrung bei der Schätzung des Behandlungseffekts.\nGrößere Bandweiten \\(h\\) erhöhen die Datenbasis der Schätzung, führen aber zu einer Annäherung der lokalen Schätzung an die globale KQ-Schätzung. Linksseitig des Schwellenwertes erzielen wir damit eine Schätzung mit hoher Güte. Rechsseitig von \\(X_i = c\\) verschlechtert sich die lokale Anpassung am Schwellenwert deutlich, weil die lineare Schätzung den tatsächlichen (nicht-linearen) Zusammenhang nicht adäquat abbilden kann. Die Schätzung des Behandlungseffekts ist hier deutlich verzerrt.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Regression Discontiniuty Designs</span>"
    ]
  },
  {
    "objectID": "RDD.html#manipulation-am-schwellenwert",
    "href": "RDD.html#manipulation-am-schwellenwert",
    "title": "\n11  Regression Discontiniuty Designs\n",
    "section": "\n11.2 Manipulation am Schwellenwert",
    "text": "11.2 Manipulation am Schwellenwert\nEine wichtige Annahmen für die Gültigkeit einer RDD-Schätzung ist, dass keine Manipulation der Gruppenzugehörigkeit am Schwellenwert vorliegt. Wenn sich Subjekte nahe des Schwellenwertes \\(c\\) — d.h. in Abhängigkeit der Laufvariable \\(X\\) — systematisch in den Confoundern \\(Z\\) unterscheiden, können wir den Backdoor-Pfad Oberhalb C → Behandlung B → Y nicht isolieren. Wir erhalten dann eine verzerrte Schätzung des Behandlungseffekts.\nIn empirischen Studien mit Individuen kann Selbstselektion auftreten: Menschen mit \\(X&lt;c\\) aber nahe \\(c\\) (hier Kontrollgruppe) könnten aufgrund unbeobachtbarer Eigenschaften \\(Z\\) die Ausprägung ihrer Laufvariable zu \\(X&gt;c\\) (hier Behandlungsgruppe) manipulieren. Wenn \\(Z\\) die Outcome-Variable beeinflusst, bleibt der Backdoor-Pfad Oberhalb C → Behandlung B → Y so bestehen.\nManipulation resultiert in Häufung von Beobachtungen am Schwellenwert. Dei Verteilung der Laufvariable kann auf diese Unregelmäßigkeit hin untersucht werden. McCrary (2008) schlägt hierfür einen Verfahren vor, das die Kontinuität der Dichtefunktion von \\(X\\) am Schwellenwert testet.\nDer Test von McCrary (2008) ist in rdd::DCdensity() implementiert. Wir zeigen die Anwendung des Tests anhand der oben simulierten Daten. Beachte, dass \\(X_i\\sim U(0, 10)\\), d.h. die Laufvariable ist bei \\(X_i = c\\) kontinuierlich verteilt. Die Nullhypothese (keine Manipulation) gilt für die simulierten Daten\n\n# McCrary-Test durchführen\np_mccrary &lt;- rdd::DCdensity(\n  runvar = X, \n  cutpoint = c, \n  plot = F\n)\n\n# p-Wert\np_mccrary\n\n[1] 0.5013939\n\n\nDer p-Wert 0.5 ist größer als jedes übliche Signifikanzniveau. Damit liegt starke Evidenz für die Nullhypothese (keine Diskontinuität) und gegen Manipulation am Schwellenwert vor.\nCattaneo, Jansson, und Ma (2020) (CMJ) schlagen eine Weiterentwicklung des McCrary-Tests vor, die höhere statistische Macht gegenüber Diskontinuitäten hat am Schwellenwert hat. Der CJM-Test ist im Paket rddensity implementiert.\n\nlibrary(rddensity)\n\n# CJM Schätzer berechnen\nCJM &lt;- rddensity(X, c = 5)\n\nMit der Funktion rddensity::rdplotdensity() erzeugen wir Abbildung 11.4.\n\n# Plot für Dichtefunktion erstellen\nplot &lt;- rdplotdensity(\n  rdd = CJM, \n  X = X, \n  # für Punkte- und Linienplots:\n  type = \"both\" \n)\n\n\n\n\n\n\nAbbildung 11.4: CJM-Test – geschätzte Dichtefunktionen der Laufvariable auf beiden Seiten des Schwellenwerts c = 5\n\n\n\n\nAbbildung 11.4 zeigt die geschätzten Dichtefunktionen. Erwartungsgemäß finden wir eine große Überlappung der zugehörigen Konfidenzbänder (schattierte Flächen) am Schwellenwert \\(c=5\\).\nMit summary() erhalten wir eine detaillierte Zusammenfassung des Tests.\n\n# Statistische Zusammenfassung des CJM-Tests\nsummary(CJM)\n\n\nManipulation testing using local polynomial density estimation.\n\nNumber of obs =       750\nModel =               unrestricted\nKernel =              triangular\nBW method =           estimated\nVCE method =          jackknife\n\nc = 5                 Left of c           Right of c          \nNumber of obs         329                 421                 \nEff. Number of obs    133                 154                 \nOrder est. (p)        2                   2                   \nOrder bias (q)        3                   3                   \nBW est. (h)           1.918               2.124               \n\nMethod                T                   P &gt; |T|             \nRobust                -0.3338             0.7385              \n\n\nP-values of binomial tests (H0: p=0.5).\n\nWindow Length              &lt;c     &gt;=c    P&gt;|T|\n0.346     + 0.346          20      21    1.0000\n0.521     + 0.544          34      37    0.8126\n0.696     + 0.742          44      57    0.2323\n0.870     + 0.939          54      64    0.4075\n1.045     + 1.137          62      77    0.2349\n1.220     + 1.334          73      98    0.0661\n1.394     + 1.532          86     106    0.1701\n1.569     + 1.729          96     124    0.0685\n1.743     + 1.927         119     140    0.2139\n1.918     + 2.124         133     154    0.2377\n\n\nGemäß des p-Werts (P &gt; |T|) von 0.74 spricht der CJM-Test noch deutlicher gegen eine Diskontinuität als der McCrary-Test.\n\n11.2.1 Case Study: Amtsinhaber-Vorteil (Lee 2008)\n\nLee (2008) untersucht den Einfluss des Amtsinhaber-Vorteils auf die Wahl von Mitgliedern des US-Repräsentantenhaus. In den meisten Wahlkreisen entfallen große Anteile der Stimmen (oder gar ausschließlich) auf demokratische und republikanische Kanditat*innen, sodass sich die Studie auf diese Parteien beschränkt. Entfällt die Mehrheit der Stimmen auf eine*n Kandiat*in, gewinnt diese*r den Sitz für den Wahlkreis. Durch die Analyse der 6558 Wahlen im Zeitraum 1946-1998 mit einem SRDD kommt die Studie zu dem Ergebnis, dass Amtsinhabende im Durchschnitt einen Vorteil von etwa 8% bis 10% bei der Wahl haben. Dieses Ergebnis kann verschiedene Ursachen haben, bspw. dass die amtierende Partei höhere finanzielle Ressourcen besitzt und von einer besseren Organisation und durch Instrumenalisierung staatlicher Strukturen für die eigenen Zwecke profitiert.\nAnhand der Datensätze house und house_binned illustrieren wir nachfolgend die Schätzung von SRDD-Modellen für den Wahlerfolg der demokratischen Partei, wenn diese Amtsinhaber ist. Wir lesen hierfür zunächst die Datensätze house und house_binned ein und verschaffen uns einen Überblick.\n\nlibrary(tidyverse)\nlibrary(modelsummary)\n\n# Daten einlesen\nhouse &lt;- read_csv(\"datasets/house.csv\")\n# Gruppierter Datensatz\nhouse_binned &lt;- read_csv(\"datasets/house_binned.csv\")\n\n# Überblick verschaffen\nglimpse(house)\n\nRows: 6,558\nColumns: 2\n$ StimmenTm1 &lt;dbl&gt; 0.1049, 0.1393, -0.0736, 0.0868, 0.3994, 0.1681, 0.2516, 0.…\n$ StimmenT   &lt;dbl&gt; 0.5810, 0.4611, 0.5434, 0.5846, 0.5803, 0.6244, 0.4873, 0.5…\n\nglimpse(house_binned)\n\nRows: 100\nColumns: 2\n$ StimmenT   &lt;dbl&gt; 0.5995600, 0.5657000, 0.4272554, 0.5637456, 0.6868627, 0.60…\n$ StimmenTm1 &lt;dbl&gt; 0.104764444, 0.135005263, -0.075690769, 0.084570886, 0.3951…\n\n\nDer Datensatz house enthält die Stimmenanteile demokratischer Kandidat*innen bei der Wahl zum Zeitpunkt \\(T\\) (\\(StimmenT\\)) sowie die Differenz zwischen demokratischen und republikanischen Stimmenanteilen bei der vorherigen Wahl, d.h. zum Zeitpunkt \\(T-1\\) (\\(StimmenTm1\\)). Der Schwellenwert für einen Wahlsieg liegt bei Stimmengleichheit, d.h. \\(StimmenTm1 = 0\\).\nhouse_binned ist eine aggregierte Version von house mit Mittelwerten von jeweils 50 gleichgroßen Intervallen oberhalb und unterhalb der Schwelle von \\(StimmenTm1 = 0\\). Dieser Datensatz eignet sich, um einen ersten Eindruck des funktionalen Zusammenhangs auf beiden Seiten zu erhalten. Wir stellen zunächst diese klassierten Daten mit ggplot2 graphisch dar.\n\n# Klassierte Daten plotten\nhouse_binned %&gt;%\n  ggplot(\n    aes(x = StimmenTm1, y = StimmenT)\n    ) +\n  geom_point() +\n  geom_vline(xintercept = 0, lty = 2)\n\n\n\n\n\n\nAbbildung 11.5: Klassierte Daten aus Lee (2008)\n\n\n\n\nDie Grafik zeigt eindeutig einen Sprung von \\(StimmenT\\) bei \\(StimmenTm1 = 0\\). Weiterhin erkennen wir, dass der Zusammenhang nahe \\(0\\) vermutlich jeweils gut durch eine lineare Funktion approximiert werden kann. Eine Modell-Spezifikation mit gleicher Steigung auf beiden Seiten des Schwellenwertes scheint hingegen weniger gut geeignet. Wir vergleichen diese Spezifikationen nachfolgend.\nZunächst fügen wir dem Datensatz eine Dummyvariable B hinzu. Diese dient als Indikator für den Wahlgewinn in der letzten Wahl und zeigt die Amtsinhaberschaft (Behandlung) an.\n\n# Behandlungsindikator B hinzufügen\nhouse &lt;- house %&gt;% \n  mutate(B = StimmenTm1 &gt; 0)\n\nglimpse(house)\n\nRows: 6,558\nColumns: 3\n$ StimmenTm1 &lt;dbl&gt; 0.1049, 0.1393, -0.0736, 0.0868, 0.3994, 0.1681, 0.2516, 0.…\n$ StimmenT   &lt;dbl&gt; 0.5810, 0.4611, 0.5434, 0.5846, 0.5803, 0.6244, 0.4873, 0.5…\n$ B          &lt;lgl&gt; TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n\n\nWir überprüfen die Laufvariable mit dem CJM-Test auf Manipulation am Schwellenwert \\(c=0\\).\n\n# CJM-Test durchführen\nCJM_Lee &lt;- rddensity(X = house$StimmenTm1)\n\n# Zusammenfassung anzeigen\nsummary(CJM_Lee)\n\n\nManipulation testing using local polynomial density estimation.\n\nNumber of obs =       6558\nModel =               unrestricted\nKernel =              triangular\nBW method =           estimated\nVCE method =          jackknife\n\nc = 0                 Left of c           Right of c          \nNumber of obs         2740                3818                \nEff. Number of obs    1297                1360                \nOrder est. (p)        2                   2                   \nOrder bias (q)        3                   3                   \nBW est. (h)           0.236               0.243               \n\nMethod                T                   P &gt; |T|             \nRobust                1.4346              0.1514              \n\n\nP-values of binomial tests (H0: p=0.5).\n\nWindow Length / 2          &lt;c     &gt;=c    P&gt;|T|\n0.004                      21      24    0.7660\n0.007                      38      46    0.4452\n0.011                      50      60    0.3909\n0.014                      73      77    0.8066\n0.018                      91     104    0.3902\n0.022                     124     132    0.6618\n0.025                     149     149    1.0000\n0.029                     163     174    0.5860\n0.032                     176     202    0.1984\n0.036                     197     223    0.2225\n\n\n\n# CJM-Plot\nplot &lt;- rdplotdensity(\n  rdd = CJM_Lee,\n  X = house$StimmenTm1, \n  type = \"both\", \n)\n\n\n\n\n\n\nAbbildung 11.6: CJM-Test – geschätzte Dichtefunktionen der Laufvariable\n\n\n\n\nAbbildung 11.6 und der p-Wert von \\(0.15\\) sind Evidenz gegen eine Manipulation am Schwellenwert.\nUm den Behandlungseffekt anhand eines SRDDs zu ermitteln, schätzen wir das Interaktionsmodell \\[\\begin{align*}\n  \\text{StimmenT}_i =&\\, \\beta_0 + \\beta_1 B_i + \\beta_2 (\\text{StimmenTm1}_i - 50)\\\\\n  +&\\, \\beta_3(\\text{StimmenTm1}_i - 50)\\times B_i + u_i\n\\end{align*}\\] zunächst für eine Bandweite von \\(h = 0.5\\). Aufgrund der Skalierung der Daten (Wahlergebnisse in %) bedeutet dies die Verwendung des gesamten Datensatzes für die Schätzung.\n\n# Interaktionsmodell schätzen\nhouse_llr1 &lt;- lm(\n  formula = StimmenT ~ B * StimmenTm1, \n  data = house\n)\n\n# Zusammenfassung anzeigen  \nmodelsummary(\n  models = house_llr1, \n  vcov = \"HC1\", # robuste Standardfehler\n  stars = T, \n  gof_map = \"nobs\", \n  output = \"gt\"\n) %&gt;% \n  tabopts\n\n\n\n\n\n\n(1)\n\n\n\n(Intercept)\n0.433***\n\n\n\n(0.004)\n\n\nBTRUE\n0.118***\n\n\n\n(0.006)\n\n\nStimmenTm1\n0.297***\n\n\n\n(0.016)\n\n\nBTRUE × StimmenTm1\n0.046*\n\n\n\n(0.018)\n\n\nNum.Obs.\n6558\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\nDer geschätzte Koeffizient von \\(B\\) (BTRUE) beträgt etwa \\(0.12\\) und ist hochsignifikant. Übereinstimmend mit Abbildung 11.5 erhalten wir also eine positive Schätzung des Behandlungseffekts. Die Interpretation ist, dass die amtierenden Demokraten bei der Wahl von einem Amtsinhabervorteil profitieren. Dieser Effekt schlägt sich als Stimmenbonus von geschätzten 12% nieder. Diese Schätzung des Behandlungseffekts könnte jedoch verzerrt sein:\n\nDie (implizite) Wahl von \\(h=0.5\\) in unserer Schätzung macht die Isolation des relevanten Frontdoor-Paths (\\(c=0\\) → Treatment → StimmenT) wenig plausibel. \\(h\\) sollte mit einer datengetriebenen Methode gewählt werden.\nWeiterhin könnte die lineare funktionale Form der Regression inadäquat sein: Die lineare Approximation der wahren Regressionsfunktion nahe des Schwellenwerts \\(0\\) könnte unzureichend sein und in einer verzerrten Schätzung des Effekts resultieren. Zur Überprüfung der Robustheit der Ergebnisse sollte mit Schätzungen anhand nicht-linearer Spezifikationen verglichen werden.\n\nUm diesen Gefahren für die Validität der Studie zu begegnen, schätzen wir nun weitere Spezifikationen. Im Folgenden verwenden wir eine Bandweitenschätzung gemäß G. Imbens und Kalyanaraman (2012).\n\n# Bandweite mit Schätzer von IK (2012) berechnen\n(\nIK_BW &lt;- \n  rdd::IKbandwidth(\n    X = house$StimmenTm1, \n    Y = house$StimmenT\n  )\n)\n\n[1] 0.2685123\n\n\nWir schätzen zunächst erneut das lineare Interaktionsmodell, diesmal jedoch mit der Bandweite IK_BW.\n\n# Lineares Interaktionsmodelle mit IK-Bandweite\nhouse_llin_IK &lt;- lm(\n  formula = StimmenT ~ B * StimmenTm1, \n  data = house %&gt;% \n    filter(\n      abs(StimmenTm1) &lt;= IK_BW\n    )\n)\n\nFür den Vergleich mit einer nicht-linearen Spezifikation schätzen wir auch ein quadratisches Interaktionsmodell.\n\n# Quadratisches Interaktionsmodell mit IK-Bandweite\nhouse_poly_IK &lt;- update(\n  object = house_llin_IK,\n  formula = StimmenT ~ B * poly(StimmenTm1, degree = 2, raw = T)\n)\n\nFür eine Gegenüberstellung der Ergebnisse verwenden wir modelsummary().\n\n# Tabellarischer Modellvergleich\nmodelsummary(\n  models = list(\n    \"Linear int.\" = house_llin_IK, \n    \"Quadratisch int.\" = house_poly_IK\n  ),  \n  vcov = \"HC1\", \n  stars = T,\n  gof_map = \"nobs\", \n  output = \"gt\"\n) %&gt;% \n  tabopts\n\n\n\n\n\n\n\n\nLinear int.\nQuadratisch int.\n\n\n\n(Intercept)\n0.450***\n0.460***\n\n\n\n(0.005)\n(0.008)\n\n\nBTRUE\n0.085***\n0.068***\n\n\n\n(0.008)\n(0.012)\n\n\nStimmenTm1\n0.360***\n\n\n\n\n(0.036)\n\n\n\nBTRUE × StimmenTm1\n0.055\n\n\n\n\n(0.059)\n\n\n\npoly(StimmenTm1, degree = 2, raw = T)1\n\n0.573***\n\n\n\n\n(0.138)\n\n\npoly(StimmenTm1, degree = 2, raw = T)2\n\n0.798\n\n\n\n\n(0.493)\n\n\nBTRUE × poly(StimmenTm1, degree = 2, raw = T)1\n\n0.036\n\n\n\n\n(0.219)\n\n\nBTRUE × poly(StimmenTm1, degree = 2, raw = T)2\n\n-1.529+\n\n\n\n\n(0.834)\n\n\nNum.Obs.\n2956\n2956\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\nTabelle 11.1: Vergleich von SRDD-Interaktionsmodellen für Lee (2008)\n\n\n\nDie Spalte (1) in Tabelle 11.1 zeigt die lokale Schätzung mit einem linearen Interaktionsmodell. Wir erhalten damit einen Behandlungseffekt von etwa \\(8.5\\%\\). Der Schätzwert fällt also etwas geringer aus als für die globale KQ-Schätzung des linearen Interaktionsmodells. Für das Modell (2) mit quadratischer Spezifikation liegt der Schätzwert mit \\(6.8\\%\\) in der selben Größenordnung. Beide Schätzungen ergeben einen signifikant von \\(0\\) verschieden Effekt. Weiterhin fällt auf, dass in beiden Modellen keine Evidenz für unterschiedliche Formen der Regressionsfunktionen auf beiden Seiten des Schwellenwerts vorliegen: sämtliche Koeffizientenschätzwerte der Interaktionsterme haben hohe Standardfehler und sind nicht signifikant. Im quadratischen Modell hat auch der Term \\(StimmenTm1^2\\) keinen signifikanten Effekt. Diese Ergebnisse deuten darauf hin, dass eine lineare Spezifikation ausreichend ist.\nSRDD-Schätzung mit LOESS\nWir illustrieren nachfolgend die Schätzung des Behandlungseffekts mit einer flexiblen und in der Praxis häufig verwendeten Methode für lokale Regression. Die nachfolgende interaktive Grafik zeigt die klassierten Daten aus Lee (2008) auf dem Intervall \\([-0.5,0.5]\\) gemeinsam mit einer nicht-parametrischen Schätzung des Zusammenhangs von StimmenT und StimmenTm1 mittels LOESS.3 Diese Implementierung von lokaler Regression nutzt einen tricube kernel. Über den Input kann eine Bandweite \\(l\\in(0,1]\\) für den LOESS-Schätzer auf beiden Seiten des Schwellenwerts \\(0\\) gewählt werden. Die Bandweite ist hier der Anteil der Beobachtungen an der gesamten Anzahl an Beobachtungen, die in die Schätzung einbezogen werden sollen.\n3 LOESS ist eine Variante von lokaler Polynom-Regression.Für die Schätzung am Schwellenwert berücksichtigte Daten sind in orange kenntlich gemacht. Die rote linie zeigt die geschätzte Regressionsfunktion über gleichmäßig verteilte Werte von StimmenTm1 auf \\([-0.5,0.5]\\). Die Grafik verdeutlicht, dass die LOESS-Methode flexibel genug ist, um lineare und nicht-lineare Zusammenhänge abbilden zu können. Wie zuvor ist eine adäquate Wahl der Bandweite wichtig:\n\nDer mit LOESS geschätzte Zusammenhang auf beiden Seiten des Schwellenwerts ist etwa linear für den voreingestellten Parameter (\\(l = 0.28\\)).\nFür größere Werte von \\(l\\) nähert sich die Schätzung weiter einem linearen Verlauf an. Die Schätzung des Effekts bleibt vergleichbar mit den Ergebnissen des linearen Interaktionsmodell (s. oben).\nFür kleinere \\(l\\) erhalten wir eine stärkere Anpassung der Schätzung an die Daten. Zu kleine Werte führen zu einer Überanpassung (overfitting). Insbesondere tendiert die geschätzte Funktion zu extremer Steigung nahe des Schwellenwerts → stark verzerrte Schätzung des Effekts!\n\n\nhtml`\n&lt;style&gt;\n.regression {\n  fill: none;\n  stroke: #000;\n  stroke-width: 1.5px;\n}\n.axis line {\n  stroke: #ddd;\n}\n.axis .baseline line {\n  stroke: #555;\n}\n.axis .domain {\n  display: none;\n} \n&lt;/style&gt;\n&lt;link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css\"&gt;\n`\n\n\n\n\n\n\n\nviewof bandwidth = Inputs.range([.01, 1], {\n  label: \"Bandweite LOESS (l)\",\n  step: .01,\n  value: .28\n});\n\nxScaleLoess = d3.scaleLinear()\n   .domain([-.55, .55])\n   .range([0, innerWidth]);\n   \nyScaleLoess = d3.scaleLinear()\n  .domain([.2, .8])\n  .range([innerHeight, 0]);\n\nlineLoess = d3.line()\n  .x(d =&gt; xScaleLoess(d[0]))\n  .y(d =&gt; yScaleLoess(d[1]));\n  \nxAxisLoess = d3.axisBottom(xScaleLoess)\n  .tickSize(innerHeight + 10)\n  .tickValues([-.5, -.25, 0, .25, .5])\n  .tickFormat(d =&gt; d);\n\nyAxisLoess = d3.axisLeft(yScaleLoess)\n  .tickSize(innerWidth + 10)\n  .tickValues([.2, .35, .5, .65, .8])\n  .tickFormat(d =&gt; d);\n\nloessRegression = d3.regressionLoess()\n  .x(d =&gt; d.StimmenTm1)\n  .y(d =&gt; d.StimmenT)\n  .bandwidth(bandwidth);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  const svg = d3.select(DOM.svg(innerWidth + margin.left + margin.right + 20, innerHeight + margin.top + margin.bottom + 20))\n  \n  const g = svg.append(\"g\")\n      .attr(\"transform\", `translate(${margin.left}, ${margin.top})`);\n\n  g.append(\"g\")\n      .attr(\"class\", \"axis\")\n      .call(xAxisLoess);\n\n  g.append(\"g\")\n    .attr(\"class\", \"axis\")\n    .attr(\"transform\", `translate(${innerWidth})`)\n    .call(yAxisLoess);\n\n  // Add X axis label:\n  g.append(\"text\")\n    .attr(\"text-anchor\", \"end\")\n    .attr(\"font-size\", 13)\n    .attr(\"x\", innerWidth)\n    .attr(\"y\", innerHeight + margin.top + 25)\n    .text(\"Differenz Stimmenanteil Demokraten letzte Wahl zur 50%-Schwelle\");\n\n  // Y axis label:\n  g.append(\"text\")\n    .attr(\"text-anchor\", \"end\")\n    .attr(\"transform\", \"rotate(-90)\")\n    .attr(\"font-size\", 13)\n    .attr(\"y\", -margin.left+10)\n    .attr(\"x\", -margin.top+10)\n    .text(\"Stimmenanteil Demokraten\");\n\n  g.selectAll(\"circle\")\n    .data(transpose(house_binned))\n    .enter().append(\"circle\")\n    .attr(\"r\", 2)\n    .attr(\"cx\", d =&gt; xScaleLoess(d.StimmenTm1))\n    .attr(\"cy\", d =&gt; yScaleLoess(d.StimmenT));\n\n  g.selectAll(\"circle\")\n   .filter( function(d){ return Math.abs(d.StimmenTm1) &lt;= bandwidth/2 } )\n   .attr(\"fill\", \"orange\")\n   .attr(\"stroke\", \"none\");\n\nfunction b(d) { return loessRegression(\n        transpose(house).filter(function(d){ return d.StimmenTm1 &lt;= 0 & d.StimmenTm1 &gt;= -.5 })\n    ); }\n\nfunction a(d) { return loessRegression(\n      transpose(house).filter(function(d){ return d.StimmenTm1 &gt; 0 & d.StimmenTm1 &lt;= .5  })\n    ); }\n\n  g.append(\"path\")\n      .attr(\"class\", \"regression\")\n      .datum(b)\n      .attr(\"d\", lineLoess)\n      .style(\"stroke\", \"red\");\n\n  g.append(\"path\")\n      .attr(\"class\", \"regression\")\n      .datum(a)\n      .attr(\"d\", lineLoess)\n      .style(\"stroke\", \"red\");\n  \n  g.append(\"text\")\n    .attr(\"x\", d =&gt; xScaleLoess(-.24))\n    .attr(\"y\", d =&gt; yScaleLoess(.55))\n    .attr(\"dy\", \".35em\")\n    .attr(\"fill\", \"#39FF14\")\n    .text(d3.format(\",.2f\")((a().slice(0))[0][1] - (b().slice(-1))[0][1]));\n  \n  g.append(\"line\")\n  .attr(\"x1\", xScaleLoess(0))\n  .attr(\"y1\", yScaleLoess((b().slice(-1))[0][1]))\n  .attr(\"x2\", xScaleLoess(0))\n  .attr(\"y2\", yScaleLoess((a().slice(0))[0][1]))\n  .attr(\"stroke\", \"#39FF14\")\n  .attr(\"stroke-width\", 2);\n  \n  /* dashed line at cutoff */\n  g.append(\"line\")\n  .attr(\"x1\", xScaleLoess(0))\n  .attr(\"y1\", 0)\n  .attr(\"x2\", xScaleLoess(0))\n  .attr(\"y2\", innerHeight)\n  .style(\"stroke\", \"black\")\n  .style(\"stroke-dasharray\", \"1\")\n  .style(\"stroke-width\", \"1\");\n  \n  /* dashed line data bw upper */\n  g.append(\"line\")\n  .attr(\"x1\", xScaleLoess(bandwidth/2))\n  .attr(\"y1\", 0)\n  .attr(\"x2\", xScaleLoess(bandwidth/2))\n  .attr(\"y2\", innerHeight)\n  .style(\"stroke\", \"blue\")\n  .style(\"stroke-dasharray\", \"4\")\n  .style(\"stroke-width\", \"1\");\n  \n  /* dashed line data bw lower */\n  g.append(\"line\")\n  .attr(\"x1\", xScaleLoess(-bandwidth/2))\n  .attr(\"y1\", 0)\n  .attr(\"x2\", xScaleLoess(-bandwidth/2))\n  .attr(\"y2\", innerHeight)\n  .style(\"stroke\", \"blue\")\n  .style(\"stroke-dasharray\", \"4\")\n  .style(\"stroke-width\", \"1\");\n\n  return svg.node();\n}\n\n\n\n\n\n\n\nAbbildung 11.7: Nicht-parametrische Regression auf beiden Seiten des Cut-offs.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Regression Discontiniuty Designs</span>"
    ]
  },
  {
    "objectID": "RDD.html#fuzzy-regression-discontinuity-design",
    "href": "RDD.html#fuzzy-regression-discontinuity-design",
    "title": "\n11  Regression Discontiniuty Designs\n",
    "section": "\n11.3 Fuzzy Regression Discontinuity Design",
    "text": "11.3 Fuzzy Regression Discontinuity Design\n\n\n\n\n\nX\nXY\nYX-&gt;Y\noberhalb c\noberhalb cX-&gt;oberhalb c\nBehandlung\nBehandlungX-&gt;Behandlung\noberhalb c-&gt;Behandlung\nZ\nZZ-&gt;X\nZ-&gt;Y\nZ-&gt;Behandlung\nBehandlung-&gt;Y\n\n\n\n\nAbbildung 11.8: Kausales Diagram für FRDD\n\n\n\n\nEin FRDD liegt vor, wenn die Zuweisung der Behandlung \\(B\\) durch die Laufvariable \\(X\\) (und möglicherweise weitere Variablen \\(Z\\)) beeinflusst wird. Im Vergleich zum SRDD ist die Behandlung dann also nicht ausschließlich durch Überschreiten des Schwellenwerts \\(X = c\\) bestimmt.\nAbbildung 11.8 zeigt den grundsätzlichen Zusammenhang. Hier genügt es weiterhin für \\(X\\) (und ggf. \\(Z\\)) zu kontrollieren, um den Pfad oberhalb \\(C\\) → Behandlung \\(B\\) → \\(Y\\) zu isolieren. Der so für Behandlung \\(B\\) ermittelte Effekt auf \\(Y\\) entspricht jedoch nicht dem “vollständigen” Behandlungseffekt, da bei \\(c\\) die Zuweisung der Behandlung nicht von \\(0\\) auf \\(100\\%\\) springt. Die Schätzung des FRDD berücksichtigt dies und skaliert den geschätzten Effekt entsprechend.\nWir betrachten zunächst den Zusammenhang \\[\\begin{align}\n  Y_i = \\beta_0 + \\beta_1 B_i + \\beta_2 (X_i - c) + u_i.\\label{eq-simpleFRDD}\n\\end{align}\\] In einem FRDD springt die Behandlungswahrscheinlichkeit am Schwellenwert \\(c\\) um \\(\\Delta p&lt;1\\). Wir können \\(B\\) also nicht als deterministische Funktion von \\(X\\), welche die Zuweisung zu Behandlungs- bzw. Kontrollgruppe am Schwellenwert \\(c\\) anzeigt (wie im SRDD), definieren. Stattdessen betrachten wir \\[\\begin{align}\n  P(B_i=1\\vert X_i) =\n  \\begin{cases}\n    g_{X_i&lt;c}(X_i), & X_i &lt; c \\\\\n    g_{X_i\\geq c}(X_i) & X_i \\geq c\n  \\end{cases}\\,. \\label{eq-BFRDD}\n\\end{align}\\] Die Funktionen \\(g_{X_i&lt;c}\\) und \\(g_{X_i\\geq c}\\) können verschieden sein. Es muss jedoch \\[g_{X_i&lt;c}(X_i = c) \\neq g_{X_i\\geq c}(X_i = c)\\] gelten. Die Behandlungsvariable \\(B_i\\) ist im FRDD also eine (binäre) Zufallsvariable, deren bedingte Wahrscheinlichkeitsfunktion \\(P(B_i=1\\vert X_i)\\) am Schwellenwert \\(c\\) eine Diskontinuität aufweist. Abbildung 11.9 zeigt heispielhafte Verläufe nicht-linearer bedingter Wahrscheinlichkeitsfunktion für die Behandlung mit einer Diskontinuität bei \\(X_i = c\\).\n\nCodelibrary(ggplot2)\nlibrary(cowplot)\n\n# Bedingte Behandlungswahrscheinlichkeit im FRDD illustrieren\nggplot() + \n  geom_function(\n    fun = ~ ifelse(\n      . &lt; 0, \n      -.1 * .^2 + .25, \n      -.1 * (.-1.5)^2 + 1\n    ), \n    n = 1000\n  ) + \n    geom_function(\n    fun = ~ ifelse(\n      . &lt; 0, \n     .35, \n     .65\n    ),\n    n = 1000, \n    lty = 2, \n    col = \"red\"\n  ) + \n  scale_x_continuous(\n    name = \"Laufvariable X\", \n    limits = c(-1.5, 1.5),\n    labels = NULL,\n    breaks = NULL\n  ) +\n  scale_y_continuous(\n    name = \"P(D=1|X)\", \n    breaks = c(0, 1), \n    limits = c(0, 1)\n  ) +\n  theme_cowplot()\n\n\n\n\n\n\nAbbildung 11.9: Bedingte Behandlungswahrscheinlichkeiten im FRDD\n\n\n\n\nDefinition \\(\\eqref{eq-BFRDD}\\) bedeutet, dass eine KQ-Schätzung von \\(\\beta_1\\) anhand \\(\\eqref{eq-simpleFRDD}\\) eine verzerrte Schätzung des Behandlungseffekts ist: Der in \\(\\widehat{\\beta}_1\\) erfasste Effekt auf \\(Y\\) ist auf einen Sprung der Behandlungswahrscheinlichkeit bei \\(X_i = c\\) um weniger als \\(100\\%\\) zurückzuführen. Der wahre Behandlungseffekt wird also unterschätzt. Daher muss \\(\\widehat{\\beta}_1\\) skaliert werden, sodass die Schätzung als Effekt einer Änderung der Behandlungswahrscheinlichkei um \\(100\\%\\) interpretiert werden kann — der erwartete Effekt, wenn ausschließlich Subjekte mit \\(X_i\\geq c\\) behandelt würden. Diese skalierte Schätzung erhalten wir mit IV-Regression (vgl. Kapitel XYZ). Hierfür nutzen wir für \\(B_i\\) die Instrumentvariable \\[\\begin{align*}\n  D_i = \\begin{cases}\n    0, & X_i &lt; c \\\\\n    1, & X_i \\geq c.\n  \\end{cases}\n\\end{align*}\\]\nAngenommen \\(g_{X_i\\geq c}(X_i) = \\alpha_0\\) und \\(g_{X_i&lt;c}(X_i) = \\alpha_0 + \\alpha_1\\) mit \\(\\alpha_0 + \\alpha_1 &lt; 1\\) (vgl. rote Funktion in Abbildung 11.8). Der FRDD-Schätzer des Behandlungseffekts ist dann \\(\\widehat{\\gamma}_\\textup{FRDD}\\) im 2SLS-Verfahren mit den Regressionen \\[\\begin{align}\n  \\begin{split}\n  (\\mathrm{I})\\qquad B_i =&\\, \\alpha_0 + \\alpha_1 D_i + \\alpha_2 (X_i - c) + e_i,\\\\\n  (\\mathrm{II})\\qquad Y_i =&\\, \\gamma_0 + \\gamma_1 \\widehat{B}_i + \\gamma_2 (X_i - c) + \\epsilon_i,\n  \\end{split}\\label{eq:FRDD_simpleIV}\n\\end{align}\\] wobei \\(\\widehat{B}_i\\) die angepassten Werte aus Stufe \\((\\mathrm I)\\) und \\(e_i\\) sowie \\(\\epsilon_i\\) Fehlterterme sind.\nAnalog zum SRDD müssen in empirischen Anwendungen geeignete Spezifikationen für die Regressionsfunktionen \\(\\eqref{eq-simpleFRDD}\\) und \\(\\eqref{eq-BFRDD}\\) gewählt und der 2SLS-Schätzer \\(\\eqref{eq:FRDD_simpleIV}\\) entsprechend angepasst werden. Ein einfaches Interaktionsmodell wäre \\[\\begin{align}\n  \\begin{split}\n  (\\mathrm{I})\\qquad B_i =&\\, \\alpha_0 + \\alpha_1 D_i + \\alpha_2 (X_i - c)\\\\\n  +&\\, \\alpha_3 (X_i - c) \\times D_i + e_i,\\\\\n  \\\\\n  (\\mathrm{II})\\qquad Y_i =&\\, \\gamma_0 + \\gamma_1 \\widehat{B}_i\\\\\n  +&\\, \\gamma_2 (X_i - c) + \\gamma_3 (X_i-c)\\times\\widehat{B}_i, \\epsilon_i\n  \\end{split},\\label{eq:FRDD_lintIV}\n\\end{align}\\] d.h. wir instrumentieren \\(B_i\\) mit \\(D_i\\) und dem Interaktionsterm \\((X_i-c)\\times D_i\\).\nWie im SRDD werden die IV-Ansätze für das FRDD \\(\\eqref{eq:FRDD_simpleIV}\\) und \\(\\eqref{eq:FRDD_lintIV}\\) in empirischen Studien unter Berücksichtigung einer Bandweite (i.d.R. dieselbe Bandweite für beide Stufen) angewendet.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Regression Discontiniuty Designs</span>"
    ]
  },
  {
    "objectID": "RDD.html#case-study-protestantische-arbeitsethik",
    "href": "RDD.html#case-study-protestantische-arbeitsethik",
    "title": "\n11  Regression Discontiniuty Designs\n",
    "section": "\n11.4 Case Study: Protestantische Arbeitsethik",
    "text": "11.4 Case Study: Protestantische Arbeitsethik\nDie Studie Beyond Work Ethic: Religion, Individual, and Political Preferences (Basten und Betz 2013) untersucht den Zusammenhang zwischen Religion, individuellen Merkmalen und politischen Präferenzen. Das Hauptaugenmerk ist die Rolle von Religiosität als Einflussfaktor auf politische Einstellungen. Die Hypothese der Autoren ist, dass Religiosität eines Individuums über den traditionellen Rahmen von Moralvorstellungen und sozialen Normen hinaus auch die politischen Präferenzen beeinflusst. Eine entsprechende Theorie wurde zu Beginn des 20. Jahrhunderts entwickelt und prominent von Max Weber (vgl. Weber 2004) vertreten. Weber argumentiert, dass die protestantische Arbeitsethik einen entscheidenden Einfluss auf die Entwicklung des Kapitalismus hatte. Laut Weber führte der protestantische Glaube an harte Arbeit, ein sparsames Leben und ethisches Verhalten zur einer in den damaligen Gesellschaften weit verbreiteten Geisteshaltung, die wirtschaftliches Wachstum förderte und den Aufstieg des Kapitalismus begünstigte.\nBasten und Betz (2013) nutzen Wahlergebnisse sowie geo- und soziodemographische Datensätze für schweizer Gemeinden, um den Zusammenhang zwischen Religiosität und politischen Präferenzen wie links-rechts-Ausrichtung, Einstellungen zur Umverteilung und Einwanderung zu untersuchen. Hierfür verwenden die Autoren ein FRDD, dass eine historisch bedingte Diskontinuität der geographischen Verteilung von evanglischer bzw. katholischer Religionszugehörigkeit zwischen den Kantonen Freiburg (überwiegend dunkelrote Region, frz. Fribourg) und Waadt (kleinere hellrote Region, frz. Vaud) ausnutzt. Die historische Verteilung der Konfessionen in der betrachteten Region im 16. Jahrhundert durch Abspaltung des Kantons Freiburg ist in Abbildung 11.10 dargestellt.\nAufgrund von Bevölkerungsbewegungen ist die Verteilung der Konfessionen zwar nicht mehr eindeutig durch die Kantonsgrenze bestimmt, jedoch sind die Gemeinden der betrachteten Kantone auch heute noch mehrheitlich protestantisch bzw. katholisch. Es ist plausibel, dass eine Prägung gemäß Webers Theorie vorliegt, sich die Gemeinden nahe der Grenz aber hinsichtlich anderer Charakteristika (insb. der Bevölkerungsstruktur) nicht systematisch unterscheiden. Somit liegt ein quai\n\n\n\n\n\n\n\nAbbildung 11.10: Historische Verteilung von Religionszugehörigkeit in Schweizer Gemeinden im 16. Jahrhundert. Quelle: Basten und Betz (2013).\n\n\n\n\nDie Ergebnisse der Studie zeigen einen signifikanten Einfluss von Protestantismus auf politische Präferenzen, die über traditionelle Moralvorstellungen hinausgehen: Die Autoren finden Hinweise, dass Einwohner evangelisch geprägter Gemeinden eher konservative soziale und politische Ansichten vertreten. Eine mögliche Erklärung für diesen Effekt ist, dass religiöse Institutionen auch eine soziale und politische Agenda verfolgen, die von den Gläubigen internalisiert wird.\n\n11.4.1 Aufbereitung der Daten\nIn diesem Kapitel zeigen wir, wie die Kernergebnisse der Studie mit R reproduziert werden können. Hierfür werden folgende Pakete benötigt.\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(vtable)\nlibrary(rdrobust)\n\nDas Papier sowie der Datensatz BastenBetz.dta sind auf der Übersichtsseite der AEA verfügbar und liegt im STATA-Format .dta vor.4\n4 Siehe alternativ das working paper, falls kein Abbonement für AEA-Journals vorliegt.\n# Datensatz einlesen\nBastenBetz &lt;- read_dta('BastenBetz.dta')\n\nDer Datensatz BastenBetz enthält Beobachtungen zu 509 schweizer Gemeinden. Eine Vielzahl an Variablen ist lediglich für Robustheits-Checks relevant. Für die Reproduktion der Kernergebnisse erstellen wir zunächst einen reduzierten Datensatz und transformieren einige Variablen.\n\n# Reduzierten Datensatz erstellen\nBastenBetz &lt;- BastenBetz %&gt;%\n  transmute(\n    gini = Ecoplan_gini,\n    prot = prot1980s,\n    bord = borderdis, \n    vaud,\n    pfl, \n    pfr, \n    pfi\n  )\n\nDie Definitionen der Variablen sind in Tabelle 11.2 gegeben. Die Präferenzen pfl, pfr und pfi basieren auf Wahlergebnissen auf Gemeindeebene zu Volksentscheiden.\n\n\n\n\n\n\n\n\nVariable\nDefinition\n\n\n\nprot\nAnteil Prothestanten im Jahr 1980 (%)\n\n\ngini\nGini-Koeffizient\n\n\nbord\nLaufdistanz zur Kantonsgrenze (Km)\n\n\nvaud\nDummyvariable: Gemeine im Kanton Waadt\n\n\npfl\nPräferenz für Freizeit (%)\n\n\npfr\nPräferenz für Umverteilung (%)\n\n\npfi\nPräferenz für wirtschaftliche Intervention des Staats (%)\n\n\n\n\n\nTabelle 11.2: BastenBetz – Variablen und Definitionen\n\n\nFür die Berechnung der optimalen Bandweite des FRDD verwenden wir einen MSE-optimalen Schätzer, der in der Funktion rdrobust::rdbwselect() implementiert ist.5\n5 Basten und Betz (2013) setzen BW = 5.01, den Durchschnitt von IK-Schätzungen über Modelle sämtlicher betrachteter Outcome-Variablen. Diese Bandweite liegt nahe des Ergebnisses von rdbwselect. Wir verwenden nachfolgend die Schätzung OB.\n# Bandweite schätzen (Bsp. für Freizeitpräferenz)\nbw_selection &lt;- rdbwselect(\n  y = BastenBetz$pfl,\n  x = BastenBetz$bord,\n  fuzzy = BastenBetz$prot, \n  bwselect = \"mserd\", \n  kernel = \"uniform\"\n) \n\n# Bandweite auslesen und zuweisen\n(OB &lt;- bw_selection$bws[1])\n\n[1] 5.078001\n\n\n\n11.4.2 Deskriptive Statistiken\nZur Reproduktion von Tabelle 1 aus Basten und Betz (2013) erzeugen wir eine nach Kantonen gruppierte Zusammenfassung der Daten und berechnen deskriptive Statistiken. Wie im Paper berücksichtigen wir hierbei nur Gemeinden innerhalb der geschätzten optimalen Bandweite OB.\n\n# Datensatz für Reproduktion von Table 1 formatieren\nT1 &lt;- BastenBetz %&gt;%\n  filter(abs(bord) &lt; OB) %&gt;%\n  mutate(\n    vaud = ifelse(\n      test = vaud == 1, \n      yes = \"Waadt\", \n      no = \"Freiburg\"\n    ),\n    prot = prot * 100\n  ) %&gt;%\n  group_by(vaud) %&gt;%\n  summarise(\n    across(\n      everything(), \n      list(\n        Mean = mean, \n        SD = sd, \n        N = length\n      )\n    )\n  ) %&gt;%\n  pivot_longer(\n    cols = -vaud,\n    names_to = c(\"variable\", \"statistic\"), \n    names_sep = \"_\"\n  )\n\nFür die tabellarische Darstellung transformieren wir in ein weites Format, sodass die Tabelle die deskriptive Statistiken spaltenweise für die Kantone zeigt.\n\n# Daten in weites Format überführen\nT1_wider &lt;- T1 %&gt;% \n  pivot_wider(\n    names_from = c(\"vaud\", \"statistic\")\n  )\n\nDie Tabelle erzeugen wir mit gt::gt().\n\n# Tabelle mit gt() erzeugen\nT1_wider %&gt;%\n  gt(rowname_col = \"Variable\") %&gt;% \n  tab_spanner_delim(\n    delim = \"_\",\n  ) %&gt;%\n tabopts\n\n\n\n\n\n\n\n\nvariable\nFreiburg\nWaadt\n\n\nMean\nSD\nN\nMean\nSD\nN\n\n\n\n\ngini\n0.302\n0.029\n49\n0.367\n0.052\n84\n\n\nprot\n9.428\n5.695\n49\n83.245\n11.411\n84\n\n\nbord\n−2.327\n1.274\n49\n2.493\n1.201\n84\n\n\npfl\n48.239\n4.774\n49\n39.508\n5.723\n84\n\n\npfr\n43.049\n2.634\n49\n39.19\n5.025\n84\n\n\npfi\n52.642\n2.94\n49\n47.086\n3.368\n84\n\n\n\n\n\n\n\nTabelle 11.3: Datensatz BastenBetz – Zusammenfassende Statistiken\n\n\n\nDie Statistiken in Tabelle 11.3 scheinen konsistent mit der (historischen) Verteilung der Religionszugehörigkeit und politischen Einstellung gemäß der Hypothese: Im überwiegend katholischen Freiburg finden wir eine größere Einkommensungleichkeit und höhere aus Wahlergebnissen abgeleitete Präferenzen für Freizeit, Umverteilung sowie staatliche Interventionen.\n\n11.4.3 Modellspezifikation und First-Stage-Ergebnisse\n\nCJM_BB &lt;- rddensity(BastenBetz$bord, c = 0, kernel = \"epanechnikov\")\nsummary(CJM_BB)\n\n\nManipulation testing using local polynomial density estimation.\n\nNumber of obs =       509\nModel =               unrestricted\nKernel =              epanechnikov\nBW method =           estimated\nVCE method =          jackknife\n\nc = 0                 Left of c           Right of c          \nNumber of obs         127                 382                 \nEff. Number of obs    69                  124                 \nOrder est. (p)        2                   2                   \nOrder bias (q)        3                   3                   \nBW est. (h)           8.531               9.57                \n\nMethod                T                   P &gt; |T|             \nRobust                0.7552              0.4501              \n\n\nP-values of binomial tests (H0: p=0.5).\n\nWindow Length              &lt;c     &gt;=c    P&gt;|T|\n1.400     + 1.400          20      20    1.0000\n2.192     + 2.308          27      47    0.0265\n2.985     + 3.215          34      60    0.0095\n3.777     + 4.123          40      72    0.0032\n4.570     + 5.031          44      84    0.0005\n5.362     + 5.939          51      95    0.0003\n6.154     + 6.846          56     100    0.0005\n6.947     + 7.754          60     106    0.0004\n7.739     + 8.662          67     114    0.0006\n8.531     + 9.570          69     124    0.0001\n\n# CJM-Plot\nplot &lt;- rdplotdensity(\n  rdd = CJM_BB,\n  X = BastenBetz$bord, \n  type = \"both\", plotN = 20, \n)\n\n\n\n\n\n\n\nDie Kantone Waadt und Freiburg haben bis heute mehrheitlich protestantische bzw. katholische Gemeinden. Die Verteilung von Protestantismus ist also, u.a. aufgrund von Bevölkerungsbewegungen, nicht mehr deterministisch. An der Kantonsgrenze besteht jedoch eine deutliche Diskontinuität im Anteil protestantischer Einwohner, die auf die historische Verteilung der Religionszugehörigkeit zurückzuführen ist. Damit kann ein FRDD implementiert werden, bei dem die Distanz zur Grenze (bord) die zentrierte Laufvariable ist und die Zugehörigkeit zum Kanton Waadt (vaud) ein Instrument für die Behandlungsvariable (prot) ist.\nWir nutzen die Funktion rdrobust::rdplot um diesen Zusammenhang für verschiedene Bandweiten anhand des linearen Interaktionsmodells \\[\\begin{align}\n  \\begin{split}\n  prot_i =&\\, \\alpha_0 + \\alpha_1 vaud_i + \\alpha_2 bord_i \\\\\n  +&\\, \\alpha_3 bord_i \\times vaud_i + u_i\n  \\end{split}\\label{eq:BBFSR}\n\\end{align}\\] grafisch darzustellen. Dies ist die First-Stage-Regression für die 2SLS-Schätzung der Behandlungseffekte.\n\n# Reproduktion von Abbildung 3 in Basten und Betz (2013)\nplots_BB &lt;- list(\n  # gesch. optimale Bandweite\n  p_OB = rdplot(\n    y = BastenBetz$prot, \n    x = BastenBetz$bord, \n    h = c(OB, OB), \n    x.label = \"Distanz zur Grenze (bord)\",\n    y.label = \"Anteil Protestanten (prot)\", \n    title = \"Gesch. Bandweite\",\n    p = 1, \n    nbins = c(6, 14), \n    masspoints = \"off\"\n  ),\n  \n  # Bandweite 10\n  p_BW10 = rdplot(\n    y = BastenBetz$prot, \n    x = BastenBetz$bord, \n    h = c(10, 10), \n    x.label = \"Distanz zur Grenze (bord)\",\n    y.label = \"Anteil Protestanten  (prot)\", \n    title = \"Bandweite = 10\",\n    p = 1, \n    nbins = c(6, 14),\n    masspoints = \"off\"\n  ),\n  \n  # Bandweite 20\n  p_BW20 = rdplot(\n    y = BastenBetz$prot, \n    x = BastenBetz$bord, \n    h = c(20, 20), \n    x.label = \"Distanz zur Grenze (bord)\",\n    y.label = \"Anteil Protestanten  (prot)\", \n    title = \"Bandweite = 20\",\n    p = 1, \n    nbins = c(6, 14),\n    masspoints = \"off\"\n  ),\n  \n  # Gesamter Datensatz\n  p_G = rdplot(\n    y = BastenBetz$prot, \n    x = BastenBetz$bord,\n    x.label = \"Distanz zur Grenze (bord)\",\n    y.label = \"Anteil Protestanten\", \n    title = \"Ges. Datensatz\",\n    p = 1, \n    nbins = c(6, 14),\n    masspoints = \"off\"\n  )\n)\n\nWir sammeln die Ergebnisse in einem Plot-Gitter mit cowplot::plot_grid().\n\n# Reproduktion von Abbildung 3 in Basten und Betz (2013)\nplot_grid(\n  plotlist = map(plots_BB, ~ .$rdplot), ncol = 2\n)\n\n\n\n\n\n\nAbbildung 11.11: First-Stage-Regressionen\n\n\n\n\nDie Grafiken in Abbildung 11.11 zeigen deutliche Hinweise auf die Diskontinuität in prot nahe der Kantonsgrenze. Die Größe des geschätzten Sprungs scheint nur wenig sensitiv gegenüber der gewählten Bandweite zu sein. Die Signifikanz des Effekts können wir anhand der jeweiligen KQ-Regressionen beurteilen.6\n6 Wir nutzen update() um die Regression mit weniger Code für verschiedene Bandweiten zu schätzen.\n# Reproduktion der First-Stage-Regressionen\n# s. Tabelle 2 in Basten und Betz (2013)\n\n# (1) BW = OB\nFS1 &lt;- lm(\n  formula = prot ~ vaud + bord + vaud * bord, \n  data = BastenBetz %&gt;% \n    filter(\n      abs(bord) &lt;= OB\n    )\n)\n\n# (2) BW = 10\nFS2 &lt;- update(\n  FS1,\n  data = BastenBetz %&gt;% \n    filter(\n      abs(bord) &lt;= 10\n    )\n)\n\n# (3) BW = 20\nFS3 &lt;- update(\n  FS1,\n  data = BastenBetz %&gt;%\n    filter(\n      abs(bord) &lt;= 20\n    )\n)\n\n# (4) Ges. Datensatz\nFS4 &lt;- update(\n  object = FS1,\n  data = BastenBetz\n)\n\n\n# Tabellarische Darstellung\nmodelsummary(\n  list(\n    \"BW = OB\"= FS1, \n    \"BW = 10\" = FS2, \n    \"BW=20\" = FS3, \n    \"Ges. Datensatz\" = FS4\n  ), \n  vcov = \"HC1\", \n  stars = T, \n  gof_map = \"nobs\", \n  output = \"gt\"\n) %&gt;%\n  tabopts()\n\n\n\n\n\n\n\n\nBW = OB\nBW = 10\nBW=20\nGes. Datensatz\n\n\n\n(Intercept)\n0.134***\n0.100***\n0.103***\n0.109***\n\n\n\n(0.017)\n(0.013)\n(0.010)\n(0.009)\n\n\nvaud\n0.671***\n0.726***\n0.756***\n0.710***\n\n\n\n(0.034)\n(0.022)\n(0.018)\n(0.014)\n\n\nbord\n0.017**\n0.001\n0.001\n0.002*\n\n\n\n(0.006)\n(0.003)\n(0.001)\n(0.001)\n\n\nvaud × bord\n-0.006\n-0.001\n-0.009***\n-0.004***\n\n\n\n(0.012)\n(0.005)\n(0.003)\n(0.001)\n\n\nNum.Obs.\n133\n207\n312\n509\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\nTabelle 11.4: First-Stage-Regressionen\n\n\n\nFür die geschätze Bandweite schätzen wir einen hochsignifikanten Sprung in prot von etwa 67% an der Kantonsgrenze. Auch für größere Bandweiten von 10km und 20km sowie für den gesamten Datensatz finden wir vergleichbare signifikante Effekte, was eine bei zunehmender Distanz zur Grenze persistente Diskrepanz der Religionszugehörigkeit bestätigt.\n\n11.4.4 Second-Stage-Ergebnisse\nWir schätzen nun den LATE von Protestantismus für die Outcome-Variablen gini, pfl, pfi und pfr, vgl. Tabelle 11.2. Die Spezifikation für die Second-Stage-Regression der FRDD-Schätzung ist \\[\\begin{align}\n  \\begin{split}\n    Y_i = \\gamma_0 + \\gamma_1 \\widehat{prot}_i +  \\gamma_2 bord_i + \\gamma_3 bord_i  \\times vaud_i + e_i\n  \\end{split},\n\\end{align}\\] wobei \\(\\widehat{prot}_i\\) angepasste Werte aus der KQ-Schätzung von \\(\\eqref{eq:BBFSR}\\) mit Bandweite OB sind. Dazu erzeugen wir zunächst eine angepasste Version des Objekts BastenBetz, welche nur Gemeinden innerhalb der Bandweite enthält.\n\n# Gemeinden innerhalb der Bandweite filtern\nBastenBetz_OB &lt;- BastenBetz %&gt;% \n  filter(\n    abs(bord) &lt;= OB\n  )\n\nZur Illustration schätzen wir nun die Second-Stage-Regression für \\(Y = pfl\\).\n\n# Second-Stage-Regression für `pfl`\nBastenBetz_OB %&gt;% \n  mutate(\n    prot_fitted = fitted(FS1)\n    ) %&gt;%\n\nlm(\n  pfl ~ prot_fitted + bord + vaud:bord, \n  data = .\n) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = pfl ~ prot_fitted + bord + vaud:bord, data = .)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.8870  -3.8621  -0.0423   3.4993  12.1636 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  50.5275     1.9721  25.621  &lt; 2e-16 ***\nprot_fitted -13.4600     3.1749  -4.240 4.24e-05 ***\nbord          0.4380     0.6528   0.671    0.503    \nbord:vaud    -0.3636     0.7939  -0.458    0.648    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.433 on 129 degrees of freedom\nMultiple R-squared:  0.383, Adjusted R-squared:  0.3686 \nF-statistic: 26.69 on 3 and 129 DF,  p-value: 1.704e-13\n\n\nDer Koeffizient prot_fitted ist der gesuchte Behandlungseffekt. Beachte, dass die von summary() berechneten Standardfehler ungültig sind, weil diese die zusätzliche Unsicherheit durch die Berechnung von \\(\\widehat{prot}\\) über die First-Stage-Regression nicht berücksichtigen. Nachfolgend nutzen wir AER::ivreg(), um komfortabel gültige (heteroskedastie-robuste) Inferenz betreiben zu können.7\n7 Die Autoren geben an, robuste SEs zu nutzen. Das scheint nicht der Fall zu sein, denn vcov = \"HC0\" liefert die Ergebinsse im Paper. Die von Stata berechneten HC1-SEs weichen ab. Dies ändert allerdings nichts an der Signifikanz der Koeffizienten. Wir nutzen vcov = \"HC1\".\n# Schätzung mit 2SlS\n# s. Tabelle 4 in Basten und Betz (2013)\n#\n# Wir instrumentieren Treatment (`prot1980s`) mit dem Schwellenindikator (`vaud`)\n# ivreg: exogene Variablen instrumentieren sich selbst, daher\n# ' | vaud * borderdis '\nlibrary(AER)\n# (1) Präferenz für Freizeit\nSS_pfl &lt;- ivreg(\n  formula = pfl ~ prot + bord:vaud + bord | vaud * bord,\n  data = BastenBetz_OB\n)\n\n# (2) Präferenz für Umverteilung\nSS_pfr &lt;- update(\n  object = SS_pfl,\n  formula = pfr ~ prot + bord:vaud + bord | vaud * bord,\n)\n\n# (3) Präferenz für Intervention\nSS_pfi &lt;- update(\n  object = SS_pfl,\n  formula = pfi ~ prot + bord:vaud + bord | vaud * bord,\n)\n\n# (4) Einkommensungleichheit\nSS_gini &lt;- update(\n  object = SS_pfl,\n  formula = pfi ~ prot + bord:vaud + bord | vaud * bord,\n)\n\n\n# Tabellarische Darstellung\nmodelsummary(\n  list(\n    \"(1) Freizeit\"= SS_pfl, \n    \"(2) Umverteilung\" = SS_pfr, \n    \"(3) Intervention\" = SS_pfi, \n    \"(4) Ungleichheit\" = SS_gini\n  ), \n  vcov = \"HC1\", \n  stars = T, \n  gof_map = \"nobs\", \n  output = \"gt\"\n) %&gt;%\n  tabopts()\n\n\n\n\n\n\n\n\n(1) Freizeit\n(2) Umverteilung\n(3) Intervention\n(4) Ungleichheit\n\n\n\n(Intercept)\n50.528***\n44.560***\n52.871***\n52.871***\n\n\n\n(1.918)\n(0.950)\n(1.063)\n(1.063)\n\n\nprot\n-13.460***\n-5.061*\n-6.487***\n-6.487***\n\n\n\n(3.161)\n(2.161)\n(1.738)\n(1.738)\n\n\nbord\n0.438\n0.444\n-0.165\n-0.165\n\n\n\n(0.639)\n(0.357)\n(0.332)\n(0.332)\n\n\nbord × vaud\n-0.364\n-0.909\n0.011\n0.011\n\n\n\n(0.811)\n(0.561)\n(0.432)\n(0.432)\n\n\nNum.Obs.\n133\n133\n133\n133\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\nTabelle 11.5: Ergebnisse der Second-Stage-Regressionen\n\n\n\nDie Koeffizienten von prot in Tabelle 11.5 sind die mit 2SLS ermittelten erwarteten Behandlungseffekte einer 100%-Reformation (d.h. von 100% katholisch zu 100% protestantisch) für eine durchschnittliche Gemeine nahe der Kantonsgrnze. Es handelt sich jeweils um einen lokalen durchschnittlichen Behandlungseffekt (LATE). Gem. der Definition der abhängigen Variablen, interpretieren wir die Koeffizienten von prot in de Regressionen (1), (2) und (3) als erwartete Prozentänderung durch Reformation. Der Koeffizient in Regression (4) gibt die erwartete Änderung des Gini-Index an. Sämtliche geschätzte Effekte sind signifikant und haben ein mit der Hypothese der Autoren konsistentes negatives Vorzeichen.\nDie Ergebnisse sind Evidenz, dass Protestantismus zu verringerter Präferenz für Freizeit, Umverteilung sowie wirtschaftspolitische Intervention seitens des Staats führt. Auch die ökonomische Ungleichheit ist signifikant geringer, als in einer durchschnittlichen vollständig katholischen Gemeinde.\n\n11.4.5 Addendum: FRDD-Schätzung mit rdrobust()\n\nDie Funktion rdrobust::rdrobust() erlaubt die Schätzung von SRDD und FRDD mit einer Vielzahl von Optionen, s. ?rdrobust. Dies erleichtert die Schätzung mehrerer Modellspezifikationenen und Bandweiten. Mit dem nachstehenden Befehl schätzen wir den LATE von Reformation auf die Präferenz für Umverteilung anhand lokaler quadratischer Regression. Der Output gibt einen Überblick der Bandweitenschätzung sowie der 2 Stufen des 2SLS-Schätzers, inkl. robuster Inferenzstatistiken.\n\npfr_rdr &lt;- rdrobust(\n  y = BastenBetz$pfr,\n  x = BastenBetz$bord,\n  fuzzy = BastenBetz$prot, \n  p = 2,\n  kernel = \"uniform\",\n  vce = \"HC1\"\n) \n\npfr_rdr %&gt;% \n  summary()\n\nFuzzy RD estimates using local polynomial regression.\n\nNumber of Obs.                  509\nBW type                       mserd\nKernel                      Uniform\nVCE method                      HC1\n\nNumber of Obs.                  127          382\nEff. Number of Obs.              85          131\nOrder est. (p)                    2            2\nOrder bias  (q)                   3            3\nBW est. (h)                  10.796       10.796\nBW bias (b)                  22.271       22.271\nrho (h/b)                     0.485        0.485\nUnique Obs.                      97          261\n\nFirst-stage estimates.\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     0.701     0.039    17.782     0.000     [0.624 , 0.778]     \n        Robust         -         -    15.837     0.000     [0.599 , 0.768]     \n=============================================================================\n\nTreatment effect estimates.\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional    -5.047     2.254    -2.239     0.025    [-9.464 , -0.629]    \n        Robust         -         -    -2.210     0.027   [-10.114 , -0.607]    \n=============================================================================\n\n\nAuch für die quadratische Spezifikation erhalten wir mit -5.047 ein vergleichbares signifikantes Ergebnis für den LATE von Protestantismus auf Umverteilung, vgl. Spalte (2) in Tabelle 11.5.\nMit der Option bwselect = \"msetwo\" kann die Bandweite jeweils für die lokale Regression links- und rechtssetig des Schwellenwerts geschätzt werden.\n\npfr_rdr %&gt;% \n  update(bwselect = \"msetwo\") %&gt;%\n  summary()\n\nFuzzy RD estimates using local polynomial regression.\n\nNumber of Obs.                  509\nBW type                      msetwo\nKernel                      Uniform\nVCE method                      HC1\n\nNumber of Obs.                  127          382\nEff. Number of Obs.              51          134\nOrder est. (p)                    2            2\nOrder bias  (q)                   3            3\nBW est. (h)                   5.340       11.387\nBW bias (b)                  13.917       22.330\nrho (h/b)                     0.384        0.510\nUnique Obs.                      97          261\n\nFirst-stage estimates.\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     0.649     0.046    14.216     0.000     [0.560 , 0.739]     \n        Robust         -         -    11.970     0.000     [0.534 , 0.743]     \n=============================================================================\n\nTreatment effect estimates.\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional    -7.487     3.378    -2.216     0.027   [-14.109 , -0.866]    \n        Robust         -         -    -2.156     0.031   [-14.750 , -0.704]    \n=============================================================================\n\n\nTrotz Diskrepanz der geschätzten Bandweiten erhalten wir eine größere aber vergleichbare Schätzung für einen negativen Effekt.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasten, Christoph, und Frank Betz. 2013. „Beyond work ethic: Religion, individual, and political preferences“. American Economic Journal: Economic Policy 5 (3): 67–91.\n\n\nCattaneo, Matias D, Michael Jansson, und Xinwei Ma. 2020. „Simple local polynomial density estimators“. Journal of the American Statistical Association 115 (531): 1449–55.\n\n\nGelman, Andrew, und Guido Imbens. 2019. „Why high-order polynomials should not be used in regression discontinuity designs“. Journal of Business & Economic Statistics 37 (3): 447–56.\n\n\nImbens, G. W., und Thomas Lemieux. 2008. „Regression discontinuity designs: A guide to practice“. Journal of econometrics 142 (2): 615–35.\n\n\nImbens, Guido, und Karthik Kalyanaraman. 2012. „Optimal bandwidth choice for the regression discontinuity estimator“. The Review of economic studies 79 (3): 933–59.\n\n\nLee, David S. 2008. „Randomized experiments from non-random selection in US House elections“. Journal of Econometrics 142 (2): 675–97.\n\n\nMcCrary, Justin. 2008. „Manipulation of the running variable in the regression discontinuity design: A density test“. Journal of Econometrics 142 (2): 698–714.\n\n\nWeber, Max. 2004. Die protestantische Ethik und der Geist des Kapitalismus. Bd. 1614. CH Beck.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Regression Discontiniuty Designs</span>"
    ]
  },
  {
    "objectID": "SyntheticControl.html",
    "href": "SyntheticControl.html",
    "title": "\n12  Synthetic Control\n",
    "section": "",
    "text": "12.1 Schätzung von Interventionseffekten mit SCM\nÄhnlich wie bei manchen Matching-Methoden wird bei SCM die Ähnlichkeit der synthetischen Einheit mit der untersuchten Einheit durch eine gewichtete Kombination von Kontrolleinheiten basierend auf ihren Prä-Interventionsmerkmalen erreicht. Seien \\(i = 1, 2, \\ldots, N\\) die Einheiten in der Stichprobe, wobei \\(i = 1\\) die behandelte Einheit und \\(i = 2, \\ldots, N\\) potenziellen Kontrolleinheiten (auch Donor pool genannt) sind. Die Daten liegen für die Perioden \\(t = 1, 2, \\ldots, T\\) vor, mit \\(T_0\\) dem Zeitpunkt direkt vor der Intervention und \\(T_1, \\ldots, T\\) den Perioden nach der Intervention.\nFür SCM bestimmen wir einen Vektor von Gewichten \\(\\mathbf{w}^* := (w_2^*, \\ldots, w_k^*)^T\\), der die Summe der quadrierten Differenzen zwischen den Ausprägungen von \\(k\\) Charakteristika der behandelten Einheit vor der Intervention, \\(X_{1,\\,m}^{\\text{Pre}}\\), \\(m=1,\\dots,k\\), und der gewichteten Summe dieser Charakteristika für die Kontrolleinheiten, \\(X_{i,\\,m}^{\\text{Pre}}\\), minimiert:\nunter der Nebenbedingung, dass \\(\\sum_{i=2}^{N} w_i = 1\\) und \\(w_i \\geq 0\\) für alle \\(i\\). Die \\(v_m\\) sind weitere Gewichte, welche die Relevanz der Variablen für die Vorhersage der Outcome-Variable der interessierenden Einheit, \\(Y_{1,\\,t}\\), beinflussen. Diese Gewichte werden meist in einem weiteren Optimierungsverfahren (bspw. mit Cross-Validation) bestimmt (vgl. A. Abadie, Diamond, und Hainmueller 2014). Als Verlustfunktion hierbei wird meist der mittlere quadratische Fehler bei der Vorhersage von \\(Y_{1,\\,t}\\) (MSPE)1 vor der Behandlung anhand der synthetischen Einheit verwendet,\n\\[\\begin{align}\n  \\sum_{t=1}^{T_0} \\left( Y_{1,\\,t} - \\sum_{i=2}^N w_i(\\mathbf{v}) Y_{i,\\,t} \\right)^2, \\label{eq:scopt2}\n\\end{align}\\] mit \\(\\mathbf{v} := (v_1,\\dots,v_k)'\\).\nDurch die Lösung des Optimierungsproblems \\(\\eqref{eq:scopt}\\) unter Berücksichtigung von \\(\\eqref{eq:scopt2}\\) erhalten wir die geschätzten Gewichte \\(\\widehat{w}_i\\), welche den Einfluss der Kontrolleinheit \\(i=2,\\dots,N\\)-ten bei der Zusammensetzung der Kontrollgruppe festlegen. Anhand der \\(\\widehat{w}_i\\) wird die Outcome-Variable der synthetischen Kontrolleinheit konstruiert, welche als Referenz für die Schätzung des kausalen Effekts der Intervention dient. Die Outcome-Variable der synthetischen Kontrollgruppe für die Nach-Interventionsperiode kann formal ausgedrückt werden als\nwobei \\(Y_{1,t}^{\\text{Synth}}\\) der Wert der Outcome-Variable \\(Y\\) für die synthetische Kontrollgruppe zum Zeitpunkt \\(t\\) und \\(Y_{i,t}\\) der entsprechende Wert des Outcomes für die \\(i\\)-te Kontrolleinheit ist. Bei SCM schätzen wir den kausalen Effekt \\(\\tau_t\\) der Intervention zum Zeitpunkt \\(t\\) als die Differenz der Post-Interventionswerte von \\(Y\\) zwischen der behandelten Einheit und dem synthetischen Doppelgänger,\n\\[\n\\widehat{\\tau}_t = Y_{1,\\,t} - Y_{1,\\,t}^{\\text{synth}},\\quad t &gt; T_0.\n\\]\nDer mit SCM geschätzte Effekt ermittelt also für \\(t &gt; T_0\\), wie sich die Intervention auf die behandelte Einheit ausgewirkt hat durch einen Vergleich mit der Situation, die eingetreten wäre, wenn die Einheit nicht behandelt worden wäre, repräsentiert durch die synthetische Kontrollgruppe.\nDer SCM-Schätzer von A. D. Abadie Alberto und Hainmueller (2010) ist im R-Paket Synth (Hainmueller, Diamond, und Abadie 2011) implementiert. Wir illustrieren die Methode nachfolgend mit einer empirischen Anwendung zu den Konsequenzen des Brexit auf die nachfolgende Entwicklung der britischen Volkswirtschaft.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Synthetic Control</span>"
    ]
  },
  {
    "objectID": "SyntheticControl.html#sec-siscm",
    "href": "SyntheticControl.html#sec-siscm",
    "title": "\n12  Synthetic Control\n",
    "section": "",
    "text": "\\[\\begin{align}\n  \\mathbf{w}^* := \\arg\\min_{\\mathbf{w}} \\sum_{m=1}^{k} v_m \\left( X_{1,\\,m}^{\\text{Pre}} - \\sum_{i=2}^{N} w_i X_{i,m}^{\\text{Pre}} \\right)^2,\\label{eq:scopt}\n\\end{align}\\]\n\n1 Engl. für Mean squared prediction error\n\n\\[\\begin{align}\n  Y_{1,\\,t}^{\\text{Synth}} = \\sum_{i=2}^{N} \\widehat{w}_i Y_{i,\\,t},\\quad t &gt; T_0,\\label{eq:dgkonst}\n\\end{align}\\]",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Synthetic Control</span>"
    ]
  },
  {
    "objectID": "SyntheticControl.html#case-study-ökonomische-kosten-des-brexit",
    "href": "SyntheticControl.html#case-study-ökonomische-kosten-des-brexit",
    "title": "\n12  Synthetic Control\n",
    "section": "\n12.2 Case Study: Ökonomische Kosten des Brexit",
    "text": "12.2 Case Study: Ökonomische Kosten des Brexit\nBorn u. a. (2019) untersuchen die ökonomischen Kosten des Brexits mit einem kausalanalytischen Forschungsansatz. Der Kern der empirischen Analyse ist eine Kombination von quasi-experimenteller Identifikation und struktureller Zeitreihenanalyse. Hiermit können nicht nur die aggregierten Kosten des EU-Ausstiegs für Großbrittanien zu quantifiziert, sondern auch die Kanäle indentifiziert werden, durch die die erwartete wirtschaftliche Desintegration die britische Makroökonomie beeinflusst hat. Hierbei identifizieren Born u. a. (2019) einen Anstieg der wirtschaftspolitischen Unsicherheit und eine Abwärtskorrektur der Wachstumserwartungen als Haupttreiber für den Rückgang der Wirtschaftsleistung.\nDer quasi-experimentelle Ansatz betrachtet das Brexit-Referendum als ein natürliches makroökonomisches Experiment und untersucht die Konsequenzn der wirtschaftlichen Desintegration für das Bruttoinlandsprodukt (BIP) im Nachfolgezeitraum mit SCM. Hierzu wird gemäß der in Kapitel 12.1 erläuterten Vorgehensweise ein syntetischer Doppelgänger für die britische Wirtschaft aus einem Donor Pool von 23 Volkswirtschaften konstruiert, und der Effekt des Referendums als Unterschied zwischen der tatsächlichen und synthetischen Trajektorien des BIP für Folgeperioden ermittelt. Die Analyse zeigt, dass das Brexit-Votum bis Ende 2018 zu einem BIP-Rückgang von etwa 1.7% bis 2.5% geführt hat.\nWie reproduzieren nun die wesentlichen Ergebnisse des SCM-Ansatzes der Studie mit R. Hierfür lesen zunächst den Datensatz brexit.csv (hier verfügbar) in R ein. Dieser enthält vierteljährliche Beobachtungen makroökonomischer Variablen für 24 Länder für den Zeitraum 1995-Q1–2021-Q4.\n\nlibrary(readr)\nlibrary(dplyr)\n\n# Datensatz 'brexit.csv' einlesen\nbrexit &lt;- read_csv(\"datasets/brexit.csv\") %&gt;%\n  as.data.frame()\n\nbrexit ist ein Datensatz mit einer Panel-Struktur. Die Zeit- und Entitätsvariablen sind Year/quarter und Country/ID. Beachte, dass die Variable Time zusätzlich das Jahr und das Quartal als numerische Variable angibt.\n\n# Überblick über 'brexit'\nglimpse(brexit)\n\nRows: 2,496\nColumns: 21\n$ Time          &lt;dbl&gt; 1995.00, 1995.00, 1995.00, 1995.00, 1995.00, 1995.00, 19…\n$ Year          &lt;dbl&gt; 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 19…\n$ quarter       &lt;chr&gt; \"Q1\", \"Q1\", \"Q1\", \"Q1\", \"Q1\", \"Q1\", \"Q1\", \"Q1\", \"Q1\", \"Q…\n$ Country       &lt;chr&gt; \"Australia\", \"Austria\", \"Belgium\", \"Canada\", \"Finland\", …\n$ real_con_raw  &lt;dbl&gt; 4.660970e+11, 1.214713e+11, 1.608240e+11, 5.558142e+11, …\n$ real_inv_raw  &lt;dbl&gt; 1.683258e+11, 5.537994e+10, 6.118800e+10, 1.894910e+11, …\n$ real_exp_raw  &lt;dbl&gt; 1.246639e+11, 6.634091e+10, 1.501160e+11, 3.353030e+11, …\n$ real_imp_raw  &lt;dbl&gt; 9.781032e+10, 7.439282e+10, 1.468880e+11, 2.572610e+11, …\n$ real_gdp_raw  &lt;dbl&gt; 8.495864e+11, 2.165699e+11, 2.910360e+11, 1.084659e+12, …\n$ real_gdp_2016 &lt;dbl&gt; 0.5080841, 0.6832146, 0.6877292, 0.6057397, 0.6370480, 0…\n$ tot_emp_raw   &lt;dbl&gt; 8077377.2, 3737003.3, 3920400.0, 13274100.0, 2050262.7, …\n$ pop_quarterly &lt;dbl&gt; 13144039.8, 6043266.6, 7666495.5, 21714093.3, 3827395.7,…\n$ lab_prod      &lt;dbl&gt; 0.9988637, 0.9863141, 0.9942753, 0.9997992, 0.9885239, 1…\n$ ConGDP        &lt;dbl&gt; 0.5486164, 0.5608871, 0.5525914, 0.5124322, 0.5244422, 0…\n$ InvGDP        &lt;dbl&gt; 0.1981267, 0.2557140, 0.2102420, 0.1747010, 0.2085606, 0…\n$ ExpGDP        &lt;dbl&gt; 0.14673485, 0.30632565, 0.51579873, 0.30913218, 0.268451…\n$ ImpGDP        &lt;dbl&gt; 0.1151270, 0.3435049, 0.5047073, 0.2371815, 0.2485870, 0…\n$ LPG           &lt;dbl&gt; -0.0078972729, -0.0093478618, 0.0033636783, 0.0054461911…\n$ EmpSha        &lt;dbl&gt; 0.6145277, 0.6183747, 0.5113679, 0.6113127, 0.5356809, 0…\n$ gdp           &lt;dbl&gt; -49.19159, -31.67854, -31.22708, -39.42603, -36.29520, -…\n$ ID            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n\n# 'Time' zeigt Jahr + Quartal\nbrexit %&gt;% \n  filter(Country == \"United Kingdom\") %&gt;% \n  select(Time) %&gt;%\n  slice_head(n = 5)\n\n     Time\n1 1995.00\n2 1995.25\n3 1995.50\n4 1995.75\n5 1996.00\n\n\nFür die Schätzung der Gewichte \\(w_i\\) für die Konstruktion des UK-Doppelgängers werden die in gelisteten Charakteristika der Volkswirtschaften verwendet.\n\n\n\n\nVariable\nDefinition\n\n\n\ngdp\nVeränderung des BIP relativ zu 2016\n\n\nConGDP\nAnteil: Konsum/BIP (%)\n\n\nInvGDP\nAnteil: Investitionen/BIP (%)\n\n\nExpGDP\nAnteil: Exporte/BIP (%)\n\n\nImpGDP\nAnteil: Importe/BIP (%)\n\n\nEmpSha\nAnteil: Beschäftigte/Erwerbsbevölkerung (%)\n\n\nLPG\nWachstum der Arbeitsproduktivität (%)\n\n\n\n\n\nTabelle 12.1: brexit – Variablen und Definitionen\n\n\nZur Berechnung von SCM mit dem R-Paket Synth müssen die Daten zunächst mit der Funktion Synth::dataprep() aufbereitet werden, s. ?Synth::dataprep() für weitere Details. Neben dem Datensatz (foo) unter expliziter Nennung der Prädiktoren (predictors) und der Outcome-Variable (dependent) übergeben wir Variablen für die Indentifikation von Einheiten (ID) und Zeitpunkten (Time), sowie Donor Pool (controls.identifier) und behandelter Einheit (treatment.identifier). Weiterhin werden die Vorbehandlungsperiode (time.predictors.prior) sowie der Zeitraum über den die Regressor-Gewichte \\(v_m\\) bestimmt werden sollen (time.optimize.ssr), festgelegt. Für letztere übergeben wir einen numerischen Vektor für sämtliche Zeitpunkte von 1995-Q1 bis zum Brexit-Referendum in 2016-Q2.\nUm einen ersten Überblick über die Entwicklung der BIP im Datensatz zu gewinnen, vergleichen wir die Zeitreihen für Donor-Pool-Länder (grau) und Großbritannien (blau) mit ggplot.\n\nlibrary(cowplot)\n\nbrexit %&gt;%\n  mutate(\n    group = ifelse(\n      Country == \"United Kingdom\", \n      yes = \"UK\", \n      no =\"else\"\n    )\n  ) %&gt;%\n  \n  ggplot(\n    mapping = aes(\n      x = Time, \n      y = gdp, \n      color = group, \n      group = Country, \n      lwd = group\n    )\n  ) +\n  scale_color_manual(\n    values = c(\n      \"UK\" = \"#00BFC4\", \"else\" = alpha(\"gray\", .75)\n    )\n  ) +\n  scale_linewidth_manual(\n    values = c(\"UK\" = 1, \"else\" = .5)\n  ) +\n  geom_line() +\n  # Brexit-Referendum\n  geom_vline(\n    xintercept = 2016.25, \n    lty = \"dotted\"\n  ) +\n  theme_cowplot() +\n  guides(\n    lwd = \"none\", \n    color = guide_legend(position = \"inside\")\n  ) +\n  theme(legend.position.inside = c(.1, .9))\n\n\n\n\n\n\nAbbildung 12.1: BIP relativ zu 2016\n\n\n\n\nAbbildung 12.1 zeigt, dass das BIP von Großbritannien zwar auch nach dem Brexit-Referendum (gepunktete Linie) gewachsen ist, jedoch vergleichsweise schwach. Eine Analyse mit SCM kann statistische Evidenz für den mutmaßlich negativen Effekt des Referendums auf das Wachstum in den Folgeperioden liefern.\nWir laden nun das Paket Synth und bereiten die Daten für die Analyse vor.\n\n# R-Paket 'Synth' laden\nlibrary(Synth)\n\n# Daten für die Optimierung vorbereiten\ndataprep_out &lt;- dataprep(\n  foo = brexit, \n  predictors = c(\n    \"ConGDP\", \"InvGDP\", \n    \"ExpGDP\", \"ImpGDP\", \n    \"LPG\", \"EmpSha\"\n  ), \n  dependent = \"gdp\", \n  unit.variable = \"ID\",\n  time.variable = \"Time\", \n  treatment.identifier = 23, \n  controls.identifier = (brexit$ID %&gt;% unique())[-23], \n  time.predictors.prior = seq(1995, 2016.25, .25),\n  time.optimize.ssr = seq(1995, 2016.25, .25),\n  unit.names.variable = \"Country\"\n)\n\nAnhand der vorbereiteten Daten dataprep_out wird nun die Bestimmung der Gewichte mit Synth::synth() durchgeführt.\n\n# Gewichte per Optimierung bestimmen\nsynth_out &lt;- synth(dataprep_out)\n\n\nX1, X0, Z1, Z0 all come directly from dataprep object.\n\n\n**************** \n searching for synthetic control unit  \n \n\n**************** \n**************** \n**************** \n\nMSPE (LOSS V): 0.6083746 \n\nsolution.v:\n 0.1488472 0.08840361 0.1480946 0.153318 0.1815462 0.2797904 \n\nsolution.w:\n 1.42199e-05 4.00616e-05 8.47284e-05 0.00014488 4.45754e-05 4.84009e-05 0.001608051 4.77577e-05 0.06654494 2.07094e-05 0.1446237 1.188e-05 2.377e-06 0.04837474 1.33542e-05 0.0001405933 4.61625e-05 0.0001592545 1.49993e-05 4.14211e-05 3.78112e-05 0.0002273924 0.737708 \n\n\nSynth::synth() gibt Infos über den Optimierungsprozess und dessen Ergebnisse automatisch in der Konsole aus. Wir können diese mit Synth::synth.tab() leicht tabellarisch zusammenfassen und mit gt::gt() darstellen.\n\n# Zusammenfassung der Ergebnisse\n(\n  tb &lt;- synth.tab(\n    synth.res = synth_out, \n    dataprep.res = dataprep_out\n  )  \n)\n\n$tab.pred\n       Treated Synthetic Sample Mean\nConGDP   0.655     0.635       0.534\nInvGDP   0.168     0.202       0.226\nExpGDP   0.254     0.219       0.454\nImpGDP   0.256     0.232       0.423\nLPG      0.003     0.003       0.003\nEmpSha   0.634     0.625       0.611\n\n$tab.v\n       v.weights\nConGDP 0.149    \nInvGDP 0.088    \nExpGDP 0.148    \nImpGDP 0.153    \nLPG    0.182    \nEmpSha 0.28     \n\n$tab.w\n   w.weights      unit.names unit.numbers\n1      0.000       Australia            1\n2      0.000         Austria            2\n3      0.000         Belgium            3\n4      0.000          Canada            4\n5      0.000         Finland            5\n6      0.000          France            6\n7      0.002         Germany            7\n8      0.000         Hungary            8\n9      0.067         Iceland            9\n10     0.000         Ireland           10\n11     0.145           Italy           11\n12     0.000           Japan           12\n13     0.000           Korea           13\n14     0.048      Luxembourg           14\n15     0.000     Netherlands           15\n16     0.000     New Zealand           16\n17     0.000          Norway           17\n18     0.000        Portugal           18\n19     0.000 Slovak Republic           19\n20     0.000           Spain           20\n21     0.000          Sweden           21\n22     0.000     Switzerland           22\n24     0.738   United States           24\n\n$tab.loss\n       Loss W    Loss V\n[1,] 0.135733 0.6083746\n\n\nFür die tabellarische Darstellung mit gt::gt() berücksichtigen wir lediglich Volkswirtschaften mit Gewicht &gt; .0001.\n\n# Darstellung mit gt()\ntb$tab.w %&gt;% \n  # Berücksichtige nur Länder mit relevanten Gewichten\n  filter(w.weights &gt; .0001) %&gt;% \n  arrange(desc(w.weights)) %&gt;% \n  gt::gt() %&gt;%\n  tabopts\n\n\n\n\n\n\n\nw.weights\nunit.names\nunit.numbers\n\n\n\n0.738\nUnited States\n24\n\n\n0.145\nItaly\n11\n\n\n0.067\nIceland\n9\n\n\n0.048\nLuxembourg\n14\n\n\n0.002\nGermany\n7\n\n\n\n\n\n\n\nTabelle 12.2: Gewichte für den synthetischen UK-Doppelgänger\n\n\n\nDer synthetische UK-Doppelgänger kann nun gemäß der Vorschrift \\(\\eqref{eq:dgkonst}\\) konstruiert werden. Wir erzeugen hierzu ein tibble-Objekt mit den entsprechenden ID-Variablen.\n\n# Doppelgänger konstruieren\ndoppelganger &lt;- left_join(\n  x = brexit, \n  y = tb$tab.w, \n  by = c(\"Country\" = \"unit.names\")\n) %&gt;% \n  select(Time, Year, Country, gdp, w.weights) %&gt;%\n  group_by(Time, Year) %&gt;%\n  summarise(\n    gdp = sum(gdp * w.weights, na.rm = T)\n  ) %&gt;%\n  mutate(type = \"Doppelgaenger\") %&gt;%\n  ungroup()\n\nglimpse(doppelganger)\n\nRows: 104\nColumns: 4\n$ Time &lt;dbl&gt; 1995.00, 1995.25, 1995.50, 1995.75, 1996.00, 1996.25, 1996.50, 19…\n$ Year &lt;dbl&gt; 1995, 1995, 1995, 1995, 1996, 1996, 1996, 1996, 1997, 1997, 1997,…\n$ gdp  &lt;dbl&gt; -36.87991, -36.70512, -36.32948, -35.72637, -35.34392, -34.57688,…\n$ type &lt;chr&gt; \"Doppelgaenger\", \"Doppelgaenger\", \"Doppelgaenger\", \"Doppelgaenger…\n\n\nFür die nachfolgenden Schritte der Analyse führen wir das beobachtete GDP für Großbritannien mit dem syntethischen GDP des Doppelgängers zusammen.\n\n# tibble mit UK-GDP erstellen\nUK &lt;- brexit %&gt;% \n  filter(Country == \"United Kingdom\") %&gt;% \n  select(Time, Year, gdp) %&gt;%\n  mutate(type = \"UK\")\n\n# UK und Doppelgänger zusammenführen\nthe_gdps &lt;- bind_rows(\n  doppelganger, UK\n)\n\nFür einen Vergleich von UK- und Doppelgänger-BIP folgen wir Born u. a. (2019) und berechnen die Differenz der BIP über den gesamten Zeitraum, die so genannte Doppelgänger-Gap.\n\n# UK-Doppelgänger-Gap berechnen\ngdp_gap &lt;- the_gdps %&gt;% \n  pivot_wider(\n    values_from = gdp, \n    names_from = \"type\"\n  ) %&gt;%\n  mutate(gdp_gap = UK - Doppelgaenger)\n\nAls ein Maß für die Unsicherheit bei der Schätzung des GDPs für den Doppelgänger berechnen Born u. a. (2019) die Standardabweichung der Doppelgänger-Gap für den Zeitraum vor dem Brexit-Referendum.\n\n# Standardabweichung der Gap vor dem Brexit-Vote\nsd_gap &lt;- gdp_gap %&gt;%\n  filter(Time &lt; 2016.25) %&gt;% \n  summarise(\n    sd = sd(gdp_gap)\n  ) %&gt;% \n  pull(sd)\n\nWir nutzen nun ggplot2::ggplot(), um den syntetischen Doppelgänger und das BIP für Großbritannien über den gesamten Zeitraum darzustellen. Für die Darstellung von Unsicherheit bei der Konstruktion des Doppelgängers unterlegen wir die Doppelgänger-Zeitreihe mit einer Schattierung in der Breite der geschätzten Standardabweichung von 0.78 für die Periode vor dem Referendum.\n\n(\n  p_gdp &lt;- ggplot() +\n    # 1-SD-Band um das Doppelgänger-GDP\n    geom_ribbon(\n      data = the_gdps %&gt;% \n        filter(type == \"Doppelgaenger\"), \n      mapping = aes(\n        x = Time, \n        ymin = gdp - sd_gap, \n        ymax = gdp + sd_gap\n      ), \n      fill = alpha(\"red\", alpha = .2), \n      color = \"white\"\n    ) +\n    # UK- und Doppelgänger-GDP\n    geom_line(\n      data = the_gdps, \n      mapping = aes(\n        x = Time, \n        y = gdp, \n        col = type\n      ),\n      lwd = 1\n    ) +\n    # Brexit-Referendum\n    geom_vline(\n      xintercept = 2016.25, \n      lty = \"dotted\"\n    ) +\n    scale_color_discrete(name = \"\") +\n    # Legende hinzufügen\n    cowplot::theme_cowplot() +\n    theme(legend.position = c(.025, .9))  \n)\n\n\n\n\n\n\nAbbildung 12.2: UK-BIP und synthetischer Doppelgänger\n\n\n\n\nAbbildung 12.2 zeigt, dass der synthetische Doppelgänger über weite Teile der Vorperiode eine gute Anpassung an das beobachtete BIP von Großbritannien aufweist, insbesondere für den Zeitraum unmittelbar vor dem Brexit-Referendum. Nach dem Referendum zeigt sich bereits nach wenigen Quartalen eine deutliche Abweichung zwischen der geschätzten und der beobachteten Trajektorie. Eine Beschränkung der in p_gdp verwendeten Datenpunkte auf einen Bereich nahe des Referendums bestärkt diese Schlussfolgerung.\n\n# Close-up im Bereich des Referendums\np_gdp +\n  scale_x_continuous(\n    limits = c(2015, 2021), \n    expand = c(0, .1)\n    ) +\n  scale_y_continuous(limits = c(-4, 12))\n\n\n\n\n\n\nAbbildung 12.3: UK-BIP und synthetischer Doppelgänger – Close-Up\n\n\n\n\nIn Abbildung 12.2 ist eine ab Mitte 2017 außerhalb des Standardabweichungsbereichs verlaufende Divergenz der Zeitreihen zu erkennen. Diese stellen wir nachfolgend anhand der Doppelgänger-Gap mit ggplot2::ggplot() dar.\n\n# BIP-Doppelgänger-Gap\nggplot(data = gdp_gap) +\n  geom_hline(yintercept = 0) +\n  geom_line(\n    mapping = aes(x = Time, y = gdp_gap),\n    lwd = 1\n  ) + \n  geom_ribbon(\n    mapping = aes(\n      x = Time, \n      ymin = gdp_gap - sd_gap, \n      ymax = gdp_gap + sd_gap\n    ), \n    fill = alpha(\"darkgray\", alpha = .2), \n    color = \"white\"\n  ) +\n  # Referendum\n  geom_vline(\n    xintercept = 2016.25,\n    lty = \"dotted\"\n  ) +\n  scale_x_continuous(\n    expand = c(0, .1), \n    limits = c(2015, 2021)\n  ) +\n  scale_y_continuous(limits = c(-6, 1.5)) +\n  cowplot::theme_cowplot()\n\n\n\n\n\n\nAbbildung 12.4: UK-BIP und synthetischer Doppelgänger – Doppelgänger-Gap\n\n\n\n\nDie in Abbildung 12.4 gezeigte Doppelgänger-Gap stimmt gut mit dem von Born u. a. (2019) geschätzten verlorenen Wachstums des BIP relativ zu 2016 um bis zu 2.5% bis Ende des Jahres 2018 überein.\nAls weiteres Maß für den Effekt des Referendums im Folgezeitraum können wir die mittlere Doppelgänger-Gap für sämtliche Beobachtungsperioden nach dem Brexit-Referendum schnell bestimmen.\n\n# Mittlerer Unterschied nach dem Brexit-Referendum\ngdp_gap %&gt;% \n  filter(Time &gt; 2016.25) %&gt;% \n  pull(gdp_gap) %&gt;% \n  mean()\n\n[1] -2.343273\n\n\n\n12.2.1 Placebo-Tests: Grafische Inferenz\nAuch für SCM sind Placebo-Tests ein hilfreiches Instrument zur Überprüfung der Gültigkeit von Studienergebnissen. Eine gründliche Placebo-Analyse kann festzustellen, ob der beobachtete Effekt tatsächlich auf die Intervention zurückzuführen ist und nicht auf unberücksichtigte (möglicherweise unbeobachtbare) Faktoren.\nEin Ansatz ist hierfür ist es, den synthetische-Doppelgänger für fiktive Interventionszeitpunkte vor dem tatsächlichen Behandlungszeitpunkt zu konstruieren, und die entsprechenden Trajektorien mit dem ursprünglichen Doppelgänger zu vergleichen. So kann die Validität der ursprünglichen Doppelgänger-Trajektorie im Hinblick auf mögliche anderweitige Ereignisse vor der Intervention geprüft werden: Doppelgänger-Trajektorien für fiktive, frühere Interventionen sollten sich nicht systematisch von der andhand von Daten bis zur tatsächlichen Intervention berechneten Trajektorie unterscheiden.\nWir definieren hierzu eine Funktion placebo(), die einen syntethischen Doppelgänger des BIP Großbritanniens mit Gewichten auf Basis eines vorgegebenen Interventionszeitpunktes (treat) zurückgibt. Abgesehen vom früheren Interventionszeitpunkt (und der damit einhergehenden verkleinerten Stichprobe) erfolgt die Berechnung der Gewichte mit derselben Spezifikation wie zuvor.\n\n# Funktion für Placebo-Doppelgänger:\n# Fiktive frühere Intervention\nplacebo &lt;- function(treat) {\n  \n  # Datenvorbereitung für fiktives Datum 'treat'\n  dataprep_out &lt;- dataprep(\n    foo = brexit, \n    predictors = c(\n      \"ConGDP\", \"InvGDP\",\n      \"ExpGDP\", \"ImpGDP\",\n      \"LPG\", \"EmpSha\"\n    ), \n    dependent = \"gdp\", \n    unit.variable = \"ID\",\n    time.variable = \"Time\", \n    treatment.identifier = 23, \n    controls.identifier = (brexit$ID %&gt;% unique())[-23], \n    time.predictors.prior = seq(1995, treat, .25),\n    time.optimize.ssr = seq(1995, treat, .25),\n    unit.names.variable = \"Country\"\n    )\n  \n  # Doppelgänger bestimmen\n  synth_out &lt;- quietly(synth)(dataprep_out)$result\n  \n  # Ergebnisse auslesen\n  tb &lt;- synth.tab(\n    synth.res = synth_out, \n    dataprep.res = dataprep_out\n    )\n  \n  return(\n    \n    # Doppelgänger konstruieren \n    left_join(\n      x = brexit, \n      y = tb$tab.w, \n      by = c(\"Country\" = \"unit.names\")\n    ) %&gt;% \n      select(Time, Country, gdp, w.weights) %&gt;%\n      group_by(Time) %&gt;%\n      summarise(\n        gdp = sum(gdp * w.weights, na.rm = T)\n      ) %&gt;%\n      mutate(type = paste0(\"Placebo\", treat))  \n    )\n  \n}\n\nWie in Born u. a. (2019) berechnen wir nun 12 Placebo-Doppelgänger des BIP von Großbritannien für fiktive Zeitpunkte eines Referendums über sämtliche Quartale im Zeitraum 2010-Q1 bis 2016-Q1. Dies ist komfortabel durch Iteration von placebo() über diese Zeitpunkte mit purrr::map_dfr() umsetzbar.\n\n# Iteration über fiktive frühere Referenden\nplacebos_tbl &lt;- map_dfr(\n  .x = seq(2010, 2016, .25), \n  .f =  \\(x) placebo(x) \n)\n\nplacebos_tbl ist ein tibble-Objekt im tidy-Format. Wir können die Placebo-Doppelgänger sowie den ursprünglich berechneten Doppelgänger und das tatsächliche BIP also ähnlich wie in Abbildung 12.2 mit ggplot2::ggplot() darstellen.\n\n# Vergleich mit Placebo-Doppelgänger\n(\n  p_UKDG &lt;- ggplot(\n    data = placebos_tbl,\n    mapping = aes(\n      x = Time, \n      y = gdp, \n      group = type\n    )\n  ) +\n    # Placebos (mit jitter)\n    geom_line(\n      lwd = .25, \n      col = \"gray80\",\n      position = position_jitter(height = .25)\n    ) +\n    # Ursprünglicher Doppelgänger\n    geom_line(\n      data = the_gdps %&gt;% \n        filter(type == \"Doppelgaenger\"), \n      mapping = aes(col = type), \n      lwd = 1\n    ) +\n    # Beobachtetes BIP\n    geom_line(\n      data = the_gdps %&gt;% \n        filter(type == \"UK\"), \n      mapping = aes(col = type), \n      lwd = 1\n    ) +\n    # Intikator für Referendum\n    geom_vline(xintercept = 2016.25, lty = \"dotted\") +\n    # Formatierung\n    cowplot::theme_cowplot() +\n    theme(legend.position = c(.05, .9))\n)\n\n\n\n\n\n\nAbbildung 12.5: Placebo-Doppelgänger\n\n\n\n\n\n# Close-up bei Referendum\np_UKDG +\n      scale_x_continuous(\n      limits = c(2015, 2021), expand = c(0, .05)\n    ) +\n      scale_y_continuous(\n      limits = c(-3, 13),  expand = c(0, 0)\n    )\n\n\n\n\n\n\nAbbildung 12.6: Placebo-Doppelgänger – Close-Up\n\n\n\n\nBeachte, dass position = position_jitter(height = .25) eine zufällige, kleine Verschiebung (jitter) der Trajektorien der Placebo-Doppelgänger für eine bessere Unterscheidbarkeit bewirkt. Abbildung 12.5 und Abbildung 12.6 zeigen, dass sich die Placebo-Pfade für fiktive frühere Referenden (grau) nicht systematisch vom ursprünglich berechneten synthetischen Doppelgänger (rot) unterscheiden. Insbesondere finden wir keinen Rückgang der synthetischen BIP relativ zum beobachteten BIP für Großbritannien vor dem Referendum. Deutliche Abweichungen vom tatsächlichen BIP ergeben sich erst jenseits der tatsächlichen Referendums. Diese Placebo-Analyse bekräftigt also die Validität der Konstruktion des “Benchmark-Doppelgängers” für die Periode bis 2016-Q2 und die Schätzung des kausalen Effekts des Referendums anhand der entsprechenden Doppelgänger-Gap.\nEin weiterer Placebo-Test in Born u. a. (2019) ist ein Vergleich der Doppelgänger-Gap Großbritanniens mit Doppelgänger-Gaps für fiktive Referenden in 2016-Q2 in Ländern mit wesentlichem Einfluss bei der Konstruktion des synthetischen Doppelgängers für Großbritannien: Die Schätzung des kausalen Effekts des Referendums auf das BIP in Großbritannien ist glaubwürdig, wenn lediglich die Doppelgänger-Gap für Großbritannien durch das Referendum beeinflusst wird, nicht aber die Doppelgänger-Gaps für Länder in der Kontrollgruppe.\nFür diese grafische Placebo-Analyse modifizieren wir die Funktion placebo() entsprechend. placebo_gap() berechnet die Doppelgänger-Gap für das mit treat identifizierte Land. Das if-Statement zu Beginn stellt sicher, dass Großbritannien nicht als Kontroll-Einheit für die Placebo-Gaps verwendet wird.\n\n# Funktion für Placebo-Gaps\nplacebo_gap &lt;- function(treat) {\n  \n  # Kontrollgruppe definieren\n  if(treat != 23) {\n    controls &lt;- (1:24)[-c(23, treat)]\n  } else {\n    controls &lt;- (1:24)[-23]\n  }\n  \n  # Daten vorbereiten\n  dataprep_out &lt;- dataprep(\n    foo = brexit, \n    predictors = c(\n      \"ConGDP\", \"InvGDP\",\n      \"ExpGDP\", \"ImpGDP\",\n      \"LPG\", \"EmpSha\"\n    ), \n    dependent = \"gdp\", \n    unit.variable = \"ID\",\n    time.variable = \"Time\", \n    treatment.identifier = treat, \n    controls.identifier = controls, \n    time.predictors.prior = seq(1995, 2016.25, .25),\n    time.optimize.ssr = seq(1995, 2016.25, .25),\n    unit.names.variable = \"Country\"\n  )\n  \n  # Gewichte bestimmen\n  synth_out &lt;- quietly(synth)(dataprep_out)$result\n  \n  # Ergebnisse zusammenfassen\n  tb &lt;- synth.tab(\n    synth.res = synth_out, \n    dataprep.res = dataprep_out\n  )\n  \n  # Doppelgänger bestimmen\n  doppel &lt;- left_join(\n    x = brexit, \n    y = tb$tab.w, \n    by = c(\"Country\" = \"unit.names\")\n  ) %&gt;% \n    select(Time, gdp, Country, w.weights) %&gt;%\n    group_by(Time) %&gt;%\n    summarise(\n      gdp_synth = sum(gdp * w.weights, na.rm = T), \n    )\n  \n  # Beobachtetes BIP auslesen\n  gdp &lt;- brexit %&gt;% filter(ID == treat) %&gt;% pull(gdp)\n  \n  return(\n    \n    # Doppelgänger-Gap berechnen\n    doppel %&gt;% \n      mutate(\n        ID = treat,\n        gdp = gdp,\n        gdp_gap = gdp - gdp_synth\n      )\n    \n  )\n  \n}\n\nFür die Berechnung der Placebo-Gaps iterieren wir über die Indizes der in Tabelle 12.2 gelisteten Volkswirtschaften der Kontrollgruppe für Großbritannien.\n\n# Indizes für \"Donor Countries\" und UK\ndonors_and_UK &lt;- brexit %&gt;% \n  select(ID, Country) %&gt;% \n  distinct() %&gt;%\n  filter(\n    Country %in% \n      c(\n        \"United States\", \"Italy\", \"Iceland\", \n        \"Luxembourg\", \"Germany\", \"United Kingdom\"\n      )\n  ) %&gt;%\n  pull(ID)\n\n\n# Placebo-Doppelgänger-Gaps berechnen\nplacebo_gaps_tbl &lt;- map_dfr(\n  .x = donors_and_UK, \n  .f =  \\(x) placebo_gap(x) \n)\n\nFür die grafische Darstellung ergänzen wir die Variable Country zur Unterscheidung der Doppelgänger-Gaps für Großbritannien und die Kontroll-Länder.\n\n# ID-Variable für UK und Kontroll-Länder\nplacebo_gaps_tbl &lt;- placebo_gaps_tbl %&gt;%\n  mutate(\n    Country = ifelse(ID == 23, \"UK\", \"else\")\n  )\n\nUm die Vergleichbarkeit der Doppelgänger-Gaps zu gewährleisten, standardisieren Born u. a. (2019) die Schätzungen der Gaps anhand der jeweiligen Mittelwerte für das Jahr 2015 und der Standardabweichungen im Zeitraum vor dem Brexit-Referendum. Wir berechnen diese Statistiken zunächst.\n\n# Mittelwerte für 2015\nmeans &lt;- placebo_gaps_tbl %&gt;% \n  group_by(ID) %&gt;% \n  filter(between(Time, 2015, 2015.75)) %&gt;% \n  summarise(\n    mean2015 = mean(gdp_gap)\n  )\n\n# Standardabweichungen vor Referendum\nsds &lt;- placebo_gaps_tbl %&gt;% \n  group_by(ID) %&gt;% \n  filter(Time &lt; 2016.25) %&gt;% \n  summarise(\n    thesd = sd(gdp_gap)\n  )\n\nMit dplyr::left_join() führen wir diese Statistiken mit placebo_gaps_tbl zusammen und berechnen die standardisierten Doppelgänger-Gaps.\n\n# Join + Standardisierung\nplacebo_gaps_std &lt;- \n  left_join(placebo_gaps_tbl, means) %&gt;% \n  left_join(sds) %&gt;%\n  mutate(gdp_gap_std = (gdp_gap - mean2015)/thesd)\n\nAnalog zum Code für Abbildung 12.4 plotten wir die Placebo-Gap-Zeitreihen mit ggplot2::ggplot().\n\n# Placebo-Gaps mit UK-Gap vergleichen\nggplot(\n  data = placebo_gaps_std,\n  mapping = aes(\n    x = Time, \n    y = gdp_gap_std,\n    group = ID,\n    lwd = Country,\n    color = Country\n  )\n) +\n  # Hilfslinie bei Differenz = 0\n  geom_hline(yintercept = 0) +\n  # Gaps\n  geom_line() +\n  # Referendum\n  geom_vline(xintercept = 2016.25, lty = \"dotted\") +\n  # Formatierung\n  scale_color_manual(\n    values = c(\"UK\" = \"steelblue\", \"else\" = alpha(\"darkgray\", .5))\n  ) +\n  scale_linewidth_manual(\n    values = c(\"UK\" = 1, \"else\" = .5)\n  ) +\n  scale_x_continuous(\n    limits = c(2015, 2021), expand = c(0, .05)\n  ) +\n  theme_cowplot() +\n  theme(legend.position = c(.05, .9))\n\n\n\n\n\n\nAbbildung 12.7: Placebo- und UK-Doppelgänger-Gaps\n\n\n\n\nAbbildung 12.7 zeigt die standardisierten Placebo-Doppelgänger-Gaps für ein fiktives Referendum zum Zeitpunkt 2016-Q2 in den 5 Kontroll-Volkswirtschaften, die für Konstruktion des BIP-Doppelgängers von Großbrittannien relevant sind (grau). Der Vergleich mit der standardisierten Doppelgänger-Gap für Großbritannien (blau). Der Verlauf der Placebo-Gaps zeigt an, dass keine Abweichungen mit negativem Trend von der Referenzlinie bei 0 (kein Unterschied zwischen beobachtetem und syntetischem BIP) nach dem Referendum vorliegen. Damit liefert die Grafik keine Hinweise auf einen Effekt fiktiver Interventionen in den Kontroll-Ländern. Für Großbritannien jedoch ist, ähnlich wie in Abbildung 12.4, ein negativer Trend nach dem Referendum deutlich erkennbar.\n\n12.2.2 Statistische Inferenz\nDie bisherigen Placebo-Tests liefern lediglich grafische Evidenz für die Signifikanz des negativen Effekts des Brexit-Referendums auf die Britische Volkswirtschaft. Methoden für statistische Inferenz für SCM sind Gegenstand aktueller Forschung. Born u. a. (2019) verwenden den End-Of-Sample Instability Test (\\(S\\)) von Andrews (2003). Dieses Verfahren kann für einen Test auf einen Strukturbruch gegen Ende einer Zeitreihe verwendet werden. In der vorliegende Studie wird der Test angewendet, um zu überprüfen, ob die Verteilung der Doppelgänger-Gap Großbritanniens für die letzen \\(m\\) Perioden jenseits des Referendums signifikant verschieden ist von Verteilung vorheriger Perioden.\nWir zeigen nachfolgend, wie diese Analyse in R mit der Funktion CPAT::Andrews.test() aus dem Paket CPAT durchgeführt werden kann. Wir testen zunächst auf eine signifikante Diskrepanz der Doppelgänger-Gap in Form eines Sturkturbruchs ab 2017 und fassen die Ergebnisse tabellarisch mit broom::tidy() und gt::gt() zusammen.\n\nlibrary(CPAT)\n\n# Andrews' (2003) Test für 2017 durchführen\nAndrews.test(\n  x = gdp_gap$gdp_gap, \n  M = which(gdp_gap$Time == 2017)\n) %&gt;% \n  broom::tidy() %&gt;% \n  gt::gt() %&gt;%\n  tabopts\n\n\n\n\n\n\n\nstatistic\np.value\nmethod\n\n\n14.196\n0.693\nAndrews' Test for Structural Change\n\n\n\n\n\n\nTabelle 12.3: Andrews’ (1993) End-of-Sample Instability Test\n\n\n\nGem. des großen \\(p\\)-Werts kann die Nullhypothese (keine strukturelle Veränderung ab 2017) nicht abgelehnt werden. Wir führen den Test nun für sämtliche Zeitpunkte ab 2017 durch und plotten die \\(p\\)-Werte nebst gepunkteten roten Hilfslinien für die gängigen Signifikanzniveaus (10%, 5%, 1%).\n\n# Andrews' (1993) test für \n# Post-Referendumsperioden\npvals_andrews &lt;- map(seq(2017, 2020.5, .25), \\(time) {\n  tibble(\n    Time = time,\n    gap = gdp_gap %&gt;% filter(Time == time) %&gt;% pull(gdp_gap),\n    pvalue = CPAT::Andrews.test(\n      x = gdp_gap$gdp_gap, \n      M = which(gdp_gap$Time == time)\n    )$p.value\n  )\n}) %&gt;% \n  bind_rows()\n\n\n# p-Werte für Post-Interventionsperioden\npvals_andrews %&gt;%\n  ggplot(mapping = aes(x = Time, y = pvalue)) + \n  geom_hline(\n    yintercept = c(.1,.05, .01), \n    lty = \"dotted\", \n    col = \"red\"\n  ) +\n  geom_line() +\n  scale_x_continuous(expand = c(0, 0)) +\n  cowplot::theme_cowplot()\n\n\n\n\n\n\nAbbildung 12.8: P-Werte für Andrews’ (2003) Test\n\n\n\n\nDer Verlauf der \\(p\\)-Werte zeigt deutlich, dass es für Zeitpunkte jenseits von 2018-Q3 Evidenz für eine strukturelle Veränderung der Doppelgänger-Gap für Großbrittannien gibt. Diese Ergebnisse untermauern die Signifikanz der in Born u. a. (2019) mit SCM gefundenen negativen Effekte des Brexit-Votums auf die Britische Volkswirtschaft weiter.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbadie, Alberto, Alexis Diamond, und Jens Hainmueller. 2014. „Comparative Politics and the Synthetic Control Method: COMPARATIVE POLITICS AND THE SYNTHETIC CONTROL METHOD“. American Journal of Political Science 59 (2): 495–510. https://doi.org/10.1111/ajps.12116.\n\n\nAbadie, Alexis Diamond, Alberto, und Jens Hainmueller. 2010. „Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California’s Tobacco Control Program.“ Journal of the American Statistical Association 105 (490): 493–505. https://doi.org/10.1198/jasa.2009.ap08746.\n\n\nAndrews, D. W. K. 2003. „End-of-Sample Instability Tests“. Econometrica 71 (6): 1661–94. https://doi.org/10.1111/1468-0262.00466.\n\n\nBorn, Benjamin, Gernot J Müller, Moritz Schularick, und Petr Sedláček. 2019. „The Costs of Economic Nationalism: Evidence from the Brexit Experiment*“. The Economic Journal 129 (623): 2722–44. https://doi.org/10.1093/ej/uez020.\n\n\nHainmueller, Jens, Alexis Diamond, und Alberto Abadie. 2011. „Synth: An R Package for Synthetic Control Methods in Comparative Case Studies“. Journal of Statistical Software 42 (13): 1–17. https://www.jstatsoft.org/v42/i13/.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Synthetic Control</span>"
    ]
  },
  {
    "objectID": "RegReg.html",
    "href": "RegReg.html",
    "title": "13  Regularisierte Regression",
    "section": "",
    "text": "13.1 Ridge Regression\nRidge Regression wurde von Hoerl und Kennard (1970) als Alternative zur KQ-Schätzung bei hoch-korrelierten Regressoren eingeführt. Die Verlustfunktion lautet \\[\\begin{align}\n  \\mathrm{RSS}(\\boldsymbol{\\beta},p=2,\\lambda) = \\mathrm{RSS}(\\boldsymbol{\\beta}) + \\lambda \\lVert\\boldsymbol{\\beta}\\rVert_2,\\label{eq:ridgeloss}\n\\end{align}\\] d.h. der Parameter \\(\\lambda\\) reguliert den Einfluss eines \\(\\ell_2\\)-Strafterms \\[\\begin{align*}\n  \\lVert\\boldsymbol{\\beta}\\rVert_2 = \\sqrt{\\sum_{j=1}^k\\beta_j^2}\n\\end{align*}\\] auf die Verlustfunktion \\(\\mathrm{RSS}(\\boldsymbol{\\beta},p=2,\\lambda)\\). Der Ridge-Schätzer ergibt sich als \\[\\begin{align}\n  \\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_\\lambda := \\arg\\min_{\\boldsymbol{\\beta}}\\mathrm{RSS}(\\boldsymbol{\\beta}) + \\lambda \\lVert\\boldsymbol{\\beta}\\rVert_2.\\label{eq:ridgereg}\n\\end{align}\\]\nFür Das Optimierungsproblem \\(\\eqref{eq:ridgereg}\\) kann wir aus den Bedingungen 1. Ordnung \\[\\begin{align}\n  -2\\boldsymbol{X}'(\\boldsymbol{Y} - \\boldsymbol{X}\\boldsymbol{\\beta}) + 2\\lambda\\boldsymbol{\\beta} = \\boldsymbol{0}\n\\end{align}\\] die analytische Lösung \\[\\begin{align}\n  \\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_\\lambda = (\\boldsymbol{X}'\\boldsymbol{X} + \\lambda\\boldsymbol{I}_p)^{-1}\\boldsymbol{X}'\\boldsymbol{Y},\\label{eq:ridgecf}\n\\end{align}\\] bestimmt werden, wobei \\(\\boldsymbol{I}_k\\) die \\(k\\times k\\) Einheitsmatrix ist. Aus Gleichung \\(\\eqref{eq:ridgecf}\\) kann die Wirkungsweise des Strafterms \\(\\lambda \\lVert\\boldsymbol{\\beta}\\rVert_2\\) abgeleitet werden: Ridge Regression modifiziert die Diagonale der zu invertierenden Matrix \\(\\boldsymbol{X}'\\boldsymbol{X}\\) durch Addition von \\(\\lambda&gt;0\\). Dies ist hilfreich, wenn\nFür eine grafische Betrachtung des Optimierungskalküls \\(\\eqref{eq:ridgereg}\\) betrachten wir die äquivalente Darstellung als Lagrange-Problem \\[\\begin{align}\n  \\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_\\lambda := \\arg\\min_{\\lVert\\boldsymbol{\\beta}\\rVert&lt;t}\\mathrm{RSS}(\\boldsymbol{\\beta}).\\label{eq:ridgeLg}\n\\end{align}\\] In der folgenden interaktiven Grafik illustrieren wir das Optimierungsproblem \\(\\eqref{eq:ridgeLg}\\) sowie den resultierenden Schätzer der Koeffizienten \\((\\beta_1, \\beta_2)\\) in einem multiplen Regressionsmodell mit den Regressoren \\(X_1\\) und \\(X_2\\).",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularisierte Regression</span>"
    ]
  },
  {
    "objectID": "RegReg.html#ridge-regression",
    "href": "RegReg.html#ridge-regression",
    "title": "13  Regularisierte Regression",
    "section": "",
    "text": "\\(k\\geq n\\) und damit \\(\\boldsymbol{X}'\\boldsymbol{X}\\) nicht invertiertbar (singulär) ist. Dann kann der KQ-Schätzer nicht berechnet werden.3 Die Inverse \\((\\boldsymbol{X}'\\boldsymbol{X} + \\lambda\\boldsymbol{I}_p)^{-1}\\) hingegen existiert unter milden Bedingungen.\nhohe Kollinearität vorliegt, sodass \\((\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\) zwar existiert, aber zu einer instablilen KQ-Schätzung mit hoher Varianz führt.\n\n3 Beispiel: X &lt;- matrix(rnorm(100), ncol = 10). Vergleiche solve(t(X) %*% X) und solve(t(X) %*% X + diag(.01, nrow = 10))\n\nDie blaue Ellipse ist die Menge aller Schätzwerte \\(\\left(\\widehat\\beta_{1},\\, \\widehat\\beta_{2}\\right)\\) für den angegebenen Wert von \\(\\mathrm{RSS}\\). Im Zentrum der Ellipse liegt der KQ-Schätzer, welcher \\(\\mathrm{RSS}\\) minimiert.\nDer blaue Kreis ist die Menge aller Koeffizienten-Paare \\((\\beta_1, \\beta_2)\\), welche die Restriktion \\(\\beta_1^2 + \\beta_2^2\\leq t\\) erfüllen. Beachte, dass die Größe des Kreises nur durch den Parameter \\(t\\) bestimmt wird, welcher für einen vorgegebenen Wertebereich variiert werden kann.\nDer blaue Punkt ist der Ridge-Schätzer \\((\\widehat\\beta^R_{1,t},\\, \\widehat\\beta^R_{2,t})\\). Dieser ergibt sich als Schnittpunkt zwischen der blauen \\(\\mathrm{RSS}\\)-Ellipse und der Restriktionsregion und variiert mit \\(t\\). Die gestrichelte rote Kurve zeigt den Ridge-Lösungspfad.\nFür kleine Werte \\(t\\) drückt die Shrinkage die geschätzten Koeffizienten Richtung 0, wobei der Lösungspfad i.d.R. nicht-linear verläuft, d.h. die Shrinkage auf den Koeffizienten ist grundsätzlich unterschiedlich. Die Lösung \\((\\widehat\\beta^R_{1,t},\\, \\widehat\\beta^R_{2,t}) = (0,0)\\) existiert nur als Grenzwert für \\(t\\to0\\).\nBeachte, dass der Effekt von \\(t\\) auf die Schätzung umgekehrt für \\(\\lambda\\) verläuft: Größere \\(\\lambda\\) führen zu stärkerer Regularisierung.\n\n\n\n\n13.1.1 Eigenschaften des Schätzers\nDer Ridge-Schätzer \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_\\lambda\\) ist nicht invariant gegenüber der Skalierung der Regressoren. Für empirische Daten sollte daher vorab eine Standardisierung der erklärenden Variablen durchgeführt werden.4 Um die Eigenschaften des Ridge-Schätzers besser zu verstehen, betrachten wir hier den Fall orthonormaler Regressoren \\(\\boldsymbol{X}_j\\).5 Dann ist \\[\\begin{align}\n  \\widehat{\\beta}^{\\mathrm{R}}_{\\lambda,\\,j} = (1+\\lambda)^{-1} \\cdot\\widehat{\\beta}_j,\\quad j = 1,\\dots,k,\\label{eq:ridgeortho}\n\\end{align}\\] d.h. der Ridge-Schätzer skaliert die KQ-Lösung mit einem von \\(\\lambda\\) abhängigen Faktor.6\n4 Bspw. mit der Funktion scale().5 Orthonormalität heißt \\(\\boldsymbol{X}_i'\\boldsymbol{X}_j = 1\\) für \\(i=j\\) und \\(0\\) sonst. Dann ist \\(\\boldsymbol{X}\\)’\\(\\boldsymbol{X} = \\boldsymbol{I}_k\\).6 \\((1+\\lambda)^{-1}\\) wird auch als Shrinkage-Faktor bezeichnet.Wir illustrieren dies, indem wir den Zusammenhang zwischen KQ- und Ridge-Schätzer im orthonormalen Fall als R-Funktion ridge_ortho() implementieren und für die Parameterwerte \\(\\lambda\\in\\{0,0.5,2\\}\\) plotten.\n\nlibrary(tidyverse)\n\n# Funktion für Rige Regression bei orthonormalen Regressoren\nridge_ortho &lt;- function(KQ, lambda) {\n  1/(1 + lambda) * KQ\n}\n\n\n# KQ-Schätzer gegen Ridge-Schätzer plotten\ndat &lt;- tibble(KQ = seq(-1, 1, .01))\n\nggplot(dat) +\n  geom_function(fun = ridge_ortho, \n                args = list(lambda =  0), \n                lty = 2) + \n  geom_function(fun = ridge_ortho, \n                args = list(lambda = .5), \n                col = \"red\") + \n  geom_function(fun = ridge_ortho, \n                args = list(lambda = 2), \n                col = \"blue\") + \n  xlim(-.4, .4) +\n  xlab(\"KQ-Schätzer von beta_1\") +\n  ylab(\"Ridge-Schätzer von beta_1\")\n\n\n\n\n\n\nAbbildung 13.1: Shrinkage des OLS-Schätzers bei Ridge Regression\n\n\n\n\nAbbildung 13.1 zeigt, dass der Ridge-Schätzer eine lineare Transformation des KQ-Schätzers (gestrichelte Linie) ist. Größere Werte des Regularisierungsparameters \\(\\lambda\\) führen zu stärkerer Shrinkage des Koeffizientenschätzers in Richtung 0. Die \\(\\ell_2\\)-Norm führt zu proportional zum Absolutwert des KQ-Schätzers verlaufender Shrinkage: Größere Koeffizienten werden stärker bestraft als kleine Koeffizienten.\nDie Eigenschaft \\[\\mathrm{E}\\left(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_{\\lambda,\\,j}\\right) = (1+\\lambda)^{-1} \\cdot \\beta_j\\] zeigt, dass \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_{\\lambda,\\,j}\\) (für fixes \\(\\lambda&gt;0\\)) nicht erwartungstreu für \\(\\beta_j\\) ist. Weiterhin ist \\[\\begin{align*}\n  \\mathrm{Var}\\left(\\widehat{\\beta}^{\\mathrm{R}}_{\\lambda,\\,j}\\right) =&\\,\n  \\mathrm{Var}\\left(\\widehat{\\beta}_j\\right) \\cdot \\left(\\frac{\\lambda}{1+\\lambda^2}\\right)\\\\\n    =&\\, \\sigma^2\\cdot \\left(\\frac{\\lambda}{1+\\lambda^2}\\right),\n\\end{align*}\\] wobei \\(\\sigma^2\\) die Varianz des Regressionsfehlers \\(u\\) ist. Wegen \\(\\lambda&lt;(1+\\lambda)^2\\) für \\(\\lambda&gt;0\\) gilt \\[\\mathrm{Var}\\left(\\widehat{\\beta}^{\\mathrm{R}}_{\\lambda,\\,j}\\right)&lt;\\mathrm{Var}\\left(\\widehat{\\beta}_j\\right).\\] Der Ridge-Schätzer hat also eine kleinere Varianz als der KQ-Schätzer. Diese Eigenschaften können auch für korrelierte Regressoren gezeigt werden.\n\n13.1.2 Ridge Regression mit glmnet\n\nWir zeigen nun anhand simulierter Daten, wie der Ridge-Lösungspfad mit dem R-Paket glmnet berechnet werden kann. Wir erzeugen zunächst Daten gemäß der Vorschrift \\[\\begin{align}\n  \\begin{split}\n  Y_i =&\\, \\boldsymbol{X}_i' \\boldsymbol{\\beta} + u_i,\\\\\n  \\\\\n  \\beta_j =&\\,  \\frac{5}{j^2}, \\qquad\\qquad\\ j=1,\\dots,5,\\\\\n  \\beta_j =&\\, -\\frac{5}{(j-5)^2}, \\quad j=6,\\dots,10,\\\\\n  \\\\\n  \\boldsymbol{X}_i \\sim&\\, N(\\boldsymbol{0}, \\boldsymbol{\\Sigma}), \\quad u_i \\overset{u.i.v.}{\\sim} N(0, 1), \\quad i = 1,\\dots,25.\n  \\end{split} \\label{eq:ridgedgp1}\n\\end{align}\\] Hierbei wird \\(\\boldsymbol{\\Sigma}\\) so definiert, dass jeder Regressor \\(N(0,1)\\)-verteilt ist und eine Korrelation von \\(0.8\\) mit allen anderen Regressoren aufweist. Mit der Vorschrift für die \\(\\beta_j\\) stellen wir sicher, dass es wenige Variablen gibt, die \\(Y\\) stark beeinflussen, da der Absolutbetrag der Koeffizienten in \\(j\\) abnimmt.7\n7 Für bessere Interpretierbarkeit der Grafischen Auswertung, wählen wir positive und negative Koeffizienten mit gleichem Bertag.\nlibrary(gendata)\nset.seed(1234)\n\n# Parameter definieren\nN &lt;- 80\nk &lt;- 10\n\ncoefs &lt;- 5/(1:(k/2))^2\nbeta &lt;- c(coefs, -coefs)\n\n# Beobachtungen simulieren\nX &lt;- as.matrix(\n  genmvnorm(\n    k = k, \n    cor = rep(.8, (k^2-k)/2), \n    n = N)\n  )\nY &lt;- X %*% beta + rnorm(N)\n\nWir schätzen nun ein Modell mit allen 10 Regressoren mit glmnet. Beachte, dass für den Ridge-Strafterm alpha = 0 gesetzt werden muss.8\n8 alpha ist ein Mischparameter im Algorithmus für elastic net, siehe ?glmnet.\nlibrary(glmnet)\n\n# Ridge-Regression anpassen\nridge_fit &lt;- glmnet(\n  x = X, \n  y = Y, \n  alpha = 0 # für Ridge-Strafterm\n)\n\nDer Lösungspfad der Ridge-Schätzung kann nach Transformation der geschätzen Koeffizienten und der zugehörigen \\(\\lambda\\)-Werte in ein langes Format überführt und komfortabel mit ggplot2 dargestellt werden.\n\n# Lambda-Sequenz auslesen\nlambdas &lt;- ridge_fit$lambda\n\n# Ridge-Schätzung für Lambdas im langen Format \nas.matrix(ridge_fit$beta) %&gt;% \n  as_tibble() %&gt;% \n  rownames_to_column(\"Variable\") %&gt;%\n  pivot_longer(-Variable) %&gt;% \n  group_by(Variable) %&gt;% \n  mutate(lambda = lambdas) %&gt;%\n  \n  # Grafik mit ggplot erzeugen\n  ggplot(\n    mapping = aes(\n      x = lambda, \n      y = value, \n      col = Variable\n    )\n  ) + \n  geom_line() +\n  ylab(\"gesch. Koeffizienten\") +\n  scale_x_log10(\"log_10(lambda)\")\n\n\n\n\n\n\nAbbildung 13.2: Lösungspfad für Ridge-Schätzung\n\n\n\n\nAbbildung 13.2 zeigt den nicht-linearen Verlauf der Shrinkage auf den geschätzten Modellkoeffizienten. Die Koeffizienten werden mit zunehmendem \\(\\lambda\\) von der KQ-Lösung ausgehend (linkes Ende der Skala) in Richtung 0 gezwungen.\nÜber die Funktion cv.glmnet() kann ein optimales \\(\\lambda\\) mit Cross Validation (CV) ermittelt werden. Ähnlich wie bei glmnet() wird für die Validierung automatisch eine \\(\\lambda\\)-Sequenz erzeugt. Wir nutzen autoplot() aus dem R-Paket ggfortify für die Visualisierung der Ergebnisse mit ggplot2.\n\nlibrary(ggfortify)\n\n# Cross-validierte Bestimmung von lambda\nridge_cvfit &lt;- cv.glmnet(\n  y = Y, \n  x = X, \n  intercept = F,\n  alpha = 0\n) \n\n# Ergebnisse plotten\nridge_cvfit %&gt;% \n  autoplot(label.n = 0)\n\n\n\n\n\n\nAbbildung 13.3: Lösungspfad für Ridge-Schätzung\n\n\n\n\nAbbildung 13.3 zeigt ridge_cvfit$lambda.min, das optimale \\(\\lambda\\) mit dem geringsten CV Mean-Squarred-Error (linke gestrichelte Linie) und ridge_cvfit$lambda.1se, das größte \\(\\lambda\\), welches innerhalb einer Standardabweichung entfernt ist (rechte gestrichelte Linie).9 Wir berechnen die Schätzung für lambda.min.\n9 Die Wahl von lambda.1se ist eine Heuristik, welche die Schätzunsicherheit berücksichtigt und zu einem “sparsameren” Modell tendiert.\n(\n  ridge_coefs &lt;- coef(\n    object = ridge_cvfit, \n    s = ridge_cvfit$lambda.min\n  )\n)\n\n11 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s1\n(Intercept)  .        \nX1           4.1302194\nX2           1.0245661\nX3           0.3139297\nX4           0.5697498\nX5           0.2928664\nX6          -4.1693524\nX7          -0.7509305\nX8          -0.3844761\nX9          -0.3841997\nX10         -0.4078514\n\n\nWir schätzen das Modell nun mit KQ und vergleichen die Koeffizienten mit der Ridge-Schätzung.\n\n# KQ-Schätzung durchführen\nKQ_fit &lt;- lm(Y ~ X - 1)\n\n# Koeffizienten auslesen und transformieren:\ntibble(\n  Ridge = as.matrix(ridge_coefs)[2:11, ],\n  KQ = KQ_fit$coefficients\n) %&gt;% \n  mutate(j = factor(1:10)) %&gt;%\n  pivot_longer(\n    cols = Ridge:KQ, \n    names_to = \"Methode\", \n    values_to = \"Koeffizient\"\n  ) %&gt;%\n\n# Bar-Plot für Koeffizientenvergleich erzeugen  \n  ggplot(\n    mapping = aes(\n      x = j, \n      y = Koeffizient, \n      fill = Methode\n    )\n  ) +\n  geom_bar(\n    position = \"dodge\", \n    stat = \"identity\", \n    width = .5\n  )\n\n\n\n\n\n\nAbbildung 13.4: Koeffizientenvergleich: Ridge vs. KQ\n\n\n\n\nDer Vergleich anhand von Abbildung 13.4 zeigt deutlich, dass Ridge Regression im Vergleich mit KQ zu absolut kleineren Koeffizientenschätzungen tendiert. Inwiefern dies Konsequenzen für die Prognosegüte der Schätzung hat, können wir Anhand eines Testdatensatzes bestimmen. Hierzu vergleichen wir die mittleren Fehler (MSE) bei der Prognose von \\(Y\\) für die Beobachtungen im Testdatensatz. Für die Simulation des Testdatensatzes nutzen wir erneut die Vorschrift \\(\\eqref{eq:ridgedgp1}\\) um 80 neue Beobachtungen zu erzeugen.\n\n# Test-Datensatz erstellen\nset.seed(4321)\n# Regressoren\nnew_X &lt;- as.matrix(\n  genmvnorm(\n    k = k, \n    cor = rep(.85, (k^2-k)/2), \n    n = N\n  )\n)\n# Abh. Variable\nnew_Y &lt;- new_X %*% beta + rnorm(N)\n\nFür beide Methoden können wir predict() für die Prognosen von \\(Y\\) für den Testdatensatz (new_Y) nutzen.\n\n# Ridge: Vorhersage von new_Y für Test-Datensatz\nY_predict_ridge &lt;- predict(\n  object = ridge_cvfit, \n  newx = new_X, \n  s = ridge_cvfit$lambda.min\n)\n\n# Ridge: MSE für Test-Datensatz berechnen\nmean((Y_predict_ridge - new_Y)^2)\n\n[1] 1.288457\n\n\nDie Vorhersage für lm() benötigt dieselben Variablennamen wie im angepassten Modell, s. KQ_fit$coefficients.\n\n# Test-Datensatz für predict.lm() formatieren\nnew_X &lt;- as.data.frame(new_X)\ncolnames(new_X) &lt;- paste0(\"X\", 1:k)\n\n# KQ: Vorhersage von new_Y für Test-Datensatz\nY_predict_KQ &lt;- predict(\n  object = KQ_fit, \n  newdata = new_X\n)\n\n# KQ: MSE für Test-Datensatz berechnen\nmean((Y_predict_KQ - new_Y)^2)\n\n[1] 29.33797\n\n\nDie Ergebnisse zeigen, dass der Ridge-Schätzer trotz seiner Verzerrung einen deutlich geringeren mittleren Vorhersagefehler für die Testdaten erzielt als der KQ-Schätzer. Diese Eigenschaft der Koeffizientenschätzung kann die Prognosegüte von Ridge Regression gegenüber der KQ-Regression verbessern.\n\n13.1.3 Beispiel: Vorhersage von Abschlussnoten in Mathe\nZur Illustration von Ridge Regression nutzen wir den Datensatz SP aus Cortez und Silva (2008).10 SP enhält Beobachtungen zu Leistungen von insgesamt 100 Schülerinnen und Schülern im Fach Mathematik in der Sekundarstufe an zwei portugiesischen Schulen. Neben der Abschlussnote in Mathe (G3, Skala von 0 bis 20) beinhaltet SP diverse demografische, soziale und schulbezogene Merkmale, die mithilfe von Schulberichten und Fragebögen erhoben wurden. Ziel ist es, ein Modell für die Prognose von G3 anzupassen.\n10 Wir verwenden eine Auszug aus dem Orignaldatensatz, der nebst ausführlicher Variablenbeschreibung hier verfügbar ist.Wir lesen zunächst die Daten (im .csv-Format) ein.\n\n# Daten einlesen\nSP &lt;- read_csv(file = \"datasets/SP.csv\")\n\nEin Überblick zeigt, dass der Großteil der Regressoren aus kategorialen Variablen mit sozio-ökonomischen Informationen besteht.\n\n# Überblick\nglimpse(SP)\n\nRows: 100\nColumns: 31\n$ school     &lt;chr&gt; \"GP\", \"GP\", \"GP\", \"MS\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\",…\n$ sex        &lt;chr&gt; \"M\", \"M\", \"F\", \"F\", \"M\", \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\",…\n$ age        &lt;dbl&gt; 17, 18, 19, 17, 16, 16, 19, 16, 16, 16, 18, 16, 15, 17, 17,…\n$ address    &lt;chr&gt; \"R\", \"R\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"R\", \"U\", \"U\",…\n$ famsize    &lt;chr&gt; \"GT3\", \"GT3\", \"LE3\", \"GT3\", \"LE3\", \"GT3\", \"GT3\", \"GT3\", \"GT…\n$ Pstatus    &lt;chr&gt; \"T\", \"T\", \"T\", \"T\", \"A\", \"T\", \"T\", \"T\", \"A\", \"T\", \"T\", \"T\",…\n$ Medu       &lt;dbl&gt; 1, 4, 3, 2, 3, 2, 0, 2, 3, 4, 4, 2, 1, 2, 2, 3, 3, 4, 4, 2,…\n$ Fedu       &lt;dbl&gt; 2, 3, 2, 2, 4, 3, 1, 1, 1, 4, 4, 2, 2, 3, 2, 3, 1, 3, 4, 2,…\n$ Mjob       &lt;chr&gt; \"at_home\", \"teacher\", \"services\", \"other\", \"services\", \"oth…\n$ Fjob       &lt;chr&gt; \"other\", \"services\", \"other\", \"at_home\", \"other\", \"other\", …\n$ reason     &lt;chr&gt; \"home\", \"course\", \"reputation\", \"home\", \"home\", \"reputation…\n$ guardian   &lt;chr&gt; \"mother\", \"mother\", \"other\", \"mother\", \"mother\", \"mother\", …\n$ traveltime &lt;dbl&gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1,…\n$ studytime  &lt;dbl&gt; 2, 3, 2, 3, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 3, 1, 2,…\n$ failures   &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ schoolsup  &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no…\n$ famsup     &lt;chr&gt; \"no\", \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", …\n$ paid       &lt;chr&gt; \"no\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"n…\n$ activities &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"y…\n$ nursery    &lt;chr&gt; \"yes\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\"…\n$ higher     &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes…\n$ internet   &lt;chr&gt; \"no\", \"yes\", \"yes\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", …\n$ romantic   &lt;chr&gt; \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"…\n$ famrel     &lt;dbl&gt; 3, 5, 4, 3, 5, 4, 3, 4, 2, 2, 1, 5, 4, 5, 3, 5, 4, 4, 5, 5,…\n$ freetime   &lt;dbl&gt; 1, 3, 2, 4, 3, 4, 4, 5, 3, 4, 4, 4, 3, 3, 4, 4, 5, 2, 3, 4,…\n$ goout      &lt;dbl&gt; 3, 2, 2, 3, 3, 3, 2, 2, 3, 4, 2, 4, 2, 3, 4, 2, 4, 2, 3, 4,…\n$ Dalc       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1,…\n$ Walc       &lt;dbl&gt; 5, 2, 2, 1, 1, 3, 1, 1, 2, 3, 2, 4, 1, 3, 3, 1, 3, 2, 1, 1,…\n$ health     &lt;dbl&gt; 3, 4, 1, 3, 5, 4, 5, 5, 4, 4, 1, 5, 5, 3, 5, 5, 1, 3, 5, 5,…\n$ absences   &lt;dbl&gt; 4, 9, 22, 8, 4, 6, 2, 20, 5, 6, 5, 0, 2, 2, 12, 0, 17, 0, 4…\n$ G3         &lt;dbl&gt; 10, 16, 11, 11, 11, 10, 9, 12, 7, 11, 16, 12, 9, 12, 12, 13…\n\n\nUm die Prognosegüte des Modells beurteilen zu können, partitionieren wir SP zufällig in einen Test- sowie einen Trainingsdatensatz (mit 30 und 70 Beobachtungen), jeweils für die Regressoren und die abhängige Variable.\n\n# ID für Beobachtungen im Testdatensatz zufällig erzeugen\nset.seed(1234)\nID &lt;- sample(1:nrow(SP), size = 30)\n\n# Regressoren aufteilen\nSP_test &lt;- SP[ID,]\nSP_train &lt;- SP[-ID,]\n\n# Abh. Variable aufteilen\nY_test &lt;- SP_test$G3\nY_train &lt;- SP_train$G3\n\nAls nächstes passen wir ein Ridge-Regressionsmodell für alle Regressoren in SP_train an und ermitteln ein optimales \\(\\lambda\\) mit Cross Validation. Beachte, dass cv.glmnet nicht für Regressoren im data.frame/tibble-Format ausgelegt ist, sondern ein matrix-Format erwartet. Wir transformieren SP_train daher mit data.matrix().\n\n# Ridge-Regression und CV für Trainingsdaten\nSP_fit_cv &lt;- cv.glmnet(\n  x = data.matrix(SP_train %&gt;% select(-G3)), \n  y = Y_train, \n  alpha = 0\n)\n\n# CV-Ergebnisse für lambda visualisieren\nSP_fit_cv %&gt;% \n  autoplot(label.n = 0)\n\n\n\n\n\n\n\nWie für das Beispiel mit simulierten Daten erhalten wir mit predict() Vorhersagen für die erzielte Punktzahl. Beachte, dass wir den MSE nicht für die Trainingsdaten SP_train, sondern für die Testdaten SP_test berechnen.\n\n# Prognose von G3 anhand des Ridge-Modells\nY_predict_ridge &lt;- predict(\n  object = SP_fit_cv, \n  newx = data.matrix(\n    SP_test %&gt;% \n      select(-G3)\n    ), \n  s = SP_fit_cv$lambda.min\n)\n\n# MSE für Testdaten berechnen\nmean((Y_predict_ridge - Y_test)^2)\n\n[1] 21.13249\n\n\nAuch in diesem empirischen Beispiel zeigt ein Vergleich der MSEs, dass Ridge Regression dem KQ-Schätzer hinsichtlich der Vorhersagegüte überlegen ist.\n\n# Modell mit KQ schätzen\nSP_fit_KQ &lt;- lm(G3 ~ ., SP_train)\n\n# Prognose\nY_predict_KQ &lt;- predict(\n  object = SP_fit_KQ, \n  newdata = SP_test %&gt;% \n    select(-G3)\n)\n\n# Testset-MSE berechnen\nmean((Y_predict_KQ - Y_test)^2)\n\n[1] 29.76893\n\n\nDer MSE für Ridge ist mit \\(21.13\\) deutlich kleiner als \\(29.77\\), der MSE für KQ.\nFür die Interpretation der Ridge-Schätzung erweitern den Code für die ggplot2-Grafik der Koeffizienten-Pfade um eine vertikale Linie des mit CV ermittelten \\(\\lambda\\) und fügen mit dem Paket ggrepel Labels für die Pfade der größten Koeffizienten hinzu.\n\nlibrary(ggrepel)\n\n# Lambda-Sequenz auslesen\nlambdas &lt;- SP_fit_cv$lambda\n\n# Ridge-Schätzung für Lambdas im langen Format \ndf &lt;- as.matrix(SP_fit_cv$glmnet.fit$beta) %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    Variable = rownames(SP_fit_cv$glmnet.fit$beta)\n  ) %&gt;%\n  pivot_longer(-Variable) %&gt;% \n  group_by(Variable) %&gt;% \n  mutate(lambda = lambdas) \n\n# Grafik mit ggplot erzeugen\ndf %&gt;%\n  ggplot(\n    mapping = aes(\n      x = lambda, \n      y = value, \n      col = Variable\n    )\n  ) + \n  geom_line() +\n  geom_label_repel(\n    data = df %&gt;% \n      filter(lambda == min(lambdas)),\n    mapping = aes(label = Variable), \n    seed = 1234,\n    size = 5, \n    max.overlaps = 8, \n    nudge_x = -.5) +\n  ylab(\"gesch. Koeffizienten\") +\n  scale_x_log10(\"log_10(lambda)\") +\n  geom_vline(\n    xintercept = SP_fit_cv$lambda.min, \n    col = \"red\", \n    lty = 2\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nAbbildung 13.5: Lösungspfad für Ridge-Schätzung\n\n\n\n\nAbbildung 13.5 gibt Hinweise darauf, dass neben der Schulzugehörigkeit und Indikatoren für schulische Leistung (bspw. failures) sozio-ökonomische Prädiktoren wie internet (Internetzugang zuhause), Pstatus (Zusammenleben der Eltern) und address/traveltime (sozialer Status) relevante Variablen zu sein scheinen.\nDas optimale \\(\\lambda_\\mathrm{cv} \\approx 0.21\\) (gestrichelte rote Linie in Abbildung 13.5) führt zu deutlicher Shrinkage, was eine mögliche Erklärung für den besseren Testset-MSE von Ridge Regression ist: Die Koeffizienten von Variablen mit wenig Erklärungskraft werden durch die Regularisierung in Richtung 0 gezwungen und reduzieren so die Varianz der Vorhersage gegenüber der (idealerweise) unverzerrten KQ-Schätzung.\n\n\n\n\n\n\nKey Facts zu Ridge Regression\n\n\n\n\nRidge-Regression regularisiert den KQ-Schätzer mit der \\(\\ell_2\\)-Norm der Koeffizienten. Diese Form von Regularisierung ist eine Alternative für KQ in Anwendungen mit mehr Regressoren als Beobachtugen (\\(k\\geq n\\)) und/oder wenn KQ aufgrund starker Kollinearität eine hohe Varianz aufweist.\nDer Ridge-Schätzer \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_\\lambda\\) ist nicht erwartungstreu. Die geschätzten Koeffizienten sind auch für \\(n\\to\\infty\\) verzerrt.\nAufgrund der verzerrten Schätzung ist statistische Inferenz für Koeffizienten mit \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{R}}_\\lambda\\) problematisch. Anstatt für strukturelle Modelle oder die Schätzung kausaler Effekte wird Ridge Regression in der Praxis daher überwiegend für Prognosen verwendet.\nDie Wahl von \\(\\lambda\\) impliziert einen Tradeoff zwischen Verzerrung und Varianz: Große \\(\\lambda\\) schrumpfen die Koeffizientenschätzer Richtung 0 (mehr Verzerrung), führen aber zu einer kleineren Varianz der Schätzung. Entsprechend können Vorhersagen mit mehr Verzerrung aber weniger Varianz als mit KQ getroffen werden.\nRidge Regression kann in R mit dem Paket glmnet berechnet werden.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularisierte Regression</span>"
    ]
  },
  {
    "objectID": "RegReg.html#lasso-regression",
    "href": "RegReg.html#lasso-regression",
    "title": "13  Regularisierte Regression",
    "section": "\n13.2 Lasso Regression",
    "text": "13.2 Lasso Regression\nLeast Absolute Shrinkage and Selection Operator (Lasso) ist ein von Tibshirani (1996) vorgeschlagener Schätzer, der die Verlustfunktion des KQ-Schätzers um einen Strafterm für die Summe der (absoluten) Größe der Koeffizienten \\(\\boldsymbol\\beta = (\\beta_1, \\dots,\\beta_k)'\\) erweitert. Die Verlustfunktion des Lasso-Schätzers von \\(\\boldsymbol{\\beta}\\) lautet \\[\\begin{align}\n\\mathrm{RSS}(\\boldsymbol{\\beta},p=1,\\lambda) = \\mathrm{RSS}(\\boldsymbol{\\beta}) + \\lambda \\lVert\\boldsymbol{\\beta}\\rVert_1.\\label{eq:lassoloss}\n\\end{align}\\] Für den Strafterm wird also die \\(\\ell_1\\)-norm \\[\n\\lVert\\boldsymbol{\\beta}\\rVert_1 = \\sum_{j=1}^k \\lvert\\beta_j \\rvert\n\\] verwendet. Der Lasso-Schätzer \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{L}}_\\lambda\\) für \\(\\boldsymbol{\\beta}\\) minimiert \\(\\eqref{eq:lassoloss}\\), \\[\\begin{align}\n\\boldsymbol{\\beta}^{\\mathrm{L}}_\\lambda = \\arg\\min_{\\boldsymbol{\\beta}} \\ \\mathrm{RSS}(\\boldsymbol{\\beta},p=1,\\lambda).\n\\end{align}\\] Entsprechend erhalten wir in Abhängigkeit von \\(\\lambda\\) ein Kontinuum an Lösungen \\[\\begin{align}\n  \\left\\{\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{L}}_\\lambda\\right\\}_{\\lambda=0}^{\\lambda=\\infty},\\label{eq:LassoPath}\n\\end{align}\\] der sogenannte Lasso-Pfad.\nDas Optimierungsproblem \\(\\eqref{eq:lassoloss}\\) hat die äquivalente Darstellung \\[\\begin{align}\n  \\begin{split}\n    \\widehat{\\boldsymbol{\\beta}}^{\\mathrm{L}}_\\lambda =&\\, \\arg\\min_{\\boldsymbol{\\beta}} \\mathrm{RSS}(\\boldsymbol{\\beta}) + \\lambda\\left(\\lVert\\boldsymbol{\\beta}\\rVert_1 - t\\right)\\\\\n    =&\\, \\arg\\min_{\\lVert\\boldsymbol{\\beta}\\rVert_1\\leq t} \\mathrm{RSS}(\\boldsymbol{\\beta}),\n  \\end{split}\\label{eq:lassolagrange}\n\\end{align}\\] welche über den Lagrange-Ansatz unter der Nebenbedingung \\(\\lVert\\boldsymbol{\\beta}\\rVert_1 \\leq t\\) gelöst werden kann.\nÄhnlich wie der KQ-Schätzer ist der Lasso-Schätzer \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{L}}_\\lambda\\) durch Bedingungen 1. Ordnung bestimmt. Diese Bedingungen lassen sich komfortabel in Matrix-Schreibweise darstellen als \\[\\begin{align}\n  -2\\boldsymbol{X}_j'(\\boldsymbol{Y} - \\boldsymbol{X}\\boldsymbol{\\beta}) + \\lambda\\cdot\\mathrm{sgn}(\\beta_j) = 0, \\quad j = 1,\\dots,k.\\label{eq:LassoFOC}\n\\end{align}\\] Aus Gleichung \\(\\eqref{eq:LassoFOC}\\) folgt, dass der Lasso-Schätzer aufgrund des Strafterms im Allgemeinen nicht algebraisch bestimmt werden kann.11\n11 Zur Bestimmung des Schätzers werden Algorithmen der nicht-linearen Optimierung genutzt.In Abhängigkeit von \\(\\lambda\\) zwingt der Lasso-Schätzer die KQ-Schätzung von \\(\\beta_j\\) zu einem (absolut) kleineren Wert: Ähnlich wie bei Ridge Regression bewirkt der \\(\\ell_1\\)-Strafterm eine mit \\(\\lambda\\) zunehmende Schrumpfung der geschätzen Koeffizienten in Richtung 0. Charakteristisch für die Lösung des Lasso-Schätzers ist, dass \\(\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{L}}_j = 0\\), wenn die Bedingung \\[\\begin{align}\n  \\left\\lvert\\boldsymbol{X}_j'(\\boldsymbol{Y} - \\boldsymbol{X}\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{L}}_\\lambda)\\right\\rvert - \\lambda/2 \\leq 0 \\label{eq:lassoselection}\n\\end{align}\\] erfüllt ist. In Abhängigkeit von \\(\\lambda\\) kann der Lasso-Schätzer folglich geschätzte Regressionskoeffizienten nicht nur in Richtung \\(0\\), sondern diese auch exakt mit \\(0\\) schätzen und damit Variablenselektion betreiben. Aufgrund der mit \\(\\lambda\\) zunehmenden Shrinkage bis die Bedingung \\(\\eqref{eq:lassoselection}\\) erfüllt und der Koeffizient gleich \\(0\\) gesetzt wird, bezeichnet man Lasso auch als einen Soft Thresholding Operator. Im nächsten Abschnitt betrachten wir die Eigenschaften von Lasso-Regularisierung unter vereinfachten Annahmen bzgl. der Regressoren.\n\n13.2.1 Lasso ist Soft Thresholding\nWir betrachten nun eine mathematische Darstellung von Selektions- und Shrinkage-Eigenschaft des Lasso-Schätzers in einem vereinfachten Modell. Wenn die Regressoren \\(\\boldsymbol{X}\\) orthonormal zueinander sind, existiert eine analytische Lösung des Lasso-Schätzers, \\[\\begin{align}\n  \\widehat{\\boldsymbol{\\beta}}^{\\mathrm{L}}_\\lambda =\n  \\begin{cases}\n    \\widehat{\\boldsymbol{\\beta}}_j - \\lambda/2 &, \\ \\ \\widehat{\\boldsymbol{\\beta}}_j &gt; \\lambda/2\\\\\n    0 &, \\ \\ \\lvert\\widehat{\\boldsymbol{\\beta}}_j\\rvert\\leq\\lambda/2\\\\\n    \\widehat{\\boldsymbol{\\beta}}_j + \\lambda/2 &, \\ \\ \\widehat{\\boldsymbol{\\beta}}_j &lt; \\lambda/2\n  \\end{cases},\\label{eq:lassoST}\n\\end{align}\\] wobei \\(\\widehat{\\boldsymbol{\\beta}}_j\\) der KQ-Schätzer von \\(\\beta_j\\) ist. Anhand von \\(\\eqref{eq:lassoST}\\) können wir die Selektionseigenschaft sowie die Schrumpfung der KQ-Koeffizientenschätzung in Abhängigkeit der durch \\(\\lambda\\) regulierten \\(\\ell_1\\)-Strafe erkennen. Für eine Visualisierung implementieren wir \\(\\eqref{eq:lassoST}\\) als R-Funktion lasso_st() und zeichnen die resultierenden Koeffizientenschätzungen für die Parameterwerte \\(\\lambda\\in\\{0, 0.2, 0.4\\}\\).\nWir definieren zunächst die Funktion lasso_st().\n\nlibrary(tidyverse)\n\n# Funktion für Lasso soft-thresholding definieren\nlasso_st &lt;- function(KQ, lambda) {\n  case_when(\n    KQ &gt; lambda/2         ~ KQ - lambda/2,\n    abs(KQ) &lt;= lambda/2   ~ 0,\n    KQ &lt; -lambda/2        ~ KQ + lambda/2,\n  )\n}\n\nIm nächsten Schritt zeichnen wir lasso_st() für eine Sequenz von KQ-Schätzwerten gegeben \\(\\lambda\\).\n\n# Sequenz von KQ-Schätzwerten für Illustration definieren\ndat &lt;- tibble(\n  KQ = seq(-1, 1, .01)\n)\n\n# Lasso-Schätzer als Funktion des KQ-Schätzers plotten\nggplot(dat) +\n  geom_function(\n    fun = lasso_st, \n    args = list(lambda = 0), \n    lty = 2\n  ) + \n  geom_function(\n    fun = lasso_st, \n    args = list(lambda = .2),\n    col = \"red\"\n  ) + \n  geom_function(\n    fun = lasso_st, \n    args = list(lambda = .4), \n    col = \"blue\"\n  ) + \n  xlim(-.4, .4) +\n  xlab(\"KQ-Schätzer von beta_1\") +\n  ylab(\"Lasso-Schätzer von beta_1\")\n\n\n\n\n\n\nAbbildung 13.6: Shrinkage und Selektion von OLS-Koeffizienten mit Lasso\n\n\n\n\nAbbildung 13.6 zeigt, dass der \\(\\ell_1\\)-Strafterm des Lasso-Schätzers zu einem linearen Verlauf der auf den KQ-Schätzer (gezeichnet für \\(\\lambda = 0\\), gestrichelte Linie) applizierten Shrinkage führt: Der Lasso-Schätzer ist eine abschnittsweise-lineare Funktion des KQ-Schätzers in \\(\\lambda\\): Je größer der Parameter \\(\\lambda\\), desto größer ist das Intervall von KQ-Schätzwerten \\([-\\lambda/2,\\lambda/2]\\), wo der Lasso-Schätzer zu Variablenselektion führt, d.h. hier den Koeffizienten \\(\\beta_j\\) als \\(0\\) schätzt (rote bzw. blaue Linie).\nAnhand von Abbildung 13.6 kann abgeleitet werden, dass der Lasso-Schätzer nicht invariant gegenüber der Skalierung der Regressoren ist: Die Stärke der Regularisierung durch \\(\\lambda\\) ist hängt von der Magnitude des KQ-Schätzers ab. Daher müssen die Regressoren vor Berechnung der Schätzung standardsiert werden. Üblich ist hierbei eine Normierung auf einen Mittelwert von \\(0\\) und eine Varianz von \\(1\\).\nDie nachstehende interaktive Grafik illustriert das Lasso-Optimierungsproblem \\(\\eqref{eq:lassolagrange}\\) sowie den resultierenden Schätzer der Koeffizienten \\((\\beta_1, \\beta_2)\\) in einem multiplen Regressionsmodell mit korrelierten Regressoren \\(X_1\\) und \\(X_2\\).\n\nDie blaue Ellipse ist die Menge aller Schätzwerte \\(\\left(\\widehat\\beta_{1},\\, \\widehat\\beta_{2}\\right)\\) für den angegebenen Wert von \\(\\mathrm{RSS}\\). Im Zentrum der Ellipse liegt der KQ-Schätzer, welcher \\(\\mathrm{RSS}\\) minimiert.\nDas graue Quadrat ist die Menge aller Koeffizienten-Paare \\((\\beta_1, \\beta_2)\\), welche die Restriktion \\(\\lvert\\beta_1\\rvert+\\lvert\\beta_2\\rvert\\leq t\\) erfüllen. Beachte, dass die Größe dieser Region nur durch den Parameter \\(t\\) bestimmt wird.\nDer blaue Punkt ist der Lasso-Schätzer \\((\\widehat{\\boldsymbol{\\beta}}^L_{1,t},\\, \\widehat{\\boldsymbol{\\beta}}^L_{2,t})\\). Dieser ergibt sich als Schnittpunkt zwischen der blauen \\(\\mathrm{RSS}\\)-Ellipse und der Restriktionsregion und variiert mit \\(t\\). Die gestrichelte rote Linie zeigt den Lasso-Lösungspfad.\nFür kleine Werte, erhalten wir starke Shrinkage auf \\(\\widehat\\beta_{1,t}\\) bis zum Wertebereich \\(t\\leq50\\), wo \\(\\widehat{\\boldsymbol{\\beta}}^L_{1,t}=0\\). Hier erfolgt Variablenselektion: Die Regularisierung führt zu einem geschätzten Modell, das lediglich \\(X_2\\) als erklärende Variable enthält. In diesem Bereich von \\(t\\) bewirkt die Shrinkage, dass \\(\\widehat{\\boldsymbol{\\beta}}^L_{2,t}\\to0\\) für \\(t\\to0\\).\n\n\n\nBeachte, dass der rote Lasso-Pfad (die Menge aller Lasso-Lösungen) äquivalent als Funktion von \\(\\lambda\\) im Optimierungsproblem \\(\\eqref{eq:lassoloss}\\) dargestellt werden kann. Implementierungen mit statistischer Software berechnen die Lasso-Lösung häufig in Abhängigkeit von \\(\\lambda\\). Ein Algorithmus hierfür ist LARS.\n\n13.2.2 Berechnung der Lasso-Lösung mit dem LARS-Algorithmus\nFür die Berechnung des Lasso-Lösungspfads kann der LARS-Algorithmus von Efron u. a. (2004) im Lasso-Modus genutzt werden.12 Der Lasso-Lösungspfad beinhaltet geschätzte Koeffizienten über ein Intervall für \\(\\lambda\\), welches sämtliche Modellkomplexitäten zwischen der (trivialen) Lösung mit maximaler Shrinkage auf allen Koeffizienten (\\(\\lambda\\) groß, alle gesch. Koeffizienten sind \\(0\\)) und der unregularisierten Lösung (\\(\\lambda = 0\\), KQ-Schätzung) abbildet. Der LARS-Algorithmus erzeugt den Lösungspfad sequentiell, sodass die Schätzung als Funktion von \\(\\lambda\\) veranschaulicht werden kann, ähnlich wie bei Ridge Regression.\n12 LARS steht für Least Angle Regression.Wir zeigen nun anhand simulierter Daten, wie Lasso-Lösungen mit dem R-Paket lars berechnet werden können. Hierfür erzeugen wir Daten gemäß der Vorschrift \\[\\begin{align}\n  \\begin{split}\n  Y_i =&\\, \\boldsymbol{X}_i' \\boldsymbol{\\beta}_v + u_i\\\\\n  \\\\\n  \\boldsymbol{\\beta}_v =&\\, (-1.25, -.75, 0, 0, 0, 0, 0, .75, 1.25)'\\\\\n  \\\\\n  \\boldsymbol{X}_i \\sim&\\, N(\\boldsymbol{0}, \\boldsymbol{I}_{9\\times9}), \\quad u_i \\overset{u.i.v.}{\\sim} N(0, 1), \\quad i = 1,\\dots,25.\n  \\end{split}\\label{eq:larsdgp}\n\\end{align}\\]\n\nlibrary(lars)\nset.seed(1234)\n\n# Parameter definieren\nN &lt;- 25\nbeta_v &lt;- c(-1.25, -.75, 0, 0, 0, 0, 0, .75, 1.25)\n\n# Beobachtungen simulieren\nX &lt;- matrix(rnorm(N * 9), ncol = 9)\nY &lt;- X %*% beta_v + rnorm(N)\n\nEntsprechend des DGP passen wir ein Modell ohne Konstante an. Damit lars::lars() den Lösungspfad des Lasso-Schätzers berechnet, muss type = \"lasso\" gewählt werden.13\n13 lars() standardisiert die Regressoren standardmäßig (aufgrund des DGPs hier nicht nötig).\n# Lösungen des Lasso-Schätzers mit LARS berechnen\n(\n  fit_lars &lt;- lars(\n    x = X, \n    y = Y, \n    intercept = F,\n    type = \"lasso\" # Wichtig: Lasso-Modus\n  )\n)\n\n\nCall:\nlars(x = X, y = Y, type = \"lasso\", intercept = F)\nR-squared: 0.858 \nSequence of LASSO moves:\n                      \nVar  9 2 8 1 3 5 4 7 6\nStep 1 2 3 4 5 6 7 8 9\n\n\nDie Zusammenfassung zeigt, dass der LARS-Algorithmus als erstes die (relevante) Variable \\(X_9\\) aktiviert.14 Mit abnehmender Regularisierung (kleinere \\(\\lambda\\)) werden in den nächsten 3 Schritten die übrigen relevanten Variablen \\(X_2\\), \\(X_8\\) und \\(X_1\\) aktiviert. Über die weiteren Schritte nähert der Algorithmus die Lösung an die saturierte Schätzung (das Modell mit allen neun Regressoren) an und aktiviert schrittweise die übrigen, irrelevanten Variablen.\n14 Aktivierung meint die Aufnahme einer Variable in der Modell gegeben eines hinreichend kleinen \\(\\lambda\\).Wir visualisieren die geschätzen Koeffizienten an jedem Schritt des Lösungspfads als Funktion von \\(\\lambda\\). In der Praxis wird der Regularisierungsparameter häufig auf der natürlichen log-Skala dargestellt.\n\n# Transformation in ein weites Format\nfit_lars$beta %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    lambda = c(fit_lars$lambda, 1e-2)\n  ) %&gt;% \n  pivot_longer(\n    cols = 1:9, \n    names_to = \"Variable\", \n    values_to = \"gesch. Koeffizient\"\n  ) %&gt;% \n  \n# Visualisierung mit ggplot  \n  ggplot(\n    mapping = aes(\n      x = log(lambda), \n      y = `gesch. Koeffizient`, \n      color = Variable\n    )\n  ) + \n  geom_line() \n\n\n\n\n\n\nAbbildung 13.7: LARS-Lösungspfad für Lasso-Schätzung\n\n\n\n\nAbbildung 13.7 zeigt, dass die Shrinkage der geschätzten Koeffizienten nach der Aktivierung rasch abnimmt und sich für kleine Werte von \\(\\lambda\\) der KQ-Lösung annähert. Wir sehen auch, dass es einen Bereich von \\(\\lambda\\)-Werten gibt, für die das wahre Modell mit den Variablen \\(X_1\\), \\(X_2\\), \\(X_8\\) und \\(X_9\\) selektiert werden kann. Je nach Ziel der Analyse kann es sinnvoll sein, ein \\(\\lambda\\) in diesem Intervall zu schätzen.\n\n13.2.3 Wahl des Regularisierungsparameters \\(\\lambda\\) für den Lasso-Schätzer\nWie zuvor bei Ridge Regression muss in empirischen Anwendungen ein Wert für den Tuning-Parameter \\(\\lambda\\) gewählt werden. Hierbei besteht die Herausforderung darin, einen geeigneten Wert zu finden, der zu wünschenswerten Eigenschaften des resultierenden Modells führt. So ist für gute Vorhersagen wichtig, dass das Modell nicht zu sehr an die Daten angepasst ist (Overfitting), um eine gute Generalisierung auf neue Daten zu ermöglichen. Gleichzeitig muss das Modell flexibel genug sein, um wesentliche Eigenschaften des datenerzeugenden Prozesses hinreichend gut zu erfassen. In der Regel wird hierbei eine sparsame Modellierung angestrebt, die nur eine Teilmenge der Prädiktoren nutzt.\nIn der Praxis werden verschiedene Verfahren verwendet, um den Wert für den Tuning-Parameter \\(\\lambda\\) zu bestimmen. Gängige Methoden sind Cross Validation (CV) und Informationskriterien. In Abhängigkeit der Methode und der Daten ergeben sich ober- oder unterparameterisierte Modelle. Aufgrund der Implementierung im R-Paket lars betrachten wir CV.15 Wir zeigen nachfolgend anhand der simulierten Daten aus dem letzten Abschnitt, wie für die LARS-Schätzung ein optimales \\(\\lambda\\) mit leave-one-out CV (LOO-CV) bestimmt werden kann. Hierzu nutzen wir lars::cv.lars() unter Verwendung derselben Argumente wie zuvor im Aufruf von lars().\n15 Chetverikov, Liao, and Chernozhukov (2020) zeigen, dass CV zu konsistenter Modellselektion führen kann.\n# LARS-Lösungen mit CV evaluieren\nfit_lars_cv &lt;- cv.lars(\n  x = X, \n  y = Y, \n  intercept = F,\n  normalize = T,\n  type = \"lasso\", \n  plot.it = F, \n  K = N # für LOO-CV\n) \n\nDas Objekt fit_lars_cv ist eine Liste mit den CV-Ergebnissen. Wir können diese einfach mit ggplot visualisieren. index ist hierbei das Verhältnis der \\(\\ell_1\\)-Norm des Lasso-Schätzers für einen spezifischen Wert von \\(\\lambda\\) und der \\(\\ell_1\\)-Norm des KQ-Schätzers. Das optimale \\(\\lambda\\) wird so implizit geschätzt. cv.error ist der mit CV geschätzte MSE.\n\n# CV-MSE\nfit_lars_cv %&gt;% \n  as_tibble() %&gt;%\n\n  ggplot(\n    mapping = aes(\n      x = index, \n      y = cv.error\n    )\n  ) + \n  geom_line() +\n  xlab(\"|beta_lambda| / |beta|\") +\n  ylab(\"CV-MSE\")\n\n\n\n\n\n\nAbbildung 13.8: CV-MSE und relative Position von \\(\\lambda\\) auf dem Lassopfad\n\n\n\n\nIn der Grafik erkennen wir ein Minimum des CV-MSEs bei etwa 0.73.\n\n# CV-MSE-minimierendes Lambda bestimmen\nID &lt;- which.min(fit_lars_cv$cv.error) # Index\n\n(\n  fraction_opt &lt;- fit_lars_cv$index[ID]\n)\n\n[1] 0.7272727\n\n\nDie geschätzten Koeffizienten für die optimale Regularisierung können mit coef() ausgelesen werden.\n\n# LARS-Lasso-Fit für optimales lambda bestimmen\ncoef(\n  object = fit_lars, \n  s = fraction_opt, \n  mode = \"fraction\"\n)\n\n[1] -0.6513191 -0.6060906 -0.1946089  0.0000000  0.0000000  0.0000000  0.0000000\n[8]  0.4977908  1.3122407\n\n\nDas Ergebnis veranschaulicht die Selektionseigenschaft von Lasso: Gemäß DGP \\(\\eqref{eq:larsdgp}\\) sind die Variablen \\(X_3\\) bis \\(X_7\\) irrelevante Prädiktoren für \\(Y\\); ihre wahren Koeffizienten sind \\(0\\). In der kreuzvalidierten Lasso-Schätzung erreicht die Regularisierung, dass die Koeffizienten der Variablen \\(X_4\\) bis \\(X_7\\) tatsächlich mit 0 geschätzt werden. Wir schätzen für das mit CV bestimmte \\(\\lambda\\) also ein leicht überspezifiziertes Modell mit den Regressoren \\(X_1\\), \\(X_2\\), \\(X_3\\), \\(X_8\\) und \\(X_9\\). Beachte, dass die Lasso-Schätzung einen Kompromiss impliziert: Die Varianz der Schätzung ist geringer als die des KQ-Schätzers im Modell mit allen Variablen.16 Aufgrund der Regularisierung sind die mit Lasso geschätzten Koeffizienten der relevanten Variablen jedoch in Richtung \\(0\\) verzerrt.\n16 Wegen \\(N=25\\) verbleiben bei der KQ-Schätzung mit 9 Regressoren nur 16 Freiheitsgrade.Einen positiven Effekt dieses Kompromisses beobachten wir anhand des mittleren Vorhersagefehlers für Daten, die nicht zur Berechnung des Schätzers verwendet wurden. Wir vergleichen den Vorhersagefehler nachfolgend anhand eines solchen simulierten Test-Datensatzes mit 25 neuen Beobachtungen. Den Vorhersagefehler bestimmen wir als MSE zwischen den vorhergesagten und den tatsächlichen Ausprägungen für \\(Y\\).\n\n# Test-Datensatz erstellen\nset.seed(4321)\nnew_X &lt;- matrix(rnorm(N * 9), ncol = 9)\nnew_Y &lt;- new_X %*% beta_v + rnorm(N)\n\n# Lasso: Vorhersage von new_Y für Test-Datensatz\nY_predict_lars &lt;- predict(\n  object = fit_lars, \n  s = fraction_opt, \n  type = \"fit\", \n  mode = \"fraction\", \n  newx = new_X\n)$fit\n\n# Lasso: MSE für Test-Datensatz berechnen\nmean((Y_predict_lars - new_Y)^2)\n\n[1] 1.419817\n\n\nWir schätzen nun das große Modell mit allen 9 Variablen mit KQ und berechnen ebenfalls den MSE der Prognosen für den Test-Datensatz.\n\n# KQ-Schätzung des großen Modells durchführen\nKQ_fit &lt;- lm(Y ~ X - 1)\n\n# Test-Datensatz für predict.lm() formatieren\nnew_X &lt;- as.data.frame(new_X)\ncolnames(new_X) &lt;- paste0(\"X\", 1:9)\n\n# KQ: Vorhersage von new_Y für Test-Datensatz\nY_predict_KQ &lt;- predict(\n  object = KQ_fit, \n  newdata = new_X\n)\n\n# KQ: MSE für Test-Datensatz berechnen\nmean((Y_predict_KQ - new_Y)^2)\n\n[1] 9.851932\n\n\nOffenbar führt die Lasso-Schätzung zu einem deutlich geringeren MSE der Vorhersage von Y für den Test-Datensatz als die KQ-Schätzung und damit zu einer höheren Vorhersagegüte. Das “sparsame” mit Lasso-Regression geschätzte Modell ist dem “großen” mit KQ geschätztem Modell in dieser Hinsicht also überlegen.\n\n\n\n\n\n\nKey Facts zu Lasso-Regression\n\n\n\n\nLasso-Regression bestraft die Verlustfunktion des KQ-Schätzers mit der \\(\\ell_1\\)-Norm der Koeffizienten.\nNeben Koeffizientenschätzung mit Shrinkage in Richtung \\(0\\) kann der Lasso-Schätzer Variablenselektion durchführen: Regressionskoeffizienten können exakt mit \\(0\\) geschätzt und so ein “sparsames”, leichter zu interpretierendes Modell gewählt werden.\nWie bei Ridge Regression impliziert die Wahl von \\(\\lambda\\) einen Bias-Variance-Tradeoff, der für Vorhersagen nützlich ist: Für größere \\(\\lambda\\) wird mehr Verzerrung induziert und möglicherweise relevante Variablen mit kleinen Koeffizienten aus dem Modell entfernt. Ein solches sparsames Modell kann eine höhere Prognosegüte haben als ein komplexes, unregularisiertes Modell.\nDer Lasso-Schätzer \\(\\widehat{\\boldsymbol{\\beta}}_\\lambda^L\\) ist nicht erwartungstreu.\nLasso Regression kann bspw. mit dem LARS-Algorithmus (Paket lars) oder mit glmnet berechnet werden.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularisierte Regression</span>"
    ]
  },
  {
    "objectID": "RegReg.html#vergleich-von-lasso--und-ridge-regression-mit-simulation",
    "href": "RegReg.html#vergleich-von-lasso--und-ridge-regression-mit-simulation",
    "title": "13  Regularisierte Regression",
    "section": "\n13.3 Vergleich von Lasso- und Ridge-Regression mit Simulation",
    "text": "13.3 Vergleich von Lasso- und Ridge-Regression mit Simulation\nIn diesem Kapitel illustrieren wir Vor- und Nachteile von Lasso- und Ridge-Regression in Prognose-Anwendungen anhand von Monte-Carlo-Simulationen. Wir betrachten hierbei datenerzeugende Prozesse, die sich hinsichtlich der Anzahl relevanter Variablen sowie der Korrelation dieser Variablen unterscheiden.\nDie grundlegende Vorschrift für die Simulationen ist \\[\\begin{align*}\n  Y_i = \\sum_{j=1}^{k=40} \\beta_j X_{i,j} + u_i, \\quad u_i \\overset{u.i.v.}{\\sim} N(0,1), \\quad i=1,\\dots,100,\n\\end{align*}\\] wobei die Regressoren \\(X_j\\) eine Varianz von \\(1\\) haben und aus einer multivariaten Normalverteilung mit Korrelation \\[\\rho\\in(0,0.5,0.8)\\] gezogen werden.\nFür die Koeffizienten \\(\\boldsymbol{\\beta}\\) unterscheiden wir zwei Szenarien. In Szenario A ist \\[\\boldsymbol{\\beta} = (1,\\dots,1)',\\] d.h. alle Variablen sind relevant und haben denselben Einfluss auf \\(Y\\). In Szenario B erzeugen wir \\(\\boldsymbol{\\beta}\\) einmalig vorab so, dass \\[\\beta_j = \\begin{cases}1,\\quad \\text{mit Wsk.  }p\\\\ 0,\\quad \\text{mit Wsk.  }1-p, \\end{cases}\\] d.h. nur eine Teilmenge der Variablen beeinflusst \\(Y\\) jeweils mit demselben Effekt \\(\\beta_j = 1\\). Die übrigen Variablen sind irrelevant.\nWir schätzen und validieren die Modelle mit glmnet().\n\n13.3.1 Prognosegüte in diversen Szenarien\n\n# Simulationsparameter definieren\nrho &lt;- c(0, 0.5, 0.8)   # Korrelation\nk &lt;- 40                 # Anz. Regressoren\nN &lt;- 100                # Anz. Beobachtungen\nn_sim &lt;- 100            # Anz. Simulationen\n\nDamit der Code für die Simulation möglichst wenig repetitiv ist, definieren wir eine Funktion cv.glmnet_MSE(), die unter Angabe der Daten X und Y, des Trainingssets train sowie des Parameters alpha den gewünschten regularisierten Schätzer under Verwendung von Cross Validation anpasst und den Testset-MSE zurückgibt.\n\n# allg. Funktion für Testset-MSE nach CV\ncv.glmnet_MSE &lt;- function(X, Y, train, alpha) {\n  \n  # Modell mit glmnet schätzen; lambda per CV bestimmen\n  fit_cv &lt;- cv.glmnet(\n    x = X[train,],\n    y =Y[train],\n    alpha = alpha\n  )\n  \n  # Vorhersagen treffen\n  Y_pred &lt;- predict(\n    object = fit_cv, \n    s = fit_cv$lambda.min, \n    newx = X[-train,])\n  \n  return(\n    # Testset-MSE berechnen\n    mean(\n      (Y[-train] - Y_pred)^2\n      )\n  )\n}\n\nWir initialisieren zunächst Matrizen, in welche die MSEs aus den 100 Simulationsdurchläufen reihenweise geschrieben werden. lasso_mse und ridge_mse haben je eine Spalte für jede Korrelation in rho\n\n# Matrizen für simulierte MSEs initialisieren...\nlasso_mse &lt;- matrix(\n  data = NA, \n  nrow = n_sim, \n  ncol = length(rho)\n) \nridge_mse &lt;- lasso_mse\n\n# ... und benennen\ncolnames(lasso_mse) &lt;- paste0(\"Kor=\", rho)\ncolnames(ridge_mse) &lt;- colnames(lasso_mse)\n\nFür die Simulation iterieren wir mit purrr::walk über den Vektor rho sowie über die Laufvariable 1:n_sim. Beide Schleifen nutzen den Syntax für anonyme Funktionen:\n\n# Die anonyme Funktion\nfunction(x) return(x)\n# ist äquivalent definiert als\n\\(x) return(x)\n\nIn jeden Simulationsdurchlauf erzeugen wir den Datensatz entsprechend der obigen Vorschrift, teilen die Daten auf und berechnen MSEs für Lasso- und Ridge-Regression mit cv.glmnet_MSE().\nSzenario A\n\n# Koeffizienten-Vektor definieren\nbeta &lt;- rep(1, k) \n\n\nlibrary(mvtnorm)\nlibrary(tidyverse)\n\nset.seed(1234)\n\n# Simulation durchführen\nwalk(1:length(rho), \\(j) {\n  \n  # Korrelationsmatrix definieren\n  Sigma &lt;- matrix(\n    data = rho[j], \n    nrow = k, \n    ncol = k\n  )\n  diag(Sigma) &lt;- 1\n  \n  walk(1:n_sim, \\(i) {\n    \n  # Daten simulieren\n  X &lt;- rmvnorm(\n    n = N, \n    mean = rep(0, k), \n    sigma = Sigma\n  )\n  Y &lt;- X %*% beta + rnorm(N)\n    \n  # Trainingsdaten definieren\n  ID_train &lt;- sample(\n    x = c(1:N), \n    size = N/2\n  )\n    \n  # Modelle mit CV schätzen und MSEs berechnen\n  # Ridge-Regression\n  ridge_mse[i, j] &lt;&lt;- cv.glmnet_MSE(\n    X = X, \n    Y = Y, \n    train = ID_train, \n    alpha = 0\n  )\n  \n  # Lasso-Regression\n  lasso_mse[i, j] &lt;&lt;- cv.glmnet_MSE(\n    X = X, \n    Y = Y, \n    train = ID_train, \n    alpha = 1\n  )\n  \n  })\n  \n})\n\nBeachte, dass hier der Super-Assignment-Operator &lt;&lt;- genutzt wird, damit walk die Matrizen ridge_mse und lasso_mse in der globalen Umgebung überschreibt.17\n17 Dies folgt aus der Definition von walk. &lt;- bewirkt hier lediglich Assignment in der Funktionsumgebung.Wir berechnen jeweils den mittleren MSEs, sammeln die Ergebnisse in einer tibble() und nutzen gt() für die tabellarische Darstellung.\n\nlibrary(gt)\n\n# Ergebnisse tabellarisch darstellen\ntibble(\n  Methode = c(\n    \"Lasso-Regression\", \n    \"Ridge-Regression\"\n  ),\n) %&gt;%\n  bind_cols(\n    bind_rows(\n      colMeans(lasso_mse),\n      colMeans(ridge_mse)  \n    )    \n  ) %&gt;%\n  gt() %&gt;%\n  tabopts\n\n\n\n\n\n\n\nMethode\nKor=0\nKor=0.5\nKor=0.8\n\n\n\nLasso-Regression\n7.17\n10.398\n7.581\n\n\nRidge-Regression\n4.841\n1.615\n1.517\n\n\n\n\n\n\n\nTabelle 13.1: Durchschnittliche Testset-MSEs für Setting A\n\n\n\nTabelle 13.1 zeigt, dass Ridge-Regression gegenüber Lasso-Regression für jede der drei betrachteten Korrelationen überlegen ist. Insbesondere bei stärker korrelierten Regressoren ist Ridge vorteilhaft.\nFür Szenario B überschreiben wir beta nach Multiplikation mit einem zufälligen binären Vektor, sodass einige der Koeffizienten \\(0\\) und die zugehörigen Variablen irrelevant für \\(Y\\) sind.\nSzenario B\n\n# Wsk. für Relevanz einer Variable\np &lt;- .3\n\n# Koeffizienten-Vektor definieren\nset.seed(123)\nbeta &lt;- beta * sample(\n  x = 0:1, \n  size = k, \n  replace = T, \n  prob = c(1-p, p)\n)\n\n# Koeffizienten prüfen\nhead(beta, n = 10)\n\n [1] 0 1 0 1 1 0 0 1 0 0\n\n\nEine wiederholung der Simulation für die modifizierten Koeffizienten beta und liefert folgende tabellarische Auswertung.\n\n\n\n\n\n\n\n\nMethode\nKor=0\nKor=0.5\nKor=0.8\n\n\n\nLasso\n2.51\n2.143\n1.923\n\n\nRidge\n3.331\n2.562\n2.014\n\n\n\n\n\n\n\nTabelle 13.2: Durchschnittliche Testset-MSEs für Szenario B\n\n\n\nDie Ergebnisse in Tabelle 13.2 zeigen, dass Ridge-Regression in Szenario B bis auf den Fall unkorrelierter Regressoren etwas schlechter abschneidet als in Szenario A. Die hohe Anzahl irrelevanter Variablen verbessert die Leistung von Lasso deutlich: Hier ist es plausibel, dass Lasso aufgrund der Thresholding-Eigenschaft die Koeffizienten einiger irrelevanten Variablen häufig exakt \\(0\\) setzt und damit ein sparsameres Modell schätzt als Ridge. Entsprechend erzielt Lasso in diesem Szenario insbesondere für \\(\\rho = 0\\) genauere Vorhersagen als Ridge Regression.\n\n13.3.2 Visualisierung des Bias-Variance-Tradeoffs bei Prognosen\nFür ein besseres Verständnis, wie sich der Regularisierungsparameter \\(\\lambda\\) auf den Bias-Variance-Tradeoff bei Prognosen mit Ridge- und Lasso-Regression auswirkt, vergleichen wir für beide Methoden nachfolgend die Abhängigkeit des MSEs der Prognose \\(\\widehat{Y}_0\\) für den Wert \\(Y_0\\) der abhängigen Variable eines Datenpunkts anhand seiner Regressoren \\(\\boldsymbol{X}_0'\\), wobei \\[\\begin{align}\n  \\text{MSE}(\\widehat{Y}_0) = \\text{Bias}(\\widehat{Y}_0)^2 + \\text{Var}(\\widehat{Y}_0) + \\text{Var}(Y_0) \\label{eq:pbvdecomp}\n\\end{align}\\] Beachte, dass \\(\\text{Var}(Y_0)\\) die durch den datenerzeugenden Prozess (und damit unvermeidbare) Varianz von \\(Y_0\\) ist, wohingegen \\(\\text{Bias}(\\widehat{Y}_0)^2\\) und \\(\\text{Var}(\\widehat{Y}_0)\\) von dem verwendeten Schätzer für \\(\\widehat{Y}_0\\) abhängt.\nFür die Simulation betrachten wir erneut Szenario A aus Kapitel 13.3.1 mit \\(50\\) Beobachtungen für ein Modell mit \\(40\\) unkorrelierten Regressoren. Wir legen zunächst die Simulationsparameter fest und erzeugen den vorherzusagenden Datenpunkt (X_0, Y_0).\n\n# Parameter festlegen\nset.seed(1234)\nn &lt;- 200 # Anz. Iterationen\nN &lt;- 50  # Anz. Beobachtungen\nk &lt;- 40  # Anz. Variablen\n\n# Korrelationsmatrix definieren\nSigma &lt;- diag(k) # Diagonalmatrix\nbeta &lt;- rep(x = 1, k)\n\n# Prognose-Ziel vorab zufällig generieren:\n\n# Regressoren\nX_0 &lt;- rmvnorm(\n  n = 1, \n  mean = rep(x = 0, k)\n)\n\n# Abh. Variable\nY_0 &lt;- X_0 %*% beta + rnorm(n = 1) %&gt;% \n  as.vector()\n\nAnhand der Simulationsergebnisse wollen wir die von der verwendeten Schätzfunktion abhängigen Komponenten von \\(\\eqref{eq:pbvdecomp}\\) untersuchen. Wir initialisieren hierzu die Listen ridge_fits und lasso_fits, in die unsere Simulationsergebnisse geschrieben werden.\n\n# Listen für Simulationsergebnisse initialisieren\nridge_fits &lt;- list()\nlasso_fits &lt;- list()\n\nWeiterhin definieren wir separate \\(\\lambda\\)-Sequenzen für Lasso- und Ridge-Schätzer.18\n18 Die Sequenzen haben wir in Abhängigkeit des DGP so gewählt, dass die Abhängigkeit der Prognosegüte von \\(\\lambda\\) gut visualisiert werden kann.\n# Lambda-Sequenzen festlegen\nlambdas_r &lt;- seq(.25, 2.5, length.out = 100)\nlambdas_l &lt;- seq(.05, 0.5, length.out = 100)\n\nFür die Simulation iterieren wir mit walk() über simulierte Datensätze und schreiben jeweils den vollständigen Output von glmnet() in die zuvor definierten Listen ridge_fits und lasso_fits.\n\n# Simulation\nwalk(1:n, \\(i) {\n  \n  # Daten simulieren\n  X &lt;- rmvnorm(\n    n = N, \n    mean = rep(0, k), \n    sigma = Sigma\n  )\n  Y &lt;- X %*% beta + rnorm(n = N, sd = 5)\n  \n  # Modelle mit glmnet schätzen\n  # Ridge-Regression\n  ridge_fits[[i]] &lt;&lt;- glmnet(\n    x = X, \n    y = Y, \n    alpha = 0, \n    intercept = F\n  )\n  # Lasso-Regression\n  lasso_fits[[i]] &lt;&lt;- glmnet(\n    x = X, \n    y = Y, \n    alpha = 1, \n    intercept = F\n  )\n  \n})\n\nWir nutzen Funktionen aus purrr und dplyr, um über die in den Simulationsdurchläufen angepassten Modelle zu iterieren. Mit predict() erhalten wir Punktvorhersagen für Y_0 für jedes \\(\\lambda\\) der zuvor definierten \\(\\lambda\\)-Sequenzen. Beachte, dass map() jeweils eine Liste mit 200 Punktvorhersagen für jedes der 100 zurückgibt. Mit list_rbind() können wir die Ergebnisse komfortabel jeweils in einer tibble sammeln.\n\n# Prognosen für Ridge-Regression\npred_r &lt;- map(\n  .x = ridge_fits, \n  .f = ~ as_tibble(\n    predict(\n      object = ., \n      s = lambdas_r, \n      newx = X_0\n    )\n  ) \n) %&gt;%\n  list_rbind() \n\n# Prognosen für Lasso-Regression\npred_l &lt;- map(\n  .x = lasso_fits, \n  .f = ~ as_tibble(\n    predict(\n      object = ., \n      s = lambdas_l, \n      newx = X_0)\n    ) \n) %&gt;%\n  list_rbind() \n\nFür die statistische Auswertung berechnen wir jeweils \\(\\text{MSE}(\\widehat{Y}_0)\\), \\(\\text{Bias}(\\widehat{Y}_0)^2\\) und \\(\\text{Var}(\\widehat{Y}_0)\\) und führen die Ergebnisse mit pivot_longer() in ein langes Format sim_data_r über. Wir berechnen weiterhin mit MSE_min_r das \\(\\lambda\\), für das wir über die Simulationsdurchläufe durchschnittlich den geringsten \\(\\text{MSE}\\) beobachten.\nRidge-Regression\n\n# Ergebnisse für Ridge-Regression zusammenfassen\nsim_data_r &lt;- tibble(\n  \n  lambda = lambdas_r,\n  \n  \"MSE\" = map_dbl(\n    .x = pred_r,  \n    .f = ~ mean((.x - Y_0)^2)\n  ),\n  \n  \"Bias^2\" = map_dbl(\n    .x = pred_r, \n    .f = ~ (mean(.x) - Y_0)^2\n  ),\n  \n  \"Varianz\" = map_dbl(\n    .x = pred_r, \n    .f = ~ var(.x)\n  )\n) %&gt;%\n  pivot_longer(\n    cols = -lambda, \n    values_to = \"Wert\",\n    names_to = \"Statistik\"\n  )\n\n# Lambda bei MSE-Minimum bestimmen\nMSE_min_r &lt;- sim_data_r %&gt;% \n  filter(\n    Statistik == \"MSE\",\n    Wert == min(Wert)\n  ) \n\nLasso-Regression\n\n# Ergebnisse zusammenfassen\nsim_data_l &lt;- tibble(\n  \n  lambda = lambdas_l,\n  \n  \"MSE\" = map_dbl(\n    .x = pred_l,  \n    .f = ~ mean((. - Y_0)^2)\n  ),\n  \n  \"Bias^2\" = map_dbl(\n    .x = pred_l, \n    .f = ~ (mean(.) - Y_0)^2\n  ),\n  \n  \"Varianz\" = map_dbl(\n    .x = pred_l, \n    .f = ~ var(.)\n  )\n) %&gt;%\n  pivot_longer(\n    cols = -lambda, \n    values_to = \"Wert\", \n    names_to = \"Statistik\"\n  )\n\n# Lambda bei MSE-Minimum bestimmen\nMSE_min_l &lt;- sim_data_l %&gt;% \n  filter(\n    Statistik == \"MSE\",\n    Wert == min(Wert)\n  ) \n\nDie Datensätze im langen Format, sim_data_r und sim_data_l, werden nun für die Visualisierung der Ergebnisse mit ggplo2 genutzt.\n# MSE, Bias^2 und Varianz gegen Lambda plotten\n\n# Ridge-Regression\nsim_data_r %&gt;%\n  ggplot(\n    mapping = aes(\n      x = lambda, \n      y = Wert, \n      color = Statistik\n    )\n  ) +\n  geom_line() +\n  geom_point(data = MSE_min_r)\n\n# Lasso-Regression\nsim_data_l %&gt;%\n  ggplot(\n    mapping = aes(\n      x = lambda, \n      y = Wert, \n      color = Statistik\n    )\n  ) +\n  geom_line() +\n  geom_point(data = MSE_min_l)\n\n\n\n\n\n\n\n\n\n(a) Ridge Regression\n\n\n\n\n\n\n\n\n\n\n\n(b) Lasso Regression\n\n\n\n\n\n\nAbbildung 13.9: Simulierte MSE-Komponenten in Abhängigkeit von Lambda\n\n\nAnhand von Abbildung 13.9 lässt sich der Bias-Variance-Tradeoff bei der Vorhersage von \\(Y_0\\) gut erkennen: Bereits für kleine \\(\\lambda\\) erzielen beide Methode eine deutliche Reduktion des MSE. Dies wir durch etwas zusätzlichen Bias, aber eine überproportionale Verringerung der Varianz erreicht. Der erkennbare funktionale Zusammenhang zeigt, dass der MSE eine konvexe Funktion von \\(\\lambda\\) ist. Damit existieren optimale \\(\\lambda\\) mit minimalem MSE (grüne Punkte), die wir mit Cross Validation schätzen können.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularisierte Regression</span>"
    ]
  },
  {
    "objectID": "RegReg.html#inferenz-für-treatment-effekt-schätzung-mit-vielen-variablen",
    "href": "RegReg.html#inferenz-für-treatment-effekt-schätzung-mit-vielen-variablen",
    "title": "13  Regularisierte Regression",
    "section": "\n13.4 Inferenz für Treatment-Effekt-Schätzung mit vielen Variablen",
    "text": "13.4 Inferenz für Treatment-Effekt-Schätzung mit vielen Variablen\nIn empirischen Studien des Effekts einer Behandlungsvariable \\(B\\) auf eine Outcome-Variable \\(Y\\) steht häufig eine Vielzahl potentieller Kontrollvariablen zur Verfügung. Häufig ist unklar, welche Variablen in das Modell aufgenommen werden sollten, um das Risiko einer verzerrten Schätzung durch ausgelassene Variablen zu vermindern und gleichzeitig eine Schätzung mit geringer Varianz zu gewährleisten. Ist der Beobachtungsumfang \\(N\\) relativ zur Variablenanzahl \\(k\\) groß, so kann die KQ-Schätzung einer langen Regression (ein Modell mit allen \\(k\\) Kontrollvariablen) gute Ergebnisse liefern. In der Praxis liegt diese wünschenswerte Situation jedoch oft nicht vor und es ist \\(k\\lesssim N\\) oder sogar \\(k&gt;N\\). Dann ist eine KQ-Schätzung des Behandlungseffekts anhand aller \\(k\\) Variablen mit hoher Varianz behaftet bzw. gar nicht möglich.19 Ein weiteres Szenario ist \\(k(N)&gt;N\\), d.h. die Anzahl der Regressoren kann mit dem Beobachtungsumfang wachsen.20 Lasso-Verfahren können dann hilfreich sein, um Determinanten von \\(Y\\) und \\(B\\) zu identifizieren und damit eine Menge an Kontrollvariablen zu selektieren, für die eine erwartungstreue und konsistente Schätzung des interessierenden Effekts wahrscheinlich ist.\n19 Beachte, dass der KQ-Schätzer bei \\(k&gt;N\\) nicht lösbar ist.20 Dieses Szenario wird unter Bedingungen bzgl. der Wachstumsrate und der Größe der Koeffizienten betrachet, s. (Belloni und Chernozhukov 2013).21 Hahn u. a. (2018) geben eine ausführliche Erläuterung dieser Problematik.Betrachte zunächst das Modell mit allen Kontrollvariablen \\(X_j\\), \\[\\begin{align}\n  Y_i = \\beta_0 + \\alpha_0 B_i + \\sum_{j=1}^k \\beta_{j} X_{i,j} + u_i, \\label{eq:lassotmt}\n\\end{align}\\] wobei einige \\(\\beta_{j}=0\\) sind und wir annehmen, dass \\(B\\) lediglich mit ein paar der \\(X_j\\) korrelliert. Die Shrinkage der geschätzten Koeffizienten aus einer naiven Lasso-Regression von \\(\\eqref{eq:lassotmt}\\) führt grundsätzlich zu einer verzerrten Schätzung des Behandlungseffekts \\(\\alpha_0\\) und damit zu ungültiger Inferenz.21\nDie Verzerrung von geschätzten Koeffizienten kann vermieden werden, indem Lasso lediglich zur Selektion von Kontrollvariablen verwendet wird. Dabei wird mit einer Lasso-Regression von \\(Y\\) auf die \\(X_j\\) eine Teilmenge von Regressoren \\(\\mathcal{S}\\) selektiert und der Treatment-Effekt anschließend mit der KQ-Schätzung von \\[\\begin{align}\n  Y_i = \\beta_0 + \\alpha_0 B_i + \\sum_{j\\in\\mathcal{S}} \\beta_{j} X_{i,j} + e_i,\n\\end{align}\\] basierend auf der Selektion \\(\\mathcal{S}\\) berechnet wird.22 Ein solcher Post-Lasso-Selection-Schätzer (Belloni und Chernozhukov 2013) ist jedoch im Allgemeinen und insbesondere in hoch-dimensionalen Settings nicht konsistent für \\(\\alpha_0\\) und nicht asymptotisch normalverteilt, da weiterhin die Gefahr einer verzerrten Schätzung durch in \\(\\mathcal{S}\\) ausgelassene Variablen besteht, die mit \\(B\\) korrelieren: Lasso selektiert Variablen \\(X_j\\), die “gut” \\(Y\\) erklären. Dabei kann nicht ausgeschlossen werden, das ein Modell gewählt wird, dass relevante Determinanten von \\(B\\) auslässt. Selbst wenn wir ein mit Lasso gewähltes Modell mit KQ (d.h. ohne Shrinkage) schätzen, würde \\(\\alpha_0\\) verzerrt geschätzt!\n22 Solche Verfahren werden Post-Selection-Schätzer gennant.Belloni, Chernozhukov, und Hansen (2014) schlagen ein alternatives Verfahren vor, dass auf Selektion der Determinanten \\(X_j\\) von \\(Y\\) und \\(B\\) basiert. Dieses Verfahren wird als Post-Double Selection bezeichnet und kann wiefolgt implementiert werden:\nPost-Double-Selection-Schätzer\n\nBestimme die Determinanten \\(X_j\\) von \\(Y\\) mit Lasso-Regression und bezeichne die Menge der selektierten Variablen als \\(\\mathcal{S}_Y\\).\nBestimme die Determinanten \\(X_j\\) von \\(B\\) mit Lasso-Regression und bezeichne die Menge der selektierten Variablen als \\(\\mathcal{S}_B\\).\nBestimme die Schnittmenge \\(\\mathcal{S}_{YB} = \\mathcal{S}_Y \\cap \\mathcal{S}_B\\). Schätze den Treatment-Effekt als \\(\\widehat{\\alpha}_0\\) in der KQ-Regression \\[\\begin{align}\n  Y_i = \\beta_0 + \\alpha_0 B_i + \\sum_{j\\in\\mathcal{S}_{YB}} \\beta_{j} X_{i,j} + v_i.\n\\end{align}\\]\n\nBelloni, Chernozhukov, und Hansen (2014) zeigen, dass \\(\\widehat{\\alpha}_0\\) aus diesem Verfahren ein asymptotisch normalverteiler Schätzer für \\(\\alpha_0\\) ist und herkömmliche t-Tests und Konfidenzintervalle gültige Inferenz erlauben.\nWir illustrieren die in diesem Abschnitt betrachteten Schätzer nun anhand simulierter Daten mit R. Die fiktive Problemstellung ist die Schätzung eines wahren Treatment-Effekts \\(\\alpha_0 = 2\\), wenn so viele potenzielle Kontrollvariablen vorliegen, dass der KQ-Schätzer gerade noch berechnet werden kann, aber aufgrund hoher Varianz unzuverlässig ist. Hierzu erzeugen wir \\(Y\\) gemäß der Vorschrift \\[\\begin{align*}\n  Y_i =&\\, \\alpha_0 B_i + \\sum_{j=1}^{k_Y} \\beta_{j}^Y X_{i,j}^Y + \\sum_{l=1}^{k_{YB}} \\beta_{l}^{YB} X_{i,l}^{YB} + u_i,\\\\\n  \\\\\n  \\beta_j^{YB} \\overset{u.i.v}{\\sim}&\\,N(10,1), \\quad \\beta_j^{Y} \\overset{u.i.v}{\\sim}U(0,1), \\quad u_i \\overset{u.i.v}{\\sim}N(0,1).\\\\\n  \\\\\n  i=&\\,1,\\dots,550\n\\end{align*}\\]\nDie Behandlungsvariable \\(B_i\\) entspricht der Vorschrift \\[\\begin{align*}\n  B_i =&\\, \\sum_{l=1}^{k_{YB}} \\beta_{l}^{YB} X_{i,l}^{YB} + e_i,\\\\\n  \\\\\n  \\beta_j^{YB} \\overset{u.i.v}{\\sim}&\\,N(2,0.2), \\quad e_i \\overset{u.i.v}{\\sim}N(0,1).\n\\end{align*}\\] Wir wählen \\(k_{YB} = k_{Y} = 25\\). Zusätzlich zu \\(B\\), den Determinanten von \\(Y\\) und \\(B\\) (\\(X^{YB}\\)) sowie den Variablen, die ausschließlich \\(Y\\) beeinflussen (\\(X^{Y}\\)) gibt es \\(k_U = 499\\) Variablen \\(X^U\\), die weder \\(Y\\) noch \\(B\\) beeinflussen und damit irrelevant für die Schätzung des Behandlungseffekts sind. Wir haben also \\(N=550\\) Beobachtungen und insgesamt \\(k = 1+k_{Y} + k_{YB} + k_{U} = 550\\) potenzielle Kontrollvariablen von denen \\(k_{YB} = 25\\) für eine unverzerrte Schätzung von \\(\\alpha_0\\) relevant sind.\nDer nachstehende Code generiert die Daten gemäß der Vorschrift.\n\nlibrary(mvtnorm)\nlibrary(tidyverse)\nset.seed(4321)\n\nn &lt;- 550      # Beobachtungen\np_Y &lt;- 25     # Determinanten Y\np_B &lt;- 25     # Determinanten B *und* Y\np_U &lt;- 499    # irrelevante Variablen \n\n# Variablen generieren\nXB &lt;- rmvnorm(n = n, sigma = diag(p_B))\nXU &lt;- rmvnorm(n = n, sigma = diag(p_U))\nXY &lt;- rmvnorm(n = n, sigma = diag(p_Y))\n\n# Stetige Behandlungsvariable erzeugen\nB &lt;- XB %*% rnorm(p_B, 2, sd = .2) + rnorm(n)\n\n# Abh. Variable erzeugen, Behandlungseffekt (ATE) ist 2\nY &lt;- 2 * B + \n  XB %*% rnorm(p_B, mean = 10) + \n  XY %*% runif(p_Y) + \n  rnorm(n)\n\n# Variablen in tibble sammeln\nX &lt;- cbind(B, XB, XU, XY) %&gt;% \n  as_tibble()\n\n# Namen zuweisen\ncolnames(X) &lt;- c(\n  \"B\", \n  paste0(\"XB\", 1:p_B), \n  paste0(\"XU\", 1:p_U),\n  paste0(\"XY\", 1:p_Y) \n)\n\nWünschenswert wäre die KQ-Schätzung des wahren Modells. Diese ergibt eine Schätzung nahe des wahren Treatment-Effekts \\(\\alpha_0 = 2\\). Unter realen Bedingungen wäre diese Regression jedoch nicht implementierbar, weil die relevanten Kovariablen XB unbekannt sind.\n\n# KQ: Wahres Modell schätzen\nlm(Y ~ B + XB - 1)$coefficients[\"B\"]\n\n       B \n1.937031 \n\n\nWir schätzen daher zunächst die “lange” Regression mit allen \\(k\\) verfügbaren Variablen mit KQ. Beachte, dass der KQ-Schätzer für \\(\\alpha_0\\) zwar implementierbar und erwartungstreu ist, jedoch eine hohe Varianz aufweist. Wegen \\(k=N=550\\) erhalten wir eine perfekte Anpassung an die Daten und können mangels Freiheitsgraden keine Hypothesentests durchführen.\n\n# KQ: Lange Regression schätzen\nlm(Y ~ . - 1, data = X)$coefficients[\"B\"]\n\n       B \n3.079497 \n\n\nDie KQ-Schätzung von \\(\\alpha_0\\) anhand der langen Regression weicht deutlich vom wahren Wert \\(\\alpha_0 = 2\\) ab.\nEine “kurze” KQ-Regression nur mit der Behandlungsvariable \\(B\\) führt wegen Korrelation mit den ausgelassenen Determinanten in XB zu einer deutlich verzerrten Schätzung.\n\n# KQ: Kurze Regression\nlm(Y ~ B - 1)$coefficients[\"B\"]\n\n       B \n6.716837 \n\n\nDie Methoden von Belloni und Chernozhukov (2013) und Belloni, Chernozhukov, und Hansen (2014) sind im R-Paket hdm implementiert. Mit den Funktionen hrm::rlasso() und hdm::rlassoEffect kann Lasso-Regression sowie Post- und Double-Post-Selection durchgeführt werden.23\n23 Diese Funktionen ermitteln ein optimales \\(\\lambda\\) mit dem in Belloni u. a. (2012) vorgeschlagenen Algorithmus.Wir berechnen zunächst den naiven Lasso-Schätzer in einem Modell mit allen Variablen.\n\nlibrary(hdm)\n\n# Naiver Post-Lasso-Schätzer\nlasso &lt;- rlasso(\n  x = X, \n  y = Y, \n  intercept = F, \n  post = F\n)\n\n# Koeffizientenschätzer auslesen\nlasso$coefficients[\"B\"] \n\n       B \n6.368456 \n\n\nAuch dieser Schätzer ist deutlich verzerrt. Problematisch ist hier nicht nur die Shrinkage auf \\(\\widehat{\\alpha}_0\\), sondern die Selektion der Variablen in XB:\n\n# Welche Variablen in XB selektiert Lasso *nicht*?\nnselektiert &lt;- which(lasso$coef[1:26] == 0)   # ID\n\n# Namen auslesen\nnames(lasso$coef[1:26])[nselektiert]\n\n[1] \"XB8\"  \"XB10\" \"XB16\" \"XB18\" \"XB20\"\n\n\nDurch das Auslassen dieser Determinanten von \\(Y\\) und \\(B\\) leidet der Lasso-Schätzer unter OVB.\nAls nächstes berechnen wir den Post-Lasso-Selection-Schätzer.\n\n# Post-Lasso-Selection-Schätzer berechnen\np_lasso &lt;- rlasso(\n  x = X,\n  y = Y, \n  intercept = F, \n  post = T\n)\n\n# Schätzung für alpha_0\np_lasso$coef[\"B\"]\n\n       B \n6.362409 \n\n\nDie Ähnlichkeit der Post-Lasso-Schätzung von \\(\\alpha_0\\) zur Lasso-Schätzung zeigt deutlich, dass die Verzerrung des Lasso-Schätzers überwiegend durch ausgelassene Variablen anstatt durch Shrinkage verursacht wird.\nMit rlassoEffect() können wir den Post-Double-Selection-Schätzer berechnen.\n\n# Post-Double-Selection-Schätzer\npds_lasso &lt;- rlassoEffect(\n  x = X %&gt;% \n    dplyr::select(-B) %&gt;% \n    as.matrix(),\n  y = Y, \n  d = B, \n  method = \"double selection\"\n)\n\n# Schnittmenge der selektierten Determinanten \n# von Y und B\n(\n  S_BY &lt;- names(\n    which(pds_lasso$selection.index)\n  )\n)\n\n [1] \"XB1\"   \"XB2\"   \"XB3\"   \"XB4\"   \"XB5\"   \"XB6\"   \"XB7\"   \"XB8\"   \"XB9\"  \n[10] \"XB10\"  \"XB11\"  \"XB12\"  \"XB13\"  \"XB14\"  \"XB15\"  \"XB16\"  \"XB17\"  \"XB18\" \n[19] \"XB19\"  \"XB20\"  \"XB21\"  \"XB22\"  \"XB23\"  \"XB24\"  \"XB25\"  \"XU209\" \"XU241\"\n[28] \"XU295\" \"XY3\"   \"XY7\"   \"XY8\"   \"XY12\"  \"XY13\"  \"XY15\"  \"XY16\"  \"XY19\" \n[37] \"XY23\" \n\n\nDouble Selection führt ebenfalls zu einem Post-Lasso-KQ-Schätzer mit allen 25 relevaten Variablen in XB. Wir selektieren allerdings deutlich weniger irrelevante Variablen aus XU als mit Single Selection und dennoch einige Determinanten von \\(Y\\) aus XY. Double Selection führt also zu einer unverzerrten Schätzen mit geringerer Varianz. Mit summary() erhalten wir gültige Inferenz bzgl. des Treatment-Effekts.\n\nsummary(pds_lasso)\n\n[1] \"Estimates and significance testing of the effect of target variables\"\n   Estimate. Std. Error t value Pr(&gt;|t|)    \nd1   1.94977    0.07127   27.36   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDer Post-Double-Selection-Schätzer liefert unter den betrachteten Verfahren die beste Schätzung von \\(\\alpha_0\\) und erlaubt gülstige statistische Inferenz. Der geschätzte Effekt ist hoch-signifikant.\n\n\n\n\n\n\nKey Facts zum Post-Double-Selection-Schätzer\n\n\n\n\nDurch die sorgfältige Auswahl von Variablen, die mit Behandlung- und Outcome-Variable zusammenhängen, ermöglicht die Double-Selection eine bessere Kontrolle über das Risiko ausgelassender Variablen in Beobachtungsstudien und ermöglicht gültige (asymptotisch normale) Inferenz.\n\nDer Post-Double-Selection-Schätzer besteht aus drei Regressionen:\n\nEs werden Variablen mit Lasso selektiert, welche die Behandlungs-Variable erklären.\nEs werden Variablen mit Lasso selektiert, welche die Outcome-Variable erklären.\nDer Post-Double-Selection-Schätzer ist der KQ-Schätzer in einer Regression, die für die Schnittmenge der ausgewählten Variablen kontrolliert.\n\n\nDank der Selektion mit Lasso kann der Schätzer auch bei hoch-dimensionalen Daten (\\(k&gt;n\\)) angewendet werden.\nPost-Double-Selection-Schätzer für Behandlungseffekte sind im R-Paket hdm implementiert.\n\n\n\n\n13.4.1 Case Study: Makroökonomisches Wachstum\nZur Illustration des Post-Double-Selection Schätzers betrachten wir eine empirische Anwendung bzgl. der Validierung von makroökonomischer Wachstumtheorie. Aus neo-klassischen Ansätzen wie dem Solow-Swan-Modell kann die Hypothese, dass Volkswirtschaften zu einem gemeinsamen Wachstumspfad hin konvergieren, abgeleitet werden. Diese Konvergenzhypothese impliziert die Existenz von Aufholeffekten: Ärmere Volkswirtschaften müssen im mittel schneller Wachsen als die Wirschaft wohlhabender Länder. Die grundlegende Spezifikation eines entsprechenden Regressionsmodells lautet \\[\\begin{align}\n  \\text{WR}_{i} = \\alpha_0 \\text{BIP0}_i + u_i, \\label{eq:growthmodel1}\n\\end{align}\\] wobei \\(\\text{WR}_{i}\\) die Wachstumsrate des Pro-Kopf-BIP in Land \\(i\\) über einen Zeitraum (typischerweise berechnet als Log-Differenz zwischen zwei Perioden) und \\(\\text{BIP0}_i\\) das (logarithmierte) Pro-Kopf-BIP zu beginn der Referenzperiode ist. Gemäß der Konvergenzhypothese muss \\(\\alpha_0&lt;0\\) sein: Je wohlhabender eine Volkswirtschaft ist, desto geringer ist das Wirtschaftswachstum.\nUm Verzerrung durch ausgelassene Kovariablen zu vermeiden, sollte das Modell \\(\\eqref{eq:growthmodel1}\\) um länder-spezifische Regressoren \\(x_{i,j}\\), die sowohl das Ausgagnsniveau \\(\\text{BIP0}\\) sowie die Wachtumsrate beinflussen, erweitert werden. Zu der großen Menge potentieller Kovariablen gehören makro- und sozio-ökonomische Maße wie bspw. die Investitionstätigkeit des Staates, Offenheit der Volkswirtschaft, das politische Umfeld, das Bildungsniveau, die Demographie usw. Eine bevorzugte Spezifikation ist daher \\[\\begin{align}\n  \\text{WR}_{i} = \\alpha_0 \\text{BIP0}_i + \\sum_{j=1}^k \\beta_j x_{i,j} + u_i,\\label{eq:growthmodel2}\n\\end{align}\\] wobei \\(\\alpha_0\\) als Behandlungseffekt interpretiert werden kann. Beachte, dass \\(\\eqref{eq:growthmodel2}\\) eine Regression in der Form von \\(\\eqref{eq:lassotmt}\\) ist.\nWir illustrieren die Schätzung von und Inferenz bzgl. \\(\\alpha_0\\) in \\(\\eqref{eq:growthmodel2}\\) mit Post-Double-Selektion für einen 90 Länder umfassenden Auszug aus dem Datensatz von Barro und Lee (2013), der als Objekt GrowthData im R-Paket hdm verfügbar ist.24\n24 Eine ausführliche Beschreibung der Variablen ist hier einsehbar.\n# Datensatz in Arbeitsumgebung verfügbar machen\nlibrary(hdm)\ndata(GrowthData)\n\n# Anzahl Beobachtungen und Variablen\ndim(GrowthData)\n\n[1] 90 63\n\n\nDie Spalte Outcome ist die jeweilige Wachstumsrate des BIP zwischen den Perioden 1965-1975 und 1975-1985 und gdpsh465 ist das reale Pro-Kopf-BIP im Jahr 1965 zu Preisen von 1980.\nWir führen zunächst eine graphische Analyse hinsichtlich des Modells einfachen Modells \\(\\eqref{eq:growthmodel1}\\) durch, indem wir gdpsh465 gegen Outcome plotten und die geschätzte Regressionsgerade einzeichnen.\n\n# Einfache grafische Analyse mit ggplot2\nGrowthData %&gt;%\n  ggplot(\n    mapping = aes(\n      x = gdpsh465, \n      y = Outcome\n    )\n  ) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F)\n\n\n\n\n\n\nAbbildung 13.10: BIP-Wachstum: Einfache Regression\n\n\n\n\nAbbildung 13.10 zeigt einen geringen positiven geschätzten Effekt \\(\\widehat{\\alpha}_0\\). Eine Auswertung mit lm() ergibt, dass der Effekt \\(\\alpha_0\\) nicht signifikant von \\(0\\) verschieden ist.\n\n# Einfache Regression durchführen, \n# Inferenz für gdpsh465 erhalten\nlm(Outcome ~ gdpsh465, data = GrowthData) %&gt;%\n  summary() %&gt;%\n  coefficients() %&gt;% \n  .[2, ]\n\n   Estimate  Std. Error     t value    Pr(&gt;|t|) \n0.001316713 0.006102200 0.215776701 0.829661165 \n\n\nDer positive Effekt aus der einfachen Schätzung widerspricht der Konvergenzhypothese. Dieses Ergebnis könnte allerdings durch Auslassen relevanter Kovariablen ungültig sein. Beispielsweise ist es plausibel, dass das Bildungsniveau einer Volkswirtschaft sowohl mit dem BIP korreliert ist als auch die Wachstumsrate beeinflusst. Dann wäre das Bildungsniveau eine relevante Kovariable, deren Auslassen zu einer verzerrten Schätzung von \\(\\alpha_0\\) führt.\nEine “lange” Regression mit allen Kovariablen ist zwar möglich, aber problematisch: Das Verhältnis von Beobachtungen (90) zu Regressoren (62) bedeutet eine hohe Unsicherheit der Schätzung.\n\n# Inferenz für alpha_0 in langer Regression\nsummary(\n  lm(Outcome ~ . - 1 , data = GrowthData)\n  ) %&gt;% \n  coefficients() %&gt;% \n  .[2, ]\n\n    Estimate   Std. Error      t value     Pr(&gt;|t|) \n-0.009377989  0.029887726 -0.313773911  0.756018518 \n\n\nDer geschätzte Koeffizient \\(\\widehat{\\alpha}_0\\) ist nun zwar negativ, liefert jedoch weiterhin keine Evidenz, dass \\(\\alpha_0\\) von 0 verschieden ist. Ein Vergleich der Standardfehler zeigt aber, dass die KQ-Schätzung aufgrund Berücksichtigung aller potentiellen Kovariablen mit deutlich größerer Varianz behaftet ist als in der einfachen KQ-Regression \\(\\eqref{eq:growthmodel1}\\)\nPost-Double-Selection erlaubt gültige Inferenz bzgl. \\(\\alpha_0\\) nach Schätzung der Menge relevanter Kovariablen. Wir weisen die entsprechenden Variablen R-Objekten zu und berechnen den Schätzer.\n\n# Variablen für Post-Double-Selection vorbereiten\n\n# abh. Variable\ny &lt;- GrowthData %&gt;% \n  pull(Outcome)\n\n# \"Treatment\"\nd &lt;- GrowthData %&gt;% \n  pull(gdpsh465)\n\n# potentielle Regressoren\nX &lt;- GrowthData %&gt;% \n  dplyr::select(\n    -Outcome, -intercept, -gdpsh465\n  )\n\n\n# Post-Double-Selection-Schätzer berechnen\nGrowth_DS &lt;- \n  rlassoEffect(\n    x = X %&gt;% \n      as.matrix(), \n    y = y, \n    d = d, \n    method = \"double selection\"\n)\n\nPost-Double-Selection wählt aus der Menge potentieller Kovariablen lediglich sieben Regressoren aus.\n\n# Selektierte Variablen einsehen\n# ID\nSelektion &lt;- Growth_DS$selection.index\n\n# Namen auslesen\nnames(\n  which(Selektion == T)\n)\n\n[1] \"bmp1l\"    \"freetar\"  \"hm65\"     \"sf65\"     \"lifee065\" \"humanf65\" \"pop6565\" \n\n\nTabelle 13.3 zeigt die Definitionen der ausgewählten Variablen.\n\n\n\n\n\n\n\n\nVariable\nBeschreibung\n\n\n\nbmp1l\nSchwarzmarktprämie d. Währung\n\n\nfreetar\nMaß für Zollbeschränkungen\n\n\nhm65\nEinschreibungsquote Uni (Männer)\n\n\nsf65\nBeschulungsquote Sekundarstufe (Frauen)\n\n\nlifee065\nLebenserwartung bei Geburt\n\n\nhumanf65\nDurschn. Bildung im Alter 25 (Frauen)\n\n\npop6565\nAnteil Bevölkerung ü. 65 Jahre\n\n\n\n\n\n\n\nTabelle 13.3: Mit PDS selektierte Variablen aus GrowthData. Referenzjahr 1965.\n\n\n\n\n# Gültige Inferenz mit dem Post-Double-Selection-Schätzer\nsummary(Growth_DS)\n\n[1] \"Estimates and significance testing of the effect of target variables\"\n   Estimate. Std. Error t value Pr(&gt;|t|)   \nd1  -0.05001    0.01579  -3.167  0.00154 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDas Ergebnis der Post-Double-Selection-Schätzung unterstützt die (bedingte) Konvergenzhypothese mit einer signifikanten negativen Schätzung \\(\\widehat{\\alpha}_0\\approx-0.05\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBarro, Robert J., und Jong Wha Lee. 2013. „A new data set of educational attainment in the world, 1950–2010“. Journal of Development Economics 104: 184–98. https://doi.org/https://doi.org/10.1016/j.jdeveco.2012.10.001.\n\n\nBelloni, Alexandre, Daniel Chen, Victor Chernozhukov, und Christian Hansen. 2012. „Sparse models and methods for optimal instruments with an application to eminent domain“. Econometrica 80 (6): 2369–429.\n\n\nBelloni, Alexandre, und Victor Chernozhukov. 2013. „Least squares after model selection in high-dimensional sparse models“. Bernoulli, 521–47.\n\n\nBelloni, Alexandre, Victor Chernozhukov, und Christian Hansen. 2014. „High-dimensional methods and inference on structural and treatment effects“. Journal of Economic Perspectives 28 (2): 29–50.\n\n\nCortez, Paulo, und Alice Maria Gonçalves Silva. 2008. „Using data mining to predict secondary school student performance“.\n\n\nEfron, Bradley, Trevor Hastie, Iain Johnstone, und Robert Tibshirani. 2004. „Least angle regression“.\n\n\nHahn, P Richard, Carlos M Carvalho, David Puelz, und Jingyu He. 2018. „Regularization and confounding in linear regression for treatment effect estimation“.\n\n\nHoerl, Arthur E, und Robert W Kennard. 1970. „Ridge regression: Biased estimation for nonorthogonal problems“. Technometrics 12 (1): 55–67.\n\n\nTibshirani, Robert. 1996. „Regression shrinkage and selection via the lasso“. Journal of the Royal Statistical Society Series B: Statistical Methodology 58 (1): 267–88.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularisierte Regression</span>"
    ]
  },
  {
    "objectID": "svm.html",
    "href": "svm.html",
    "title": "\n14  Support Vector Machines\n",
    "section": "",
    "text": "14.1 Trennende Hyper-Ebenen\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\n\nset.seed(1234)\n\nlsvm_data &lt;- bind_rows(\n  tibble(\n    x1 = rnorm(50, mean = -2),\n    x2 = rnorm(50, mean = -2),\n    klasse = factor(rep(1, 50))\n  ),\n  tibble(\n    x1 = rnorm(50, mean = 2),\n    x2 = rnorm(50, mean = 2),\n    klasse = factor(rep(2, 50))\n  )\n)\nWir betrachten zunächst ein Klassifikationsproblem mit zwei Prädiktoren \\(X_1\\), \\(X_2\\) für die binäre Outcome-Variable \\(Y\\). Die nächste Abbildung stellt diese Beobachtungen im Prädiktorraum graphisch dar. Mit geom_function() fügen wir dem Plot drei mögliche Trenneben hinzu, welche die Klassen perfekt seprarieren.\nlibrary(ggplot2)\nlibrary(cowplot)\n# Visualisiere den Datensatz mit ggplot2\nggplot(\n  data = lsvm_data,\n  mapping = aes(x = x1, y = x2, color = klasse)\n  ) +\n  geom_point(size = 3) +\n  labs(x = \"X1\", y = \"X2\") +\n  # Mögliche Trennebenen\n  geom_function(fun = \\(x)  -1.5 * x, col = \"black\") +\n  geom_function(fun = \\(x)  -2.5 * x, col = \"black\") +\n  geom_function(fun = \\(x)  -.4 * x, col = \"black\") +\n  lims(y = c(-5, 6)) +\n  theme_cowplot() +\n  theme(legend.position = \"top\")\n\n\n\nPerfekt linear speparierbare Beobachtungen",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "svm.html#trennende-hyper-ebenen",
    "href": "svm.html#trennende-hyper-ebenen",
    "title": "\n14  Support Vector Machines\n",
    "section": "",
    "text": "14.1.1 Trennebenen\nSVM für Klassifikation basiert auf der Idee, eine Trennhyperebene \\[\\begin{align}\n  \\boldsymbol{w}'\\boldsymbol{X} + b = 0 \\label{eq:sepplane}\n\\end{align}\\] im Prädiktorraum zu finden, welche die Datenpunkten hinsichtlich ihrer Klasse möglichst gut trennt.\nDefiniere die abhängige Variable als \\[\\begin{align*}\n  y =\n  \\begin{cases}\n    -1, & \\textup{wenn Beob. $i$ Klasse A hat,}\\\\\n     1, & \\textup{wenn Beob. $i$ Klasse B hat}\n  \\end{cases}\n\\end{align*}\\]\nWähle \\(\\boldsymbol{w}\\) und \\(b\\) so, dass die Ungleichung\n\\[\\begin{align}\n  y_i(\\boldsymbol{w}'\\boldsymbol{X}_i + b) \\geq 0 \\label{eq:sepcond}\n\\end{align}\\]\nerfüllt ist, d.h.\n\\[\\begin{align*}\n\\boldsymbol{w}'\\boldsymbol{X}_i + b \\geq 0  & \\quad \\textup{für} \\quad y_i = 1,\\\\\n  \\boldsymbol{w}'\\boldsymbol{X}_i + b \\leq 0  & \\quad \\textup{für} \\quad y_i = -1.\n\\end{align*}\\]\nIn unserem Beispiel mit zwei Regressoren \\(X_1\\), \\(X_2\\) ist \\(\\boldsymbol{w} = (w_1,\\,w_2)'\\) und \\(\\boldsymbol{X}=(X_1,\\,X_2)'\\). Der Prädiktorrraum ist also zwei-dimensional. Jede Hyperebene in diesem Raum ist ein-dimensional, kann also formal durch eine Geraden-Gleichung der Form \\[\\begin{align}\n  x_2 = \\alpha_1\\cdot x_1 + \\alpha_0 \\label{eq:gg}\n\\end{align}\\] dargestellt werden: Durch Umformen von \\(\\eqref{eq:sepplane}\\) erhalten wir \\[\\begin{align}\n  &\\, w_1 \\cdot x_1 + w_2 \\cdot x_2 + b = 0 \\notag \\\\\n  \\\\\n\\Leftrightarrow &\\, x_2 = -\\left(\\frac{w_1}{w_2}\\right) \\cdot x_1 - \\frac{b}{w_2} \\label{eq:ggt}\n\\end{align}\\]\nIn Abbildung XYZ erkennen wir, dass die Beobachtungen durch eine Gerade mit negativer Steigung perfekt separiert werden können.\nWenn die Daten perfekt separierbar sind (und die Regressoren kontinuierlich sind) gibt es unendlich viele Ebenen, die Bedingung \\(\\eqref{eq:sepcond}\\) erfüllen. Welche Hyperebene (also welche Parameter \\(\\boldsymbol{w},\\,b\\)) gewählt werden sollen, muss durch weitere Bedingungen festgelegt werden.\n\n14.1.2 Maximal Margin Classifier\nEin Maximal Margin Classifier (MMC) wählt die trennende Ebene wie folgt: Bestimme \\(\\{\\boldsymbol{w},b\\}\\) und \\(M\\) unter der Bedingung \\(\\sum_{j=1}^k w_j^2 = 1\\) so, dass für sämtliche Beobachtungen gilt, dass\n\\[\\begin{align*}\n    Y_i(\\boldsymbol{w}'\\boldsymbol{X}_i + b) \\geq M, \\quad \\forall i \\in \\{1, \\dots, n\\},\n\\end{align*}\\]\nwobei \\(M\\), der minimale Abstand aller Beobachtungen zur trennenden Ebene – die Margin – maximiert wird.\nBeobachtungen, die exakt die Distanz \\(M\\) zur Trennebene haben, sind die Support-Vektoren (SV). Beachte: Ausschließlich die SV definieren die angepasste Entschiedungsgrenze des MMC.\nWir trainieren nun einen MMC für klasse in lsvm_data mit e1071::svm(). Mit kernel = \"linear\" legen wir fest, dass eine lineare Entscheidungsgrenze im zwei-dimensionalen Raum von \\((X_1,X_2)\\) gefunden werden soll. Die Parameter cost = 1e8 und scale = F erzwingen wir eine “harte” Margin (mehr dazu im Abschnitt zu SVMs unten) sowie keine Skalierung der Daten. Mit coef() lesen wir die trainierten Koeffizienten aus.\n\nlibrary(e1071)\n\n# Trainiere das Modell\nmodel_mm &lt;- svm(\n  klasse ~ x1 + x2, \n  data = lsvm_data, \n  kernel = \"linear\", \n  cost = 1e8,\n  scale = F\n)\n\n# Trainierte Koeffizienten des MMC auslesen\n(\n  coefs &lt;- coef(model_mm) %&gt;%\n    set_names(\n      c(\"b\", \"w1\", \"w2\")\n    )\n)\n\n          b          w1          w2 \n 0.06210578 -0.52107776 -0.69398704 \n\n\nZur Vereinfachung der graphischen Darstellung normieren wir den Vektor der Koeffizienten c(\"w1\", \"w2\").\n\n# Koeffizienten normieren\nnrm &lt;- sqrt(coefs[\"w1\"]^2 + coefs[\"w2\"]^2) # Euklidische Norm\ncoefs &lt;- coefs / nrm\n\nMit den angepassten Koeffizienten in coefs können wir anhand der Umformung \\(\\eqref{eq:ggt}\\) die mit MMC ermittelte trennende Gerade berechnen. Wir definieren \\(\\alpha_0\\) und \\(\\alpha_1\\) wie in \\(\\eqref{eq:gg}\\).\n\n# Koeffizienten der trennenden Geraden berechnen\n# alpha0\n(\n  alpha0 &lt;- unname(-coefs[\"b\"] / coefs[\"w2\"])\n)\n\n[1] 0.08949127\n\n# alpha1  \n(\n  alpha1 &lt;- unname(-coefs[\"w1\"] / coefs[\"w2\"])\n)\n\n[1] -0.7508465\n\n\nDer Eintrag SV des im Objekt model_mm gespeicherten Outputs von svm() enthält die Support-Vektoren des MMC.\n\n# Support-Vektoren auslesen\n(\n  sv &lt;- model_mm$SV %&gt;% \n  as_tibble()\n)\n\n# A tibble: 3 × 2\n      x1     x2\n   &lt;dbl&gt;  &lt;dbl&gt;\n1 0.416  -1.66 \n2 3.01   -0.732\n3 0.0518  1.49 \n\n\nMit geom_function() zeichnen wir nun die ermittelte Trennlinie in den Punkteplot der Regressoren ein. Wir ergänzen die Grafik um die Marginallinien und Hervorhebungen der in sv gespeicherten Support-Vektoren.\n\n# Beobachtungen in MMC-Trennlinien einzeichnen\nggplot(\n  data = lsvm_data, \n  mapping = aes(x = x1, y = x2, color = klasse)\n  ) +\n    labs(x = \"X1\", y = \"X2\") +\n    # Trennlinie\n    geom_function(\n      fun = \\(x) alpha0 + alpha1 * x, col = \"black\"\n    ) +\n    # Marginallinien\n    geom_function(fun = \\(x) -(coefs[1] + 1/nrm)/coefs[3] + alpha1 * x, \n                  col = \"black\", linetype = \"dashed\") +\n    geom_function(fun = \\(x) -(coefs[1] - 1/nrm)/coefs[3] + alpha1 * x, \n                  col = \"black\", linetype = \"dashed\") +\n    # Support-Vektoren\n    geom_point(\n      data = sv, \n      mapping = aes(x = x1, y = x2),\n      size = 5,\n      shape = 1,\n      inherit.aes = F\n    ) +\n    # Beobachtungen\n    geom_point(size = 2) +\n    lims(y = c(-3, 6), x = c(-3.8, 3.8)) +\n    theme_cowplot() +\n    theme(legend.position = \"top\")\n\n\n\n\n\n\nAbbildung 14.1: Maximal Margin Classifier: Trennebene, Margin und SV\n\n\n\n\nDie Grafik zeigt, dass der MMC in model_mm tatsächlich eine perfekte Klassifzierung der Datenpunkte erreicht: Die gewählte Trennlinie (schwarze Linie) maximiert die jeweils durch die Marginallinien (getrichelte Linien) definierte Margin. Der MMC wird lediglich durch die 3 schwarz umrandeten Beobachtungen (die Support-Vektoren) definiert.\nWir können die Genauigkeit der Klassifikation von model_mm für die Trainingsdaten mit yardstick::accuracy auswerten.\n\nlibrary(yardstick)\n\n# Genauigkeit der Klassifizierung für Trainingsdaten\nmodel_mm %&gt;% \n  predict() %&gt;%\n  bind_cols(\n    estimate = ., \n    truth = lsvm_data$klasse\n  ) %&gt;%\n  accuracy(truth, estimate)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary             1\n\n\nEin Nachteil des MMC ist, dass das training der separierenden Grenze empfindlich gegenüber Ausreißern und hoher Varianz in den Daten ist: Da der MMC die Trennlinie so positioniert, dass der Abstand (die “Margin”) zu den nächstgelegenen Trainingsdatenpunkten maximal wird, können selbst wenige ungewöhnliche Beobachtungen die Lage der Grenze stark beeinflussen, was die Robustheit des Classifier beeinträchtigt und zu einer schlechteren Generalisierung auf neue Daten führen kann.\nAbbildung 14.2 illustriert die Sensibilität des MMC durch Hinzufügen nur einer zusätzlichen ungewöhnlichen Beobachtung der Klasse 1 mit \\(X_1 = -.5\\) und \\(X_2 = 3\\)2: Das Optimierungskalkül des MMC führt zu einer deutlich verschobenen Trennlinie im Vergleich zu ABZ.\n2 Der Code hierfür ist lsvm_data %&gt;% add_row(tibble(x1 = -.5, x2 = 3, klasse = factor(1))).\n\n\n\n\n\n\nAbbildung 14.2: Effekt von Ausreißern auf Maximum Margin Classifier\n\n\n\n\nEine Erweitung des MMC, die dieses Problem adressiert ist der (Soft Margin) Support Vector Classifier (SVC). Der Ausreißer lässt bestimmte Fehlklassifizierungen zu, um eine bessere Balance zwischen Genauigkeit auf dem Trainingsdatensatz und Generalisierungsfähigkeit des Modells zu finden.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "svm.html#sec-scv",
    "href": "svm.html#sec-scv",
    "title": "\n14  Support Vector Machines\n",
    "section": "\n14.2 Support Vector Classifier",
    "text": "14.2 Support Vector Classifier\nEin MMC strebt eine strikte Trennung der Datenklassen durch eine Hyperebene mit einer “harten” Margin, deren Grenzen durch die Support-Vektoren definierten werden (hard margin). Ein SVC erlaubt eine “weiche” Margin (soft margin) die zu weniger Empfindlichkeit der ermittelten Enscheidungsgrenze gegenüber Rauschen und Ausreißern führt. Diese weiche Margin lässt zu, dass Beobachtungen innerhalb der Margin liegen und erlaubt auch Fehlklassifikationenen. Die Support-Vektoren des SVC sind sämtliche Beobachtungen, di im Bereich der Margin liegen.\nWie beim MMC ist es das Ziel des SVC, die Parameter \\(\\{\\boldsymbol{w}, b\\}\\) so zu bestimmen, dass die Margin maximiert wird, jedoch dürfen einige Datenpunkte innerhalb der Margin oder sogar auf der falschen Seite der trennenden Hyperebene (Fehlerklassifizierung) liegen. Ein SVC kann so eine (nicht perfekt) trennende Hyperbene finden, wenn das Optimierungskalkühl des MMC aufgrund der Überlappung der Klassen unlösbar ist.\nHierzu wird die Ebene unter Berücksichtigung einer weiteren Bedingung für die Optimierung ermittelt, die auf Slack-Variablen \\(\\epsilon_i \\geq 0\\) basiert,\n\\[\\begin{align}\n  \\sum_{i=1}^{n} \\epsilon_i \\leq C. \\label{eq:svcreg}\n\\end{align}\\]\nDie \\(\\epsilon_i\\) sind größer 0 für Beobachtungen, welche die Margin verletzen, wobei die Größe von \\(\\epsilon_i\\) die Intensität der Verletzung misst. Für eine Beobachtung \\(i\\) innerhalb der Margin, aber auf der korrekten Seite der Trennebene ist \\(0\\leq\\epsilon_i&lt;1\\). Liegt die Beobachtung auf der falschen Seite der Ebene, ist \\(\\epsilon_i&gt;1\\).\n\\(C\\geq0\\) ist ein Regularisierungsparameter, der die Balance zwischen der Maximierung der Margin und der Anzahl der tolerierten Fehlklassifikationen regelt.3 Ein hoher Wert von \\(C\\) ermöglicht mehr Verletzungen der Margin und mehr Fehlklassifikationen, was meist zu einer breiteren Margin führt. Ein kleinerer Wert von \\(C\\) hingegen erzielt eine kleinere Margin mit weniger Verletzungen und Fehlklassifikationen. \\(C\\) ist also ein Tuning-Parameter der einen Bias-Varianz-Tradeoff regelt: Für kleine \\(C\\) (kleine Margins) wird die trennende Ebene gut an die Trainingsdaten angepasst, hat also wenig Verzerrung aber tendentiell größere Varianz. Für große \\(C\\) (breite Margin) ist die Varianz beim Bestimmen der trennenden Ebene tendentiell kleiner, jedoch kann die Verzerrung groß sein. Diese Abwägung ist wichtig für die Güte der Klassifizierung neuer Beobachtung. In empirischen Anwendungen wird \\(C\\) mit Cross-Validation bestimmt.\n3 Für \\(C=0\\) muss \\(\\sum_{i=1}^{n} \\epsilon_i = 0\\) sein. Der SVC entspricht dann dem MMC.Nicht perfekt separierbare Klassen\n\n# nicht exakt separierbare Daten einlesen\nlsvm_nonsep_data &lt;- readRDS(file = \"datasets/lsvm_nonsep_data.RDS\")\n\n\nggplot(\n  data = lsvm_nonsep_data, \n  mapping = aes(x = x1, y = x2, color = klasse)\n  ) +\n    labs(\n      x = \"X1\", \n      y = \"X2\", \n      title = \"Nicht perfekt separierbare Klassen\"\n    ) +\n    # Beobachtungen\n    geom_point(size = 2) +\n    theme_cowplot() +\n    theme(legend.position = \"top\")\n\n\n\n\n\n\nAbbildung 14.3: Nicht perfekt separierbare Klassen\n\n\n\n\nEin MMC kann für den in Abbildung 14.3 gezeigten Datensatz lsvm_nonsep_data keine Lösung finden: Der Algorithmus iteriert bis zu einer maximalen Anzahl an Durchläufen.\nEin SVC hingegen kann angepasst werden: Mit cost = 1 räumen wir ein “Budget” für Verletzungen der Margin und Fehlklassifikationen ein.4\n4 Der Parameter cost in svm() ist ein Langrange-Multiplikator und verhält sich daher invers zu \\(C\\) in \\(\\eqref{eq:svcreg}\\): Große Werte für cost führen zu viel Regularisierung (kleine Margin) und umgekehrt.\n# Trainiere ein SVC-Modell\nmodel_svc &lt;- svm(\n  klasse ~ x1 + x2, \n  data = lsvm_nonsep_data, \n  kernel = \"linear\", \n  cost = 1, \n  scale = F\n)\n\nmodel_svc\n\n\nCall:\nsvm(formula = klasse ~ x1 + x2, data = lsvm_nonsep_data, kernel = \"linear\", \n    cost = 1, scale = F)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  1 \n\nNumber of Support Vectors:  36\n\n\n\n# Trainierte Koeffizienten des SVC auslesen\n(\n  coefs &lt;- coef(model_svc) %&gt;%\n    set_names(\n      c(\"b\", \"w1\", \"w2\")\n    )\n)\n\n        b        w1        w2 \n-1.998007 -1.525909 -1.336617 \n\nnrm &lt;- sqrt(coefs[\"w1\"]^2 + coefs[\"w2\"]^2)\ncoefs &lt;- coefs / nrm\n\n\nalpha0 &lt;- unname(-coefs[\"b\"] / coefs[\"w2\"])\nalpha1 &lt;- unname(-coefs[\"w1\"] / coefs[\"w2\"])\n\nsv &lt;- model_svc$SV %&gt;% \n  as_tibble()\n\nAnalog zu model_mmc können wir die trennende Gerade gemeinsam mit den Margins sowie den SV graphisch darstellen.\n\nggplot(\n  data = lsvm_nonsep_data, \n  mapping = aes(x = x1, y = x2, color = klasse)\n  ) +\n    labs(x = \"X1\", y = \"X2\") +\n    # Trennlinie\n    geom_function(\n      fun = \\(x) alpha0 + alpha1 * x, col = \"black\"\n    ) +\n    # Marginallinien\n    geom_function(fun = \\(x) -(coefs[1] + 1/nrm)/coefs[3] + alpha1 * x, \n                  col = \"black\", linetype = \"dashed\") +\n    geom_function(fun = \\(x) -(coefs[1] - 1/nrm)/coefs[3] + alpha1 * x, \n                  col = \"black\", linetype = \"dashed\") +\n    # Support-Vektoren\n    geom_point(\n      data = sv, \n      mapping = aes(x = x1, y = x2),\n      size = 5,\n      shape = 1,\n      inherit.aes = F\n    ) +\n    # Beobachtungen\n    geom_point(size = 2) +\n    theme_cowplot() +\n    theme(legend.position = \"top\")\n\n\n\n\n\n\nAbbildung 14.4: Soft Margin Support Vector Classifier\n\n\n\n\nAufgrund der Soft Margin des SVC werden hier Fehlklassifikationen in Kauf genommen. Beachte, dass sowohl Beobachtungen auf und innerhalb der Margin als auch fehlklassifizierte Beobachtungen die Support-Vektoren der trennenden Geraden sind in Abbildung 14.4 sind.\n\n# Genauigkeit der Klassifizierung für Trainingsdaten\nmodel_svc %&gt;% \n  predict() %&gt;%\n  bind_cols(\n    estimate = ., \n    truth = lsvm_nonsep_data$klasse\n  ) %&gt;%\n  accuracy(truth, estimate)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary          0.85\n\n\nNicht-lineare und unscharfe Entscheidungsgrenzen\nIn empirischen Anwendungen gibt es selten exakt lineare Entscheidungsgrenzen. Wir präsentieren nun ein Beispiel für eine nicht-lineare Entscheidungsgrenze, bei der SVC dennoch eine hilfreiche Approximation der Entscheidungsgrenze liefern kann und diskutieren anschließend Support Vector Machines als Generalisierung von SVC zur Anpassung nicht-linearer Grenzen.\nDer Datensatz data_wavy enthält gleichverteilte Beobachtungen \\((X_1, X_2)\\) die gemäß der sinuidalen Entscheidungsgrenze\n\\[\\begin{align*}\n  g(x_1) =  x_1 + 3\\cdot \\sin(1.2 \\cdot x_1),\n\\end{align*}\\]\nin zwei Klassen eingeteilt sind: Beoabchtungen gehören zu Klasse 1, wenn \\(x_2 + \\varepsilon &gt; g(x_1)\\) und ansonsten zu Klasse 2. Der (normalverteilte) Fehlerterm \\(\\varepsilon\\) führt zu einer unscharfen Trennung der Klassen, d.h. der Trainingsdatensatz ist selbst bei Abgleich mit der (nachfolgend als unbekannt angenommenen) Funktion \\(g(x_1)\\) nicht vollständig korrekt klassifizierbar. Wir suchen dennoch einen Classifier, der \\(g(x_1)\\) gut approximiert, also den deterministischen Teil der Entscheidungsregel möglichst gut lernt.\n\n# data_wavy einlesen\ndata_wavy &lt;- readRDS(\"datasets/data_wavy.RDS\")\n\nZunächst plotten wir die Beobachtungen in data_wavy gemeinsam mit der deterministischen Entscheidungsgrenze \\(g(x_1)\\).\n\n# Visualisiere den Datensatz mit ggplot2\nggplot(\n  data = data_wavy,\n  mapping = aes(\n    x = x1, \n    y = x2, \n    color = klasse\n    )\n  ) +\n  geom_point() +\n  labs(\n    x = \"X1\",\n    y = \"X2\"\n  ) +\n  # deterministische Entscheidungsgrenze\n  geom_function(\n    fun = \\(x) x + 3 * sin(x * 1.2), \n    lwd = 1.25\n  ) +\n  coord_equal(expand = F) +\n  theme_cowplot() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nAbbildung 14.5: data_wavy: Beobachtungen mit nicht-linearer Entscheidungsgrenze\n\n\n\n\nEin SVC kann die nicht-lineare Entscheidungsgrenze nicht abbilden, erreicht jedoch aufgrund der groben Teilbarkeit des Regressorraums hinsichtlich der Klassen durch eine (von links unten nach rechts oben verlaufende) Grade eine akzeptable Fehlklassifikationrate. Wir zeigen dies mit R.\n\n# Trainiere SV Classifier mit Cost-Kriterium\nsvc_wavy &lt;- svm(\n  klasse ~ ., \n  data = data_wavy, \n  kernel = \"linear\",\n  cost = 1,\n  scale = F\n)\n\nsvc_wavy\n\n\nCall:\nsvm(formula = klasse ~ ., data = data_wavy, kernel = \"linear\", cost = 1, \n    scale = F)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  1 \n\nNumber of Support Vectors:  849\n\n\n\n# Erzeuge einen Grid für die Vorhersagen\ngrid &lt;- expand_grid(\n  x1 = seq(min(data_wavy$x1), max(data_wavy$x1), length.out = 150),\n  x2 = seq(min(data_wavy$x2), max(data_wavy$x2), length.out = 150)\n) \n\ngrid &lt;- grid %&gt;%\n  # Vorhersagen auf dem Grid\n  mutate(\n    klasse = predict(svc_wavy, grid)\n  )\n\n\n# Daten + Entscheidungsgrenze mit ggplot\nggplot() +\n  geom_point(\n    data = grid, \n    mapping = aes(x = x1, y = x2, color = klasse), alpha = 0.1) +\n  geom_point(\n    data = data_wavy,\n    mapping = aes(x = x1, y = x2, color = klasse)) +\n  geom_function(\n    fun = \\(x) x + 3 * sin(x * 1.2), \n    col = \"#00BFC4\", \n    lwd = 1.25\n  ) +\n  labs(x = \"X1\", y = \"X2\") +\n  coord_equal(expand = F) +\n  theme_cowplot() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nAbbildung 14.6: data_wavy: Mit SVC trainierte und deerministische Entscheidungsgrenze\n\n\n\n\n\n# Anteil falscher Klassifzikationen für data_wavy\n1 - mean(predict(svc_wavy) == data_wavy$klasse)\n\n[1] 0.1835\n\n\nDiese Fehlerrate kann durch Verfahren, die nicht-lineare Entscheidungsgrenzen lernen können, deutlich verringert werden. Der nächste Abschnitt erläutert eine entsprechende Erweiterung des SVC: Support Vector Machines.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "svm.html#support-vector-machines",
    "href": "svm.html#support-vector-machines",
    "title": "\n14  Support Vector Machines\n",
    "section": "\n14.3 Support Vector Machines",
    "text": "14.3 Support Vector Machines\nSei \\(S\\) die Indexmenge der zu findenden Support-Vektoren für den SVC. Mann kann zeigen (vgl. Hastie, Tibshirani, und Friedman 2013), dass die zu lernende Entscheidungsregel des SVC die Form\n\\[\\begin{align}\n  f(x) = b + \\alpha_i \\sum_{i\\in S} \\langle\\boldsymbol{x},\\boldsymbol{x}_j\\rangle \\label{eq:svck}\n\\end{align}\\]\nhat, wobei \\(\\langle\\boldsymbol{x}_i,\\boldsymbol{x}_j\\rangle\\) das innere Produkt der Vektoren \\(\\boldsymbol{x}_i\\) und \\(\\boldsymbol{x}_j\\) meint,\n\\[\\begin{align*}\n  \\langle\\boldsymbol{x}_i,\\boldsymbol{x}_j\\rangle := \\sum_{l=1}^k x_{i,l} x_{j,l}.\n\\end{align*}\\]\nDie Funktion \\(\\langle\\boldsymbol{x}_i,\\boldsymbol{x}_j\\rangle\\) ist ein Beispiel für eine Kernel-Funktion \\(K(\\boldsymbol{x}_i,\\boldsymbol{x}_j)\\), der lineare Kernel. Ein Kernel \\(K\\) berechnet die “Ähnlichkeit” zwischen zwei Datenpunkten. Mit \\(K(\\boldsymbol{x}_i,\\boldsymbol{x}_j)=\\langle\\boldsymbol{x}_i,\\boldsymbol{x}_j\\rangle\\) wird die lineare Übereinstimmung der Vektoren \\(\\boldsymbol{x}_i\\) und \\(\\boldsymbol{x}_j\\) im ursprünglichen Regressorraum gemessen und damit eine lineare Klassifikationsregel \\(f(x)\\) gesucht.5\n5 Geometrisch wird die Ähnlichkeit durch den Winkel zwischen den Regressor-Vektoren gemessen: \\(\\langle\\boldsymbol{x}_i \\boldsymbol{x}_j\\rangle = \\|\\boldsymbol{x}_i\\| \\|\\boldsymbol{x}_j\\| \\cos(\\theta)\\), wobei \\(\\theta\\) der Winkel zwischen \\(\\boldsymbol{x}_i\\) und \\(\\boldsymbol{x}_j\\) ist.6 Der Kernel Trick ist hilfreich, weil die explizite Transformation der Daten rechenaufwendig oder sogar unlösbar sein kann. Kernel-Funktionen hingegen können schnell berechnet werden.Eine Support Vector Machine (SVM) ersetzt das innere Produkt in \\(\\eqref{eq:svck}\\) durch einen nicht-linearen Kernel \\(K\\). Dadurch können SVMs auch hochgradig nicht-lineare Entscheidungsgrenzen lernen: Um die nicht-lineare Beziehungen in den Daten zu modellieren, werden die Regressoren mit einem \\(K\\) implizit so transformiert, dass sie in einem (möglicherweise hoch-dimensionalen) Raum einfacher hinsichtlich ihrer Klasse getrennt werden können. Diese Transformation muss jedoch dank des Kernels nicht explizit berechnet werden — dies ist der Kernel Trick.6\nTypische nicht-lineare Kernel-Funktionen sind\n\n\nRadialer Kernel\n\\[\\begin{align*}\n  K(\\boldsymbol{x}_i,\\boldsymbol{x}_j) = \\exp\\bigg(-\\gamma \\sum_{l=1}^k (x_{lj} - x_{li})^2\\bigg)\n\\end{align*}\\]\n\n\nPolynomialer Kernel (Ordnung \\(d\\))\n\\[\\begin{align*}\n  K(\\boldsymbol{x}_i,\\boldsymbol{x}_j) = \\bigg(c + \\sum_{l=1}^k x_{lj} \\cdot x_{li} \\bigg)^d, \\quad c\\in\\mathbb{R},\\ d&gt;1\n\\end{align*}\\]\n\n\nSigmoidaler Kernel\n\\[\\begin{align*}\n  K(\\boldsymbol{x}_i,\\boldsymbol{x}_j) = \\tanh\\bigg(\\gamma\\sum_{l=1}^k x_{lj} \\cdot x_{li} + c \\bigg), \\quad c\\in\\mathbb{R}\n\\end{align*}\\]\n\n\nZusätzlich zu dem Tuning-Parameter \\(C\\) müssen also für das Training einer Entscheidungsgrenze die Skalierungs- bzw. Positions-Parameter \\(\\gamma\\) und \\(c\\) für den Kernel gewählt werden. Letzteres erfolgt in empirischen Anwendungen häufig mit Cross Validation.\n\n14.3.1 Der Kernel-Trick\nUm den Kernel Trick zu veranschaulichen, betrachten wir ein binäres Klassifikationsproblem mit einer nicht-linearen Entscheidungsgrenze im Regressorraum für \\((X_1,X_2)\\in\\mathbb{R}^2\\), bei dem die Klassenzugehörigkeit der Beobachtungen konzentrisch ist. Wir lesen hierzu den Datensatz data_concentric ein.\n\n# konzentrische Daten einlesen\ndata_concentric &lt;- readRDS(\n  file = \"datasets/data_concentric.RDS\"\n)\n\nFür einen Verständnis der Verteilung der Klassenzugehörigkeit im Regressorraum visualisieren wir den Datensatz data_concentric mit ggplot().\n\nlibrary(ggplot2)\nlibrary(cowplot)\n\n# `data_concentric` visualisieren\nggplot(\n  data = data_concentric,\n  mapping = aes(x = X1, y = X2, color = klasse)\n) +\n  geom_point() +\n  # Seitenverhältnis auf 1 setzten\n  coord_equal() +\n  theme_cowplot() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nAbbildung 14.7: Konzentrisch angeordnete Klassenzugehörigkeit\n\n\n\n\nDie Klassen der in Abbildung 14.7 gezeigten Beobachtungen sind aufgrund der konzentrischen Verteilung nicht linear separierbar – die Entscheidungsgrenze hat eine kreisförmige Struktur. Der Kernel-Trick nutz hier eine implizite Daten abbildung in einen höherdimensionalen Raum, in dem die Klassen leichter zu trennen sind.\nWir suchen hierfür einen Kernel \\(K(\\boldsymbol{x}_i, \\boldsymbol{x}_j)\\), der “ähnliche” Funktionswerte annimmt, wenn die Datenpunkte \\(i\\) und \\(j\\) zu derselben Klasse gehören. Für die in der Abbildung gezeigte Verteilung empfiehlt sich der polynomiale Kernel mit dem Grad \\(d = 2\\). Dieser Kernel berechnet die Ähnlichkeit zwischen zwei Datenpunkten \\(\\boldsymbol{x}_i\\) und \\(\\boldsymbol{x}_j\\) im Regressorraum als\n\\[\\begin{align*}\n  K(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\left( \\sum_{l=1}^k x_{li} \\cdot x_{lj} \\right)^2.\n\\end{align*}\\]\nDer polynomiale Kernel entspricht einer quadratischen Beziehung zwischen den Datenpunkten und ist so in der Lage, kreisförmige oder konzentrische Zusammenhänge im Regressorraum zu erfassen. Die Trennbarkeit der Beobachtungen aufgrund ähnlicher Werte des Kernels für Beobachtungen der selben Klasse kann hierbei durch eine explizite Transformation der Datenpunkte in einen höherdimensionalen Raum dargestellt werden. In unserem zwei-dimensionalen Beispiel ist diese Transformation\n\\[\\begin{align}\n  \\begin{pmatrix}\n    x = x_1\\\\ y = x_2\n  \\end{pmatrix}\n  \\rightarrow\n  \\begin{pmatrix}\n    x = x_1\\\\\n    y = x_2\\\\\n    z = x_1^2 + x_2^2.\n  \\end{pmatrix}\n  \\label{eq:polykerntrans}\n\\end{align}\\]\nDie Abbildung \\(\\eqref{eq:polykerntrans}\\) projiziert die Daten in den drei-dimensionalen Raum \\(\\mathbb{R}^3\\), wobei die zusätzliche Koordinate \\(z = x_1^2 + x_2^2\\) die (quadratische) Distanz einer Beobachtung vom Ursprung darstellt. In diesem transformierten Raum sind konzentrisch angeordnete Klassen anhand des Wertes von \\(z\\) linear separierbar, was geometrisch einer trennenden Hyperebene entspricht. Durch die Wahl des polynomialen Kernels wird so eine nicht-lineare Trennlinie im ursprünglichen Regressor-Raum \\(\\mathbb{R}^2\\) modelliert, welche die kreisförmige Entscheidungsgrenze erfassen kann.\nUm dies grafisch zu veranschaulichen, transformieren wir die Datenpunkte in data_concentric gemäß der Funktion \\(\\eqref{eq:polykerntrans}\\) nach \\(\\mathbb{R}^3\\).\n\n# Funktion: Explizite Transformation durch \n# polynomiellen Kernel mit d = 2\npoly_transform &lt;- function(X) {\n    X1 &lt;- X[, 1]\n    X2 &lt;- X[, 2]\n    X_trans &lt;- tibble(X1, X2, z = X1^2 + X2^2)\n    return(X_trans)\n}\n\n# Transformation furchführen\nX_transformed &lt;- poly_transform(\n  data_concentric %&gt;% \n    dplyr::select(X1, X2)\n)\n\n# Transformierte Beobachtungen\nslice_head(X_transformed, n = 5)\n\n# A tibble: 5 × 3\n       X1      X2     z\n    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1  0.0380  0.535  0.288\n2  0.863  -0.208  0.788\n3 -0.514  -0.380  0.409\n4 -0.936  -0.0886 0.883\n5 -0.794   0.557  0.940\n\n\nIm nächsten Schritt berechnen wir für die transformierten Daten X_transformed unter Verwendung der Klassen in data_concentric$klasse einen SVC, also eine SVM mit linearem Kernel: Wir suchen eine trennende Ebene für die transformierten Daten X_transformed in \\(\\mathbb{R}^3\\).\n\n# SVM mit einem linearen Kernel \n# auf den transformierten Daten trainieren\nsvm_model_poly_trans &lt;- svm(\n  x = X_transformed, \n  y = data_concentric$klasse, \n  kernel = \"linear\"\n)\n\nsvm_model_poly_trans\n\n\nCall:\nsvm.default(x = X_transformed, y = data_concentric$klasse, kernel = \"linear\")\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  1 \n\nNumber of Support Vectors:  27\n\n\nDie interaktive Abbildung 14.8 zeigt\n\ndie transformierten Datenpunkte in \\(\\mathbb{R}^3\\),\ndie mit dem Modell-Objekt svm_model_poly_trans berechnete trennende Hyperebene (grün) und\ndie Support-Vektore der Hyperebene (grau hinterlegt).\n\n\n\n\n\n\n\n\nAbbildung 14.8: data_concentric: In R3 linear separierbaren Datenpunkte und trennende Hyperebene\n\n\n\nBeachte, dass die grüne Hyperebene die Daten nahezu perfekt trennt:\n\n# Anteil falscher Klassifzikationen für data_concentric\n1 - mean(predict(svm_model_poly_trans) == data_concentric$klasse)\n\n[1] 0.0025\n\n\n\n# SVM mit poly Kernel auf data_concentric trainiert\nsvm_model_poly &lt;- svm(\n  x = data_concentric %&gt;% dplyr::select(X1, X2), \n  y = data_concentric$klasse, \n  kernel = \"polynomial\", \n  coef0 = 0, # c = 0\n  degree = 2, # d = 2\n  gamma = 1, \n  cost = 1\n)\n\nsvm_model_poly\n\n\nCall:\nsvm.default(x = data_concentric %&gt;% dplyr::select(X1, X2), y = data_concentric$klasse, \n    kernel = \"polynomial\", degree = 2, gamma = 1, coef0 = 0, cost = 1)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  polynomial \n       cost:  1 \n     degree:  2 \n     coef.0:  0 \n\nNumber of Support Vectors:  24\n\n\nDie SVM-Modelle svm_model_poly_trans und svm_model_poly liefern identische Klassifzierungen der Beobachtungen in data_concentric.\n\nsum(\n  predict(svm_model_poly_trans) == predict(svm_model_poly)\n)\n\n[1] 399\n\n\nAhand der Komponenten des Objekts svm_model_poly und mit Vorhersagen der Klassen für ein Gitter von Punkten können wir die angepasste kreisförmige Entscheidungsgrenze sowie die Margins im urspünglichen Prädiktorraum \\(\\mathbb{R}^2\\) visualisieren. Abbildung 14.9 zeig das Ergebnis.\n\ngrid &lt;- expand_grid(\n  x1 = seq(min(data_concentric$X1), max(data_concentric$X1), length.out = 150),\n  x2 = seq(min(data_concentric$X2), max(data_concentric$X2), length.out = 150)\n) \n\ndist &lt;- attributes(\n  predict(svm_model_poly, grid, decision.values = T)\n)$decision.values\n\ngrid &lt;- grid %&gt;%\n  # Vorhersagen auf dem Grid\n  mutate(\n    distanz = dist,\n    klasse = predict(svm_model_poly, grid)\n  )\n\n# Support-Vektoren\nSV &lt;- data_concentric %&gt;% \n  slice(svm_model_poly$index)\n\nggplot(\n  data = data_concentric,\n  mapping = aes(x = X1, y = X2, color = klasse)\n) +\n  geom_point(grid, mapping = aes(x = x1, y = x2, color = klasse), alpha = .01) +\n  geom_point() +\n  geom_contour(\n    data = grid,\n    aes(x = x1, y = x2, z = dist),\n    breaks = 0,  # Decision boundary\n    color = \"black\",\n  ) +\n  geom_contour(\n    data = grid,\n    aes(x = x1, y = x2, z = dist),\n    breaks = c(-1, 1),  # Marginal lines\n    linetype = \"dashed\",\n    size = 0.5\n  ) +\n  geom_point(\n    data = SV, inherit.aes = F,\n    mapping = aes(x = X1, y = X2),\n    shape = 1, size = 3\n  ) +\n  coord_equal(expand = 0) +\n  theme_cowplot() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nAbbildung 14.9: SVM mit Polynomial-Kernel erfasst kreisförmige Entscheidungsgrenze\n\n\n\n\nDank des Kernel-Tricks können SVMs praktisch beliebige Formen von nicht-linearen Entscheidungsregeln lernen. Für die sinuidal verlaufende Entscheidungsgrenze im Datensatz data_wavy (vgl. Abbildung 14.5) ist ein radialer Kernel hilfreich:\nDer radiale Kernel ist dem polynomiellen Kernel bei der Anpassung hochflexibler Entscheidungsgrenzen überlegen, weil er selbst stark variierende Muster dank großer lokale Flexibilität besser erfassen kann. Ein polynomialer Kernel hingegen ist durch seine funktionale Form und den Polynomgrad \\(d\\) beschränkt. Bei komplexen Grenzen muss \\(d\\) für mehr Flexibilität der SVM groß gewählt werden, was Overfitting begünstigen kann.\nFür einen Vergleich mit der in Abbildung 14.5 visualisierten Anpassung des SVC trainieren nun eine SVM für data_wavy mit radialem Kernel als radial_svm_model für die Standardwerte cost = 1, coef0 = 0 und gamma = 1.\n\n# SVM mit RBF-Kernel anpassen\nradial_svm_model &lt;- svm(\n  klasse ~ x2 + x1, \n  data = data_wavy, \n  kernel = \"radial\", \n  cost = 1,\n  coef0 = 0,\n  gamma = 1\n)\n\nWie zuvor plotten wir die trainierte Entscheidungsgrenze sowie die Margin gemeinsam mit mit den Datenpunkte im Regressor-Raum.\n\n# Erzeuge einen Grid für die Vorhersagen\ngrid &lt;- expand_grid(\n  x1 = seq(\n    min(data_wavy$x1), \n    max(data_wavy$x1), \n    length.out = 150),\n  x2 = seq(\n    min(data_wavy$x2), \n    max(data_wavy$x2), \n    length.out = 150\n  )\n) \n\ndist &lt;- attributes(predict(radial_svm_model , grid, decision.values = T))$decision.values\n\ngrid &lt;- grid %&gt;%\n  # Vorhersagen auf dem Grid\n  mutate(\n    dist = dist,\n    klasse = predict(radial_svm_model , grid)\n  )\n\nSV &lt;- data_wavy %&gt;% \n  slice(radial_svm_model$index)\n\n\n# Visualisiere die Entscheidungsgrenze mit ggplot2\nggplot() +\n  geom_point(\n    data = grid, \n    mapping = aes(x = x1, y = x2, color = klasse), alpha = 0.05) +\n  geom_point(\n    data = data_wavy,\n    mapping = aes(x = x1, y = x2, color = klasse)) +\n  \n  geom_contour(\n    data = grid,\n    aes(x = x1, y = x2, z = dist),\n    breaks = 0,  # Decision boundary\n    color = \"black\",\n  ) +\n  geom_contour(\n    data = grid,\n    aes(x = x1, y = x2, z = dist),\n    breaks = c(-1, 1),  # Marginal lines\n    linetype = \"dashed\",\n    size = 0.5,\n    color = \"black\"\n  ) +\n  geom_point(\n    data = SV, inherit.aes = F,\n    mapping = aes(x = x1, y = x2),\n    shape = 1, \n    size = 3\n  ) +\n  \n  coord_equal() +\n  scale_x_continuous(\"X1\", expand = c(0, 0)) +\n  scale_y_continuous(\"X2\", expand = c(0, 0)) +\n  theme_cowplot() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nAbbildung 14.10: data_wavy mit RBF-SVM trainierte Entscheidungsgrenze\n\n\n\n\nEin Vergleich von Abbildung 14.10 mit Abbildung 14.6 zeigt, dass die SVM mit radialem Kernel im Objekt radial_svm_model den nicht-linearen Verlauf der sinuidalen Entscheidungsgrenze deutlich besser erfasst als der SVC in svc_wavy. Der Anteil fehlklassifizierter Beobachtungen liegt bei weniger als 10%.\n\n# Fehlerrate von 'radial_svm_model'\n1 - mean(\n  predict(radial_svm_model) == data_wavy$klasse\n)\n\n[1] 0.09\n\n\n\n14.3.2 Overfitting\nDer in Abschnitt Kapitel 14.2 beschriebene Einfluss des cost-Parameters auf die Klassifikationsleistung von SVCs überträgt sich auch auf SVMs. Bei der Anpassung nichtlinearer Entscheidungsgrenzen mit nichtlinearen Kernelfunktionen müssen neben cost auch die Kernel-Parameter sorgfältig gewählt werden, um Overfitting zu vermeiden. Dabei bestehen Interdependenzen zwischen den Parametern. In den hier behandelten Beispielen mit einem zweidimensionalen Merkmalsraum lassen sich diese Effekte gut grafisch darstellen: Bei einer SVM mit radialem Kernel bewirken höhere gamma-Werte eine stärker lokal geprägte Entscheidungsgrenze, die sich enger um benachbarte Datenpunkte legt. Höhere cost-Werte führen zu engeren Margins, wodurch weniger Support-Vektoren zugelassen werden. Dies kann die Varianz der Entscheidungsgrenze erhöhen, da der Algorithmus sich stärker an einzelne Datenpunkte anpasst und die Schätzung tendenziell weiter von der wahren Grenze abweichen kann. Zu große Werte führen also jeweils zu einer Überanpassung an die Daten.\nWir illustrieren Overfitting durch eine ungünstige Parameterwahl für eine SVM mit Radial-Kernek und den Datensatz data_wavy.\n\n# SVM mit ungünstiger Parameter-Kombination\nradial_svm_overfit &lt;- svm(\n  klasse ~ x2 + x1, \n  data = data_wavy, \n  kernel = \"radial\", \n  cost = 1000,\n  gamma = 10\n)\n\n\ngrid &lt;- expand_grid(\n  x1 = seq(min(data_wavy$x1), max(data_wavy$x1), length.out = 150),\n  x2 = seq(min(data_wavy$x2), max(data_wavy$x2), length.out = 150)\n) \n\ndist &lt;- attributes(predict(radial_svm_overfit , grid, decision.values = T))$decision.values\n\ngrid_overfit &lt;- grid %&gt;%\n  # Vorhersagen auf dem Grid\n  mutate(\n    dist = dist,\n    klasse = predict(radial_svm_overfit , grid)\n  )\n\n\n# Visualisiere die Entscheidungsgrenze mit ggplot2\nggplot() +\n  geom_point(\n    data = grid_overfit, \n    mapping = aes(x = x1, y = x2, color = klasse), alpha = 0.1) +\n  geom_point(\n    data = data_wavy,\n    mapping = aes(x = x1, y = x2, color = klasse)) +\n  \n  geom_contour(\n    data = grid_overfit,\n    aes(x = x1, y = x2, z = dist),\n    breaks = 0,  # Decision boundary\n    color = \"black\",\n  ) +\n  geom_contour(\n    data = grid_overfit,\n    aes(x = x1, y = x2, z = dist),\n    breaks = c(-1, 1),  # Marginal lines\n    linetype = \"dashed\",\n    size = 0.5,\n    color = \"black\"\n  ) +\n  \n  coord_equal() +\n  scale_x_continuous(\"X1\", expand = c(0, 0)) +\n  scale_y_continuous(\"X2\", expand = c(0, 0)) +\n  theme_cowplot() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nAbbildung 14.11: data_wavy: Overfitting der Entscheidungsgrenze mit RBF-SVM\n\n\n\n\n\n# Fehlklassifikationsrate bei Overfitting:\n1 - mean(\n  predict(radial_svm_overfit) == data_wavy$klasse\n  )\n\n[1] 0.058\n\n\nDas Modell radial_svm_overfit leidet unter Overfiting: In Abbildung 14.11 sehen wir, dass die gelernte Entscheidungsgrenze zu stark durch lokale Muster aufgrund der Überlappung der Klassen geprägt ist. Entsprechend ist die Klassifikation der für das Training genutzen Datenpunkte für radial_svm_overfit etwas genauer radial_svm_model. Diese Überanpassung resultiert tendentiell in einer schlechteren Generalisierung von radial_svm_overfit in Vorhersagen unbekannter Datenpunkte.\nMit e1071::tune() können wir optimale Parameterwerte aus einem Grid mit Cross Validation schätzen. Wir übergeben hierzu die Methode (METHOD), die Spezifikation des Modells wie in svm() und dem Argument ranges die zu berücksichtigenden Parameter-Werte als Liste.\n\nset.seed(1234)\n\n# Cross-Validation für Parameter in radialer SVM\nradial_svm_tuned &lt;- tune(\n  METHOD = svm, \n  klasse ~ x2 + x1, \n  data = data_wavy, \n  kernel = \"radial\",\n  # Parameter-Werte festlegen\n  ranges = list(\n    cost = c(1, 2, 4, 6),\n    gamma = c(0, .01, .05, .1, .25, .5, .8, 1, 2, 3)\n  )  \n)\n\n# CV-Ergebnisse auslesen\nradial_svm_tuned$performances\n\n   cost gamma  error dispersion\n1     1  0.00 0.4960 0.02282786\n2     2  0.00 0.4960 0.02282786\n3     4  0.00 0.4960 0.02282786\n4     6  0.00 0.4960 0.02282786\n5     1  0.01 0.1835 0.03308659\n6     2  0.01 0.1835 0.03240799\n7     4  0.01 0.1830 0.03110913\n8     6  0.01 0.1830 0.03038640\n9     1  0.05 0.1745 0.03148633\n10    2  0.05 0.1720 0.02679138\n11    4  0.05 0.1665 0.02273641\n12    6  0.05 0.1630 0.02335713\n13    1  0.10 0.1595 0.02254009\n14    2  0.10 0.1495 0.02033743\n15    4  0.10 0.1450 0.01914854\n16    6  0.10 0.1420 0.01584649\n17    1  0.25 0.1360 0.01429841\n18    2  0.25 0.1295 0.01657475\n19    4  0.25 0.1230 0.01475730\n20    6  0.25 0.1190 0.01370320\n21    1  0.50 0.1150 0.01452966\n22    2  0.50 0.1080 0.02188353\n23    4  0.50 0.1000 0.02108185\n24    6  0.50 0.0910 0.01696401\n25    1  0.80 0.1005 0.01992346\n26    2  0.80 0.0915 0.02041922\n27    4  0.80 0.0885 0.01901023\n28    6  0.80 0.0875 0.01703754\n29    1  1.00 0.0950 0.01840894\n30    2  1.00 0.0900 0.02041241\n31    4  1.00 0.0865 0.01650757\n32    6  1.00 0.0850 0.01763834\n33    1  2.00 0.0890 0.01940790\n34    2  2.00 0.0860 0.01897367\n35    4  2.00 0.0905 0.01992346\n36    6  2.00 0.0905 0.01921371\n37    1  3.00 0.0900 0.02000000\n38    2  3.00 0.0915 0.01716748\n39    4  3.00 0.0925 0.01859659\n40    6  3.00 0.0920 0.01798147\n\n\nerror ist die mithilfe von Cross-Validation geschätzte Fehlklassifikationsrate für unbekannte Beobachtungen. Die Ergebnisse verdeutlichen, dass die Modellleistung stark von der Fähigkeit des radialen Kernels abhängt, Nicht-Linearitäten zu erfassen: Bereits geringe gamma-Werte führen zu einer signifikanten Reduktion der Fehlklassifikationsrate im Vergleich zu einem trivialen Modell mit gamma = 0.7\n7 Für gamma = 0 wird lediglich eine Konstante angepasst, es findet also extremes “Underfitting” statt: Das Modell ist dann nicht besser als die zufällige Klassifikation mit Wahrscheinlichkeiten, die den relativen Häufigkeiten der Klassen im Datensatz entsprechen (baseline accuracy).\n# Beste Parameter-Kombination anzeigen\nradial_svm_tuned$best.parameters\n\n   cost gamma\n32    6     1\n\n# Beste SVM auslesen\nradial_svm_tuned$best.model\n\n\nCall:\nbest.tune(METHOD = svm, train.x = klasse ~ x2 + x1, data = data_wavy, \n    ranges = list(cost = c(1, 2, 4, 6), gamma = c(0, 0.01, 0.05, \n        0.1, 0.25, 0.5, 0.8, 1, 2, 3)), kernel = \"radial\")\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  6 \n\nNumber of Support Vectors:  450",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "svm.html#beispiel-vorhersage-von-kaufentscheidungen",
    "href": "svm.html#beispiel-vorhersage-von-kaufentscheidungen",
    "title": "\n14  Support Vector Machines\n",
    "section": "\n14.4 Beispiel: Vorhersage von Kaufentscheidungen",
    "text": "14.4 Beispiel: Vorhersage von Kaufentscheidungen\nDer mit dem R-Paket ISLR (James u. a. 2021) Datensatz OJ enthält Beobachtungen von 1070 Orangensaft-Verkäufen bei denen der Kunde entweder die Mark “Citrus Hill” (CH) oder “Minute Maid” (MM) gekauft hat. Tabelle 14.1 enthält eine Beschreibung der verfügbaren Variablen.\n\n\n\n\n\n\n\n\nVariable\nBeschreibung\n\n\n\nPurchase\nKauf von CH oder MM Orangensaft\n\n\nWeekofPurchase\nKaufwoche\n\n\nStoreID\nID des Geschäfts\n\n\nPriceCH\nPreis für Citrus Hill\n\n\nPriceMM\nPreis für Minute Maid\n\n\nDiscCH\nRabatt auf Citrus Hill\n\n\nDiscMM\nRabatt auf Minute Maid\n\n\nSpecialCH\nSonderangebot für Citrus Hill\n\n\nSpecialMM\nSonderangebot für Minute Maid\n\n\nLoyalCH\nMarkentreue für Citrus Hill\n\n\nSalePriceMM\nVerkaufspreis für Minute Maid\n\n\nSalePriceCH\nVerkaufspreis für Citrus Hill\n\n\nPriceDiff\nUnterschied Verkaufspreise MM - CH\n\n\nStore7\nVerkauf in Geschäft 7 (Ja/Nein)\n\n\nPctDiscMM\nProzentualer Rabatt für MM\n\n\nPctDiscCH\nProzentualer Rabatt für CH\n\n\nListPriceDiff\nUnterschied Listenpreise MM - CH\n\n\nSTORE\nWelches der 5 möglichen Geschäfte\n\n\n\n\n\n\n\nTabelle 14.1: Datensatz OJ: Übersicht der Variablen\n\n\n\nNach Laden des Pakets fügen wir den Datensatz OJ der Arbeitsumgebung hinzu. Wir transformieren WeekofPurchase in eine leichter intepretierbare Wochenangabe und definieren die kategorischen Variablen als Typ factor.\n\n\n\n\n\n\n\n\nDer nächste Code-Chunk produziert eine Grafik für die Gesamtenwicklung der Verkäufe über das Jahr.\n\n\n\n\n\n\n\n\nMit ggcorrplot::ggcorrplot() erzeugen wir einen Korrelationsplot für die numerischen Variablen des Datensatzes, um die linearen Zusammenhänge zwischen den Preis-, Rabatt- und Verkaufsvariablen einzuschätzen.\n\n\n\n\n\n\n\n\nDie Abbildung zeigt, dass es starke Korellation zwischen Discount-Indikatoren (DiscCH, DiscMM) und den jeweiligen Prozent-Beträgen (PctDiscCH, PctDiscMM) sowie der Preis-Differenz (PriceDiff) gibt. Die Variablen STORE und Store7 enthalten redundante Informationen über das Geschäft, in dem der Verkauf stattfand. Wir entfernen diese Variablen bevor der Datensatz mit rsample::initial_split() in Trainings- und Test-Partitionen eingeteilt wird.\n\n\n\n\n\n\n\n\nMit recipe::recipe() können Datensätze systematisch für die Anwendung von Machine-Learning-Methoden vorbereitet werden. Im nachfolgenden Code-Chunk erstellen wir das recipe blueprint: Wir skalieren und zentrieren wir numerische Variablen, was sicherstellt, dass die Regresoren auf einer vergleichbaren Skala liegen, sodass distanzbasierte Algorithmen nicht durch unterschiedlliche Skalierungen Variablen beeinflusst werden. Kategorische Variablen mit mehr als zwei Ausprägungen werden mithilfe von One-Hot-Encoding in binäre Dummy-Variablen umgewandelt. Dieser Ansatz sorgt für eine konsistente und reproduzierbare Datenvorverarbeitung jenseits der Anwendungen mit SVMs.\n\n\n\n\n\n\n\n\nMit recipes::prep() bestimmen wir die nötigen Parameter für die Vorbereitung von Datensätzen mit dem in oj_train enthaltenen. recipes::bake() wendet die Transformationen an. Wir zeugen eine Übersicht der transformierten Trainingsdaten mit skimr::skim.\n\n\n\n\n\n\n\n\nDie nächsten Code chunks zeigen, wie parsnip SVMs mit lineare, polynomialen und radialen Kernel-funktionen für die Klassifikation der gekauften Orangensaft-Marke Purchase angepasst werden können. Hierbei wird die Implmentierung von ksvm() im Paket kernlab genutzt. Für jeden dieser drei Ansätze definieren wir einen separaten Workflow für 10-fache Cross Validation der jeweiligen Modellparameter.8 Als Kriterium für die Modellleistung verwernden wir accuracy, den Anteil korrekter Klassifikationen. Nach Anpassung des als optimal ermittelten Modells evaluieren wir die Vorhersagegüte der Modelle auf dem Testdatensatz bake_train.\n8 Aufgrund der geringen Rechenleistung der für die interaktiven Code-Chunks genutzen Webr-Implementierung betrachten wir hier beispielhafte, kleine Parameter-Gitter.SVM: Linearer Kernel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSVM: Radialer Kernel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSVM: Polynomialer Kernel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFür einen tabellarischen Vergleich der Performance der Modelle auf den Testdatensatz sammeln wir die Ergebnisse in einer Liste ergebnisse und verwenden purrr::map_dfr() für die Transformation in eine tibble mit der ID-Variable Modell. Die Metrik accuracy tabellieren wir mit gt::gt().\n\n\n\n\n\n\n\n\nDie Auswertung zeigt, dass die hier betrachteten SVMs mit nicht-linearer Kernelfunktion den SVC mit linearem Kernel hinsichtlich der Genauigkeit der Vorhersagen auf dem Testdatensatz nicht schlagen können.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "svm.html#zusammenfassung",
    "href": "svm.html#zusammenfassung",
    "title": "\n14  Support Vector Machines\n",
    "section": "\n14.5 Zusammenfassung",
    "text": "14.5 Zusammenfassung\nIn diesem Kapitel haben wir Support Vector Machines (SVM) zur Klassifikation eingeführt, eine Machine-Learning-Methode, die sich besonders für komplexe und nicht-lineare Probleme in hochdimensionalen Datensätzen eignet. Das Ziel von SVMs ist es, eine Trennlinie oder -ebene im Regressor-Raum zu finden, die zwei Klassen von Datenpunkten optimal voneinander abgrenzt. Hierfür ermittelt ein Algorithmus sogenannte Support-Vektoren – eine Teilmenge der Datenpunkte, welche die angepasste Entscheidungsgrenze bestimmen.\nSupport Vector Machines (SVMs) sind eine Weiterentwicklung des klassischen Maximal Margin Classifiers (MMC). Während der MMC eine harte Trennlinie sucht, die die Daten perfekt in zwei Klassen unterteilt, funktioniert dieser Ansatz nur dann gut, wenn die Klassen klar separierbar sind. Bei realen Datensätzen, in denen Überlappungen oder Ausreißer auftreten, scheitert der MMC-Algorithmus. Hier kann ein Soft Margin Classifier (SMC) angewendet werden, eine Verallgemeinerung des MMC, die Fehlklassifikationen zulässt. Diese zusätzliche Flexibilität erweitert die Anwendbarkeit auf reale, häufig nicht perfekt linear trennbare Klassen.\nFür Klassifikationsprobleme mit nicht-linearen Entscheidungsgrenzen wird das Prinzip des SMCs durch den sogenannten Kernel-Trick zu einer SVM erweitert. Hierbei werden die Daten implizit in einen höherdimensionalen Raum projiziert, ohne dass die Transformation explizit durchgeführt werden muss. In diesem Raum lassen sich ursprünglich komplexe Entscheidungsgrenzen durch eine (lineare) Hyperebene modellieren. Unterschiedliche Kernel-Funktionen wie der radiale Kernel oder der polynomiale Kernel erlauben es, verschiedene Formen nicht-linearer Entscheidungsgrenzen zu erfassen und anzupassen.\nDie Implementierung von SVMs in R erfolgt effizient durch Pakete wie e1071 und kernlab. Wir haben gezeigt, wie Datensätze mithilfe von parsnip im tidymodels-Framework systematisch vorbereitet, SVM-Modelle spezifiziert, durch Cross-Validation optimiert und vergleichen werden können.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHastie, T., R. Tibshirani, und J. Friedman. 2013. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer Series in Statistics. Springer New York.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, und Rob Tibshirani. 2021. ISLR: Data for an Introduction to Statistical Learning with Applications in R. https://CRAN.R-project.org/package=ISLR.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, und Robert Tibshirani. 2017. An introduction to statistical learning: With applications in R. Corrected at 8th printing 2017. Springer texts in statistics. New York: Springer.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "trees.html",
    "href": "trees.html",
    "title": "15  Baum-basierte Methoden",
    "section": "",
    "text": "15.1 Entscheidungsbäume\nEin Entscheidungsbaum ist ein Modell, das auf der Basis von hierarchischen Bedingungen bzgl. der Regressoren Vorhersagen für die Outcome-Variable trifft. Jeder Baum beginnt mit einem Wurzelknoten (root node) und verzweigt sich binär. Jede Verzweigung (split) stellt eine Bedingung dar, die auf einem bestimmten Regressor basiert. Der Baum trifft Entscheidungen, indem er diese Bedingungen sukzessive überprüft, bis er zu einem Blattknoten (leaf node / terminal node) gelangt, der die finale Vorhersage liefert. Hierbei handelt es sich eine Mehrheitsentscheidung für Klassifikation und einen Mittelwert, jeweils gebildet anhand Beobachten des Trainingsdatensatzes im leaf node.\nAbbildung 15.1 zeigt ein einfaches Beispiel eines Entscheidungsbaums zur Klassifikation der Kreditwürdigkeit einer Person. Die Klassfikation erfolgt, in dem die Beobachtung basierend auf den Merkmalen Alter, Einkommen und Eigentum durch den Baum geleitet wird. Zunächst wird geprüft, die Person 30 Jahre oder jünger ist. Fall ja, entscheidet der Baum anhand des Einkommens: Bei einem Jahreseinkommen von 40.000 oder weniger wird die Person als wenig kreditwürdig klassifiziert, bei höherem Einkommen als mäßig kreditwürdig. Für Personen älter als 30 Jahre überprüft das Modell lediglich, ob die Person eine Immobilie besitzt, um zwischen mäßiger Kreditwürdigkeit und guter Bonität zu unterscheiden.\nexdectree\n1\nAlter &lt;= 30?2\nEinkommen &lt;= 40 Tsd.?1-&gt;2\nJa3\nEigentum?1-&gt;3\nNein4\nStatus: Niedrig2-&gt;4\nJa5\nStatus: Mittel2-&gt;5\nNein3-&gt;5\nNein6\nStatus: Hoch3-&gt;6\nJa\n\n\n\nAbbildung 15.1: Entscheidungsbaum: Klassifikation von Kreditwürdigkeit",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Baum-basierte Methoden</span>"
    ]
  },
  {
    "objectID": "trees.html#training-von-bäumen",
    "href": "trees.html#training-von-bäumen",
    "title": "15  Baum-basierte Methoden",
    "section": "\n15.2 Training von Bäumen",
    "text": "15.2 Training von Bäumen\nZur Konstruktion von Binär-Bäumen werden etablierte Algorithmen wie Classification and Regression Trees (CART von Breiman u. a. (1984) verwendet. Die wesentliche Vorgehensweise für das Training eines Baums \\(T\\) ist wie folgt:\n\n\n\n\n\n\nCART-Algorithmus\n\n\n\n\n\nSplitting: Beginnend am root node sucht der Algorithmus nach der “besten” Regel, die Daten anhand eines Merkmals in zwei Gruppen zu teilen. Die Qualität des Splits wird in Abhängigkeit der Definition der Outcome-Variable beurteilt:\n\nBei Klassifikation: Die Reinheit (purtity) der Klassen in den unmittelbar nachfolgen nodes wird maximiert. Ein gängiges Kriterium hierfür ist der Gini-Koeffizient.1\nBei Regression: Der MSE bei Vorhersage des Outcomes durch Mittelwertbildung für Beobachtungen in den unmittelbar nachfolgenden nodes wird minimiert.\n\n\n\nRekursion: Der Prozess wird rekursiv fortgesetzt, bis Abbruchkriterien greifen eine weitere Verzewigung verhindern:\n\nDie maximale Baumtiefe (tree depth) ist erreicht\nDie leaf nodes sind hinreichend “rein”: Alle Beobachtungen in einem leaf node gehören zur gleichen Klasse oder die Verbesserung des Loss durch weitere Splits fällt unter einen festgelegten Schwellenwert\nWeitere Splits führen zu leaf nodes, die eine Mindestanzahl an Beobachtungen (minimum split) unterschreiten würden\n\n\n\nPruning: Um Überanpassung an die Trainingsdaten zu vermeiden, kann der Baum beschnitten werden (pruning). Der Grundgedanke ist, dass tief verzweigte Bäume die Trainingsdaten zwar gut modellieren können, aber schlecht auf neue, unbekannte Daten generalisieren.\nBei cost complexity (CP) pruning werden, beginnend auf Ebene der leaf nodes sukuzessive Äste entfernt, und eine Balance zwischen Komplexität des Baums und dem Anpassungsfehler zu finden. Ähnlich wie bei regularisierter KQ-Schätzung (Kapitel 13), wird die Verlustfunktion \\(L\\) um einen Strafterm für die Komplexität erweitert. Der Effekt der Strafe wird durch den CP-Parameter \\(\\alpha\\in[0,1]\\) geregelt,\n\\[\\begin{align*}\n   L_{\\alpha}(T) = L(T) + \\alpha \\lvert T\\rvert,\n\\end{align*}\\]\n\n\nfür einen Baum \\(T\\) mit Komplexitätsmaß \\(\\lvert T\\rvert\\) (Anzahl der leaf nodes) (Hastie, Tibshirani, und Friedman 2013).\n\n\n1 Der Gini-Koeffizient \\(0\\leq G\\leq1\\) misst die Homogenität der Outcome-Variable für die Beobachtungen eines Knotens. \\(G=0\\) ergibt sich bei vollständiger “Reinheit” (alle Beobachtungen im Knoten gehören zur gleichen Klasse). \\(G &gt; 0\\) zeigt Heterogenität der Klassen an, die mit \\(G\\) zunimmtZur Demonstation der Schätzung von Regressionsbäumen mit R betrachten wir nachfolgend den Datensatz MASS::Bosten. Ziel hierbei ist es, mittlere Hauswerte medv in Stadteilen von Boston, MA vorherzusagen. Wir verwenden hierzu Funktionen aus dem Paket parsnip.\nZunächst transformieren wir den Datensatz in ein tibble-Objekt und definieren Trainings- und Test-Daten.\n\n\n\n\n\n\n\n\nparsnip bietet eine vereinheitlichetes Framework für das Training von Modellen mit R und eine flexible API für Machine Learning. Wir definieren zunächst mit parsnip::decision_tree() eine Spezifikation zum Training von Entschieundgsmodellen und übergeben beispielhaft einen CP-Parameter \\(\\alpha=.1\\). Mit parsnip::set_engine wählen wir das Paket raprt. Der hier implementierte Agorithmus ist CART. Zuletzt legen wir mit parsnip::set_mode() fest, dass der Algorithmus für Regression durchgeführt werden soll.\n\n\n\n\n\n\n\n\nDer Output in tree_fit$fit zeigt, dass CP-Pruning zu einem kleinen Baum mit 3 Hierarchie-Ebenen geführt hat. Die Struktur zeigt, dass lstat und rm für Splitting-Regeln (split) verwendet werden, wie viele Beobachtungen den nodes zugeordnet sind (n), den Wert der Verlustfunktion (deviance) sowie den Durchschnitt von medv für jede node (yval). Für die drei leaf nodes (gekennzeichnet mit *) ist yval die Vorhersage der Outcome-Varibale für entsprechend gruppierte Beobachtungen.\nEine leichter interpretierbare Darstellung der Entscheidungsregeln des angepassten Baums in tree_fit$fit erhalten wir mit rattle::fancyRpartPlot().\n\n\n\n\n\n\n\n\nAbbildung 15.2 zeigt Beobachtungen von rm und lstat, die hinsichtlich ihrer in drei Klassen eingeteilten Ausprägung von medv eingefärbt sind. Die durch den CART-Algorithmus gelernten Entscheidungsregeln sind als farbige Paritionen des Regressorraums dargestellt.\n\n\n\n\n\n\n\nAbbildung 15.2: Partitionierung des Regressorraums für lstat und rm durch Regressionsbaum\n\n\n\n\nFür eine datengetriebene Wahl des CP-Parameters \\(\\alpha\\) kann Cross Validation (CV) verwendet werden. Hierzu erstellen wir zunächst eine parsnip-Spezifikation mit cost_complexity = tune::tune() in decision_tree() und erstellen einen workflow mit parsnip::workflow()\n\n\n\n\n\n\n\n\nMit rsample::vfold_cv() definieren wir den CV-Prozess: 10-fold CV mit 2 Wiederholungen. tune::tune_grid() führt CV anhand des in tree_wf_cv definierten workflows durch. Hierbei werden in cp_grid festgelegte Werte von cost_complexity berücksichtigt. Die mit yardstick::metric_set(rmse) festgelegte Verlustfunktion ist der mittlere quadratische Fehler (RMSE).2\n2 Die hier verwedete Funktion ist yardstick::rmse().\n\n\n\n\n\n\n\nMit workflowsets::autoplot() kann der CV-RMSE für als Funktion des CP-Parameter leicht grafisch betrachtet dargestellt werden.\n\n\n\n\n\n\n\n\nFür eine tabellierte Übersicht der besten Modelle kann tune::show_best() verwendet werden. tune::select_best() liest die beste Parameter-Kombination aus.\n\n\n\n\n\n\n\n\nAnhand tree_fit_cv trainieren wir die finale Spezifikation.\n\n\n\n\n\n\n\n\nDer geringe CP-Parameter führt zu einem großen Entscheidungsbaum.3\n3 Die Dimension der Grafik wurde hier zwecks Darstellung des gesamten Baums gewählt. print(final_tree_fit$fit) druckt die Entscheidungsregeln in die R-Konsole (hierzu die letzte Zeile ausführen).\n\n\n\n\n\n\n\nZur Beurteilung der Relevanz von Variablen für die Reduktion des Anpassungsfehlers (variable importance) kann der Eintrag variable.importance des rpart-Objekts herangezogen werden. Variable importance misst hier die Gesamtreduktion der Fehlerquadratsumem über alle Knoten, an denen die jeweilige Variable für Splits verwendet wird.\n\n\n\n\n\n\n\n\nDie Werte von Variable Importance zeigen, dass der mit CV ermittelte Baum alle Regressoren in boston_train für Splits nutzt, wobei lstat und rm die relevantesten Variablen sind.\nAnhand von Vorhersagen für medv mit dem Test-Datensatz boston_test können wir das naive Baum-Modell tree_fit mit dem durch CV ermittelten Modell tree_fit_cv hinsichtlich des Vorhersagefehlers für ungesehene Beobachtungen vergleich. yardstick::metric() berechnet hierzu automatisch gängige Statistiken für Regressionsprobleme.\n\n\n\n\n\n\n\n\nDer Vergleich zeigt eine bessere Vorsageleistung des großen Baums in tree_fit_cv. In diesem Fall scheint CP-Pruning wenig hilfreich zu sein. Tatsächlich liefert ein Baum mit \\(\\alpha=0\\) bessere Vorhersagen als tree_fit_cv (überprüfe dies!).",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Baum-basierte Methoden</span>"
    ]
  },
  {
    "objectID": "trees.html#bagging",
    "href": "trees.html#bagging",
    "title": "15  Baum-basierte Methoden",
    "section": "\n15.3 Bagging",
    "text": "15.3 Bagging\nBagging ist eine Ensemble-Modelle, die durch aus einer Kombination von vielen Entscheidungsbäumen bestehen. Bagging steht für Bootstrap Aggregating und nutzt einen Algorithmus, bei dem Bäume auf zufälligen Stichproben aus dem Trainingsdatensatz angepasst werden: Jeder Baum wird auf einer Bootstrap-Stichprobe (siehe Kapitel 5) trainiert, die durch zufällige Züge (mit Zurücklegen) erstellt wird. Nach dem Training aggregiert Bagging die Vorhersagen aller Bäume des Ensembles.\nDer Vorteil von Bagging gegenüber einem einzelnen Entscheidungsbaum ist, dass die Varianz der Vorhersage deutlich reduziert werden kann: Einzelne Entscheidungsbäume neigen dazu, Muster in den Trainingsdaten zu lernen, die sich zufällig aus der Zusammensetzung der Stichprobe ergeben und nicht repräsentativ für Zusammenhänge zwischen den Regressoren und der Outcome-Variable sind. Diese Überanpassung führt zu hoher Varianz auf von Vorhersagen für ungesehene Daten. Durch das Training vieler Bäume auf unterschiedlichen zufälligen Stichproben aus den Trainingsdaten und das anschließende Aggregieren kann der negative Effekt der Überanpassung auf die Unsicherheit der Vorhersage einzelner Bäume reduziert werden.\nEine Bagging-Spezifikation kann mit parsnip::bag_tree() festgelegt werden. Mit times = 500 wird definiert, dass der Bagging-Algorithmus ein Ensemble mit 500 Bäumen (mit CART) anpassen soll. Das Training und die Vorhersage auf den Testdaten erfolgt analog zur Vorgehensweise in Kapitel 15.1.\n\n\n\n\n\n\n\n\nDie Auswertung auf den Testdatensatz ergibt eine deutliche Verbesserung der Vorhersageleistung gegenüber einem einfachen Regressionsbaum.\nObwohl die Bäume beim Bagging auf unterschiedlichen Stichproben trainiert werden, kann innerhalb des Ensembles dennoch eine deutliche Korrelation vorliegen: Da jeder Baum auf alle Regressoren für Splits zugreift, können trotz Bootstrapping ähnliche (unverteilhafte) Muster aus dem Datensatz erlernt werden, was sich nachteilig auf die Generalisierungsfähigkeit auswirken kann. Diese Korrelation mindert die Effektivität von Bagging, da stark korrelierte Bäume dazu neigen, ähnliche Fehler zu machen.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Baum-basierte Methoden</span>"
    ]
  },
  {
    "objectID": "trees.html#sec-brf",
    "href": "trees.html#sec-brf",
    "title": "15  Baum-basierte Methoden",
    "section": "\n15.4 Random Forests",
    "text": "15.4 Random Forests\nRandom Forests erweitern Bagging, indem zusätzlich bei jedem Knoten innerhalb jedes Baumes eine zufällige Teilmenge der Regressoren als potentielle Variable für die Split-Regel ausgewählt wird. Dies führt zu einer Reduktion der Korrelation zwischen den Bäumen, was die Genauigkeit verbessert und das Risiko von Overfitting weiter verringert.\nIn R erstellen wir die Spezifikation mit parsnip::rand_forest(). Der Parameter mtry legt fest, wie viele Regressoren \\(m\\) zufällig für jeden Split zur Verfügung stehen. Wir nutzen den im randomForest-Paket implementierten Algorithmus und legen in set_engine() fest, dass die von randomForest::randomForest() berechnete Fehler-Metrik im Output-Objekt ausgegeben wird (tree.err = TRUE). Um die Spezifikation für verschiedene Werte von mtry anwenden zu können, implementieren wir die Spezifikation innerhalb einer Wrapper-Funktion rf_spec_mtry(). Mit purrr::map() iterieren wir rf_spec_mtry() über drei verschiedene Werte für den Tuning-Parameter mtry (4, 6 und 10 Variablen).4\n4 Eine Faustregel für die Wahl von \\(m\\) bei \\(k\\) verfügbaren Regressoren ist \\(m\\approx\\sqrt{k}\\).\n\n\n\n\n\n\n\nFür eine Beurteilung des Vorhersageleistung dieser drei Modelle können wir den Out-of-Bag-Fehler (OOB) verwenden:\nDer OOB-Fehler ist eine Schätzung des Generalisierungsfehlers ohne einen separaten Testdatensatzes. Bei Random Forests (und Bagging) ist dies aufgrund der Berechnung des Ensembles für Bootstrap-Stichproben möglich: Grob ein Drittel der Beobachtungen des Datensatzes sind nicht Teil der Stichprobe, die für das Training jedes Baums im Ensemble genereiert werden.5 Diese nicht gezogenen Datenpunkte sind OOB-Beobachtungen. Der OOB-Fehler des Ensembles ist der durchschnittliche Fehler für die aggregierten Vorhersagen der Bäume des Forests.\n5 Beachte, dass beim Bootstrap \\(n\\) aus \\(n\\) Beobachtungen mit Zurücklegen gezogen werden. Die Wahrscheinlicht, dass eine Beobachtung nicht gezogen wird (“Out-of-Bag”), ist \\((1-1/n)^n\\approx37\\%\\).Der OOB-Fehler kann auch verwendet werden, um die erforderliche Größe des Random Forests zu beurteilen: Eine größere Anzahl von Bäumen reduziert tendenziell die Varianz der Vorhersagen und verbessert die Generalisierungsfähigkeit. Allerdings nimmt dieser Effekt ab, und ab einer bestimmten Baumanzahl sind weitere Verbesserungen marginal. Obwohl das Risiko von Überanpassung durch viele Bäume aufgrund des Bagging minimal ist, kann es bei großen Datensätzen sinnvoll sein, kleinere Wälder zu trainieren, um den Rechenaufwand zu verringern. Wir plotten hierfür den OOB-Fehler für das Modell mit mtry = 10 gegen die Anzahl der Bäume.\n\n\n\n\n\n\n\n\nDie Grafik zeigt, dass die Verbesserung des OOB-Fehlers jenseits von 250 Beobachtungen deutlich nachlässt, sodass ein Training von 500 Bäumen ausreichend scheint.\nZur Beurteiliung der Vorhersagegüte mit dem Testdatensatz gehen wir analog zum Training vor und iterieren mit map() über rf_fits, die Liste der angepassten Modelle.\n\n\n\n\n\n\n\n\nÄhnlich wie für einen einzelnen Baum kann die Relevanz von Variablen anhand der Reduktion der Loss-Funktion durch das Ensemble beurteilt werden. Für einen einfachen Vergleich der Variable Importance für den Random Forests mit mtry = 10 in rf_fits$rf_mtry10_fit nutzen wir ggRandomForests::gg_vimp().\n\n\n\n\n\n\n\n\nDie Grafik bestärkt unsere Schlussfolgerung aus der Analyse des (mit CART trainierten) einzelnen Entscheidungsbaums in Kapitel 15.1, dass rm und lstat die wichtigsten Regressoren für die Vorhersage von medv sind.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Baum-basierte Methoden</span>"
    ]
  },
  {
    "objectID": "trees.html#sec-boosting",
    "href": "trees.html#sec-boosting",
    "title": "15  Baum-basierte Methoden",
    "section": "\n15.5 Boosting",
    "text": "15.5 Boosting\nBoosting ist eine leistungsstarke Ensemble-Methode für Vorhersagen, die kleine Modelle (oft Entscheidungsbäume geringer Tiefe) sukzessiv trainiert und zu einem starken Modell kombiniert. Anders als bei Random Forests, bei denen viele Bäume unabhängig voneinander auf zufälligen Stichproben der Daten trainiert werden, geht ein Boosting-Algorithmuss sequentiell vor: Jeder nachfolgende Baum wird darauf optimiert, die Fehler des vorherigen Modells zu reduzieren. Die Idee hierbei ist es, iterativ “schwache” Modelle zu erzeugen, die eine gute Anpassung für Datenpunkte liefern, die in den vorherigen Durchläufen schlecht vorhergesagt wurden.\nFür einen Trainingsdatensatz \\(\\{(x_i, y_i)\\}_{i=1}^n\\), wobei \\(x_i\\) die Input-Features und \\(y_i\\) Beobachtungen des Outcomes sind, kann Boosting wiefolgt durchgeführt werden.\n\n\n\n\n\n\nBoosting für Regression\n\n\n\n\nInitialisierung: Initialisiere das Boosting-Modell als \\(\\widehat{F}_0(x)\\). Setze die Residuen \\(r^0_i=y_i\\) für alle \\(i\\)\n\nIteration: Wiederhole die folgenden Schritte für \\(b = 1,2,\\dots,B\\) mit \\(B\\) hinreichend groß:\n2.1 Base Learner: Trainiere Baum \\(T_b\\) mit \\(\\{(\\boldsymbol{x}_i, r^{b-1}_i)\\}_{i=1}^n\\) für die Vorhersage des Fehlers der vorherigen Iteration \\(r^{b-1}\\).\n2.2 Aktualisierung: Aktualisiere das Boosting-Modell,\n\\[\\begin{align*}\n   \\widehat{F}_{b}(\\boldsymbol{x}) = \\widehat{F}_{b-1}(\\boldsymbol{x}) + \\eta \\cdot T_{b}(\\boldsymbol{x}),\n   \\end{align*}\\]\nwobei \\(\\eta\\) die (oft klein gewählte) Lernrate ist.\n2.3 Fehlerberechnung: Berechne die Residuen \\(r^b_i\\) als Differenzen zwischen dem tatsächlichen Werten \\(y_i\\) und den Vorhersage des aktuellen Modells \\(\\widehat{F}_m(\\boldsymbol{x}_i)\\),\n\\[\\begin{align*}\n   r^b_i = y_i - \\widehat{F}_b(\\boldsymbol{x}_i).\n   \\end{align*}\\]\n\n\nOutput: Gib das finale Modell aus:\n\\[\\begin{align*}\n   \\widehat{F}(\\boldsymbol{x}) := \\sum_{b=1}^B \\eta\\cdot \\widehat{F}^b(\\boldsymbol{x})\n\\end{align*}\\]\n\n\n\n\nDer Parameter \\(0\\leq\\eta\\leq0\\) steuert, wie stark der Einfluss jedes neuen Baumes auf das Modell ist. Eine kleine Lernrate führt dazu, dass viele Bäume benötigt werden, was Vorhersagen (ähnlich wie bei Bagging) stabiler macht. Beachte die sequentielle Natur des Trainings: Die \\(r^b_i\\) in Schritt 2.3 sind die zu vorhersagenden Outcome-Variable für den nächsten Baum. \\(T_{b+1}\\) wird trainiert wird, um den Fehler des bisherigen Modells \\(\\widehat{F}_b\\) zu erklären.\nFür die Anwendung auf MASS::Boston in R nutzen wir den im Paket gbm implementierten Gradient-Boosting-Algorithmus. Bei Gradient Boosting wird jeder Baum so trainiert, dass er den negativen Gradienten einer Verlustfunktion approximiert, also die Richtung des größten Fehlers. Das Modell wird schrittweise verbessert, indem es entlang des Gradienten aktualisiert wird, um die Vorhersagegüe zu optimieren; siehe Hastie, Tibshirani, und Friedman (2013) für eine detaillierte Erläuterung.\nMit dem nachfolgenden Code-Chunk trainieren wir ein Boosting-Modell für Regression mit 5000 einfachen Bäumen (n.trees = 5000) mit einer maximalen Tiefe von 2 (interaction.depth = 2), d.h. es folgen maximal 2 Entscheidungs-Regeln nacheinander. Um das Risiko von Overfitting gering zu halten, erlauben wir nur Splits, die zu mindestens zwei Beobachtungen in resultierenden nodes führen (n.minobsinnode = 2). Die Lernrate (Beitrag der Base Learner zum Ensemble) wird typischerweise klein (und in Abhängigkeit von n.trees) gewählt (shrinkage = 0.001).6\n6 Je kleiner die Lernrate, desto größer sollte n.trees gewählt werden.\n\n\n\n\n\n\n\nFür die Vorhersagen auf dem Test-Datensatz legen wir mit n.trees = gbm_model$n.trees fest, dass das gesamte Ensemble genutzt werden soll.\n\n\n\n\n\n\n\n\nDie Ergebnisse zeigen, dass Gradient Boosting bereits für die naive Parameterwahl im Aufruf von gbm::gbm() zu einer Verbesserung der Vorhersageleistung gegenüber den Random-Forest-Modellen führt.\nAnstatt n.trees = 5000 können wir n.trees in predict() einen Vektor mit verschiedenen Ensemble-Größen übergeben. Für n.trees = 5000 erhalten wir Vorhersagen für jeden Status, den das Boosting-Modell im Training nach seiner Initialisierung bis zu der in gbm::gbm() festgelgten Größe durchläuft. Anhand dieser Vorhersagen können wir die Generalisierungsfähigkeit des Modells in Abhängigkeit der gewählten Lernrate und der Größe beurteilen, in dem wir den RMSE für den gesamten Trainingsprozess berechnen. Für eine leichtere Interpretation erzeugen wir eine Grafik ählich wie bei der OOB-Analyse des Random-Forest-Modells.\n\n\n\n\n\n\n\n\nDie Grafik zeigt eine schnelle Verbesserung des Out-of-sample-Fehlers mit der Größe des Ensembles. Für die gewählte Lernrate scheinen 5000 Bäume adäquat zu sein.\nAnalog zu Bagging und Random Forests können wir die Relevanz der Regressoren in Boston für die Vorhersage von medv anhand der mit summary() berechneten (relativen) Variable Importance für die Anpassung auf den Trainingsdatensatz einschätzen.\n\n\n\n\n\n\n\n\nObwohl erneut lstat und rm als die wichtigsten Prädiktoren gelistet sind, identifiziert Gradient Boosting im Gegensatz zu Bagging und Random Forests lstat als die Variable mit der größten Vorhersagekraft für medv.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Baum-basierte Methoden</span>"
    ]
  },
  {
    "objectID": "trees.html#causal-trees-und-causal-forests",
    "href": "trees.html#causal-trees-und-causal-forests",
    "title": "15  Baum-basierte Methoden",
    "section": "\n15.6 Causal Trees und Causal Forests7\n",
    "text": "15.6 Causal Trees und Causal Forests7\n\nBaum-Algorithmen sind vielversprechende Ansätze zur Schätzung kausaler Effekte, insbesondere in Situationen, in denen die Bestimmung heterogener Effekte gewünscht ist: Der Vorteil von Baum-Methoden liegt darin, dass sie nicht-parametrisch sind: Der Regressorraum wird adaptiv in Partitionen unterteilt, um auf Basis dieser Aufteilung differenzierte Vorhersagen für die Zielvariable zu treffen. Diese Eigenschaft kann für Kausalanalysen hilfreich sein, da wir in vielen empirischen Anwendungen die Effekte einer Behandlung nicht nur im Durchschnitt für die betrachtete Population, sondern differenzierter schätzen möchten: Ein durchschnittlicher Behandlungseffekt (engl. average treatment effect, ATE) kann nicht ausreichend informativ für unsere Forschungsfrage sein, bspw. wenn wir erwarten, dass eine politische Intervention unterschiedliche Auswirkungen auf verschiedene Bevölkerungsgruppen hat. Idealerweise möchten wir \\(\\tau_i\\) bestimmen, den individuellen Behandlungseffekt einer Beobachtung \\(i\\). Das fundamentale Problem der Kausalinferenz ist, dass \\(\\tau_i\\) nicht ermittelt werden kann (s. u.), sodass wir unser Ziel abschwächen müssen. Statt \\(\\tau_i\\) suchen wir einen Behandlungseffekt in Abhängigkeit von beobachtbaren Charakteristiken \\(\\boldsymbol{X}\\) für Untergruppen der Population, einen conditional average treatment effect (CATE). Im Potential-Outcomes-Framework ist der CATE definiert als\n\\[\\begin{align*}\n  \\tau(\\boldsymbol{x}) = \\textup{E}\\big(Y^{(1)} - Y^{(0)}\\big\\vert \\boldsymbol{X} = \\boldsymbol{x}\\big),\n\\end{align*}\\]\nwobei \\(Y^{(1)}\\) und \\(Y^{(0)}\\) die potenziellen Outcomes darstellen, wenn eine Behandlung erfolgt bzw. nicht erfolgt. In der Praxis beobachten wir jedoch nur \\(Y_i = Y_i^{(B_i)}\\), wobei \\(B_i\\) der Behandlungsindikator für die Beobachtung \\(i\\) ist, sodass \\(\\tau(\\boldsymbol{x}_i)\\) nicht direkt beobachtet werden kann. Unter der Annahme, dass nach Kontrolle für (beobachtbare) \\(\\boldsymbol{X}\\) die Zuordnung zur Behandlung quasi-zufällig ist (unconfoundedness), formal\n\\[\\begin{align*}\nY_i^{(0)},\\,Y_i^{(1)} \\perp B_i \\vert \\boldsymbol{X}_i,\n\\end{align*}\\]\nkann \\(\\tau(\\boldsymbol{x})\\) geschätzt werden: Wir können Outcome-Differenzen zwischen behandelten und nicht behandelten Beobachtungen als kausal interpretieren, da unbeobachtete Faktoren die Ergebnisse nicht verzerren.\nCART und andere traditionelle Entscheidungsbaum-Algorithmen sind für die Schätzung heterogener Behandlungseffekte jedoch ungeeignet. Dafür gibt es zwei wesentliche Ursachen:\n\n\nDas Splitting-Kriterium\nDas Splitting-Kriterium des CART-Algorithmus optimiert die Aufteilungen der Beobachtungen in jedem Knoten, um die Genauigkeit von Vorhersagen für die Outcome-Variable \\(Y\\) durch Minimierung der Heterogienität (Klassifikation) oder des MSE (Regression) zu optimieren. Diese Kriterien zielen also darauf ab, die Homogenität innerhalb der Blätter hinsichtlich \\(Y\\) zu maximieren.\nFür die Schätzung heterogener kausaler Effekte ist ein solches Splitting jedoch nicht zielführend. Statt Knoten zu formen, in denen \\(Y\\) möglichst homogen ist, benötigen wir für die Schätzung von Behandlungseffekten grundsätzlich Aufteilungen, bei denen sich \\(Y\\) zwischen den behandelten und unbehandelten Individuen innerhalb der Knoten unterscheidet.8 Das Splitting sollte zu Blättern führen, die hinsichtlich des geschätzten Behandlungseffekts möglichst heterogen sind.\nDie Wahl des Splitting-Kriterium für die Schätzung kausaler Effekte mit Bäumen ist nicht trivial: Ein natürliches Kriterium ist der mittlere quadratische Fehler bei der Vorhersage von \\(\\tau\\),\n\\[\\begin{align*}\n    \\textup{MSE}_\\tau = \\frac{1}{n} \\sum_{i=1}^n (\\tau_i - \\widehat{\\tau}_i(\\boldsymbol{X}_i))^2.\n  \\end{align*}\\]\n\\(\\textup{MSE}_\\tau\\) ist jedoch nicht direkt berechenbar: Aufgrund der nicht-beobachtbaren individuellen Behandlungseffekte \\(\\tau_i\\) müsste \\(\\textup{MSE}_\\tau\\) selbst geschätzt werden!9\n\n\nData leakage\nData leakage tritt auf, wenn Informationen aus dem Trainingsprozess in den Modellvalidierungs- oder Schätzprozess einfließen. Bei der Anpassung des Baums berücksichtigt der Algorithmus idealerweise Informationen über \\(Y\\) und \\(B\\) im Splitting-Prozess, um die besten Aufteilungen zu finden. Die hiezu verwendeten Datenpunkte definieren damit den zu schätzten CATE anhand der durch Partionierung gebildeten Blätter. Wenn dieselben Datenpunkte auch für die tatsächliche Schätzung des CATE mit dem trainierten Baum verwendet werden, besteht die Gefahr von Überanpassung und somit verzerrten Schätzungen.\n\n\n8 Wenn die Kontroll- und Behandlungsbeobachtungen in einem Blatt sehr ähnliche Outcomes \\(Y\\) haben, können wir den Effekt nicht schätzen.9 Bei “herkömmlichen” Regressionsbäumen besteht dieses Problem nicht, weil das Splitting-Kriterium Abweichungen von den wahren, beobachteten Werten von \\(Y\\) misst.\n15.6.1 Causal Trees\nDer Causal Tree Algorithmus von Athey und Imbens (2016) modifiziert den CART-Algorithmus für die Schätzung heterogener Behandlungseffekte. In diesem Kontext wird die Vorgehensweise als „ehrlich“ (honest) bezeichnet, wenn nicht dieselben Informationen sowohl zur Auswahl des Modells (die Partitionierung des Regressorraums durch Splits) als auch zur Schätzung anhand dieses Modells verwendet werden. Athey und Imbens (2016) adressieren das Data-Leakage-Problem durch zufällige Aufteilung des Datensatzes in eine Teilmenge \\(\\mathcal{S}^{tr}\\) für das Training des Baums und eine Teilmenge \\(\\mathcal{S}^{est}\\) für die Schätzung der Behandlungseffekte.\nFür die Erläuterung von honest splitting führen wir folgende Notation aus Athey und Imbens (2016) ein:\n\n\\(\\mathcal{S}^{te}\\) ist ein hypothetischer Testdatensatz\n\\(\\Pi\\) ist eine Partition, d.h. eine Aufteilung des Regressorraums von \\(\\boldsymbol{X}\\)10\nWir definieren die Schätzung des CATE anhand der Beobachtungen \\(\\mathcal{S}^{est}\\): Der CATE \\(\\widehat{\\tau}(\\boldsymbol{X}_i,\\mathcal{S}^{est},\\Pi)\\) ist die Differenz der Mittelwerte von \\(Y_i\\) für Behandlungs- und Kontrollbeobachtungen in dem aus \\(\\Pi\\) resultierenden Blatt für \\(\\boldsymbol{X}_i\\).\n\n10 \\(\\Pi\\) sammelt also die Entschidungsregeln eines Baums und ist äquivalent zu \\(T\\) in den füheren Kapiteln.11 Die Notation \\(\\textup{E}_{\\mathcal{S}^{est},\\,\\mathcal{S}^{te}}\\) meint, dass die Erwartung über \\(\\mathcal{S}^{est}\\), und \\(\\mathcal{S}^{te}\\) gebildet wird.Für die Wahl der Splits (die Partitionierung \\(\\Pi\\)) für den Causal Tree schlagen Athey und Imbens (2016) statt der Minimierung des MSE der Vorhersagen \\(\\widehat{Y}\\) (wie bei Regressionsbäumen) die Minimierung des MSE für den CATE vor. Das Vorgehen hierbei ist honest in dem Sinn, dass der erwartete Schätzfehler für ungesehene Beobachtungen \\(\\mathcal{S}^{te}\\) anhand einer Paritionierung \\(\\Pi\\) und entsprechenden Schätzungen der Behandlungseffekte \\(\\widehat\\tau\\) mit unabhängigen Datensätzen \\(\\mathcal{S}^{tr}\\) bzw. \\(\\mathcal{S}^{est}\\) minimiert wird. Das hierzu verwendete Splitting-Kriterium ist eine Schätzung des Erwartungswerts von \\[\\begin{align*}\n  \\textup{MSE}(\\mathcal{S}^{est},\\mathcal{S}^{te},\\Pi) = \\frac{1}{n^{te}} \\sum_{i=1}^{n^{te}} \\big(\\tau_i - \\widehat{\\tau}(\\boldsymbol{X}_i,\\mathcal{S}^{est},\\Pi)\\big)^2,\n\\end{align*}\\] der erwartete11 mittlere quadratische Fehler der heterogenen Behandlungseffekte,\n\\[\\begin{align*}\n  \\textup{EMSE}(\\Pi) = \\textup{E}_{\\mathcal{S}^{est},\\,\\mathcal{S}^{te}}\\big[\\textup{MSE}(\\mathcal{S}^{est},\\mathcal{S}^{te},\\,\\Pi)\\big].\n\\end{align*}\\]\nEine hilfreiche Umformung für \\(\\textup{EMSE}\\) ist\n\\[\\begin{align*}\n  \\textup{EMSE}(\\Pi) = \\textup{Var}_{\\mathcal{S}^{est},\\boldsymbol{X}_i} \\big[\\widehat\\tau(\\boldsymbol{X}_i,\\mathcal{S}^{est},\\Pi)\\big] - \\textup{E}_{\\boldsymbol{X}_i}\\big[\\tau^2(\\boldsymbol{X}_i,\\Pi)\\big] + \\textup{E}[\\tau_i^2],\n\\end{align*}\\]\ndenn Athey und Imbens (2016) zeigen, wie die ersten beiden Summanden empirisch geschätzt werden können. Der Term \\(\\textup{E}[\\tau_i^2]\\) ist nicht schätzbar (unbeobachteter individueller Behandlungseffekt \\(\\tau_i\\)), kann aber vernachlässigt werden, da er nicht von \\(\\Pi\\) oder den Daten abhängt und somit eine Konstante ist, die sich beim Vergleich des geschätzen EMSE für verschiedene \\(\\Pi\\) rauskürzt.\nDies sorgt für konsistente Schätzungen. Athey und Imbens (2016) zeigen, dass die Minimierung des EMSE sowohl eine ausgewogene Verteilung der behandelten und unbehandelten Individuen als auch eine genaue Schätzung des Behandlungseffekts innerhalb jedes Knotens gewährleistet.\n\n\n\n\n\n\nAlgorithmus: Causal Tree\n\n\n\n\nPasse den Baum an: teile den Regressorraum mit binären Entscheidungsregeln rekursiv in Partitionen \\(\\Pi\\):\n\nAn jedem Knoten wird die Aufteilung so gewählt, dass die Schätzung des erwarteten mittleren quadratischen Fehlers () über alle möglichen binären Aufteilungen \\(\\Pi\\) minimiert wird.\nStelle sicher, dass eine Mindestanzahl von behandelten und Kontroll-Einheiten in jedem Blatt des so angepassten Baums nicht unterschritten wird.\n\n\nBestimmte mit Cross-Validation die Tiefe \\(d^*\\) der Partition, die eine Schätzung des MSE der Behandlungseffekte minimiert.\nEhalte die Partition \\(\\Pi^*\\) durch das Beschneiden von \\(\\Pi\\) auf die Tiefe \\(d^*\\): Entferne Blätter, die die geringste Verbesserung der Anpassung bieten. Dieser Schritt liefert den finalen Baum.\nSchätze die Behandlungseffekte in jedem Blatt von \\(\\Pi^*\\) mit den Beobachtungen in \\(\\mathcal{S}^{est}\\).\n\n\n\n\nnl_effects &lt;- readRDS(file = \"datasets/nl_effects.Rds\")\n\nTibshirani u. a. (2024)\n\nlibrary(dplyr)\nlibrary(grf)\n# Fit a causal tree (one single tree)\ncausal_tree &lt;- causal_forest(\n  X = nl_effects %&gt;% select(X1:X3) %&gt;% as.matrix(),\n  Y = nl_effects$Y,\n  W = nl_effects$B, \n  num.trees = 1, \n  honesty = FALSE, \n  min.node.size = 150\n)\n\ncausal_tree\n\nGRF forest object of type causal_forest \nNumber of trees: 2 \nNumber of training samples: 5000 \nVariable importance: \n    1     2     3 \n0.000 0.000 0.956 \n\n\nAbbildung 15.3\n\nthe_tree &lt;- get_tree(causal_tree, index = 1)\n\nplot(the_tree)\n\n\n\n\n\n\nAbbildung 15.3: Mit grf::causal_forest geschätzter Causal Tree\n\n\n\n15.6.2 Causal Forests\nCausal Forests sind eine Erweiterung von Random Forests, die speziell entwickelt wurden, um individuelle Behandlungseffekte*(engl. Individual Treatment Effects, ITE) zu schätzen. Der Hauptunterschied zu Random Forests besteht darin, dass Causal Forests nicht nur Vorhersagen treffen, sondern auch den kausalen Effekt einer Behandlung \\(B\\) auf das Outcome \\(Y\\) schätzen. Der kausale Effekt wird oft als Differenz der Ergebnisse unter verschiedenen Bedingungen (z. B. mit und ohne Behandlung) modelliert.\n\n\n\n\n\n\n\n\n\nAspekt\nRandom Forests\nCausal Forests\n\n\n\nZiel\nVorhersage von Zielvariablen\nSchätzung individueller Behandlungseffekte\n\n\nErgebnisse\nGlobale Vorhersagen\nHeterogene, individuelle Effekte\n\n\nTrainingsdaten\nOutcome und Prädiktoren\nBehandlung, Outcome und Prädiktoren\n\n\nBeispiel\nEinkommensvorhersage auf Basis von Merkmalen\nEffekt einer Werbekampagne auf verschiedene Kunden\n\n\nSchätzung\nBedingte Erwartung \\(\\textup{E}[Y|X]\\)\n\nBedingter Behandlungseffekt \\(\\textup{E}[Y(1) - Y(0)|X]\\)\n\n\n\nVerwendung\nKlassifikation und Regression\nKausalanalyse und individuelle Wirkungsschätzung\n\n\n\n\n\nTabelle 15.1: Vergleich von Random Forests und Causal Forests\n\n\nIn diesem Beispiel verwenden wir das Paket grf (Generalized Random Forests) in R, um einen Causal Forest zu trainieren und die individuellen Behandlungseffekte zu schätzen.\nNaive Methode mit Random Forest\n\nlibrary(tidyverse)\n# Daten ins Long-Format umwandeln für einfachere Facettierung\nnl_long &lt;- nl_effects %&gt;% \n  select(X1, X2, X3, tau) %&gt;% \n  pivot_longer(cols = starts_with(\"X\"), names_to = \"Variable\", values_to = \"Value\")\n\n# Erstelle den facettierten Plot\nggplot(\n  data = nl_long,\n  mapping = aes(x = Value, y = tau)) + \n  geom_point(alpha = 0.3, size = .5) + \n  facet_wrap(~Variable, scales = \"free\") + \n  theme_minimal() + \n  labs(\n    x = \"Wert der Prädiktoren\",\n    y = \"Wahrer Behandlungseffekt (tau)\"\n  )\n\n\n\n\n\n\nAbbildung 15.4: Zusammenhang zwischen X1, X2, X3 und dem wahren Behandlungseffekt tau\n\n\n\n\n\n# In langes Format überführen\ndf_long &lt;- nl_effects %&gt;% \n    select(X1, X2, X3, B) %&gt;% \n    pivot_longer(\n      cols = starts_with(\"X\"),\n      names_to = \"Variable\",\n      values_to = \"Value\"\n    )\n\n# Facetting nach Variable\nggplot(df_long, aes(x = Value, fill = factor(B))) + \n    geom_density(alpha = 0.5) + \n    facet_wrap(~Variable, scales = \"free\") + \n    labs(\n      x = \"Prädiktoren-Werte\",\n      fill = \"Behandlung (B)\"\n    ) +\n    theme_minimal() + \n    theme(legend.position = \"top\")\n\n\n\n\n\n\nAbbildung 15.5: Korrelation zwischen Behandlungsindikator (B) und relevanten Kovariablen\n\n\n\n\n\nlibrary(grf)\nlibrary(randomForest)\nlibrary(tidymodels)\n\nthe_split &lt;- initial_split(data = nl_effects, prop = .8)\nnl_effects_train &lt;- training(the_split)\nnl_effects_test &lt;- testing(the_split)\n\n# Variablen in Matrizen / Vektoren überführen\nX &lt;- nl_effects_train %&gt;% \n  select(starts_with(\"X\")) %&gt;% \n  as.matrix()\nB &lt;- nl_effects_train %&gt;% pull(B)\nY &lt;- nl_effects_train %&gt;% pull(Y)\ntau &lt;- nl_effects_train %&gt;% pull(tau)\n\n\n# Propensity Score Schätzen\nB_hat_mod &lt;- regression_forest(\n  X = X,\n  Y = B,\n  tune.parameters = \"all\"\n)\n\nB_hat &lt;- B_hat_mod$predictions\n\n# Outcome Schätzen\nY_hat_mod &lt;- regression_forest(\n  X = X,\n  Y = Y,\n  tune.parameters = \"all\"\n)\n\nY_hat &lt;- Y_hat_mod$predictions\n\n# Causal Forest trainieren\ncf &lt;- causal_forest(\n  X = X, \n  Y = Y, \n  W = B, \n  Y.hat = Y_hat,\n  W.hat = B_hat\n)\n\n\n# Schritt 4: Vorhersage des durchschnittlichen Behandlungseffekts\n# Causal Forest\ntau.cf &lt;- average_treatment_effect(cf)\ncat(\"Durchschnittlicher Behandlungseffekt mit Causal Forest:\", tau.cf[1], \"\\n\")\n\nDurchschnittlicher Behandlungseffekt mit Causal Forest: -0.2468697 \n\n\n\n# Schritt 5: Individuelle Schätzungen der Behandlungseffekte\n# Causal Forest - Individuelle Behandlungseffekte (CATE)\ntau.hat.cf &lt;- predict(cf)$predictions\n\n# Wahrer Behandlungseffekt tau\nhead(tau.hat.cf)\n\n[1] -0.0642255 -0.6270295 -1.0991273  3.1437400 -5.1925531  0.5135410\n\n\n\ntibble(\n    x = nl_effects_train$tau,\n    y = predict(cf)$predictions\n) %&gt;%\n    ggplot(aes(x  =x, y = y)) +\n    geom_point(alpha= .3, size = 1) +\n    geom_abline(intercept = 0, slope = 1, col = \"red\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n# ATE CF\nmean(tau.hat.cf)\n\n[1] -0.3033085\n\n\n\n# Schritt 6: Evaluierung - Vergleich der Genauigkeit der beiden Modelle\n# MSE für Causal Forest\nmse.cf &lt;- sqrt(mean((tau.hat.cf - tau)^2))\ncat(\"MSE des Causal Forest:\", mse.cf, \"\\n\")\n\nMSE des Causal Forest: 1.450783 \n\n\nTest set\n\nsqrt(mean((predict(object = cf, newdata = nl_effects_test %&gt;% select(-Y, -tau, -B))$predictions - nl_effects_test$tau)^2))\n\n[1] 1.468642\n\n\n\n# ATE CF pred\nmean(predict(object = cf, newdata = nl_effects_test %&gt;% select(-Y, -tau, -B))$predictions)\n\n[1] -0.4234307",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Baum-basierte Methoden</span>"
    ]
  },
  {
    "objectID": "trees.html#zusammenfassung",
    "href": "trees.html#zusammenfassung",
    "title": "15  Baum-basierte Methoden",
    "section": "\n15.7 Zusammenfassung",
    "text": "15.7 Zusammenfassung\nIn diesem Kapitel haben wir die Anwendung baum-basierter Methoden in R diskutiert. Darunter Entscheidungsbäume, Bagging, Random Forests und Boosting. Entscheidungsbäume sind Modelle, die die Daten anhand binärer Entscheidungsregeln sukzessiv in kleinere, homogene Gruppen aufgeteilt werden. Baum-Modelle bieten intuitive Interpretierbarkeit, neigen jedoch zur Überanpassung, was durch Beschneiden (Pruning) vermieden werden kann. Die Vorhersage einzelner Bäume ist tendentiell mit hoher Varianz verbunden. Random Forests kombinieren mit Bagging viele Entscheidungsbäume, die auf zufälligen Teilmengen der Daten und Merkmale trainiert werden. Durch die Aggregation der Vorhersagen vieler Bäume reduziert der Random Forest die Varianz und verbessert so die Vorhersagegenauigkeit. Boosting-Methoden mit Entscheidungsbäumen trainieren kleine Bäume sukzessive, wobei jeder weitere Baum zur Korrektur der gegenwärtigen Fehler des Ensembles trainiert wird. Gradient Boosting nutzt den Gradienten der Verlustfunktion, um die Vorhersagequalität des Ensembles optimieren.\nFür alle Methoden wurden Implementierungen im parsnip-Framework in R vorgestellt. Zudem wurde gezeigt, wie die Vorhersagegüte durch Testdatensätze beurteilt und die Bedeutung einzelner Variablen mit Variable-Importance-Metriken analysiert werden kann.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAthey, Susan, und Guido Imbens. 2016. „Recursive partitioning for heterogeneous causal effects“. Proceedings of the National Academy of Sciences 113 (27): 7353–60. https://doi.org/10.1073/pnas.1510489113.\n\n\nBreiman, L., J. Friedman, C. J. Stone, und R. A. Olshen. 1984. Classification and Regression Trees. Taylor & Francis.\n\n\nHastie, T., R. Tibshirani, und J. Friedman. 2013. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer Series in Statistics. Springer New York.\n\n\nTibshirani, Julie, Susan Athey, Erik Sverdrup, und Stefan Wager. 2024. grf: Generalized Random Forests. https://CRAN.R-project.org/package=grf.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Baum-basierte Methoden</span>"
    ]
  },
  {
    "objectID": "Machine Learning.html",
    "href": "Machine Learning.html",
    "title": "\n16  Neuronale Netzwerke\n",
    "section": "",
    "text": "16.1 Grundlagen und Vokabeln\nNN bestehen aus einer (often großen) Anzahl so genannter künstlicher Neuronen. Ein Neuron ist eine mathematische Funktion, die mehrere Eingaben empfängt, diese unter Verwendung von Gewichten linear kombiniert und eine Ausgabe durch Verwendung einer Aktivierungsfunktion generiert.\nDie Neuronen eines NN sind in Schichten (Layers) organisiert. Jedes Layer verarbeitet die Eingabedaten und gibt die Ergebnisse an das nächste Layer weiter, wobei die Neuronen verschiedener Layer miteinander verknüpft werden. Während das Eingabe-Layer (Input) die “Rohdaten” (bspw. beobachtete Regressorwerte) aufnimmt und sie an die erste versteckte Schicht (Hidden Layer) weiterleitet, ist die Hauptaufgabe der Neuronen in den Hidden Layers, komplexe Muster und Merkmale in den Daten zu erkennen und zu verarbeiten. Jedes Hidden Layer transformiert die empfangenen Daten anhand seiner Neuronen, bevor diese an das nächste Layer weitergeleitet werden. Das letzte Layer in einem neuronalen Netzwerk ist das Ausgabe-Layer (Output Layer), das die endgültige Vorhersage für die Outcome-Variable basierend auf den verarbeiteten Daten liefert.\nDie Stärke der Verknüpfungen zwischen den Neuronen wird durch die Gewichte \\(w\\) bestimmt, welche während des Trainingsprozesses angepasst werden, um das Modell hinsichtlich der (Vorhersage) einer Zielvariable zu optimieren. Die \\(w\\) bestimmen, wie stark die Aktivierung eines Neurons in einer Schicht die Aktivierung der Neuronen in der nächsten Schicht beeinflusst. Das Netzwerk kann so tiefe und abstrakte Strukturen eines Datensatzes abbilden.\nNNEX\nX1\nX1V1\nV1(A)X1-&gt;V1\nw11V2\nV2(A)X1-&gt;V2\nw12V3\nV3(A)X1-&gt;V3\nw13X2\nX2X2-&gt;V1\nw21X2-&gt;V2\nw22X2-&gt;V3\nw23Y\nY(A)V1-&gt;Y\nw31V2-&gt;Y\nw32V3-&gt;Y\nw33\n\n\n\nAbbildung 16.1: Neuronales Netzwerk mit einem Hidden Layer\nAngenommen wir interessieren uns für die Vorhersage einer Outcome-Variable \\(Y\\) mit den Regressoren \\(X_1\\) und \\(X_2\\). Abbildung 16.1 zeigt ein mögliches NN mit 3 Neuronen \\(V_1\\), \\(V_2\\), \\(V_3\\) in einem Hidden Layer. Die Neuronen im Hidden Layer empfangen Eingaben aus dem Input Layer, bestehend aus Beobachtungen der Variablen \\(X_1\\) und \\(X_2\\), und gewichten diese Informationen gemäß der Vorschrift\n\\[\\begin{align*}\n  h_i = A\\left(\\sum_{j=1}^{2} w_{ji} \\cdot x_j + b_i\\right) \\quad \\text{für } i = 1, 2, 3.\n\\end{align*}\\]\nHierbei sind \\(w_{ji}\\) die Gewichte der Verbindung von Input \\(j\\) zu Neuron \\(i\\) und \\(b_i\\) ist ein Bias.1 \\(A(\\cdot)\\) ist eine Aktivierungsfunktion, die in Abhängigkeit der zu modellierenden Daten gewählt wird.\nDas Ausgabe-Neuron für \\(Y\\) verarbeitet die Informationen aus dem Hidden Layer ebenfalls anhand einer Linearkombination, die mit einer Aktivierungsfunktion transformiert wird,\n\\[\\begin{align*}\n  y = A\\left(\\sum_{i=1}^{3} w_{i} \\cdot h_i + b_y\\right).\n\\end{align*}\\]\nEin solches NN “lernt” Relationen zwischen \\(Y\\) und den Regressoren \\(X_1\\) und \\(X_2\\), indem die Gewichte anhand eines Algorithmus derart gewählt werden, dass der Fehler zwischen den vorhergesagten und den tatsächlichen Werten von \\(Y\\) — gemessen mit einer Verlustfunktion (Loss-Funktion) — minimiert wird. Dieser Lernprozess erfolgt unter Verwendung numerischer Optimierungsverfahren wie Gradientenabstieg (Gradient Descent).",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Neuronale Netzwerke</span>"
    ]
  },
  {
    "objectID": "Machine Learning.html#sec-nn-basics",
    "href": "Machine Learning.html#sec-nn-basics",
    "title": "\n16  Neuronale Netzwerke\n",
    "section": "",
    "text": "1 Der Bias ist analog zur Konstante in einer Regression.\n\n\n\n16.1.1 Training Neuronaler Netze\nDer Anpassungsprozess eines NN an einen Datensatz (Training) wird grob durch folgende Schritte bestimmt:\n\nDas Netz (Gewichte) wird initialisiert.\nDie Inputs jeder Beobachtung im Trainingsdatensatz werden durch das NN geleitet (Forward Pass): Jedes Layer transformiert die Daten mit Hilfe von Gewichten und Aktivierungsfunktionen, um eine Vorhersage von \\(Y\\) zu erzeugen.\nDer Loss wird berechnet, indem die Vorhersage von \\(Y\\) mit dem tatsächlichen Wert verglichen wird. Die Verlustfunktion wird entsprechend der Definition von \\(Y\\) gewählt. Typische Verlustfunktionen sind Quadratic Loss (analog zur Schätzung von linearen Regressionsmodellen mit KQ) oder Logistic Loss (analog zu logistischer Regression).\n\nZur Anpassung der Gewichte wird der Gradient2 der Verlustfunktion hinsichtlich der Gewichte des NN ermittelt.3 Ein Gradient-Descent-Algorithmus bestimmt, in welche Richtung die Gewichte verändert werden müssen, um den Vorhersagefehler zu verringern.\nFür diese Berechnung wird ein Backward Pass (auch Backpropagation genannt) genutzt. Hierbei wird der anhand des Ausgabelayers ermittelte Loss rückwärts durch das Netzwerk propagiert, um die Gewichte so anzupassen, dass der Fehler bei der Vorhersage von \\(Y\\) minimiert wird.\n\n\nDie Gewichte werden in kleinen Schritten, die durch die so genannte Lernrate bestimmt werden, in Richtung des negativen Gradienten angepasst. Dies bewirkt, dass die Gewichte so verändert werden, dass der Loss im Vergleich zur letzten Iteration verringert wird.\nUm den Lernprozess effizienter und stabiler zu machen, nutzen moderne Algorithmen weitere Schritte, bspw. eine Kombination von Gradientenabstieg mit Momentum. Dies beschleunigt die Anpassung der Gewichte und stabilisiert den Lernprozess. Fortgeschrittene Methoden verwenden adaptiven Lernraten, die die Schrittgröße für jedes Gewicht individuell anpassen können.\n\n\n2 Der Gradient einer Funktion \\(f(\\boldsymbol{x}) = f(x_1, x_2, \\ldots, x_k)\\) ist der Vektor der partiellen Ableitungen: \\(\\nabla f = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_k} \\right)\\). \\(\\nabla f(\\boldsymbol{x})\\) zeigt die Richtung und Stärke der steilsten Änderung von \\(f\\) am Punkt \\(\\boldsymbol{x}\\) an.3 \\(\\nabla f\\) ist in NN grundsätzlich unbekannt. Gradient-Desenct-Algorithmen verwenden numerische Verfahren, um den Gradienten anhand von \\(f\\) zu approximieren.Die Schritte 4 und 5 werden wiederholt, bis ein Abbruchkriterium erfüllt ist: Der Fehler ist ausreichend klein, oder weitere Iterationen bewirken keine signifikante Änderung des Gradienten.\nEpochen und Iterationen\nDer Gesamte Prozess wird für mehrere Epochen (Epocs) durchlaufen, in denen jeweils der gesamte Trainingsdatensatz durch das NN geleitet wird. Um das Training auch für große Datensätze durchführen zu können, werden die Trainingsdaten hierbei üblicherweise in zufällig zusammengesetzen, kleineren Datensätzen (Batches) gruppiert. In jeder Epoche erfolgt die Anpassung der Gewichte für jedes durch das Netz geleitete Batch (jede Iteration):\n\n\nEpoche\n\n\nBatch\nForward Pass \\(\\rightarrow\\) Loss-Berechnung \\(\\rightarrow\\) Backpropagation \\(\\rightarrow\\) Gradient-Descent-Update\n\n\nBatch\nForward Pass \\(\\rightarrow\\) Loss-Berechnung \\(\\rightarrow\\) Backpropagation \\(\\rightarrow\\) Gradient-Descent-Update\n…\n\n\n\n\nEpoche\n     ...\n…\n\n\nFür das Training eines NN sind mehrere Epochen notwendig, weil ein einzelner Durchlauf der Daten oft nicht ausreicht, um die zugrundeliegenden Muster zu lernen. Durch Anpassung über mehrere Epochen können die Gewichte des Modells verfeinert werden, was insbesondere die Fähigkeit zur Generalisierung für ungesehene Daten verbessert. Die zufällige Einteilung der Daten in Batches zu Beginn jeder Epoche verhindert unter anderem, dass das NN lediglich die Reihenfolge der durchgeleiteten Datenpunkte lernt.\nDie Anzahl an zu durchlaufender Epochen ist ein Tuning-Parameter: Zu wenige Epochen führen zu einer schlechten Anpassung an die Daten, während zu viele Epochen das Risiko von Overfitting erhöhen. Um den Vorhersagefehler für ungesehene Daten einzuschätzen, wird ein Testdatensatz vorbehalten. Dieser Datensatz wird während des Trainings nicht zum Anpassen der Gewichte genutzt, sondern erst nach Abschluss einer Epoche für die Berechnung der Vorhersagequalität herangezogen. So kann jeweils nach dem Durchlauf einer Epoche beurteilt werden, wie gut das Modell auf neue, unbekannte Daten generalisiert. Hierbei können ein hoher Vorhersagefehler für den Testdatensatz und ein (viel) geringerer Fehler für den Trainingsdatensatz nach mehreren Epochen auf Overfitting hinweisen. Im empirischen Teil dieses Kapitels diskutieren wir (grafische) Methoden zur Beurteilung der Anpassung des Modells.\nBeim Training von NN können sogenannte Callback-Funktionen eingesetzt werden, um den Anpassungsprozess unter Einbezug von Zwischenergebnissen zu bestimmten Zeitpunkten während des Trainingsprozesses, z. B. am Ende jeder Epoche oder nach einer bestimmten Anzahl von Iterationen, zu evaluieren. Callbacks werden verwendet, um bestimmte Aktionen auszuführen, wie das Anpassen der Lernrate oder das Überwachen der Trainingsleistung: Ein Callback kann das Training automatisch stoppen (Early Stopping), wenn Anzeichen von Overfitting erkannt werden, beispielsweise wenn die Vorhersagegüte auf dem Test-Datensatz über mehrere Epochen hinweg stagniert. Dadurch wird ein unnötiges Fortsetzen des Trainings vermieden und ein Verlust der Generalisierungsfähigkeit auf neuen Daten verhindert.\nWir fassen die wichtigsten Begriffe für die Beschreibung von NN nachfolgend kurz zusammen.\nWesentliche Definitionen\n\nLayer: Eine Ebene von Neuronen im neuronalen Netzwerk. Es gibt das Eingabe-Layer, versteckte Layers (Hidden Layers) und das Ausgabe-Layer. Jedes Layer verarbeitet Informationen aus dem vorangegangenen Layer und gibt die Ergebnisse an das nächste Layer weiter.\nInput: Die Eingangsdaten oder Merkmale, die in das NN eingespeist werden. Jeder Input wird durch ein oder mehrere Neuronen im Eingabe-Layer repräsentiert.\nOutput: Das Ergebnis, welches das NN nach der Verarbeitung der Inputs liefert. Der Output wird durch die Neuronen im Output-Layer des NN erstellt.\nNeuron: Die kleinste Komponente eines NN. Jedes Neuron empfängt Inputs, multipliziert sie mit Gewichten, addiert einen Bias und gibt das Ergebnis nach Anwendung einer Aktivierungsfunktion weiter: Ein Neuron ist also eine mathematische Funktion, die Inputs aus dem vorherigen Layer mit einer transformierten Linearkombination verarbeitet und das Ergebnis das nächste Layers weiterleitet.\nForward Pass: Leitung der Trainingsdaten durch das NN und Berechnung der Vorhersage des Outcomes.\nLoss-Funktion: Mathematische Funktion, welche die Güte der Vorhersage des NN für das Outcome quantifiziert. Der Loss ist eine Funktion der zu trainierenden Parameter des NN.\nBackward Pass / Backpropagation: Ermittlung des Gradienten der Loss-Funktion durch Verkettung des Effekts der Gewichte über die Layers des NN.\n\nAktivierungsfunktion: Eine mathematische Funktion, die auf die gewichtete Summe der Eingaben eines Neurons angewendet wird. Die Aktivierungsfunktion bestimmt, ob ein Neuron aktiviert wird. Beispiele sind\n\\[\\begin{align*}\n    \\textup{ReLU}(z) =& \\max(0, z), \\\\[.5ex]\n    \\sigma(z) =&\\, \\frac{1}{1 + e^{-z}}, \\\\[.5ex]\n    \\tanh(z) =&\\, \\frac{e^z - e^{-z}}{e^z + e^{-z}}.\n  \\end{align*}\\]\n\nEpoche: Ein Trainingszyklus, bei dem der gesamte Trainingsdatensatz, aufgeteilt in Batches, das NN durchläuft.\nBatches: Zufällig eingeteilte Teilmengen der Beobachtungen des Trainingsdatensatzes.\nCallback: Eine Funktion, die im Zuge der Überwachung des des Trainings-Prozesses automatisch ausgeführt wird, um Aktionen wie Lernratenanpassung oder Trainingsstopp zu auszulösen.\n\nIm nächsten Abschnitt erläutern wir die Optimierung der Gewichte mit Gradient Descent beispielhaft anhand interaktiver Visualisierungen.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Neuronale Netzwerke</span>"
    ]
  },
  {
    "objectID": "Machine Learning.html#optimierung-mit-gradient-descent",
    "href": "Machine Learning.html#optimierung-mit-gradient-descent",
    "title": "\n16  Neuronale Netzwerke\n",
    "section": "\n16.2 Optimierung mit Gradient Descent",
    "text": "16.2 Optimierung mit Gradient Descent\nGradient Descent ist ein iteratives Optimierungsverfahren zur Minimierung einer differenzierbaren Zielfunktion \\(f(w)\\). Ausgehend von einem Startwert \\(w_0\\) aktualisiert der Algorithmus die Variable \\(w\\) schrittweise gemäß einer Lernrate \\(\\eta\\) in die entgegengesetzte Richtung des Gradienten \\(\\nabla f(w)\\) der Funktion an der aktuellen \\(w\\). Mit \\(\\nabla f(w)\\) wird mathematisch die Richtung des steilsten Anstiegs von \\(f(w)\\) im Punkt \\(w\\) ermittelt. Der Algorithmus vollzieht eine Veränderung von \\(w\\) in die entgegengesetzten Richtung – die Richtung mit dem schnellsten Abstieg (Descent) der Zielfunktion.\nDer folgende Algorithmus zeigt die grundlegende Vorgehensweise des Gradientenabstiegsverfahrens für einen einziegen zu optimierenden Parameter \\(w\\) unter Einbeziehung eines Momentum-Terms \\(v_t\\).4 Der Momentum-Term dient dazu, das Konvergenzverhalten zu verbessern und lokale Minima effektiver zu überwinden. Die Stärke des Momentums \\(v_t\\) wird durch den Momentum-Faktor \\(\\alpha \\in [0,1)\\) bestimmt.\n4 In der Literatur wird \\(v_t\\) häufig auch als Velocity bezeichnet.\\[\\begin{align*}\n  \\small\n  & \\textbf{Algorithmus: Gradientenabstiegsverfahren mit Momentum} \\\\\n  & \\textup{Initialisiere: }\\\\[.5ex]\n  & \\quad w_0 \\text{ (Startpunkt) }\\\\\n  & \\quad \\eta \\text{ (Lernrate) }\\\\\n  & \\quad \\alpha \\text{ (Momentum-Faktor) }\\\\\n  & \\quad v_0 = 0 \\text{ (Anfangsmomentum) } \\\\[1em]\n  & \\text{Iteriere für } t = 0, 1, 2, \\dots \\text{ bis Konvergenz:} \\\\[.5ex]\n  & \\quad \\text{1. Berechne den Gradienten: } \\nabla f(w_t) \\\\\n  & \\quad \\text{2. Aktualisiere den Momentum-Term: } v_{t+1} = \\alpha v_t - \\eta \\nabla f(w_t) \\\\\n  & \\quad \\text{3. Aktualisiere die Position: } w_{t+1} = w_t + v_{t+1} \\\\\n  & \\quad \\text{4. Überprüfe das Abbruchkriterium } |\\nabla f(w_t)| &lt; \\epsilon\\text{ (für ein kleines $\\epsilon&gt;0$)} \\\\\n\\end{align*}\\]\nIn der nachfolgenden interaktiven Visualisierung illustrieren wir die Minimierung einer univariaten Funktion \\(\\color{blue}{f(w_t)}\\) über \\(w_t\\) anhand des obigen Algorithmus mit Lernrate \\(\\eta = .001\\) und Momentum-Faktor \\(\\alpha = .925\\).\nDer Gradient\\(\\color{orange}{\\nabla f(w_t)}\\) ist hier die 1. Ableitung von \\(\\color{blue}{f(w_t)}\\) nach \\(w_t\\). Die Richtung der Änderung von \\(\\color{blue}{f(w_t)}\\) in \\(w_t\\) wird durch den orangenen Pfeil angezeigt. Beachte, wie sich der Gradient bei Variation des Start-Punkts mit dem Slider ändert. Während die Animation der Optimierung mit Gradient Descent läuft, zeigt der lilane Pfeil das Momentum (Velocity \\(v_t\\)) für Schirtt \\(t\\) an.5 Der Algorithmus iteriert die Schritte 1. bis 3. solange, bis das Abbruchkriterium \\(|\\textcolor{orange}{\\nabla f(w_t)}| &lt; \\epsilon = 0.001\\) erreicht ist, die Änderung in \\(\\color{orange}{\\nabla f(w_t)}\\) also hinreichend klein ist, dass ein Parameterwert \\(w_t\\) mit \\(\\color{blue}{f(w_t)}\\) nahe des (globalen) Minimums von \\(f\\) plausibel ist.\n5 Unterschiedliche Längen der Pfeile zeigen hier nicht Änderungen der tatsächlichen Beträge, sondern dienen lediglich der Interpretierbakeit der Grafik.Folgende Eigenschaften der Optimierung mit Gradient Descent können anhand der Parameter geprüft werden:\n\n\nFür Startpunkte mit großen Werten des Gradienten beginnt der Algorithmus mit einem starken Momentum: Der Abstieg in Richtung des negativen Gradients erfolgt also in großen Schritten, sodass die Optimierung schneller erfolgt als für Startpunkte in flachen Regionen von \\(\\color{blue}{f}\\).\nDieser Effekt des Momentum auf den Pfad der zu optimierenden Parameter bei Gradient Descent ist vergleichbar mit dem Effekt der Schwerkraft auf eine Murmel, die auf einer hügeligen Oberfläche rollt: Anfangs gewinnt die Murmel an Geschwindigkeit und bewegt sich beschleunigt in Richtung des steilsten Gefälles. In flacheren Regionen wird die Bewegung langsamer und die Murmel kann in Tälern stecken bleiben, ähnlich wie der Optimierungsprozess in flachen Regionen von \\(\\color{blue}{f}\\) langsamer verläuft oder gar stoppt, weil ein Abbruchkriterium erfüllt ist (geringe Änderung des Gradienten). Das Momentum hilft, auch in solchen flachen Bereichen weiter voranzukommen, indem es dem Parameterpfad eine gewisse “Trägheit” verleiht, die es ermöglicht, flache Stellen schneller zu durchqueren und die Optimierung effizienter zu gestalten.\n\nBei ungünstiger Wahl der Parameter konvergiert der Algorithmus nicht zum globalen Minimum, sondern stoppt im lokalen Minimum bei \\(w = -0.5\\). Dies unterstreicht die Notwendigkeit, die Hyperparameter Lernrate \\(\\eta\\) und Momentum-Faktor \\(\\alpha\\) sorgfältig zu wählen, beispielsweise indem die Modellgüte nach erfolgter Anpassung für verschiedene Parameter-Kombinationen verglichen wird.\n\nIn empirischen Anwendungen ist es für eine hohe Modellgüte eines neuronalen Netzwerks nicht unbedingt erforderlich, das globale Minimum zu finden: Viele Optimierungsprobleme weisen zahlreiche lokale Minima auf, die eine ausreichend gute Annäherung an das Optimum bieten können. Besonders bei hochdimensionalen Optimierungsproblemen mit komplexen Loss-Funktionen können diese lokalen Minima zufriedenstellende Lösungen darstellen. In einigen Fällen existiert möglicherweise kein globales Minimum, und der Algorithmus konvergiert zwangsläufig zu einem stabilen lokalen Minimum, das dennoch eine gute Performance gewährleistet. Daher kann es sinnvoller sein, Algorithmen zu verwenden, die das Erreichen einer robusten Lösung legen, anstatt strikt nach dem globalen Minimum zu suchen.\nIn Software-Implementierungen für Machine und Deep Learning wie tensorflow und keras werden fortgeschrittene Techniken wie Momentum Tuning oder Stochastic Gradient Descent (SGD) eingesetzt, um die Wahrscheinlichkeit zu erhöhen, dass der Algorithmus nicht in einem (ungünstigen) lokalen Minimum endet. Ein für die Anpassung von NN häufig verwendeter Algorithmus, der SGD verwendet, ist Adaptive Moment Estimation (Adam). Wir verwenden u.a. den Adam-Optimizer in den empirischen Beispielen.\n\n\nIn empirischen Anwendungen sind die zu lernenden Zusammenhänge komplex und damit die Anzahl der zu optimierenden Parameter eines NN häufig groß. Der oben erläuterte Algorithmus für Gradient Descent mit Momentum kann einfach auf Optimierungsprobleme mit \\(k\\) Parametern generalisiert werden. Dann ist \\(\\boldsymbol{w}_t\\) ein Vektor mit \\(k\\) Gewichten, \\(\\boldsymbol{v}_{t+1}\\) eine vektorwertige Funktion von \\(\\boldsymbol{v}_t\\) und \\(\\nabla f(\\boldsymbol{w}_t)\\) mit Dimension \\(k\\) und \\(f(\\boldsymbol{w}_t)\\) ist eine Oberfläche in einem \\(k+1\\)-dimensionalen Raum.\nDie nachfolgende interaktive Grafik illustriert Gradient Descent mit Momentum für \\(k=2\\) zu optimierende Gewichte. Statt der Parameter des Algorithmus kann hier die Form der zu optimierenden Funktion manipuliert werden, sodass bis zu 6 Extremstellen vorliegen können. Der rote Punkt zeigt den Verlauf der Optimierung von \\(\\boldsymbol{w}_t\\).\nDie Animation verdeutlicht, dass lokale Minima insbesondere in höheren Dimensionen herausfordernd für Optimierungsalgorithmen sind: Durch Variation der Extrema lassen sich leicht Funktionen \\(f(w_1,w_1)\\) konstruieren, für die Gradient Descent mit den voreingestellten Parametern nicht gegen das globale Minimum konvergiert, sofern vorhanden. Ein günstiger Initialwert für \\(\\boldsymbol{w}_t\\) kann die Wahrscheinlichkeit von Stops in lokalen Minima verringern: Grid Search Initialization wertet die Funktion über ein gleichmäßiges Gitter von Werten für \\(\\boldsymbol{w}_t\\) aus und wählt als Startwert \\(\\boldsymbol{w}_{0,\\textup{init}}\\) den Punkt mit dem minimalen Funktionswert von \\(f\\) über alle Punkte im Gitter.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Neuronale Netzwerke</span>"
    ]
  },
  {
    "objectID": "Machine Learning.html#funktionale-zusammenhänge-lernen-regression",
    "href": "Machine Learning.html#funktionale-zusammenhänge-lernen-regression",
    "title": "\n16  Neuronale Netzwerke\n",
    "section": "\n16.3 Funktionale Zusammenhänge lernen: Regression",
    "text": "16.3 Funktionale Zusammenhänge lernen: Regression\nFür einen leichten Einstieg in die Modellierung funktionaler Zusammenhänge durch NN mit statistischer Programmierung in R betrachten wir zunächst den einfachsten Zusammenhang zwischen einer Outcome-Variable \\(Y\\) und einem Regressor \\(X\\): Die einfache lineare Funktion \\[\\begin{align*}\n  Y = w_1 X + b,\n\\end{align*}\\] wobei der Regressionskoeffizient \\(w_1\\) den Einfluss von \\(X\\) auf \\(Y\\) misst und \\(b\\) eine Konstante ist. Gemäß der Definitionen in Kapitel 16.1 kann dieser Funktionale Zusammenhang als NN ohne Hidden Layer dargestellt werden, wobei \\(X\\) ein Input-Neuron ist, dessen Information mit \\(w_1\\) gewichtet an das Output Layer mit einem einzigen Neuron für \\(Y\\) weitergegeben wird. Die Konstante \\(b\\) ist ein Bias, der als von \\(X\\) unabhängiger Einfluss von \\(Y\\) behandelt wird, vgl. Abbildung 16.2.\n\n\n\n\n\nNEURALNET\nX\nXY\nYX-&gt;Y\nw1B\n1B-&gt;Y\nBias (b)\n\n\n\nAbbildung 16.2: Neuronales Netzwerk: Lineare Regression mit einer Variable und Konstante\n\n\n\n\nFür die Illustration der Schätzung des in Kapitel 16.1 dargestellten NN verwenden wir \\(n=1000\\) simulierte Datenpunkte gemäß der Vorschrift \\[\\begin{align}\n  Y = 5 + 3 \\cdot X + u\n\\end{align}\\] mit \\(X\\sim U[0,10]\\) und \\(u\\sim N(0,1)\\).\n\n# Daten simulieren\nset.seed(1234)\n\nn &lt;- 1000\nx &lt;- runif(n, min = 0, max = 10)\ny &lt;- 5 + 3 * x + rnorm(n) \n\nFür das Training von NN verwenden wir das Python-Paket keras. Hierzu muss lediglich eine lokale Python-Installation vorhanden sein.\nDie in diesem Kapitel betrachteten NN sind sequentielle NN. Solche Modelle können in keras mit der Funktion keras_model_sequential() definiert werden. Die Struktur des Modells kann über eine Verkettung von Funktionen für Layers (keras::layer_dense()) und Aktivierungen (keras::layer_activation()) definiert werden.\nFür die Implementierung des Modells in Abbildung 16.2 wählen wir mit units = 1 und input_shape = 1 ein Modell mit einem Neuron im Output Layer, das skalare Informationen verarbeitet. activation = 'linear' in layer_dense() führt zu der Aktivierungsfunktion \\(A(x) = x\\), d.h. die Ausgabe des Input Layers ist die gewichtete Summe der Eingaben plus Bias, ohne eine zusätzliche Transformation.\n\nlibrary(dplyr)\nlibrary(keras)\n\n# NN für einfache Regression\nmodel &lt;- keras_model_sequential() %&gt;%\n  layer_dense(\n    units = 1, \n    input_shape = 1, \n    activation = 'linear'\n    )\n\n# Modell-Definition prüfen\nmodel\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense (Dense)                      (None, 1)                       2           \n================================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nDie Übersicht zeigt, dass model aus einem Layer für skalare Inputs und Outputs sowie zwei trainierbaren Parameters (\\(w_1\\) und \\(b\\)) besteht.\nBevor das im Objekt model definierte Modell trainiert werden kann, muss der Code kompiliert werden. Dieser Vorgang ist notwendig, da sämtliche Berechnungen in Python durchgeführt werden. Der Python-Code wird beim kompilieren in eine Zwischendarstellung (Bytecode) übersetzt, die dann von der Python-Interpreter-Laufzeitumgebung ausgeführt wird.6\n6 Im Gegensatz zu Python ist R eine interpretierte Programmiersprache. Kompilierung von R-Ccode ist daher nicht notwendig.Mit keras::compile() kompilieren wir das Modell und wählen als Optimierungsfunktion Adam mit einer Lernrate von \\(.01\\). Die Loss-Funktion wird über das Argument loss festgelegt, hier der mittlere absolute Fehler, \\[\\begin{align*}\n  \\textup{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\lvert y_i - \\widehat{y}_i\\rvert.\n\\end{align*}\\]\n\n# Modell kompilieren\nmodel %&gt;% \n  compile(\n    optimizer = optimizer_adam(learning_rate = 0.01),\n    loss = 'mean_absolute_error'\n)\n\nDie Kompilierung erfolgt meist innerhalb von Sekundenbruchteilen und geschieht in-place: Eine Zuweisung des kompilierten Modells in model ist nicht notwendig.\nUm das Modell zu trainieren verwenden wir keras::fit(). Neben den (simulierten) Daten übergeben wir die Anzahl der zudurchlaufenden Epochen epocs. Über das Argument validation_split legen wir fest, dass 20% der Datensatzes zufällig ausgewählt und als Test-Datensatz für die Modell-Validierung während des Trainings genutzt werden sollen.\n\n# Modell trainieren\nhistory_snn &lt;- model %&gt;% \n  fit(\n    x = x, \n    y = y, \n    epochs = 50, \n    validation_split = .2\n)\n\nEpoch 1/50\n25/25 - 0s - loss: 17.4493 - val_loss: 16.3791 - 419ms/epoch - 17ms/step\nEpoch 2/50\n25/25 - 0s - loss: 15.9267 - val_loss: 14.8964 - 97ms/epoch - 4ms/step\nEpoch 3/50\n25/25 - 0s - loss: 14.4136 - val_loss: 13.4081 - 88ms/epoch - 4ms/step\nEpoch 4/50\n25/25 - 0s - loss: 12.8847 - val_loss: 11.9392 - 85ms/epoch - 3ms/step\nEpoch 5/50\n25/25 - 0s - loss: 11.3714 - val_loss: 10.4542 - 84ms/epoch - 3ms/step\nEpoch 6/50\n25/25 - 0s - loss: 9.8430 - val_loss: 8.9833 - 87ms/epoch - 3ms/step\nEpoch 7/50\n25/25 - 0s - loss: 8.3326 - val_loss: 7.4892 - 86ms/epoch - 3ms/step\nEpoch 8/50\n25/25 - 0s - loss: 6.8056 - val_loss: 6.0124 - 87ms/epoch - 3ms/step\nEpoch 9/50\n25/25 - 0s - loss: 5.2863 - val_loss: 4.5325 - 85ms/epoch - 3ms/step\nEpoch 10/50\n25/25 - 0s - loss: 3.7661 - val_loss: 3.0508 - 86ms/epoch - 3ms/step\nEpoch 11/50\n25/25 - 0s - loss: 2.2770 - val_loss: 1.6876 - 86ms/epoch - 3ms/step\nEpoch 12/50\n25/25 - 0s - loss: 1.2858 - val_loss: 1.2269 - 86ms/epoch - 3ms/step\nEpoch 13/50\n25/25 - 0s - loss: 1.1312 - val_loss: 1.1798 - 87ms/epoch - 3ms/step\nEpoch 14/50\n25/25 - 0s - loss: 1.1029 - val_loss: 1.1570 - 85ms/epoch - 3ms/step\nEpoch 15/50\n25/25 - 0s - loss: 1.0803 - val_loss: 1.1328 - 87ms/epoch - 3ms/step\nEpoch 16/50\n25/25 - 0s - loss: 1.0584 - val_loss: 1.1086 - 87ms/epoch - 3ms/step\nEpoch 17/50\n25/25 - 0s - loss: 1.0368 - val_loss: 1.0852 - 85ms/epoch - 3ms/step\nEpoch 18/50\n25/25 - 0s - loss: 1.0149 - val_loss: 1.0538 - 86ms/epoch - 3ms/step\nEpoch 19/50\n25/25 - 0s - loss: 0.9948 - val_loss: 1.0335 - 85ms/epoch - 3ms/step\nEpoch 20/50\n25/25 - 0s - loss: 0.9749 - val_loss: 1.0075 - 91ms/epoch - 4ms/step\nEpoch 21/50\n25/25 - 0s - loss: 0.9560 - val_loss: 0.9841 - 87ms/epoch - 3ms/step\nEpoch 22/50\n25/25 - 0s - loss: 0.9385 - val_loss: 0.9590 - 87ms/epoch - 3ms/step\nEpoch 23/50\n25/25 - 0s - loss: 0.9218 - val_loss: 0.9400 - 86ms/epoch - 3ms/step\nEpoch 24/50\n25/25 - 0s - loss: 0.9068 - val_loss: 0.9180 - 86ms/epoch - 3ms/step\nEpoch 25/50\n25/25 - 0s - loss: 0.8907 - val_loss: 0.8966 - 86ms/epoch - 3ms/step\nEpoch 26/50\n25/25 - 0s - loss: 0.8770 - val_loss: 0.8782 - 86ms/epoch - 3ms/step\nEpoch 27/50\n25/25 - 0s - loss: 0.8628 - val_loss: 0.8619 - 87ms/epoch - 3ms/step\nEpoch 28/50\n25/25 - 0s - loss: 0.8499 - val_loss: 0.8433 - 88ms/epoch - 4ms/step\nEpoch 29/50\n25/25 - 0s - loss: 0.8394 - val_loss: 0.8217 - 86ms/epoch - 3ms/step\nEpoch 30/50\n25/25 - 0s - loss: 0.8271 - val_loss: 0.8102 - 86ms/epoch - 3ms/step\nEpoch 31/50\n25/25 - 0s - loss: 0.8171 - val_loss: 0.7957 - 85ms/epoch - 3ms/step\nEpoch 32/50\n25/25 - 0s - loss: 0.8089 - val_loss: 0.7853 - 85ms/epoch - 3ms/step\nEpoch 33/50\n25/25 - 0s - loss: 0.8012 - val_loss: 0.7771 - 93ms/epoch - 4ms/step\nEpoch 34/50\n25/25 - 0s - loss: 0.7951 - val_loss: 0.7688 - 86ms/epoch - 3ms/step\nEpoch 35/50\n25/25 - 0s - loss: 0.7898 - val_loss: 0.7614 - 86ms/epoch - 3ms/step\nEpoch 36/50\n25/25 - 0s - loss: 0.7836 - val_loss: 0.7571 - 86ms/epoch - 3ms/step\nEpoch 37/50\n25/25 - 0s - loss: 0.7819 - val_loss: 0.7475 - 85ms/epoch - 3ms/step\nEpoch 38/50\n25/25 - 0s - loss: 0.7765 - val_loss: 0.7458 - 85ms/epoch - 3ms/step\nEpoch 39/50\n25/25 - 0s - loss: 0.7735 - val_loss: 0.7404 - 83ms/epoch - 3ms/step\nEpoch 40/50\n25/25 - 0s - loss: 0.7698 - val_loss: 0.7366 - 86ms/epoch - 3ms/step\nEpoch 41/50\n25/25 - 0s - loss: 0.7674 - val_loss: 0.7324 - 85ms/epoch - 3ms/step\nEpoch 42/50\n25/25 - 0s - loss: 0.7662 - val_loss: 0.7292 - 86ms/epoch - 3ms/step\nEpoch 43/50\n25/25 - 0s - loss: 0.7660 - val_loss: 0.7281 - 86ms/epoch - 3ms/step\nEpoch 44/50\n25/25 - 0s - loss: 0.7663 - val_loss: 0.7240 - 86ms/epoch - 3ms/step\nEpoch 45/50\n25/25 - 0s - loss: 0.7650 - val_loss: 0.7230 - 86ms/epoch - 3ms/step\nEpoch 46/50\n25/25 - 0s - loss: 0.7627 - val_loss: 0.7214 - 85ms/epoch - 3ms/step\nEpoch 47/50\n25/25 - 0s - loss: 0.7616 - val_loss: 0.7172 - 85ms/epoch - 3ms/step\nEpoch 48/50\n25/25 - 0s - loss: 0.7613 - val_loss: 0.7164 - 86ms/epoch - 3ms/step\nEpoch 49/50\n25/25 - 0s - loss: 0.7616 - val_loss: 0.7191 - 86ms/epoch - 3ms/step\nEpoch 50/50\n25/25 - 0s - loss: 0.7601 - val_loss: 0.7155 - 86ms/epoch - 3ms/step\n\n\nDer Output zeigt die Enwicklung des Loss (MAE) für Vorhersagen des Trainingsdatensatzes (loss) und für den Test-Datensatz (val_loss) für alle 25 Epochen. Diese Informationen können mit plot() einfach visualisiert werden.\n\nlibrary(ggplot2)\nlibrary(cowplot)\nlibrary(purrr)\n\n# Entwicklung des Loss über Epochen plotten\nplot(history_snn) +\n  labs(\n    x = \"Epoche\",\n    y = \"Wert der Verlustfunktion\"\n  ) +\n  theme_cowplot() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nAbbildung 16.3: Einfaches lineares NN: Entwicklung des Loss für 25 Epochen\n\n\n\n\nAbbildung 16.3 zeigt, dass sich sowohl die Anpassung des NN auf dem Trainingsdatenstz als auch die Generalisierung auf dem Testdatensatz innerhalb der ersten Epochen dramatisch verbessert. Jenseits der 15. Epoche hingegen bewirken weitere Trainingszyklen keine weitere Verbesserung des Loss.\nMit keras::get_weights() können wir die optimierten Parameter aus dem Modell-Objekt auslesen.\n\n# Gewichtung und Bias des trainierten NN auslesen\nmodel %&gt;% \n  keras::get_weights() %&gt;% \n  flatten_dbl() %&gt;% \n  set_names(\n    c(\"w_1\", \"bias\")\n  )\n\n     w_1     bias \n3.008101 4.924770 \n\n\nDas NN hat den funktionalen Zusammengang zwischen x und y erfolgreich gelernt: Die optimierten Parameter-Werte bias und w_1 liegen nahe der wahren Parameter. Bei Parameter sind mit ihren KQ-Schätzungen vergleichbar.7\n7 Beachte, dass die KQ-Schätzung der Einfachheit halber hier den gesamten Datensatz nutzt und daher präziser sein kann als das NN.\n# lineares Modell\nlm_model &lt;- lm(\n  formula = y ~ x\n  )\n\nsummary(lm_model)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91933 -0.62956  0.01084  0.63819  2.73178 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.04449    0.06028   83.69   &lt;2e-16 ***\nx            2.98893    0.01031  290.01   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9486 on 998 degrees of freedom\nMultiple R-squared:  0.9883,    Adjusted R-squared:  0.9883 \nF-statistic: 8.411e+04 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\n\n# Koeffizienten der KQ-Schätzung auslesen\ncoef(lm_model)\n\n(Intercept)           x \n   5.044491    2.988928 \n\n\nMit predict() erhalten wir Vorhersagen des NN und können so beispielsweise die Residuen für den gesamten Datensatz mit denen der KQ-Schätzung vergleichen.\n\n# Residuen vergleichen\n tibble(\n   NN = y - model %&gt;% predict(x),\n   lm = lm_model$residuals\n ) %&gt;%\n  \n  ggplot(mapping = aes(x = NN, y = lm)) +\n  geom_point(alpha = .5, color = \"steelblue\") +\n  theme_cowplot()\n\n32/32 - 0s - 72ms/epoch - 2ms/step\n\n\n\n\n\n\n\nAbbildung 16.4: Vergleich von Residuen für NN und KQ-Schätzung\n\n\n\n\nAbbildung 16.4 zeigt eine gute Korrespondenz zwischen der Anpassung des NN und der KQ-Schätzung des einfachen linearen Modells.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Neuronale Netzwerke</span>"
    ]
  },
  {
    "objectID": "Machine Learning.html#multiple-regression",
    "href": "Machine Learning.html#multiple-regression",
    "title": "\n16  Neuronale Netzwerke\n",
    "section": "\n16.4 Multiple Regression",
    "text": "16.4 Multiple Regression\nEin neuronales Netz für multiple Regression kann als eine Erweiterung des Netzes für einfache Regression betrachtet werden. Das Netz enthält nun mehrere Input-Neuronen, von denen jedes eine der unabhängigen Variablen \\(X_1, X_2, \\dots, X_k\\) repräsentiert. Diese Input-Neuronen sind mit einem einzigen Output-Neuron verbunden, das die Vorhersage für \\(Y\\) liefert. Jede dieser Verbindungen wird mit einem Gewicht \\(w_i\\) multipliziert, das die Stärke des Einflusses der jeweiligen unabhängigen Variable \\(X_i\\) auf die abhängige Variable \\(Y\\) repräsentiert. Wie im einfachen Modell gibt es einen Bias-Term \\(b\\), der ähnlich wie in der einen konstanten Einfluss darstellt.\nDie Struktur eines NN für multiple Regression ist in Abbildung 16.5 dargestellt. In diesem Beispiel gibt es drei unabhängige Variablen \\(X_1\\), \\(X_2\\) und \\(X_3\\), die jeweils ein eigenes Input-Neuron haben und mit dem Output-Neuron \\(Y\\) verbunden sind. \\(Y\\) ist eine Linear-Kombination der Inputs, gewichtet mit den jeweiligen Gewichten \\(w_1\\), \\(w_2\\) und \\(w_3\\), sowie dem Bias \\(b\\).\n\n\n\n\n\nNEURALNET\nX1\nX1SUM\n∑X1-&gt;SUM\nw1X2\nX2X2-&gt;SUM\nw2X3\nX3X3-&gt;SUM\nw3Y\nYSUM-&gt;Y\nB\n1B-&gt;SUM\nb\n\n\n\nAbbildung 16.5: Neuronales Netzwerk: Multiple lineare Regression\n\n\n\n\nUm die Vorgehensweise in R zu zeigen, generieren wir zunächst \\(n=250\\) Datenpunkte gemäß der Vorschrift \\[\\begin{align}\n  Y = 5 + 3 \\cdot X_1 + 2\\cdot X_2 - 1.5 \\cdot X_k + u\n\\end{align}\\] mit \\(X_1,X_2,X_3 \\sim\\textup{u.i.v.} N(0, 1)\\) und \\(u\\sim N(0,1)\\).\n\n# Erstellen von Trainingsdaten\nset.seed(42)\n\nn &lt;- 250\nk &lt;- 3\n\nX &lt;- matrix(rnorm(n = n * k), ncol = k)\nw &lt;- c(3, 2, -1.5)\n\nY &lt;- 5 + X %*% w + rnorm(n)\n\nAnschließend definieren wir ein einfaches NN und fügen ein Layer hinzu. Da wir eine multiple Regression durchführen, wählen wir input_shape = k, wobei k die Anzahl der unabhängigen Variablen ist. Wie im einfachen Modell ist die Aktivierungsfunktion linear, da wir an der Anpassung von \\(Y\\) mit einer linearen Kombination der Inputs interessiert sind.\n\n# Erstellen und Kompilieren des Modells\nmodel &lt;- keras_model_sequential() %&gt;%\n  layer_dense(\n    units = 1, \n    input_shape = k, \n    activation = 'linear'\n  )\n\n# Modelldefinition prüfen\nmodel\n\nModel: \"sequential_1\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_1 (Dense)                    (None, 1)                       4           \n================================================================================\nTotal params: 4\nTrainable params: 4\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nWir kompilieren das Modell mit dem mittleren quadratischen Fehler (mean squared error, MSE) und SGD als Loss-Funktion mit einer moderaten Lernrate.\n\nmodel %&gt;% \n  compile(\n    loss = 'mean_squared_error',\n    optimizer = optimizer_sgd(learning_rate = 0.01)\n  )\n\nDie Anpassung des Modells erfolgt wie bei einfacher Regression mit keras::fit().\n\n# Training des Modells\nhistory_mnn &lt;- model %&gt;% \n  fit(\n    x = X, \n    y = Y, \n    validation_split = .2,\n    epochs = 25\n  )\n\nEpoch 1/25\n7/7 - 0s - loss: 38.3905 - val_loss: 32.7356 - 307ms/epoch - 44ms/step\nEpoch 2/25\n7/7 - 0s - loss: 29.3786 - val_loss: 25.1954 - 36ms/epoch - 5ms/step\nEpoch 3/25\n7/7 - 0s - loss: 22.5195 - val_loss: 19.3879 - 35ms/epoch - 5ms/step\nEpoch 4/25\n7/7 - 0s - loss: 17.3129 - val_loss: 15.1332 - 34ms/epoch - 5ms/step\nEpoch 5/25\n7/7 - 0s - loss: 13.4103 - val_loss: 11.8057 - 35ms/epoch - 5ms/step\nEpoch 6/25\n7/7 - 0s - loss: 10.3898 - val_loss: 9.1894 - 35ms/epoch - 5ms/step\nEpoch 7/25\n7/7 - 0s - loss: 8.0201 - val_loss: 7.2339 - 34ms/epoch - 5ms/step\nEpoch 8/25\n7/7 - 0s - loss: 6.2253 - val_loss: 5.8595 - 35ms/epoch - 5ms/step\nEpoch 9/25\n7/7 - 0s - loss: 4.9764 - val_loss: 4.8528 - 35ms/epoch - 5ms/step\nEpoch 10/25\n7/7 - 0s - loss: 4.0437 - val_loss: 4.0504 - 34ms/epoch - 5ms/step\nEpoch 11/25\n7/7 - 0s - loss: 3.3104 - val_loss: 3.4789 - 34ms/epoch - 5ms/step\nEpoch 12/25\n7/7 - 0s - loss: 2.7748 - val_loss: 2.9960 - 34ms/epoch - 5ms/step\nEpoch 13/25\n7/7 - 0s - loss: 2.3315 - val_loss: 2.6592 - 35ms/epoch - 5ms/step\nEpoch 14/25\n7/7 - 0s - loss: 2.0093 - val_loss: 2.3985 - 35ms/epoch - 5ms/step\nEpoch 15/25\n7/7 - 0s - loss: 1.7653 - val_loss: 2.2088 - 35ms/epoch - 5ms/step\nEpoch 16/25\n7/7 - 0s - loss: 1.5860 - val_loss: 2.0859 - 37ms/epoch - 5ms/step\nEpoch 17/25\n7/7 - 0s - loss: 1.4453 - val_loss: 1.9906 - 34ms/epoch - 5ms/step\nEpoch 18/25\n7/7 - 0s - loss: 1.3416 - val_loss: 1.9086 - 33ms/epoch - 5ms/step\nEpoch 19/25\n7/7 - 0s - loss: 1.2629 - val_loss: 1.8457 - 37ms/epoch - 5ms/step\nEpoch 20/25\n7/7 - 0s - loss: 1.2032 - val_loss: 1.8069 - 34ms/epoch - 5ms/step\nEpoch 21/25\n7/7 - 0s - loss: 1.1609 - val_loss: 1.7969 - 35ms/epoch - 5ms/step\nEpoch 22/25\n7/7 - 0s - loss: 1.1269 - val_loss: 1.7767 - 35ms/epoch - 5ms/step\nEpoch 23/25\n7/7 - 0s - loss: 1.1030 - val_loss: 1.7572 - 34ms/epoch - 5ms/step\nEpoch 24/25\n7/7 - 0s - loss: 1.0870 - val_loss: 1.7448 - 34ms/epoch - 5ms/step\nEpoch 25/25\n7/7 - 0s - loss: 1.0700 - val_loss: 1.7339 - 34ms/epoch - 5ms/step\n\n\n\n# Entwicklung des Loss über Epochen plotten\nplot(history_mnn) +\n  labs(\n    x = \"Epoche\",\n    y = \"Wert der Verlustfunktion\"\n  ) +\n  theme_cowplot() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nAbbildung 16.6: NN für mult. Regression: Entwicklung des Loss für 25 Epochen\n\n\n\n\nWie bei der einfachen Regression zeigt ein Vergleich der angepassten Gewichte mit den KQ-Schätzungen eines entsprechenden linearen Regressionsmodells ähnliche Ergebnisse beider Ansätze.\n\n# Gewichtung und Bias des trainierten NN auslesen\nmodel %&gt;% \n  keras::get_weights() %&gt;% \n  flatten_dbl() %&gt;% \n  set_names(\n    c(\"w_1\", \"w_2\", \"w_3\", \"bias\")\n  )\n\n      w_1       w_2       w_3      bias \n 2.946030  1.925088 -1.457486  4.794597 \n\n\n\n# Multiples lineares Modell mit KQ schätzen\nlm_model &lt;- lm(\n  formula = Y ~ X\n)\n\nsummary(lm_model)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3129 -0.7286  0.0900  0.7439  3.4086 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.01583    0.06843   73.30   &lt;2e-16 ***\nX1           2.97449    0.07028   42.32   &lt;2e-16 ***\nX2           1.94345    0.07061   27.52   &lt;2e-16 ***\nX3          -1.47278    0.06914  -21.30   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.079 on 246 degrees of freedom\nMultiple R-squared:  0.9271,    Adjusted R-squared:  0.9262 \nF-statistic:  1042 on 3 and 246 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Neuronale Netzwerke</span>"
    ]
  },
  {
    "objectID": "Machine Learning.html#nicht-lineare-zusammenhänge",
    "href": "Machine Learning.html#nicht-lineare-zusammenhänge",
    "title": "\n16  Neuronale Netzwerke\n",
    "section": "\n16.5 Nicht-Lineare Zusammenhänge",
    "text": "16.5 Nicht-Lineare Zusammenhänge\nIn diesem Abschnitt verwenden trainieren wir ein NN, um eine logistische Regression durchzuführen. Dieser Ansatz wird häufig verwendet, um eine binäre Outcome-Variablen \\(Y\\) zu modellieren, also Variablen, die zwei mögliche Ausgänge haben (oft als 0 oder 1 dargestellt), siehe Kapitel 4.2.3 für Details. Anstatt die Eingaben lediglich linear zu kombinieren, verwenden wir eine Sigmoid-Aktivierungsfunktion8, \\[\\begin{align*}\n  \\sigma(z) = \\frac{1}{1 + \\exp(-z)},\n\\end{align*}\\] welche die Ausgaben auf einen Wertebereich zwischen 0 und 1 abbildet. Dadurch kann das NN Wahrscheinlichkeiten \\(P(Y=1\\vert \\boldsymbol{X} = \\boldsymbol{x})\\) vorhersagen, die anschließend für die Klassifikation von Beobachtungen verwendet werden können.\n8 Die Sigmoid-Aktivierungsfunktion entspricht der logistischen Funktion \\(\\Lambda(z)\\) aus Kapitel 4.2.3.Für die Illustration der Schätzung mit keras verwenden wir den DGP aus Kapitel 4.2.2.\n\n# Erstellen von Trainingsdaten\nset.seed(1234)\n\nn &lt;- 500\nX &lt;- rnorm(n = n, mean = 5, sd = 2) # Regressor\nP &lt;- pnorm(-4 + 0.7 * X)\nY &lt;- as.integer(runif(n) &lt; P)\n\nAbbildung 16.7 zeigt ein einfaches NN für eine binäre Outcome-Variable.\n\n\n\n\n\nNNlogit\nX\nX1SUM\n∑X-&gt;SUM\nw1sigmoid\nσSUM-&gt;sigmoid\nY\nYsigmoid-&gt;Y\nB\n1B-&gt;SUM\nb\n\n\n\nAbbildung 16.7: Neuronales Netzwerk mit Sigmoid-Aktivierungsfunktion\n\n\n\n\nNach der Definition des NN wird das Modell mit dem Binary-Cross-Entropy-Loss (BCEL) und dem Adam-Optimierer kompiliert. BCEL ist für binäre Klassifikationsprobleme geeignet: Diese Loss-Funktion misst die die Unterschiede zwischen den vorhergesagten Wahrscheinlichkeiten \\(\\widehat{p}_i\\) und den tatsächlichen binären Zielen \\(y_i\\), \\[\\begin{align*}\n  \\textup{BCEL} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\cdot \\log(\\widehat{p}_i) + (1 - y_i) \\cdot \\log(1 - \\widehat{p}_i) \\right].\n\\end{align*}\\] Als weitere zu berechnende Metrik wählen wir \\(\\textup{Accuracy}\\), ein geläufiges Maß zur Bewertung der Leistung von Klassifikationsmodellen. \\(\\textup{Accuracy}\\) gibt an, wie oft das Modell korrekte Vorhersagen getroffen hat, ausgedrückt als Verhältnis der Anzahl der korrekten Vorhersagen zur Gesamtzahl der Vorhersagen, \\[\\begin{align*}\n  \\text{Accuracy} = \\frac{\\textup{TP} + \\textup{TN} }{ \\textup{TP} + \\textup{TN} + \\textup{FP} + \\textup{FN} } = \\frac{\\textup{Anz. korrekte Vorhersagen}}{\\textup{Anz. alle Vorhersagen}}.\n\\end{align*}\\] Hierbei sind \\(\\textup{TP}\\) und \\(\\textup{FP}\\) die Anazhl korrekter (true positive) und falscher (false positiv) Vorhersagen für Beobachtungen mit \\(y_i = 1\\). \\(\\textup{TN}\\) und \\(\\textup{FN}\\) sind analog für Beobachtungen mit tatsächlichen Werten \\(y_i = 0\\) definiert.\nDie Vorhersage von \\(y_i\\) zur Berechnung von \\(\\textup{Accuracy}\\) erfolgt durch keras::fit() standardmäßig nach der Regel \\[\\begin{align*}\n  \\hat{y}_i =\n  \\begin{cases}\n    1 & \\text{wenn } \\hat{p}_i \\geq 0.5, \\\\\n    0 & \\text{wenn } \\hat{p}_i &lt; 0.5.\n  \\end{cases}\n\\end{align*}\\]\nWir definieren nachchfolgend das Modell-Objekt und passen das NN über 150 Epochen an.\n\n# Erstellen und Kompilieren des Modells\nmodel_nn_logit &lt;- keras_model_sequential() %&gt;%\n  layer_dense(\n    units = 1, \n    input_shape = 1, \n    activation = 'sigmoid' # &lt;= für Logit-Modell\n  )\n\nmodel_nn_logit\n\nModel: \"sequential_2\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_2 (Dense)                    (None, 1)                       2           \n================================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\n________________________________________________________________________________\n\n\n\n# Modell kompilieren\nmodel_nn_logit %&gt;% \n  compile(\n    loss = 'binary_crossentropy', # Für BCEL\n    optimizer = optimizer_adam(learning_rate = 0.01),\n    metrics = 'accuracy'\n  )\n\n\n# Anpassen des Modells\nhistory_nn_logit &lt;- model_nn_logit %&gt;% \n  fit(\n    x = X, \n    y = Y, \n    validation_split = .2,\n    epochs = 150,\n    verbose = F\n  )\n\nDie Zusammenfassung der Anpassung für die letzte Epoche in history_nn_logit zeigt ergibt eine Genauigkeit von über 80% auf dem Validierungsdatensatz.\n\nhistory_nn_logit\n\n\nFinal epoch (plot to see history):\n        loss: 0.3992\n    accuracy: 0.8025\n    val_loss: 0.3415\nval_accuracy: 0.87 \n\n\n\n# Entwicklung des Loss über Epochen plotten\nplot(history_nn_logit) +\n  labs(\n    x = \"Epoche\",\n    y = \"\"\n  ) +\n  theme_cowplot() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nAbbildung 16.8: NN für Logit-Regression: Entwicklung der Metriken für 25 Epochen\n\n\n\n\n\n# Gewicht und Bias extrahieren\nmodel_nn_logit %&gt;% \n  keras::get_weights() %&gt;% \n  flatten_dbl() %&gt;% \n  set_names(\n    c(\"w_1\", \"bias\")\n  )\n\n       w_1       bias \n 0.9631591 -5.3512850 \n\n\nFür einen Vergleich der Vorhersagegüter mit logistischer Regression schätzen wir zunächst ein entsprechendes GLM mit glm().\n\n# Logistisches Modell mit glm() anpassen\nglm_mod &lt;- glm(\n  formula = Y ~ X, \n  family = binomial(link = \"logit\")\n)\n\nsummary(glm_mod)\n\n\nCall:\nglm(formula = Y ~ X, family = binomial(link = \"logit\"))\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -7.3289     0.6520  -11.24   &lt;2e-16 ***\nX             1.3140     0.1191   11.04   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 673.01  on 499  degrees of freedom\nResidual deviance: 375.96  on 498  degrees of freedom\nAIC: 379.96\n\nNumber of Fisher Scoring iterations: 6\n\n\nWir erzeugen nun einen Testdatensatz mit 250 Beobachtungen gemäß des oben gewählten DGP.\n\nset.seed(4321)\n\nn &lt;- 250\ndata_new &lt;- tibble(\n  X = rnorm(n = n, mean = 5, sd = 2),\n  P = pnorm(-4 + 0.7 * X),\n  Y = as.integer(runif(n) &lt; P)\n)\n\nFür die neuen Datenpunkte data_new erzeugen wir Vorhersagen von \\(P(Y=1\\vert X=x)\\) mit model_nn_logit und glm_mod und erweitern data_new um diese.\n\n# Vorhersagen für Trainingsdaten erstellen\npredictions_nn_logit &lt;- model_nn_logit %&gt;% \n  predict(data_new$X) %&gt;% \n  c()\n\n8/8 - 0s - 43ms/epoch - 5ms/step\n\npredictions_glm_logit &lt;- predict(\n    glm_mod, \n    newdata = data_new, \n    type = \"response\"\n  )\n\n# Zusammenfassen\nresults &lt;- data_new %&gt;% \n  mutate(\n    nn_logit = predictions_nn_logit,\n    glm_logit = predictions_glm_logit\n)\n\nslice_head(results, n = 10)\n\n# A tibble: 10 × 5\n       X      P     Y nn_logit glm_logit\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1  4.15 0.136      1    0.205    0.132 \n 2  4.55 0.208      0    0.276    0.206 \n 3  6.44 0.693      1    0.700    0.755 \n 4  6.68 0.751      1    0.748    0.810 \n 5  4.74 0.248      0    0.314    0.250 \n 6  8.22 0.960      1    0.929    0.970 \n 7  4.41 0.180      0    0.248    0.177 \n 8  5.39 0.411      0    0.461    0.439 \n 9  7.48 0.892      0    0.865    0.924 \n10  3.56 0.0660     0    0.128    0.0661\n\n\nFür eine erste Beurteilung anhand der Vorhersagen auf dem Testdatensatz plotten wir die vorhergesagten Wahrscheinlichkeiten als Funktion von X gemeinsam mit den tatsächlichen Ausprägungen der Outcome-Variable Y.\n\nlibrary(ggplot2)\nlibrary(cowplot)\n\n# Plot der tatsächlichen Werte gegen die vorhergesagten Wahrscheinlichkeiten\nggplot(\n  data = results,\n  mapping =  aes(x = X, y = Y)\n  ) +\n  geom_point(\n    position = position_jitter(height = 0.05), \n    alpha = 0.5\n  ) +\n  geom_line(\n     mapping = aes(y = nn_logit),\n    col = \"darkred\"\n  ) +\n  geom_smooth(\n    method = \"glm\", \n    method.args = list(family = \"binomial\"), \n    se = FALSE\n  ) +\n  labs(\n    title = \"Logistische Regression vs. NN\",\n       x = \"x\",\n       y = \"Schätzung v. P(Y=1|X=x)\"\n  ) +\n  theme_cowplot()\n\n\n\n\n\n\n\nDie geschätzten Wahrscheinlichkeitsfunktionen zeigen eine gute Übereinstimmung. Um die Vorhersagegüte von model_nn_logit und model_glm_logit genauer zu untersuchen, erstellen wir Plots der jeweilgen Receiver Operating Characteristic (ROC). ROC zeigt den Zusammenhang zwischen der True Positive Rate (TPR), dem Anteil korrekter Vorhersagen für \\(y_i=1\\) (auch Sensitivität gennant) und der False Positive Rate (FPR), dem Anteil falscher Vorhersagen für \\(y_i=1\\) in Abhängigkeit des Schwellenwerts von \\(\\widehat{p}\\) für die Klassifikation des Outcomes einer Beobachtung \\(y_i = 1\\). Es gilt\n\\[\\begin{align*}\n  TPR =&\\, \\frac{TP}{TP + FN},\\\\\n  \\\\\n  FPR =&\\, \\frac{FP}{FP + TN}.\n\\end{align*}\\]\nDer Schwellenwert \\(\\widehat{p}\\) reguliert den Trade-Off zwischen \\(\\textup{FPR}\\) und \\(\\textup{TPR}\\): Kleine \\(\\widehat{p}\\) führen tendenziell zu großer \\(\\textup{TPR}\\) (gut), aber auch zu großer \\(\\textup{FPR}\\) (schlecht). Für ein Modell, das zufällig klassifiziert, entspricht die ROC-Kurve der Winkelhalbierenden. Wünschenwert ist ein verlauf der ROC-Kurve möglichst oberhalb der Winkelhalbierenden.\nEine ROC-Kurve kann in R mit dem plotROC anhand der Vorhergesagten und tatsächlichen Werte der Outcome-Varibale berechnet und geplottet werden. Hierzu transformieren wir results in langes Format und verwenden ggplot() mit dem Layer geom_roc().\n\nlibrary(plotROC)\nlibrary(tidyr)\n\nroc_data &lt;- results %&gt;%\n  pivot_longer(\n    cols = glm_logit:nn_logit,\n    names_to = \"model\", \n    values_to = \"pp\"\n    )\n\n# ROC-Kurve plotten\n  ggplot(\n    data = roc_data, \n    mapping = aes(\n      m = pp, \n      d = Y, \n      colour = model\n    )\n  ) +\n  geom_roc() +\n  style_roc() + \n  facet_wrap(~ model) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nAbbildung 16.9: ROC-Kurve für logistische Modelle\n\n\n\n\nAbbildung 16.9 zeigt sehr ähnliche ROC-Kurven für model_nn_logit und model_glm_logit.9 Für die Quantifizierung der Vorhersageleistung wird Area under the Curve (AUC), die Fläche unterhalb der ROC-Kurve, herangezogen. Mit plotROC::calc_auc() kann AUC aus dem geom_roc()-Layer berechnet werden. Für ein geschätztes Modell \\(\\widehat{M}\\) gilt \\(0\\leq\\textup{AUC}(\\widehat{M})\\leq1\\).10\n9 Beide Plots enthalten Indikatoren des ROC für bestimmte Schwellenwerte \\(\\widehat{p}\\).10 Ein geschätzes, dass nicht besser als raten ist, gilt \\(\\textup{AUC}(\\widehat{M})=.5\\).\n# AUC berechnen\nroc_data %&gt;%\n  group_by(model) %&gt;%\n  summarise(\n    AUC = calc_auc(\n      ggplot(mapping = aes(m = pp, d = Y)) +\n        geom_roc()\n    )$AUC\n  )\n\n# A tibble: 2 × 2\n  model       AUC\n  &lt;chr&gt;     &lt;dbl&gt;\n1 glm_logit 0.890\n2 nn_logit  0.890\n\n\nDer Vergleich der AUC-Statistiken beider Modelle für den Testdatensatz zeigt, dass das NN ähnlich gut klassifizert, wie das GLM mit logistischer Link-Funktion.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Neuronale Netzwerke</span>"
    ]
  },
  {
    "objectID": "Machine Learning.html#empirisches-beispiel-boston-housing",
    "href": "Machine Learning.html#empirisches-beispiel-boston-housing",
    "title": "\n16  Neuronale Netzwerke\n",
    "section": "\n16.6 Empirisches Beispiel: Boston Housing",
    "text": "16.6 Empirisches Beispiel: Boston Housing\nIn diesem Beispiel illustrieren wir die Anwendung von (NN) für den Boston Housing-Datensatz MASS::Boston. Boston enthält enthält verschiedene Charakteristika von Häusern in Boston, wie z.B. Kriminalitätsrate, Anzahl der Räume und Alter der Gebäude. Ziel ist esm den Medianpreis der Häuser (medv) vorherzusagen.\nFür die Analyse verwenden wir insbesondere die Pakete tidymodels und recipes. Das recipes-Paket erlaubt eine automatisierte und leicht reproduzierbare Vorbereitung von Datensätzen für statistische Modellierung mit tidymodels. Es stellt Verben zur Konstruktion von Pipelines mit Schritten wie Skalierung, Kodierung und Handling fehlender Werte bereit. Diese Schritte werden mit einem Rezept (recipe()) definiert, vorbereitet (prep()) und dann auf Trainings- und/oder Testdaten angewendet (bake()).\nZunächst laden wir den Boston Housing-Datensatz und teilen boston_tbl in Trainings- und Testdaten auf. Hierbei verwenden wir 80% der Daten für das Training und reservieren 20% zum Testen.\n\nlibrary(tidymodels)\n\n# Datensatz in tibble umwandeln \nboston_tbl &lt;- as_tibble(MASS::Boston)\n\n# Trainings- und Testdaten einteilen \n# (80% / 20%)\nset.seed(1234)\n\nsplit &lt;- initial_split(boston_tbl, prop = 0.8)\ntrain_data &lt;- training(split)\ntest_data &lt;- testing(split)\n\n# Zielvariable und Prädiktoren aufteilen\ntrain_x &lt;- select(train_data, -medv)\ntrain_y &lt;- train_data$medv\n\ntest_x &lt;- select(test_data, -medv)\ntest_y &lt;- test_data$medv\n\nDa neuronale Netze empfindlich auf unterschiedlich skalierte Eingabedaten reagieren (ähnlich wie regularisierte Schätzer, vgl. Kapitel 13), normalisieren wir sämtliche (numerischen) Prädiktoren, ausgenommen chas und rad, mit recipes::step_normalize().\n\n# Daten normalisieren\n\n# Rezept\nrecipe_obj &lt;- recipe(\n  formula = medv ~ ., \n  data = train_data\n  ) %&gt;%\n  step_normalize(\n    all_predictors(), -chas, -rad\n  )\n\n# Vorbreiten\ndata_prep &lt;- prep(\n  x = recipe_obj, \n  training = train_data\n)\n\n# Anwenden\ntrain_x &lt;- bake(\n  object = data_prep, \n  new_data = train_data\n  ) %&gt;% \n  select(-medv)\n\ntest_x &lt;- bake(\n  object = data_prep, \n  new_data = test_data) %&gt;% \n  select(-medv)\n\nZunächst führen wir eine KQ-Regression durch, um den Zusammenhang zwischen sämtlichen verfügbaren Prädiktoren und dem Zielwert medv zu schätzen. Hierfür nutzen wir das linear_reg()-Modell aus tidymodels und passen es mit der Methode der kleinsten Quadrate (KQ) an.\n\n# Lineare Regression mit KQ\nkq_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\nkq_fit &lt;- fit(\n  object = kq_spec, \n  formula = medv ~ ., \n  data = train_data\n)\n\n# KQ-Vorhersagen für Testdaten\nkq_predictions &lt;- predict(\n  object = kq_fit, \n  new_data = test_data\n  ) %&gt;%\n  bind_cols(test_data) %&gt;%\n  rename(kq_pred = .pred)\n\nFür das NN wählen wir eine Spezifikation mit zwei Hidden Layers, die jeweils 64 Neuronen haben. Mit input_shape = ncol(train_x) verarbeitet das 1. Hidden Layer die Inputs sämtlicher Prädiktoren in boston_tbl. Als Aktivierungsfunktion verwenden wir Recified Linear Unit, \\[\\begin{align}\n      \\textup{ReLU}(z) \\max(0, z).\\\\\n\\end{align}\\]\nDie ReLU-Aktivierungsfunktion setzt also negative Werte auf 0 und lässt positive Werte unverändert. In NN ermöglicht sie eine sparsame anpassung und ist numerisch leicht handhabbar bei der Berechnung des Gradienten: Die Verwendung von Layers mit ReLU verringert das Vanishing-Gradient-Problem.11\n11 Bei nicht-linearen Aktivierungsfunktionen kann der Gradient der Loss-Funktion extrem klein werden, was das Training von NN mit vielen Layers schwierig machen kann.Das Output Layer des Netzes besteht aus einem einzigen Neuron, dass einen numerischen Wert (den geschätzten Hauspreis) zurückgibt.\n\n# NN mit Keras erstellen\nmodel &lt;- keras_model_sequential() %&gt;%\n  # Hidden Layer 1\n  layer_dense(\n    units = 64, \n    activation = \"relu\", \n    input_shape = ncol(train_x)\n  ) %&gt;%\n  # Hidden Layer 2\n  layer_dense(\n    units = 64, \n    activation = \"relu\"\n  ) %&gt;%\n  # Output Layer\n  layer_dense(units = 1)\n\n# Modell kompilieren\nmodel %&gt;% compile(\n  optimizer = \"rmsprop\",\n  loss = \"mse\",\n  metrics = \"mae\"\n)\n\nWir trainieren das NN über 100 Epochen mit einem Batch-Größe von 32 Datenpunkten.\n\n# Modell trainieren\nhistory &lt;- model %&gt;% \n  fit(\n    x = as.matrix(train_x), \n    y = train_y,\n    epochs = 100,\n    batch_size = 32,\n    validation_split = 0.2,\n    verbose = 0\n  )\n\nMit dem trainierten Nachdem das NN trainiert ist, verwenden wir model, um Vorhersagen auf den Testdaten test_data zu machen.\n\n# Vorhersagen mit trainiertem NN\nnn_pred &lt;- model %&gt;% \n  predict(as.matrix(test_x))\n\n4/4 - 0s - 70ms/epoch - 17ms/step\n\n# Testdaten erweitern\nnn_predictions &lt;- test_data %&gt;%\n  mutate(\n    nn_pred = c(nn_pred)\n  )\n\nUm die Leistung der Modelle zu vergleichen, berechnen wir den MSE für beide Modelle.\n\n# MSE berechnen\nols_mse &lt;- kq_predictions %&gt;%\n  summarise(mse = mean((medv - kq_pred)^2)) %&gt;%\n  pull(mse)\n\nnn_mse &lt;- nn_predictions %&gt;%\n  summarise(mse = mean((medv - nn_pred)^2)) %&gt;%\n  pull(mse)\n\nc(\n  \"MSE KQ\" = ols_mse,\n  \"MSE NN\" = nn_mse\n)\n\n  MSE KQ   MSE NN \n28.79773 16.88401 \n\n\nDa das NN ist in der Lage ist, nicht-lineare Zusammenhänge zwischen den Prädiktoren und der Zielvariablen zu modellieren, liefert hat es eine höhere Vorhersagegenauigkeit auf dem Testdatensatz.\nFür eine Visualisierung der Vorhersagen tragen wir den wahren und die vorhergesagten Werte von medv für beide Modelle in einem Punkteplot ab.\n\n# KQ- und NN-Vorhersagen sammeln\npreds &lt;- kq_predictions %&gt;%\n  select(medv, kq_pred) %&gt;%\n  mutate(nn_pred = nn_predictions$nn_pred)\n\n# Visualisierung der Ergebnisse mit ggplot2\npreds %&gt;%\n  ggplot(aes(x = medv)) +\n  geom_point(aes(y = kq_pred, color = \"OLS\"), alpha = 0.6) +\n  geom_point(aes(y = nn_pred, color = \"Neuronales Netz\"), alpha = 0.6) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\") +\n  labs(\n    x = \"Wahre Werte (medv)\",\n    y = \"Vorhersagen\",\n    color = \"Modell\"\n  ) +\n  theme_minimal() +\n  scale_color_manual(\n    values = c(\n      \"OLS\" = \"red\", \n      \"Neuronales Netz\" = \"steelblue\"\n    )\n  )\n\n\n\n\n\n\nAbbildung 16.10: Boston Housing: KQ vs. NN\n\n\n\n\nAbbildung 16.10 zeigt, dass KQ und NN weitgehend vergleichbare Vorhersagen von medv auf dem Testdatensatz liefern. Das NN scheint jedoch im Mittel besser in der Vorhersage hoher Verkaufspreise zu sein – möglicherweise weil extreme Preise auf nicht-lineare Beziehungen zwischen bestimmten Regressoren und medv zurückzuführen sind, die in einer linearen KQ-Regression nicht erfasst werden können.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Neuronale Netzwerke</span>"
    ]
  },
  {
    "objectID": "Machine Learning.html#case-study-vorhersage-von-immobilienpreisen",
    "href": "Machine Learning.html#case-study-vorhersage-von-immobilienpreisen",
    "title": "\n16  Neuronale Netzwerke\n",
    "section": "\n16.7 Case Study: Vorhersage von Immobilienpreisen",
    "text": "16.7 Case Study: Vorhersage von Immobilienpreisen\n\nlibrary(AmesHousing)\nhousing &lt;- make_ames()\n\n\n# Load the necessary libraries\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(tigris)\n\n# Retrieve basemap for Ames, Iowa using the tigris package\nplaces_map &lt;- places(\n  state = 'IA', \n  cb = TRUE, \n  progress = F\n) %&gt;%\n  st_as_sf()\n\n# Filter for Ames city\names_map &lt;- places_map %&gt;% \n  filter(NAME == \"Ames\")\n\nhouses &lt;- housing %&gt;%\n    select(Latitude, Longitude, Sale_Price) %&gt;%\n    mutate(\n      Sale_Price = cut(log(Sale_Price, base = 2), breaks = 5, labels = FALSE)\n    ) %&gt;%\n    st_as_sf(coords = c(\"Longitude\", \"Latitude\"), \n             crs = 4326, agr = \"constant\")\n\nrainbow_colors &lt;- rainbow(5, rev = TRUE)\n\n\n# Plot the map with just the outline of Ames\nggplot() +\n  geom_sf(data = ames_map, color = \"black\", fill = alpha(\"black\", alpha = 0)) +\n  geom_sf(data = houses, mapping = aes(color = factor(Sale_Price)), size = .2) +\n  theme_map() +\n  scale_color_manual(\n    name = \"log(Verkaufspreis)\", \n    values = rainbow_colors, \n    labels = levels(factor(houses$Sale_Price))\n  ) +\n  theme(legend.position = \"bottom\", legend.direction = \"horizontal\") +\n  ggtitle(\"Verkaufte Häuser in Ames, Iowa\")\n\n\n\n\n\n\n\n\nhousing %&gt;%\n  ggplot(mapping = aes(x = Year_Built, y = Sale_Price)) +\n  geom_point(alpha = .5, fill = \"steelblue\") +\n  theme_cowplot()\n\n\n\n\n\n\n\n\n# Split the data into training and testing sets\nset.seed(1234)\n\nlibrary(tidymodels)\n\nsplit &lt;- initial_split(housing, prop = 0.8)\n\nhousing_train &lt;- training(split)\nhousing_test &lt;- testing(split)\n\n# Separate the predictors and the outcome\nhousing_train_x &lt;- housing_train %&gt;% select(-Sale_Price)\nhousing_train_y &lt;- housing_train$Sale_Price\n\nhousing_test_x &lt;- housing_test %&gt;% select(-Sale_Price)\nhousing_test_y &lt;- housing_test$Sale_Price\n\n\nblueprint &lt;- recipe(\n  Sale_Price ~ ., \n  data = housing_train) %&gt;%\n  step_nzv(all_nominal()) %&gt;%\n  step_other(all_nominal(), threshold = .01, other = \"other\") %&gt;%\n  step_integer(matches(\"(Qual|Cond|QC|Qu)$\")) %&gt;%\n  step_YeoJohnson(all_numeric(), -all_outcomes()) %&gt;%\n  step_center(all_numeric(), -all_outcomes()) %&gt;%\n  step_scale(all_numeric(), -all_outcomes()) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)\n\nprepare &lt;- prep(blueprint, training = housing_train)\nprepare\n\n\nbaked_train &lt;- bake(prepare, new_data = housing_train)\nbaked_test &lt;- bake(prepare, new_data = housing_test)\n\nbaked_train\n\n# A tibble: 2,344 × 190\n   Lot_Frontage  Lot_Area Overall_Qual Overall_Cond Year_Built Year_Remod_Add\n          &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;\n 1       1.15    0.350         -0.0340       -0.487     0.843           0.654\n 2       0.277  -0.0336        -0.800        -0.487    -0.155          -0.839\n 3      -0.129  -0.0213        -0.800        -1.76     -1.75           -1.66 \n 4      -0.909  -3.19          -0.0340        0.479     0.277           0.365\n 5       0.407  -0.223         -1.68          1.26     -0.0554         -0.694\n 6      -1.88    0.0595         1.28         -0.487     1.01            0.847\n 7       0.407   0.000506       0.655        -0.487     0.943           0.750\n 8       0.144  -0.377         -0.0340       -0.487     0.910           0.895\n 9      -0.0461 -1.52          -0.0340       -0.487     0.111          -0.453\n10       1.98    0.501          1.28         -0.487     0.876           0.654\n# ℹ 2,334 more rows\n# ℹ 184 more variables: Mas_Vnr_Area &lt;dbl&gt;, Exter_Qual &lt;dbl&gt;, Exter_Cond &lt;dbl&gt;,\n#   Bsmt_Qual &lt;dbl&gt;, BsmtFin_SF_1 &lt;dbl&gt;, BsmtFin_SF_2 &lt;dbl&gt;, Bsmt_Unf_SF &lt;dbl&gt;,\n#   Total_Bsmt_SF &lt;dbl&gt;, Heating_QC &lt;dbl&gt;, First_Flr_SF &lt;dbl&gt;,\n#   Second_Flr_SF &lt;dbl&gt;, Low_Qual_Fin_SF &lt;dbl&gt;, Gr_Liv_Area &lt;dbl&gt;,\n#   Bsmt_Full_Bath &lt;dbl&gt;, Bsmt_Half_Bath &lt;dbl&gt;, Full_Bath &lt;dbl&gt;,\n#   Half_Bath &lt;dbl&gt;, Bedroom_AbvGr &lt;dbl&gt;, Kitchen_AbvGr &lt;dbl&gt;, …\n\n\n\nlibrary(glmnet)\n\n# Prepare the data for glmnet (which requires matrices)\nx_train_glmnet &lt;- as.matrix(baked_train %&gt;% select(-Sale_Price))\ny_train_glmnet &lt;- log10(baked_train$Sale_Price)\n\nx_test_glmnet &lt;- as.matrix(baked_test %&gt;% select(-Sale_Price))\n\n# Fit a Ridge Regression model\nridge_model &lt;- glmnet(x_train_glmnet, y_train_glmnet, alpha = 0)\n\n# Use cross-validation to find the optimal lambda\ncv_ridge &lt;- cv.glmnet(x_train_glmnet, y_train_glmnet, alpha = 0)\nbest_lambda &lt;- cv_ridge$lambda.min\n\n# Predict on the test set using the best lambda\nridge_preds_log &lt;- predict(cv_ridge, s = best_lambda, newx = x_test_glmnet)\n\n# Convert predictions back from log scale\nridge_preds &lt;- as.numeric(10^ridge_preds_log)\n\n# Evaluate the performance\nmae_vec(\n  truth = housing_test_y, \n  estimate = ridge_preds\n)\n\n[1] 14695.86\n\n\n\nx_train &lt;- select(baked_train, -Sale_Price) %&gt;% as.matrix()\ny_train &lt;- baked_train %&gt;% pull(Sale_Price)\n\nx_test &lt;- select(baked_test, -Sale_Price) %&gt;% as.matrix()\ny_test &lt;- baked_test %&gt;% pull(Sale_Price)\n\nnetwork &lt;- keras_model_sequential() %&gt;% \n  layer_dense(units = 512, activation = \"relu\", input_shape = ncol(x_train)) %&gt;%\n  layer_dense(units = 512, activation = \"relu\") %&gt;%\n  layer_dense(units = 512, activation = \"relu\") %&gt;%\n  layer_dense(units = 512, activation = \"relu\") %&gt;%\n  layer_dense(units = 512, activation = \"relu\") %&gt;%\n  layer_dense(units = 1)\n\nnetwork %&gt;%\n  compile(\n    optimizer = optimizer_rmsprop(learning_rate = 0.01),\n    loss = \"msle\",\n    metrics = c(\"mae\")\n  )\n\nset.seed(1234)\n\nhistory &lt;- network %&gt;% fit(\n  x_train,\n  y_train,\n  epochs = 30,\n  batch_size = 32,\n  validation_split = 0.2,\n  callbacks = list(\n        callback_early_stopping(patience = 10, restore_best_weights = TRUE),\n        callback_reduce_lr_on_plateau(factor = 0.2, patience = 4)\n    )\n)\n\nEpoch 1/30\n59/59 - 2s - loss: 2.6801 - mae: 68863.8047 - val_loss: 0.2711 - val_mae: 77059.9766 - lr: 0.0100 - 2s/epoch - 29ms/step\nEpoch 2/30\n59/59 - 0s - loss: 0.3143 - mae: 98887.8047 - val_loss: 0.4397 - val_mae: 161632.1250 - lr: 0.0100 - 499ms/epoch - 8ms/step\nEpoch 3/30\n59/59 - 0s - loss: 0.2246 - mae: 76933.0938 - val_loss: 0.1350 - val_mae: 55139.1758 - lr: 0.0100 - 498ms/epoch - 8ms/step\nEpoch 4/30\n59/59 - 0s - loss: 0.1730 - mae: 70248.9609 - val_loss: 0.0686 - val_mae: 42032.3672 - lr: 0.0100 - 492ms/epoch - 8ms/step\nEpoch 5/30\n59/59 - 0s - loss: 0.1217 - mae: 56412.5195 - val_loss: 0.1327 - val_mae: 55897.4141 - lr: 0.0100 - 487ms/epoch - 8ms/step\nEpoch 6/30\n59/59 - 0s - loss: 0.1111 - mae: 51840.2422 - val_loss: 0.0468 - val_mae: 32525.9043 - lr: 0.0100 - 491ms/epoch - 8ms/step\nEpoch 7/30\n59/59 - 0s - loss: 0.0850 - mae: 44002.2148 - val_loss: 0.0963 - val_mae: 50788.2852 - lr: 0.0100 - 485ms/epoch - 8ms/step\nEpoch 8/30\n59/59 - 0s - loss: 0.0819 - mae: 44281.1836 - val_loss: 0.1088 - val_mae: 67397.4844 - lr: 0.0100 - 481ms/epoch - 8ms/step\nEpoch 9/30\n59/59 - 0s - loss: 0.0696 - mae: 40577.3477 - val_loss: 0.0743 - val_mae: 40895.9688 - lr: 0.0100 - 483ms/epoch - 8ms/step\nEpoch 10/30\n59/59 - 0s - loss: 0.0596 - mae: 36760.8672 - val_loss: 0.0261 - val_mae: 22681.9336 - lr: 0.0100 - 485ms/epoch - 8ms/step\nEpoch 11/30\n59/59 - 0s - loss: 0.0580 - mae: 36272.9648 - val_loss: 0.0180 - val_mae: 15472.0615 - lr: 0.0100 - 489ms/epoch - 8ms/step\nEpoch 12/30\n59/59 - 0s - loss: 0.0570 - mae: 35091.3867 - val_loss: 0.0754 - val_mae: 51871.4961 - lr: 0.0100 - 479ms/epoch - 8ms/step\nEpoch 13/30\n59/59 - 0s - loss: 0.0440 - mae: 29530.9336 - val_loss: 0.0862 - val_mae: 44767.3125 - lr: 0.0100 - 482ms/epoch - 8ms/step\nEpoch 14/30\n59/59 - 0s - loss: 0.0479 - mae: 32043.8887 - val_loss: 0.0805 - val_mae: 54268.2891 - lr: 0.0100 - 479ms/epoch - 8ms/step\nEpoch 15/30\n59/59 - 0s - loss: 0.0412 - mae: 29175.3594 - val_loss: 0.0269 - val_mae: 22318.1484 - lr: 0.0100 - 495ms/epoch - 8ms/step\nEpoch 16/30\n59/59 - 0s - loss: 0.0086 - mae: 12245.5010 - val_loss: 0.0170 - val_mae: 14487.9668 - lr: 0.0020 - 485ms/epoch - 8ms/step\nEpoch 17/30\n59/59 - 0s - loss: 0.0078 - mae: 11720.7324 - val_loss: 0.0196 - val_mae: 16752.9180 - lr: 0.0020 - 484ms/epoch - 8ms/step\nEpoch 18/30\n59/59 - 0s - loss: 0.0070 - mae: 11038.4990 - val_loss: 0.0167 - val_mae: 14753.1855 - lr: 0.0020 - 485ms/epoch - 8ms/step\nEpoch 19/30\n59/59 - 0s - loss: 0.0069 - mae: 11079.9805 - val_loss: 0.0160 - val_mae: 14316.6523 - lr: 0.0020 - 485ms/epoch - 8ms/step\nEpoch 20/30\n59/59 - 0s - loss: 0.0067 - mae: 10853.2559 - val_loss: 0.0171 - val_mae: 14301.3926 - lr: 0.0020 - 486ms/epoch - 8ms/step\nEpoch 21/30\n59/59 - 0s - loss: 0.0062 - mae: 10195.1113 - val_loss: 0.0175 - val_mae: 14863.8672 - lr: 0.0020 - 483ms/epoch - 8ms/step\nEpoch 22/30\n59/59 - 0s - loss: 0.0063 - mae: 10472.6641 - val_loss: 0.0174 - val_mae: 14883.5322 - lr: 0.0020 - 481ms/epoch - 8ms/step\nEpoch 23/30\n59/59 - 0s - loss: 0.0059 - mae: 9997.8916 - val_loss: 0.0186 - val_mae: 15775.3262 - lr: 0.0020 - 479ms/epoch - 8ms/step\nEpoch 24/30\n59/59 - 0s - loss: 0.0042 - mae: 8284.0127 - val_loss: 0.0164 - val_mae: 14226.6836 - lr: 4.0000e-04 - 498ms/epoch - 8ms/step\nEpoch 25/30\n59/59 - 0s - loss: 0.0041 - mae: 8079.7988 - val_loss: 0.0164 - val_mae: 14222.3291 - lr: 4.0000e-04 - 485ms/epoch - 8ms/step\nEpoch 26/30\n59/59 - 0s - loss: 0.0040 - mae: 7974.5630 - val_loss: 0.0164 - val_mae: 14259.8604 - lr: 4.0000e-04 - 488ms/epoch - 8ms/step\nEpoch 27/30\n59/59 - 0s - loss: 0.0039 - mae: 7852.2417 - val_loss: 0.0164 - val_mae: 14279.8174 - lr: 4.0000e-04 - 485ms/epoch - 8ms/step\nEpoch 28/30\n59/59 - 0s - loss: 0.0037 - mae: 7661.8911 - val_loss: 0.0164 - val_mae: 14294.8623 - lr: 8.0000e-05 - 488ms/epoch - 8ms/step\nEpoch 29/30\n59/59 - 0s - loss: 0.0037 - mae: 7651.1865 - val_loss: 0.0164 - val_mae: 14320.3291 - lr: 8.0000e-05 - 489ms/epoch - 8ms/step\n\n\n\nhistory\n\n\nFinal epoch (plot to see history):\n    loss: 0.003737\n     mae: 7,651\nval_loss: 0.01637\n val_mae: 14,320\n      lr: 0.00008 \n\n\n\nplot(history) + \n  scale_y_log10() +\n  theme_cowplot() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBishop, Christopher M. 2007. Pattern recognition and machine learning. Information Science and Statistics. New York, NY: Springer Science+Business Media, LLC.\n\n\nChollet, Francois, und J. J. Allaire. 2018. Deep Learning with R. New York: Manning Publications Co. LLC.\n\n\nGoodfellow, Ian, Yoshua Bengio, und Aaron Courville. 2016. Deep Learning. MIT Press.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Neuronale Netzwerke</span>"
    ]
  },
  {
    "objectID": "Literatur.html",
    "href": "Literatur.html",
    "title": "Literatur",
    "section": "",
    "text": "Abadie, Alberto, Alexis Diamond, and Jens Hainmueller. 2014.\n“Comparative Politics and the Synthetic Control Method:\nCOMPARATIVE POLITICS AND THE SYNTHETIC CONTROL METHOD.”\nAmerican Journal of Political Science 59 (2): 495–510. https://doi.org/10.1111/ajps.12116.\n\n\nAbadie, Alberto, and Guido W. Imbens. 2008. “On the Failure of the\nBootstrap for Matching Estimators.” Econometrica. Journal of\nthe Econometric Society 76 (6): 1537–57. https://doi.org/10.3982/ECTA6474.\n\n\nAbadie, Alberto, and Jann Spiess. 2022. “Robust Post-Matching\nInference.” Journal of the American Statistical\nAssociation 117 (538): 983–95. https://doi.org/10.1080/01621459.2020.1840383.\n\n\nAbadie, Alexis Diamond, Alberto, and Jens Hainmueller. 2010.\n“Synthetic Control Methods for Comparative Case Studies:\nEstimating the Effect of California’s Tobacco Control Program.”\nJournal of the American Statistical Association 105 (490):\n493–505. https://doi.org/10.1198/jasa.2009.ap08746.\n\n\nAbrams, David S. 2012a. “Estimating the Deterrent Effect of\nIncarceration Using Sentencing Enhancements.” American\nEconomic Journal: Applied Economics 4 (4): 32–56. https://doi.org/10.1257/app.4.4.32.\n\n\nAbrams, David S. 2012b. “Replication Data for: Estimating the\nDeterrent Effect of Incarceration Using Sentencing Enhancements.”\nICPSR - Interuniversity Consortium for Political; Social Research. https://doi.org/10.3886/E113838V1.\n\n\nAcemoglu, Daron, Giuseppe De Feo, and Giacomo Davide De Luca. 2020.\n“Weak States: Causes and Consequences of the Sicilian\nMafia.” The Review of Economic Studies. https://doi.org/10.1093/restud/rdz009.\n\n\nAcemoglu, Daron, Simon Johnson, James A. Robinson, and Pierre Yared.\n2008a. “Income and Democracy.” American Economic\nReview 98 (3): 808–42. https://doi.org/10.1257/aer.98.3.808.\n\n\n———. 2008b. “Replication Data for: Income and Democracy.”\nICPSR - Interuniversity Consortium for Political; Social Research. https://doi.org/10.3886/E113251V1.\n\n\nAdireksombat, Kampon. 2010. “The Effects of the 1993 Earned Income\nTax Credit Expansion on the Labor Supply of Unmarried Women.”\nPublic Finance Review 38 (1): 11–40. https://doi.org/https://doi.org/10.1177/1091142109358626.\n\n\nAllaire, JJ, Yihui Xie, Christophe Dervieux, Jonathan McPherson, Javier\nLuraschi, Kevin Ushey, Aron Atkins, et al. 2024. Rmarkdown: Dynamic\nDocuments for r. https://github.com/rstudio/rmarkdown.\n\n\nAndrews, D. W. K. 2003. “End-of-Sample Instability Tests.”\nEconometrica 71 (6): 1661–94. https://doi.org/10.1111/1468-0262.00466.\n\n\nArellano, Manuel, and Stephen Bond. 1991. “Some Tests of\nSpecification for Panel Data: Monte Carlo Evidence and an Application to\nEmployment Equations.” The Review of Economic Studies 58\n(2): 277. https://doi.org/10.2307/2297968.\n\n\nAthey, Susan, and Guido Imbens. 2016. “Recursive Partitioning for\nHeterogeneous Causal Effects.” Proceedings of the National\nAcademy of Sciences 113 (27): 7353–60. https://doi.org/10.1073/pnas.1510489113.\n\n\nAustin, P. 2011. “An Introduction to Propensity Score Methods for\nReducing the Effects of Confounding in Observational Studies.”\nMultivariate Behavioral Research 46 (3): 399–424. https://doi.org/10.1080/00273171.2011.568786.\n\n\nAustin, Peter C., and Dylan S. Small. 2014. “The Use of\nBootstrapping When Using Propensity-Score Matching Without Replacement:\nA Simulation Study.” Statistics in Medicine 33 (24):\n4306–19. https://doi.org/10.1002/sim.6276.\n\n\nAustin, Peter C., and Elizabeth A. Stuart. 2017. “Estimating the\nEffect of Treatment on Binary Outcomes Using Full Matching on the\nPropensity Score.” Statistical Methods in Medical\nResearch 26 (6): 2505–25. https://doi.org/10.1177/0962280215601134.\n\n\nBarro, Robert J., and Jong Wha Lee. 2013. “A New Data Set of\nEducational Attainment in the World, 1950–2010.” Journal of\nDevelopment Economics 104: 184–98. https://doi.org/https://doi.org/10.1016/j.jdeveco.2012.10.001.\n\n\nBasten, Christoph, and Frank Betz. 2013. “Beyond Work Ethic:\nReligion, Individual, and Political Preferences.” American\nEconomic Journal: Economic Policy 5 (3): 67–91.\n\n\nBelloni, Alexandre, Daniel Chen, Victor Chernozhukov, and Christian\nHansen. 2012. “Sparse Models and Methods for Optimal Instruments\nwith an Application to Eminent Domain.” Econometrica 80\n(6): 2369–429.\n\n\nBelloni, Alexandre, and Victor Chernozhukov. 2013. “Least Squares\nAfter Model Selection in High-Dimensional Sparse Models.”\nBernoulli, 521–47.\n\n\nBelloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2014.\n“High-Dimensional Methods and Inference on Structural and\nTreatment Effects.” Journal of Economic Perspectives 28\n(2): 29–50.\n\n\nBishop, Christopher M. 2007. Pattern Recognition and Machine\nLearning. Information Science and Statistics. New York, NY:\nSpringer Science+Business Media, LLC.\n\n\nBodory, Hugo, Lorenzo Camponovo, Martin Huber, and Michael Lechner.\n2020. “The Finite Sample Performance of Inference Methods for\nPropensity Score Matching and Weighting Estimators.” Journal\nof Business & Economic Statistics. https://doi.org/10.2139/ssrn.2731969.\n\n\nBorn, Benjamin, Gernot J Müller, Moritz Schularick, and Petr Sedláček.\n2019. “The Costs of Economic Nationalism: Evidence from the Brexit\nExperiment*.” The Economic Journal 129 (623): 2722–44.\nhttps://doi.org/10.1093/ej/uez020.\n\n\nBreiman, L., J. Friedman, C. J. Stone, and R. A. Olshen. 1984.\nClassification and Regression Trees. Taylor & Francis.\n\n\nCallaway, Brantly, and Pedro H. C. Sant’Anna. 2021.\n“Difference-in-Differences with Multiple Time Periods.”\nJournal of Econometrics 225 (2): 200–230. https://doi.org/10.1016/j.jeconom.2020.12.001.\n\n\nCameron, A. Colin, Jonah B. Gelbach, and Douglas L. Miller. 2008.\n“Bootstrap-Based Improvements for Inference with Clustered\nErrors.” Review of Economics and Statistics 90 (3):\n414–27. https://doi.org/10.1162/rest.90.3.414.\n\n\n———. 2011. “Robust Inference with Multiway Clustering.”\nJournal of Business &Amp; Economic Statistics 29 (2):\n238–49. https://doi.org/10.1198/jbes.2010.07136.\n\n\nCattaneo, Matias D, Michael Jansson, and Xinwei Ma. 2020. “Simple\nLocal Polynomial Density Estimators.” Journal of the American\nStatistical Association 115 (531): 1449–55.\n\n\nChang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke,\nYihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara\nBorges. 2024. Shiny: Web Application Framework for r. https://CRAN.R-project.org/package=shiny.\n\n\nChollet, Francois, and J. J. Allaire. 2018. Deep Learning with\nr. New York: Manning Publications Co. LLC.\n\n\nCortez, Paulo, and Alice Maria Gonçalves Silva. 2008. “Using Data\nMining to Predict Secondary School Student Performance.”\n\n\nCutrera, Antonino. 1900. La Mafia E I Mafiosi.\n(Palermo, IT: Reber).\n\n\nDahl, Robert Alan. 1971. Polyarchy: Participation and Opposition:\nParticipation and Opposition. New Haven: Yale Univ. Press.\n\n\nDi Tella, Rafael, and Ernesto Schargrodsky. 2004. “Do Police\nReduce Crime? Estimates Using the Allocation of Police Forces After a\nTerrorist Attack.” American Economic Review 94 (1):\n115–33. https://doi.org/10.1257/000282804322970733.\n\n\nEfron, B. 1979. “Bootstrap Methods: Another Look at the\nJackknife.” The Annals of Statistics 7 (1). https://doi.org/10.1214/aos/1176344552.\n\n\nEfron, Bradley, Trevor Hastie, Iain Johnstone, and Robert Tibshirani.\n2004. “Least Angle Regression.”\n\n\nEissa, N., and J. B. Liebman. 1996. “Labor Supply Response to the\nEarned Income Tax Credit.” The Quarterly Journal of\nEconomics 111 (2): 605–37. https://doi.org/10.2307/2946689.\n\n\nFearon, James D., and David D. Laitin. 2003. “Ethnicity,\nInsurgency, and Civil War.” American Political Science\nReview 97 (01): 75–90. https://doi.org/10.1017/s0003055403000534.\n\n\nGelman, Andrew, and Guido Imbens. 2019. “Why High-Order\nPolynomials Should Not Be Used in Regression Discontinuity\nDesigns.” Journal of Business & Economic Statistics\n37 (3): 447–56.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep\nLearning. MIT Press.\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-Differences with\nVariation in Treatment Timing.” Journal of Econometrics\n225 (2): 254–77. https://doi.org/10.1016/j.jeconom.2021.03.014.\n\n\nHahn, P Richard, Carlos M Carvalho, David Puelz, and Jingyu He. 2018.\n“Regularization and Confounding in Linear Regression for Treatment\nEffect Estimation.”\n\n\nHainmueller, Jens. 2012. “Entropy Balancing for Causal Effects: A\nMultivariate Reweighting Method to Produce Balanced Samples in\nObservational Studies.” Political Analysis 20 (1):\n25–46. https://doi.org/10.1093/pan/mpr025.\n\n\nHainmueller, Jens, Alexis Diamond, and Alberto Abadie. 2011.\n“Synth: An r Package for Synthetic Control Methods in Comparative\nCase Studies.” Journal of Statistical Software 42 (13):\n1–17. https://www.jstatsoft.org/v42/i13/.\n\n\nHájek, J. 1971. “Comment on ‘an Essay on the Logical\nFoundations of Survey Sampling’ by Basu, d.”\nFoundations of Statistical Inference 236.\n\n\nHansen, Lars Peter. 1982. “Large Sample Properties of Generalized\nMethod of Moments Estimators.” Econometrica 50 (4):\n1029. https://doi.org/10.2307/1912775.\n\n\nHastie, T., R. Tibshirani, and J. Friedman. 2013. The Elements of\nStatistical Learning: Data Mining, Inference, and Prediction.\nSpringer Series in Statistics. Springer New York.\n\n\nHill, Jennifer, and Jerome P. Reiter. 2006. “Interval Estimation\nfor Treatment Effects Using Propensity Score Matching. Statistics in\nMedicine.” Statistics in Medicine 25 (13): 2230–56. https://doi.org/10.1002/sim.2277.\n\n\nHirano, Keisuke, Guido Imbens, and Geert Ridder. 2003. “Efficient\nEstimation of Average Treatment Effects Using the Estimated Propensity\nScore.” Econometrica 71 (4): 1161–89. https://doi.org/10.1111/1468-0262.00442.\n\n\nHoerl, Arthur E, and Robert W Kennard. 1970. “Ridge regression: Biased estimation for nonorthogonal\nproblems.” Technometrics 12 (1): 55–67.\n\n\nHuntington, Samuel P. 1991. The Third Wave: Democratization in the\nLate Twentieth Century: Democratization in the Late Twentieth\nCentury. Norman, OK: Univ. of Oklahoma Press.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to\nResearch Design and Causality. Chapman; Hall/CRC. https://doi.org/10.1201/9781003226055.\n\n\nImbens. 2016. “Matching on the Estimated Propensity Score.”\nEconometrica 84 (2): 781–807. https://doi.org/10.3982/ecta11293.\n\n\nImbens, G. W., and Thomas Lemieux. 2008. “Regression Discontinuity\nDesigns: A Guide to Practice.” Journal of Econometrics\n142 (2): 615–35.\n\n\nImbens, Guido, and Karthik Kalyanaraman. 2012. “Optimal Bandwidth\nChoice for the Regression Discontinuity Estimator.” The\nReview of Economic Studies 79 (3): 933–59.\n\n\nJacobson, Louis S., Robert J. LaLonde, and Daniel G. Sullivan. 1993.\n“Earnings Losses of Displaced Workers.” The American\nEconomic Review 83 (4): 685–709. http://www.jstor.org/stable/2117574.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2021.\nISLR: Data for an Introduction to Statistical Learning with\nApplications in r. https://CRAN.R-project.org/package=ISLR.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2017. An Introduction to Statistical Learning: With Applications in\nr. Corrected at 8th printing 2017. Springer Texts in Statistics.\nNew York: Springer.\n\n\nLee, David S. 2008. “Randomized Experiments from Non-Random\nSelection in US House Elections.” Journal of\nEconometrics 142 (2): 675–97.\n\n\nLiu, Regina Y. 1988. “Bootstrap Procedures Under Some Non-i.i.d.\nModels.” The Annals of Statistics 16 (4): 1696–1708. https://doi.org/10.1214/aos/1176351062.\n\n\nLove, Thomas. 2004. “Graphical Display of Covariate\nBalance.” Presentation.\n\n\nMcCrary, Justin. 2008. “Manipulation of the Running Variable in\nthe Regression Discontinuity Design: A Density Test.” Journal\nof Econometrics 142 (2): 698–714.\n\n\nMiguel, Edward, Shanker Satyanath, and Ernest Sergenti. 2004.\n“Economic Shocks and Civil Conflict: An Instrumental Variables\nApproach.” Journal of Political Economy 112 (4): 725–53.\nhttps://doi.org/10.1086/421174.\n\n\nNickell, Stephen. 1981. “Biases in Dynamic Models with Fixed\nEffects.” Econometrica 49 (6): 1417. https://doi.org/10.2307/1911408.\n\n\nRosenbaum, Paul R., and Donald R. Rubin. 1983. “The Central Role\nof the Propensity Score in Observational Studies for Causal\nEffects.” Biometrika 70 (1): 170–84. https://doi.org/10.1017/cbo9780511810725.016.\n\n\nRueschemeyer, Dietrich, Evelyne H. Stephens, and John D. Stephens. 1992.\nCapitalist Development and Democracy. Cambridge: Polity Pr.\n\n\nSargan, J. D. 1958. “The Estimation of Economic Relationships\nUsing Instrumental Variables.” Econometrica 26 (3): 393.\nhttps://doi.org/10.2307/1907619.\n\n\nTibshirani, Julie, Susan Athey, Erik Sverdrup, and Stefan Wager. 2024.\nGrf: Generalized Random Forests. https://CRAN.R-project.org/package=grf.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via\nthe Lasso.” Journal of the Royal Statistical Society Series\nB: Statistical Methodology 58 (1): 267–88.\n\n\nWeber, Max. 2004. Die Protestantische Ethik Und Der Geist Des\nKapitalismus. Vol. 1614. CH Beck.\n\n\nWickham, H., and an O’Reilly Media Company Safari. 2021. Mastering\nShiny. O’Reilly Media, Incorporated. https://books.google.de/books?id=ha1CzgEACAAJ.\n\n\nWooldridge, Jeffrey. 2010. Econometric Analysis of Cross Section and\nPanel Data. Second edition. Cambridge, Massachusetts: MIT.\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2023. R Markdown:\nThe Definitive Guide: The Definitive Guide. Chapman; Hall/CRC. https://doi.org/10.1201/9781138359444.\n\n\nXie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R\nMarkdown Cookbook. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown-cookbook.",
    "crumbs": [
      "Literatur"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kausalanalyse und maschinelles Lernen mit R",
    "section": "",
    "text": "1 Einleitung\nMit der zunehmenden Verfügbarkeit großer Datenmengen und damit einhergehend komplexeren Forschungsfragen ist der Einsatz fortschrittlicher statistischer Verfahren in den empirischen Wirtschaftswissenschaften notwendig geworden, die über traditionelle Regressionsansätze hinausgehen. Insbesondere Methoden der Kausalanalyse und des maschinellen Lernens haben sich in den letzten Jahren als Teil eines modernen Methoden-Baukastens etabliert und gewinnen auch jenseits des akademischen Bereichs zunehmend an Bedeutung: Die Fähigkeit mit statistischer Analyse in Produktivumgebungen fundierte Aussagen über Ursache-Wirkungs-Zusammenhänge zu treffen und leistungsfähige Vorhersagemodelle zu erstellen, ist für Absolventen wirtschaftswissenschaftlicher Studiengänge in vielen Berufsfeldern zu einer arbeitsmarktrelevanten Qualifikation geworden.\nDieses Online-Kompendium, Kausalanalyse und Maschinelles Lernen mit R (KMLR), bietet Studierenden eine interaktive Lernplattform, die darauf ausgerichtet ist, in quantitativen Lehrveranstaltungen vermittelte Inhalte im Selbststudium zu vertiefen und praktisch anzuwenden. KMLR bietet eine Vielzahl dynamischer Elemente, darunter interaktive Grafiken und Anwendungen mit Illustrationen statistischer Konzepte in Observable, sowie eine mit webR erzeugte R-Konsole.1 Hiemit können Studierende R-Code direkt im Browser ausführen, um in Fallstudien statistische Analysen eigenständig zu replizieren, die Ergebnisse nachzuvollziehen und zu lernen, wie diese reproduzierbar aufzubereitet werden können – ein wesentlicher Bestandteil moderner wissenschaftlicher Praxis. Für ein besseres Verständnis theoretischer Konzepte kann der vorgegebene R-Code modifiziert werden (beispielsweise zur Anpassung der Parameter von Schätzfunktionen oder Simulation), um durch eigenständiges Experimentieren ein tieferes Verständnis der behandelten Methoden zu erlangen.\nAls offene Bildungsressource (open educational ressource, OER) ermöglichen wir Dozierenden, ihre bestehenden Lehrveranstaltungen durch den Einsatz von KLMR um relevante Themen zu erweitern und ihren Studierenden eine innovative und praxisnahe Lernerfahrung zu bieten.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Einleitung</span>"
    ]
  },
  {
    "objectID": "index.html#inhalt",
    "href": "index.html#inhalt",
    "title": "Kausalanalyse und maschinelles Lernen mit R",
    "section": "\n1.1 Inhalt",
    "text": "1.1 Inhalt",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Einleitung</span>"
    ]
  },
  {
    "objectID": "index.html#konventionen-und-hinweise",
    "href": "index.html#konventionen-und-hinweise",
    "title": "Kausalanalyse und maschinelles Lernen mit R",
    "section": "\n1.2 Konventionen und Hinweise",
    "text": "1.2 Konventionen und Hinweise\nWir verwenden folgende Konventionen hinsichtlich der Formatierung und Einbindung von R-Code:\nMit so formatiertem Text referenzieren wir einzeilige Bestandteile komplexerer Code-Statements, häufig R-Objekte oder Funktionsargumente. Solcher Inline-Code wird häufig im Fließtext verwendet, um nicht ausführbare und ausführbare Code-Anweisungen besser unterscheidbar zu machen und insbesondere letztere zu erläutern.\nGrundsätzlich wird ausführbarer mehrzeiliger R-Code in Form von Blöcken dargestellt, die als Code-Chunks bezeichnet werden. In Code-Chunks wird gezeigt, wie die im Fließtext beschriebenen Schritte einer Analyse mit R umgesetzt werden können. Für eine bessere Nachvollziehbarkeit einzelner Bestandteile des Codes werden durch # gekennzeichnete # Kommentare verwendet.2\n2 Code-Chunks können per Klick auf das Clipboard-Symbol  (bei mouse-over) in die Zwischenablage kopiert werden.Beispiel: Der nächste Code-Chunk definiert das tibble-Objekt dat mit vier Beobachtungen der Variable Alter und berechnet das Durchschnittsalter.\n\nlibrary(tibble)\n\n# Beispiel-Datensatz erzeugen  \ndat &lt;- tibble(\n  Alter = c(28, 34, 23, 45)\n)\n\n# Durchschnittsalter berechnen\nmean(dat$Alter)\n\n[1] 32.5\n\n\nDas Ergebnis der Berechnung wird direkt unterhalb des Code-Chunks dargestellt. Der gezeigte Output eines Chunks entspricht meist einer solchen Ausgabe in der R-Konsole oder einer Grafik, wie im nächsten Beispiel.\n\n# Alter plotten\nplot(dat$Alter)\n\n\n\n\n\n\nAbbildung 1.1: Die mit plot(dat$Alter) erzeugte Grafik\n\n\n\n\nWebR\nEin Großteil des gezeigten R-Codes wird in WebR-Chunks eingebunden. Ein WebR-Chunk kann am besten als ein interaktiver R-Editor verstanden werden: R-Code kann per Klick auf Run Code zur Auswertung an eine im Hintergrund laufende R-Session übergeben werden.3 Wie bei “normalen” Code-Chunks werden die Ergebnisse direkt unter dem Webr-Chunk angezeigt. Der in einem WebR-Chunk vorgegebene Code kann beliebig angepasst werden. Ein Klick auf Start Over stellt den ursprünglichen Zustand wieder her.\n3 Eine Alternative ist die aus RStudio bekannte shortcut CTRL+Enter (MacOS: CMD+Enter).Der nachfolgende WebR-Chunk macht die Definition von dat und die Berechnung des Durchschnittsalters unmittelbar im Browser reproduzierbar.\n\n\n\n\n\n\n\n\nBei der Interkation mit WebR-Chunks ist Folgendes zu beachten:\nDie WebR-Session wird lokal im Browser ausgeführt. Die Performanz bei komplexeren Berechnungen ist daher von dem verwendeten Broweser und der Hardware des genutzen Endgerätes abghängig. Auch hinsichtlich typischer Limitationen von Mobilgeräten empfehlen wir, KLMR auf einem Notebook oder PC aufzurufen.\nDie WebR-Chunks werden bei jeder Aktualisierung des Browser-Tabs auf den durch uns (die Autoren) definierten Zustand zurückgesetzt. Das Bedeutet, dass durch den Nutzer geschriebener Code, berechnete Ergebnisse und definierte Objekte bei einer Aktualisierung gelöscht werden.\nInnerhalb eines Kapitels sind die WebR-Chunks durch die im Hintergrund laufende R-Session verknüpft und greifen damit auf den selben Speicher für R-Objekte zu. Wir nutzen diese Eigenschaft für prozedurale Analysen, in denen bspw. zunächst einen Datensatz geladen und aufbreitet wird, um im folgenden Chunk ein ökonometrisches Modell zu schätzen.\nSämtliche für die Analyse nötigen Pakete sind bereits vorinstalliert und müssen nicht durch den Nutzer installiert werden.4\n4 Eine Installation weiterer Pakete durch Nutzer*innen ist möglich, sofern diese Pakete in einer WebR-kompatiblen Version vorliegen. Installation und laden eines Pakets erfolgen mit library(Paketname).WebR-Chunks und “nicht-interaktiver” R-Code sind nicht miteinander verknüpft: R-Objekte, die wir in normalen Code-Chunks definieren, sind nicht notwendigerweise in WebR-Chunks verfügbar.\nInsbsondere in Fallstudien werden WebR-Chunks oft automatisch beim Laden des Kapitels ausgeführt, um die Nachvollziehbarkeit der Erläuterungen im Fließtext zu gewährleisten. Hierbei werden oft relevante R-Pakete heruntergeladen, installiert und Berechnungen durchgeführt. Die WebR-Chunk sind erst nach Abschluss dieses Prozesses interaktiv. Der Status des Installationsprozesses wird unten rechts im Browser angezeigt. Bei laufenden Berechnung erscheint in der Kopfzeile aller Chunks ein Indikator.5\n5 Laufende Vorbereitungen und Berechnungen werden durch  angezeigt.Der nächste Code-Chunk simuliert eine für 5 Sekunden laufende Berechnung und die anschließende Ausgabe eines Ergebnisses.\n\n\n\n\n\n\n\n\nInteraktive Visualisierungen\nIn theorie-lastigen Abschnitten des Kompendiums setzen wir interaktive Grafiken und Applikationen (Apps) ein, um komplexere mathematische Konzepte zu veranschaulichen. Diese meist in Observable-Notebooks implementierten Grafiken können durch Variation der Parametern über verschiedene Input-Elemente (meist Slider) angepasst werden.6 Die nachfolgende App ist ein Beispiel aus Kapitel 4.\n6 Der Quellcode dieser Grafiken ist unter den eingebetteten Links in Observable-Notebooks verfügbar. Die Notebooks sind ebenfalls OER und können in HTML-Lehrmaterialien eingebettet werden.\n\nOftmals geben wir detaillierte Erläuterungen zu voreingestellten Parameterkombination und geben weitere Beispiele zu interessanten Eigenschaften des gezeigten Studienobjekts, die für bestimmte Einstellungen beobachtet werden können.7 Diese Beispiele sind nicht erschöpfend und wir laden Lehrende und Studierende herzlich zum experimentieren ein!\n7 Hier zum Beispiel, dass der Schnittpunkt der gezeigten Wahrscheinlichkeitsfunktionen lediglich für \\(\\beta_0 = 0\\) unabhängig von \\(\\beta_1\\) ist.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Einleitung</span>"
    ]
  },
  {
    "objectID": "index.html#verschiedenes-und-danksagungen",
    "href": "index.html#verschiedenes-und-danksagungen",
    "title": "Kausalanalyse und maschinelles Lernen mit R",
    "section": "\n1.3 Verschiedenes und Danksagungen",
    "text": "1.3 Verschiedenes und Danksagungen\nUnterstützung mit Issues bei github, hypothesis\nKLMR und die verlinkten Lehrmaterialien sind im Rahmen des von der Stiftung Innovationen in der digitalen Hochschullehre geförderten Projekts Kausalanalyse, Maschinelles Lernen und Reproduzierbare Forschung im Förderprogramm Freiraum 2022 entstanden. Wir bedanken uns für die finanzielle Unterstützung.\nWir danken Ilona Braun für ihre unverzichtbare Hilfe bei der Projektadministration, Jens Klenke und Lennard Maßmann für anregende Impulse und ihre Hilfe bei Erprobung und Evaluation der Materialien im Lehrbetrieb sowie unseren wissenschaftlichen Hilfskräften für ihre Unterstützung bei Revisionen.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Einleitung</span>"
    ]
  },
  {
    "objectID": "Matching.html#zusammenfassung",
    "href": "Matching.html#zusammenfassung",
    "title": "\n6  Matching\n",
    "section": "\n6.6 Zusammenfassung",
    "text": "6.6 Zusammenfassung\nIn diesem Kapitel haben wir gezeigt, wie Matching-Methoden genutzt werden, um kausale Effekte aus nicht-experimentellen Daten zu schätzen. Selektierende Ansätze wie exaktes Matching, Coarsened Exact Matching und Propensity Score Matching stellen die Vergleichbarkeit von Behandlungs- und Kontrollgruppen her, indem gezielt Beobachtungen mit ähnlich verteilten Kovariablen ausgewählt werden. Gewichtendes Matching passt Beobachtungen durch Gewichte an, um die Verteilung von Counfoundern für beide Gruppen statistisch zu anzugleichen, ohne eine explizite Selektion von Beobachtungen vorzunehmen. Fortgeschrittene Verfahren kombinieren Matching mit Regression Adjustment, um verbleibende Unterschiede mit parametrischer Modellierung auszugleichen, während Doubly Robust Estimation selbst bei Fehlspezifikationen im Outcome-Modell oder des Matchings konsistente Schätzungen liefert. Bootstrap-Verfahren ermöglichen die Berechnung verlässlicher Standardfehler und verbessern die Robustheit der Inferenz, gerade für komplexe Matching-Ansätze.\nWir haben gezeigt, wie diese Methoden effizient in R implementiert werden können, insbesondere mit Paketen wie MatchIt und weightit für das Matching sowie boot für Bootstrap-basierte Ansätze.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbadie, Alberto, und Guido W. Imbens. 2008. „On the Failure of the Bootstrap for Matching Estimators.“ Econometrica. Journal of the Econometric Society 76 (6): 1537–57. https://doi.org/10.3982/ECTA6474.\n\n\nAbadie, Alberto, und Jann Spiess. 2022. „Robust Post-Matching Inference.“ Journal of the American Statistical Association 117 (538): 983–95. https://doi.org/10.1080/01621459.2020.1840383.\n\n\nAustin, P. 2011. „An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies“. Multivariate Behavioral Research 46 (3): 399–424. https://doi.org/10.1080/00273171.2011.568786.\n\n\nAustin, Peter C., und Dylan S. Small. 2014. „The use of bootstrapping when using propensity-score matching without replacement: A simulation study.“ Statistics in Medicine 33 (24): 4306–19. https://doi.org/10.1002/sim.6276.\n\n\nAustin, Peter C., und Elizabeth A. Stuart. 2017. „Estimating the Effect of Treatment on Binary Outcomes Using Full Matching on the Propensity Score.“ Statistical Methods in Medical Research 26 (6): 2505–25. https://doi.org/10.1177/0962280215601134.\n\n\nBodory, Hugo, Lorenzo Camponovo, Martin Huber, und Michael Lechner. 2020. „The Finite Sample Performance of Inference Methods for Propensity Score Matching and Weighting Estimators.“ Journal of Business & Economic Statistics. https://doi.org/10.2139/ssrn.2731969.\n\n\nHainmueller, Jens. 2012. „Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies“. Political Analysis 20 (1): 25–46. https://doi.org/10.1093/pan/mpr025.\n\n\nHájek, J. 1971. „Comment on ‚An essay on the logical foundations of survey sampling‘ by Basu, D“. Foundations of Statistical Inference 236.\n\n\nHill, Jennifer, und Jerome P. Reiter. 2006. „Interval estimation for treatment effects using propensity score matching. Statistics in Medicine“. Statistics in Medicine 25 (13): 2230–56. https://doi.org/10.1002/sim.2277.\n\n\nHirano, Keisuke, Guido Imbens, und Geert Ridder. 2003. „Efficient Estimation of Average Treatment Effects Using the Estimated Propensity Score.“ Econometrica 71 (4): 1161–89. https://doi.org/10.1111/1468-0262.00442.\n\n\nImbens. 2016. „Matching on the Estimated Propensity Score.“ Econometrica 84 (2): 781–807. https://doi.org/10.3982/ecta11293.\n\n\nLove, Thomas. 2004. „Graphical display of covariate balance“. Presentation.\n\n\nRosenbaum, Paul R., und Donald R. Rubin. 1983. „The central role of the propensity score in observational studies for causal effects“. Biometrika 70 (1): 170–84. https://doi.org/10.1017/cbo9780511810725.016.\n\n\nWooldridge, Jeffrey. 2010. Econometric Analysis of Cross Section and Panel Data. Second edition. Cambridge, Massachusetts: MIT.",
    "crumbs": [
      "Kausale Inferenz",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matching</span>"
    ]
  }
]