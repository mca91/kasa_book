<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="de" xml:lang="de"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>13&nbsp; Regularisierte Regression – Kausalanalyse und maschinelles Lernen mit R</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./svm.html" rel="next">
<link href="./SyntheticControl.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/live-runtime/live-runtime.js" type="module"></script>
<link href="site_libs/quarto-contrib/live-runtime/live-runtime.css" rel="stylesheet"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Keine Treffer",
    "search-matching-documents-text": "Treffer",
    "search-copy-link-title": "Link in die Suche kopieren",
    "search-hide-matches-text": "Zusätzliche Treffer verbergen",
    "search-more-match-text": "weitere Treffer in diesem Dokument",
    "search-more-matches-text": "weitere Treffer in diesem Dokument",
    "search-clear-button-title": "Zurücksetzen",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Abbrechen",
    "search-submit-button-title": "Abschicken",
    "search-label": "Suchen"
  }
}</script><script type="module" src="site_libs/quarto-ojs/quarto-ojs-runtime.js"></script><link href="site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">
<meta name="robots" content="noindex">
<script>
  MathJax = {
    tex: {
      tags: 'ams'  // should be 'ams', 'none', or 'all'
    }
  };
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.js" integrity="sha512-aoZChv+8imY/U1O7KIHXvO87EOzCuKO0GhFtpD6G2Cyjo/xPeTgdf3/bchB10iB+AojMTDkMHDPLKNxPJVqDcw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css">
<style>
  .panel-tabset .tab-content {
    border: 0;
    padding: 1em 0 0 0;
  }
  
  .panel-tabset .nav-item a {
    border-radius: 5px 5px 0 0;
  }
  
  .scientific_borders {
    border: 0;
    border-top: 2px solid black !important; 
    border-bottom: 2px solid black !important;
  }
  .table:not(.gt_table) > :not(caption)>*>* {
    border-bottom-width: 0;
  }
  .table:not(.gt_table) > thead {
    border-bottom: 1px solid black;
  }
  .soft-box-shadow {
    border: 1px solid rgba(233,236,239,.9) !important;
    border-radius: .5rem !important;
    background-color: rgba(250,250,250,.9) !important;
    box-shadow: 0px 1px 2px rgba(0,0,0,.1),
                0px 3px 7px rgba(0,0,0,.1),
                0px 12px 30px rgba(0,0,0,.08);
    margin-top: 2rem !important;
    margin-bottom: 2.5rem !important;
    padding: .25rem !important;
  }
  .obs-soft-box-shadow {
    border: 1px solid rgba(233,236,239,.9) !important;
    border-radius: .5rem !important;
    background-color: white !important;
    box-shadow: 0px 1px 2px rgba(0,0,0,.1),
                0px 3px 7px rgba(0,0,0,.1),
                0px 12px 30px rgba(0,0,0,.08);
    margin-top: 2rem !important;
    margin-bottom: 2.5rem !important;
    padding: .25rem !important;
  }
  
</style>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    
    var gt_tables = document.querySelectorAll(".gt_table");
    gt_tables.forEach(function(table) {
      table.classList.remove("table-striped");
    });
    
    var tables = document.querySelectorAll("table.table:not(.gt_table)");
    tables.forEach(function(table) {
      table.classList.remove("table-striped");
      table.classList.add("scientific_borders");
    });
    
    document.querySelectorAll("div.sourceCode").forEach(function(block) {
      block.classList.add("soft-box-shadow");
    });
    
    document.querySelectorAll("div.bg-white").forEach(function(block) {
      block.classList.remove("bg-white");
    });
    
const elements = document.querySelectorAll('[id^="qwebr-interactive-area"]');

    elements.forEach(element => {
        element.classList.add('box-shadow');
    });
    
    document.querySelectorAll('[id^="webr"]').forEach(function(block) {
      block.classList.add("box-shadow");
    });
    
        document.querySelectorAll('.card-header').forEach(function(block) {
      block.classList.add("box-shadow");
    });
    
  });
</script><style>
.qwebr-code-output-stdout {background-color: powderblue;}

.qwebr-button-run {
 width = 100%; 
}

.centered-caption {
   text-align: center;
}
</style>
<script src="https://cdn.jsdelivr.net/npm/quizdown@latest/public/build/quizdown.js"></script><script>quizdown.init();</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script><link rel="stylesheet" href="custom_styles.css">
</head>
<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Seitenleiste umschalten" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./RegReg.html">Machine Learning</a></li><li class="breadcrumb-item"><a href="./RegReg.html"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Regularisierte Regression</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Seitenleiste umschalten" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Suchen" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./"></a><a href="./index.html">Kausalanalyse mit R</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Lesemodus umschalten">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Suchen"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Einführung</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Grundlagen</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Abschnitt umschalten">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R_Einfuehrung.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistische Programmierung mit R</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./RR.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Reproduzierbarkeit</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Reg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Simulation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Simulation</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Kausale Inferenz</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Abschnitt umschalten">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Matching.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./FixedEffects.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Panel-Daten</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./IV.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">IV-Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./DiD.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Difference-in-Differences</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./EventStudies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Event Studies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./RDD.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Regression Discontiniuty Designs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./SyntheticControl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Synthetic Control</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Abschnitt umschalten">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./RegReg.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Regularisierte Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./svm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./trees.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Baum-basierte Methoden</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Machine Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Neuronale Netzwerke</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Literatur.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Literatur</span></a>
  </div>
</li>
    </ul>
</div>
    <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title"><a href="./index.html">Übersicht</a></h2>
   
  <ul>
<li>
<a href="#ridge-regression" id="toc-ridge-regression" class="nav-link active" data-scroll-target="#ridge-regression"><span class="header-section-number">13.1</span> Ridge Regression</a>
  <ul>
<li><a href="#eigenschaften-des-sch%C3%A4tzers" id="toc-eigenschaften-des-schätzers" class="nav-link" data-scroll-target="#eigenschaften-des-sch%C3%A4tzers"><span class="header-section-number">13.1.1</span> Eigenschaften des Schätzers</a></li>
  <li><a href="#ridge-regression-mit-glmnet" id="toc-ridge-regression-mit-glmnet" class="nav-link" data-scroll-target="#ridge-regression-mit-glmnet"><span class="header-section-number">13.1.2</span> Ridge Regression mit <code>glmnet</code></a></li>
  <li><a href="#beispiel-vorhersage-von-abschlussnoten-in-mathe" id="toc-beispiel-vorhersage-von-abschlussnoten-in-mathe" class="nav-link" data-scroll-target="#beispiel-vorhersage-von-abschlussnoten-in-mathe"><span class="header-section-number">13.1.3</span> Beispiel: Vorhersage von Abschlussnoten in Mathe</a></li>
  </ul>
</li>
  <li>
<a href="#lasso-regression" id="toc-lasso-regression" class="nav-link" data-scroll-target="#lasso-regression"><span class="header-section-number">13.2</span> Lasso Regression</a>
  <ul>
<li><a href="#lasso-ist-soft-thresholding" id="toc-lasso-ist-soft-thresholding" class="nav-link" data-scroll-target="#lasso-ist-soft-thresholding"><span class="header-section-number">13.2.1</span> Lasso ist Soft Thresholding</a></li>
  <li><a href="#berechnung-der-lasso-l%C3%B6sung-mit-dem-lars-algorithmus" id="toc-berechnung-der-lasso-lösung-mit-dem-lars-algorithmus" class="nav-link" data-scroll-target="#berechnung-der-lasso-l%C3%B6sung-mit-dem-lars-algorithmus"><span class="header-section-number">13.2.2</span> Berechnung der Lasso-Lösung mit dem LARS-Algorithmus</a></li>
  <li><a href="#wahl-des-regularisierungsparameters-lambda-f%C3%BCr-den-lasso-sch%C3%A4tzer" id="toc-wahl-des-regularisierungsparameters-lambda-für-den-lasso-schätzer" class="nav-link" data-scroll-target="#wahl-des-regularisierungsparameters-lambda-f%C3%BCr-den-lasso-sch%C3%A4tzer"><span class="header-section-number">13.2.3</span> Wahl des Regularisierungsparameters <span class="math inline">\(\lambda\)</span> für den Lasso-Schätzer</a></li>
  </ul>
</li>
  <li>
<a href="#vergleich-von-lasso--und-ridge-regression-mit-simulation" id="toc-vergleich-von-lasso--und-ridge-regression-mit-simulation" class="nav-link" data-scroll-target="#vergleich-von-lasso--und-ridge-regression-mit-simulation"><span class="header-section-number">13.3</span> Vergleich von Lasso- und Ridge-Regression mit Simulation</a>
  <ul>
<li><a href="#sec-pdz" id="toc-sec-pdz" class="nav-link" data-scroll-target="#sec-pdz"><span class="header-section-number">13.3.1</span> Prognosegüte in diversen Szenarien</a></li>
  <li><a href="#visualisierung-des-bias-variance-tradeoffs-bei-prognosen" id="toc-visualisierung-des-bias-variance-tradeoffs-bei-prognosen" class="nav-link" data-scroll-target="#visualisierung-des-bias-variance-tradeoffs-bei-prognosen"><span class="header-section-number">13.3.2</span> Visualisierung des Bias-Variance-Tradeoffs bei Prognosen</a></li>
  </ul>
</li>
  <li>
<a href="#inferenz-f%C3%BCr-treatment-effekt-sch%C3%A4tzung-mit-vielen-variablen" id="toc-inferenz-für-treatment-effekt-schätzung-mit-vielen-variablen" class="nav-link" data-scroll-target="#inferenz-f%C3%BCr-treatment-effekt-sch%C3%A4tzung-mit-vielen-variablen"><span class="header-section-number">13.4</span> Inferenz für Treatment-Effekt-Schätzung mit vielen Variablen</a>
  <ul>
<li><a href="#case-study-makro%C3%B6konomisches-wachstum" id="toc-case-study-makroökonomisches-wachstum" class="nav-link" data-scroll-target="#case-study-makro%C3%B6konomisches-wachstum"><span class="header-section-number">13.4.1</span> Case Study: Makroökonomisches Wachstum</a></li>
  </ul>
</li>
  </ul></nav>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./RegReg.html">Machine Learning</a></li><li class="breadcrumb-item"><a href="./RegReg.html"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Regularisierte Regression</span></a></li></ol></nav><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-regreg" class="quarto-section-identifier"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Regularisierte Regression</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><p>In diesem Kapitel betrachten wir Varianten von Koeffizientenschätzern im linearen Modell <span class="math display">\[\begin{align}
Y_i = \beta_1 X_{1,i} + \dots + \beta_k X_{k,i} + u_i, \quad i = 1,\dots,n,\label{eq:slm}
\end{align}\]</span> deren Motivation die Schätzung von <span class="math inline">\(\boldsymbol{\beta} := (\beta_1, \dots,\beta_k)'\)</span> in Anwendungen ist, in denen der KQ-Schätzer <span class="math display">\[\begin{align}
  \begin{split}
  \widehat{\boldsymbol{\beta}} =&amp;\, \arg\min_{\boldsymbol{\beta}}\mathrm{RSS}(\boldsymbol{\beta})\\
  =&amp;\,  \arg\min_{\boldsymbol{\beta}}  \sum_{i=1}^n\left(Y_i-\beta_1 X_{1,i} + \dots + \beta_k X_{k,i}\right)^2
  \end{split}\label{eq:KQLoss}
\end{align}\]</span> keine stabile Schätzung zulässt oder nicht eindeutig definiert ist, und damit gar nicht erst berechnet werden kann. Solche Szenarien ergeben sich in der empirischen Forschung, wenn die Regressoren stark korreliert sind und/oder das Modell viele Regressoren enthält (<span class="math inline">\(k\lesssim n\)</span>), oder das Regressionsproblem hoch-dimensional ist (<span class="math inline">\(k&gt;n\)</span>).</p>
<p>Regularisierte Regressionsschätzer begegnen dieser Problematik mit einer Modifikation der Verlustfunktion <span class="math inline">\(\mathrm{RSS}\)</span> in <span class="math inline">\(\eqref{eq:KQLoss}\)</span>, <span class="math display">\[\begin{align}
  \mathrm{RSS}(\boldsymbol{\beta}, p, \lambda) := \mathrm{RSS}(\boldsymbol{\beta}) + \lambda\lVert\boldsymbol{\beta}\rVert_p.
\end{align}\]</span> Hierbei ist <span class="math inline">\(\lambda&gt;0\)</span> ein Tuning-Parameter und <span class="math inline">\(p\geq1\)</span> definiert die <span class="math inline">\(p\)</span>-Norm des Koeffizientenvektors, <span class="math display">\[\begin{align}
  \lVert\boldsymbol{\beta}\rVert_p := \left(\sum_{j=1}^k \lvert\beta_j\rvert^{p}\right)^{1/p}&gt;0.\label{eq:pnorm}
\end{align}\]</span></p>
<p>Wegen <span class="math inline">\(\lambda\lVert\boldsymbol{\beta}\rVert_p&gt;0\)</span> kann die <span class="math inline">\(p\)</span>-Norm des Koeffizientenvektors <span class="math inline">\(\boldsymbol{\beta}\)</span> das Optimierungsproblem <span class="math display">\[\min_{\boldsymbol{\beta}} \mathrm{RSS}(\boldsymbol{\beta}, p, \lambda) \vert\, p,\, \lambda\]</span> derart restringieren, dass die geschätzten Koeffizienten <span class="math display">\[\begin{align*}
  \widehat{\boldsymbol{\beta}}_{p,\,\lambda} := \arg\min_{\boldsymbol{\beta}} \mathrm{RSS}(\boldsymbol{\beta}, p, \lambda)
\end{align*}\]</span> im Erwartungswert absolut kleiner ausfallen als bei der KQ-Schätzung: Der Schätzer ist in Richtung 0 verzerrt.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Dieser Effekt der Regularisierung wird in der Literatur als <em>Shrinkage</em> bezeichnet.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Beachte, dass für <span class="math inline">\(\lambda=0\)</span> die Verlustfunktion des KQ-Schätzers folgt.</p></div><div id="fn2"><p><sup>2</sup>&nbsp;D.h. wir wählen <span class="math inline">\(p\)</span>, um einen Schätzer mit für die konkrete Anwendung hilfreichen Eigenschaften zu erhalten.</p></div></div><p>Die grundlegenden Eigenschaften des Schätzers <span class="math inline">\(\widehat{\boldsymbol{\beta}}_{p,\,\lambda}\)</span> werden maßgeblich durch den Parameter <span class="math inline">\(p\)</span> bestimmt, der hinsichtlich des zu lösenden Regressionsproblems <em>a priori</em> gewählt wird.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>Shrinkage ist eine Motivation für die Anwendung regularisierter Schätzer in Modellen, die auch mit KQ geschätzt werden könnten. Um dies zu verstehen, nehmen wir an, dass die Gauss-Markov-Annahmen in <span class="math inline">\(\eqref{eq:slm}\)</span> gelten. Dann hat der KQ-Schätzer die kleinste Varianz unter allen <em>unverzerrten</em> Schätzern. Aufgrund der Shrinkage fallen regularisierte Schätzer zwar nicht unter das Gauss-Markov-Theorem, können dafür aber eine geringere Varianz haben als KQ. Schätzer mit solchen Eigenschaften sind nützlich, wenn eine unverzerrte Schätzung von <span class="math inline">\(\boldsymbol{\beta}\)</span> nicht unser primäres Ziel ist: Für Vorhersagen kann es hilfreich sein, etwas Verzerrung bei der Koeffizientenschätzung in Kauf zu nehmen, um eine hinreichend große Varianzreduktion zu erreichen, sodass ein geringerer erwarteter Vorhersagefehler als für KQ resultiert. Hierbei liegt, eine Abwägung zwischen Verzerrung und Varianz (<em>Bias Variance Tradeoff</em>) vor, der durch den Regularisierungsparameter <span class="math inline">\(\lambda\)</span> beeinflusst wird.</p>
<p>Für die Berechnung des Schätzers in empirischen Anwendungen wird <span class="math inline">\(\lambda\)</span> meist datengetrieben (mit <a href="https://de.wikipedia.org/wiki/Kreuzvalidierungsverfahren">Cross Validation</a> oder einem Informationskriterium) geschätzt oder mit einer analytisch fundierten Faustregel gewählt.</p>
<p>Nachfolgend betrachten wir zwei häufig verwendete regularisierte Schätzer, die sich durch die Wahl <span class="math inline">\(p=1\)</span> (Lasso Regression) bzw. <span class="math inline">\(p=2\)</span> (Ridge Regression) ergeben und illustrieren ihre Anwendung mit R.</p>
<section id="ridge-regression" class="level2 page-columns page-full" data-number="13.1"><h2 data-number="13.1" class="anchored" data-anchor-id="ridge-regression">
<span class="header-section-number">13.1</span> Ridge Regression</h2>
<p>Ridge Regression wurde von <span class="citation" data-cites="HoerlKennard1970">Hoerl und Kennard (<a href="Literatur.html#ref-HoerlKennard1970" role="doc-biblioref">1970</a>)</span> als Alternative zur KQ-Schätzung bei hoch-korrelierten Regressoren eingeführt. Die Verlustfunktion lautet <span class="math display">\[\begin{align}
  \mathrm{RSS}(\boldsymbol{\beta},p=2,\lambda) = \mathrm{RSS}(\boldsymbol{\beta}) + \lambda \lVert\boldsymbol{\beta}\rVert_2,\label{eq:ridgeloss}
\end{align}\]</span> d.h. der Parameter <span class="math inline">\(\lambda\)</span> reguliert den Einfluss eines <span class="math inline">\(\ell_2\)</span>-Strafterms <span class="math display">\[\begin{align*}
  \lVert\boldsymbol{\beta}\rVert_2 = \sqrt{\sum_{j=1}^k\beta_j^2}
\end{align*}\]</span> auf die Verlustfunktion <span class="math inline">\(\mathrm{RSS}(\boldsymbol{\beta},p=2,\lambda)\)</span>. Der Ridge-Schätzer ergibt sich als <span class="math display">\[\begin{align}
  \widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda := \arg\min_{\boldsymbol{\beta}}\mathrm{RSS}(\boldsymbol{\beta}) + \lambda \lVert\boldsymbol{\beta}\rVert_2.\label{eq:ridgereg}
\end{align}\]</span></p>
<p>Für Das Optimierungsproblem <span class="math inline">\(\eqref{eq:ridgereg}\)</span> kann wir aus den Bedingungen 1. Ordnung <span class="math display">\[\begin{align}
  -2\boldsymbol{X}'(\boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\beta}) + 2\lambda\boldsymbol{\beta} = \boldsymbol{0}
\end{align}\]</span> die analytische Lösung <span class="math display">\[\begin{align}
  \widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda = (\boldsymbol{X}'\boldsymbol{X} + \lambda\boldsymbol{I}_p)^{-1}\boldsymbol{X}'\boldsymbol{Y},\label{eq:ridgecf}
\end{align}\]</span> bestimmt werden, wobei <span class="math inline">\(\boldsymbol{I}_k\)</span> die <span class="math inline">\(k\times k\)</span> Einheitsmatrix ist. Aus Gleichung <span class="math inline">\(\eqref{eq:ridgecf}\)</span> kann die Wirkungsweise des Strafterms <span class="math inline">\(\lambda \lVert\boldsymbol{\beta}\rVert_2\)</span> abgeleitet werden: Ridge Regression modifiziert die Diagonale der zu invertierenden Matrix <span class="math inline">\(\boldsymbol{X}'\boldsymbol{X}\)</span> durch Addition von <span class="math inline">\(\lambda&gt;0\)</span>. Dies ist hilfreich, wenn</p>
<ul>
<li><p><span class="math inline">\(k\geq n\)</span> und damit <span class="math inline">\(\boldsymbol{X}'\boldsymbol{X}\)</span> nicht invertiertbar (singulär) ist. Dann kann der KQ-Schätzer nicht berechnet werden.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> Die Inverse <span class="math inline">\((\boldsymbol{X}'\boldsymbol{X} + \lambda\boldsymbol{I}_p)^{-1}\)</span> hingegen existiert unter milden Bedingungen.</p></li>
<li><p>hohe Kollinearität vorliegt, sodass <span class="math inline">\((\boldsymbol{X}'\boldsymbol{X})^{-1}\)</span> zwar existiert, aber zu einer instablilen KQ-Schätzung mit hoher Varianz führt.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Beispiel: <code>X &lt;- matrix(rnorm(100), ncol = 10)</code>. Vergleiche <code>solve(t(X) %*% X)</code> und <code>solve(t(X) %*% X + diag(.01, nrow = 10))</code></p></div></div><p>Für eine grafische Betrachtung des Optimierungskalküls <span class="math inline">\(\eqref{eq:ridgereg}\)</span> betrachten wir die äquivalente Darstellung als Lagrange-Problem <span class="math display">\[\begin{align}
  \widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda := \arg\min_{\lVert\boldsymbol{\beta}\rVert&lt;t}\mathrm{RSS}(\boldsymbol{\beta}).\label{eq:ridgeLg}
\end{align}\]</span> In der folgenden interaktiven Grafik illustrieren wir das Optimierungsproblem <span class="math inline">\(\eqref{eq:ridgeLg}\)</span> sowie den resultierenden Schätzer der Koeffizienten <span class="math inline">\((\beta_1, \beta_2)\)</span> in einem multiplen Regressionsmodell mit den Regressoren <span class="math inline">\(X_1\)</span> und <span class="math inline">\(X_2\)</span>.</p>
<ul>
<li><p>Die blaue Ellipse ist die Menge aller Schätzwerte <span class="math inline">\(\left(\widehat\beta_{1},\, \widehat\beta_{2}\right)\)</span> für den angegebenen Wert von <span class="math inline">\(\mathrm{RSS}\)</span>. Im Zentrum der Ellipse liegt der KQ-Schätzer, welcher <span class="math inline">\(\mathrm{RSS}\)</span> minimiert.</p></li>
<li><p>Der blaue Kreis ist die Menge aller Koeffizienten-Paare <span class="math inline">\((\beta_1, \beta_2)\)</span>, welche die Restriktion <span class="math inline">\(\beta_1^2 + \beta_2^2\leq t\)</span> erfüllen. Beachte, dass die Größe des Kreises nur durch den Parameter <span class="math inline">\(t\)</span> bestimmt wird, welcher für einen vorgegebenen Wertebereich variiert werden kann.</p></li>
<li><p>Der blaue Punkt ist der Ridge-Schätzer <span class="math inline">\((\widehat\beta^R_{1,t},\, \widehat\beta^R_{2,t})\)</span>. Dieser ergibt sich als Schnittpunkt zwischen der blauen <span class="math inline">\(\mathrm{RSS}\)</span>-Ellipse und der Restriktionsregion und variiert mit <span class="math inline">\(t\)</span>. Die gestrichelte rote Kurve zeigt den Ridge-Lösungspfad.</p></li>
<li><p>Für kleine Werte <span class="math inline">\(t\)</span> drückt die Shrinkage die geschätzten Koeffizienten Richtung 0, wobei der Lösungspfad i.d.R. nicht-linear verläuft, d.h. die Shrinkage auf den Koeffizienten ist grundsätzlich unterschiedlich. Die Lösung <span class="math inline">\((\widehat\beta^R_{1,t},\, \widehat\beta^R_{2,t}) = (0,0)\)</span> existiert nur als Grenzwert für <span class="math inline">\(t\to0\)</span>.</p></li>
<li><p>Beachte, dass der Effekt von <span class="math inline">\(t\)</span> auf die Schätzung umgekehrt für <span class="math inline">\(\lambda\)</span> verläuft: Größere <span class="math inline">\(\lambda\)</span> führen zu stärkerer Regularisierung.</p></li>
</ul>
<iframe width="100%" height="500" frameborder="0" src="https://observablehq.com/embed/45a59e74a4330581?cells=viewof+m%2Cchart114">
</iframe>
<section id="eigenschaften-des-schätzers" class="level3 page-columns page-full" data-number="13.1.1"><h3 data-number="13.1.1" class="anchored" data-anchor-id="eigenschaften-des-schätzers">
<span class="header-section-number">13.1.1</span> Eigenschaften des Schätzers</h3>
<p>Der Ridge-Schätzer <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda\)</span> ist nicht invariant gegenüber der Skalierung der Regressoren. Für empirische Daten sollte daher vorab eine Standardisierung der erklärenden Variablen durchgeführt werden.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> Um die Eigenschaften des Ridge-Schätzers besser zu verstehen, betrachten wir hier den Fall orthonormaler Regressoren <span class="math inline">\(\boldsymbol{X}_j\)</span>.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> Dann ist <span class="math display">\[\begin{align}
  \widehat{\beta}^{\mathrm{R}}_{\lambda,\,j} = (1+\lambda)^{-1} \cdot\widehat{\beta}_j,\quad j = 1,\dots,k,\label{eq:ridgeortho}
\end{align}\]</span> d.h. der Ridge-Schätzer skaliert die KQ-Lösung mit einem von <span class="math inline">\(\lambda\)</span> abhängigen Faktor.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;Bspw. mit der Funktion <code><a href="https://rdrr.io/r/base/scale.html">scale()</a></code>.</p></div><div id="fn5"><p><sup>5</sup>&nbsp;Orthonormalität heißt <span class="math inline">\(\boldsymbol{X}_i'\boldsymbol{X}_j = 1\)</span> für <span class="math inline">\(i=j\)</span> und <span class="math inline">\(0\)</span> sonst. Dann ist <span class="math inline">\(\boldsymbol{X}\)</span>’<span class="math inline">\(\boldsymbol{X} = \boldsymbol{I}_k\)</span>.</p></div><div id="fn6"><p><sup>6</sup>&nbsp;<span class="math inline">\((1+\lambda)^{-1}\)</span> wird auch als <em>Shrinkage-Faktor</em> bezeichnet.</p></div></div><p>Wir illustrieren dies, indem wir den Zusammenhang zwischen KQ- und Ridge-Schätzer im orthonormalen Fall als R-Funktion <code>ridge_ortho()</code> implementieren und für die Parameterwerte <span class="math inline">\(\lambda\in\{0,0.5,2\}\)</span> plotten.</p>
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Funktion für Rige Regression bei orthonormalen Regressoren</span></span>
<span><span class="va">ridge_ortho</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">KQ</span>, <span class="va">lambda</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fl">1</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span> <span class="op">+</span> <span class="va">lambda</span><span class="op">)</span> <span class="op">*</span> <span class="va">KQ</span></span>
<span><span class="op">}</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell page-columns page-full">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># KQ-Schätzer gegen Ridge-Schätzer plotten</span></span>
<span><span class="va">dat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span>KQ <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">.01</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">dat</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_function.html">geom_function</a></span><span class="op">(</span>fun <span class="op">=</span> <span class="va">ridge_ortho</span>, </span>
<span>                args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>lambda <span class="op">=</span>  <span class="fl">0</span><span class="op">)</span>, </span>
<span>                lty <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_function.html">geom_function</a></span><span class="op">(</span>fun <span class="op">=</span> <span class="va">ridge_ortho</span>, </span>
<span>                args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>lambda <span class="op">=</span> <span class="fl">.5</span><span class="op">)</span>, </span>
<span>                col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_function.html">geom_function</a></span><span class="op">(</span>fun <span class="op">=</span> <span class="va">ridge_ortho</span>, </span>
<span>                args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>lambda <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>, </span>
<span>                col <span class="op">=</span> <span class="st">"blue"</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/lims.html">xlim</a></span><span class="op">(</span><span class="op">-</span><span class="fl">.4</span>, <span class="fl">.4</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">"KQ-Schätzer von beta_1"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"Ridge-Schätzer von beta_1"</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display page-columns page-full">
<div id="fig-ridgeortho" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-ridgeortho-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="RegReg_files/figure-html/fig-ridgeortho-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-ridgeortho-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Abbildung&nbsp;13.1: Shrinkage des OLS-Schätzers bei Ridge Regression
</figcaption></figure>
</div>
</div>
</div>
<p><a href="#fig-ridgeortho" class="quarto-xref">Abbildung&nbsp;<span>13.1</span></a> zeigt, dass der Ridge-Schätzer eine lineare Transformation des KQ-Schätzers (gestrichelte Linie) ist. Größere Werte des Regularisierungsparameters <span class="math inline">\(\lambda\)</span> führen zu stärkerer Shrinkage des Koeffizientenschätzers in Richtung 0. Die <span class="math inline">\(\ell_2\)</span>-Norm führt zu proportional zum Absolutwert des KQ-Schätzers verlaufender Shrinkage: Größere Koeffizienten werden stärker bestraft als kleine Koeffizienten.</p>
<p>Die Eigenschaft <span class="math display">\[\mathrm{E}\left(\widehat{\boldsymbol{\beta}}^{\mathrm{R}}_{\lambda,\,j}\right) = (1+\lambda)^{-1} \cdot \beta_j\]</span> zeigt, dass <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{\mathrm{R}}_{\lambda,\,j}\)</span> (für fixes <span class="math inline">\(\lambda&gt;0\)</span>) nicht erwartungstreu für <span class="math inline">\(\beta_j\)</span> ist. Weiterhin ist <span class="math display">\[\begin{align*}
  \mathrm{Var}\left(\widehat{\beta}^{\mathrm{R}}_{\lambda,\,j}\right) =&amp;\,
  \mathrm{Var}\left(\widehat{\beta}_j\right) \cdot \left(\frac{\lambda}{1+\lambda^2}\right)\\
    =&amp;\, \sigma^2\cdot \left(\frac{\lambda}{1+\lambda^2}\right),
\end{align*}\]</span> wobei <span class="math inline">\(\sigma^2\)</span> die Varianz des Regressionsfehlers <span class="math inline">\(u\)</span> ist. Wegen <span class="math inline">\(\lambda&lt;(1+\lambda)^2\)</span> für <span class="math inline">\(\lambda&gt;0\)</span> gilt <span class="math display">\[\mathrm{Var}\left(\widehat{\beta}^{\mathrm{R}}_{\lambda,\,j}\right)&lt;\mathrm{Var}\left(\widehat{\beta}_j\right).\]</span> Der Ridge-Schätzer hat also eine kleinere Varianz als der KQ-Schätzer. Diese Eigenschaften können auch für korrelierte Regressoren gezeigt werden.</p>
</section><section id="ridge-regression-mit-glmnet" class="level3 page-columns page-full" data-number="13.1.2"><h3 data-number="13.1.2" class="anchored" data-anchor-id="ridge-regression-mit-glmnet">
<span class="header-section-number">13.1.2</span> Ridge Regression mit <code>glmnet</code>
</h3>
<p>Wir zeigen nun anhand simulierter Daten, wie der Ridge-Lösungspfad mit dem R-Paket <code>glmnet</code> berechnet werden kann. Wir erzeugen zunächst Daten gemäß der Vorschrift <span class="math display">\[\begin{align}
  \begin{split}
  Y_i =&amp;\, \boldsymbol{X}_i' \boldsymbol{\beta} + u_i,\\
  \\
  \beta_j =&amp;\,  \frac{5}{j^2}, \qquad\qquad\ j=1,\dots,5,\\
  \beta_j =&amp;\, -\frac{5}{(j-5)^2}, \quad j=6,\dots,10,\\
  \\
  \boldsymbol{X}_i \sim&amp;\, N(\boldsymbol{0}, \boldsymbol{\Sigma}), \quad u_i \overset{u.i.v.}{\sim} N(0, 1), \quad i = 1,\dots,25.
  \end{split} \label{eq:ridgedgp1}
\end{align}\]</span> Hierbei wird <span class="math inline">\(\boldsymbol{\Sigma}\)</span> so definiert, dass jeder Regressor <span class="math inline">\(N(0,1)\)</span>-verteilt ist und eine Korrelation von <span class="math inline">\(0.8\)</span> mit allen anderen Regressoren aufweist. Mit der Vorschrift für die <span class="math inline">\(\beta_j\)</span> stellen wir sicher, dass es wenige Variablen gibt, die <span class="math inline">\(Y\)</span> stark beeinflussen, da der Absolutbetrag der Koeffizienten in <span class="math inline">\(j\)</span> abnimmt.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;Für bessere Interpretierbarkeit der Grafischen Auswertung, wählen wir positive und negative Koeffizienten mit gleichem Bertag.</p></div></div><div class="cell">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">gendata</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1234</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Parameter definieren</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">80</span></span>
<span><span class="va">k</span> <span class="op">&lt;-</span> <span class="fl">10</span></span>
<span></span>
<span><span class="va">coefs</span> <span class="op">&lt;-</span> <span class="fl">5</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">k</span><span class="op">/</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span>
<span><span class="va">beta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">coefs</span>, <span class="op">-</span><span class="va">coefs</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Beobachtungen simulieren</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/gendata/man/genmvnorm.html">genmvnorm</a></span><span class="op">(</span></span>
<span>    k <span class="op">=</span> <span class="va">k</span>, </span>
<span>    cor <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">.8</span>, <span class="op">(</span><span class="va">k</span><span class="op">^</span><span class="fl">2</span><span class="op">-</span><span class="va">k</span><span class="op">)</span><span class="op">/</span><span class="fl">2</span><span class="op">)</span>, </span>
<span>    n <span class="op">=</span> <span class="va">N</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="va">X</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">beta</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Wir schätzen nun ein Modell mit allen 10 Regressoren mit <code>glmnet</code>. Beachte, dass für den Ridge-Strafterm <code>alpha = 0</code> gesetzt werden muss.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn8"><p><sup>8</sup>&nbsp;<code>alpha</code> ist ein Mischparameter im Algorithmus für <a href="https://en.wikipedia.org/wiki/Elastic_net_regularization">elastic net</a>, siehe <code><a href="https://glmnet.stanford.edu/reference/glmnet.html">?glmnet</a></code>.</p></div></div><div class="cell">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://glmnet.stanford.edu">glmnet</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Ridge-Regression anpassen</span></span>
<span><span class="va">ridge_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet</a></span><span class="op">(</span></span>
<span>  x <span class="op">=</span> <span class="va">X</span>, </span>
<span>  y <span class="op">=</span> <span class="va">Y</span>, </span>
<span>  alpha <span class="op">=</span> <span class="fl">0</span> <span class="co"># für Ridge-Strafterm</span></span>
<span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Der Lösungspfad der Ridge-Schätzung kann nach Transformation der geschätzen Koeffizienten und der zugehörigen <span class="math inline">\(\lambda\)</span>-Werte in ein langes Format überführt und komfortabel mit <code>ggplot2</code> dargestellt werden.</p>
<div class="cell page-columns page-full">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Lambda-Sequenz auslesen</span></span>
<span><span class="va">lambdas</span> <span class="op">&lt;-</span> <span class="va">ridge_fit</span><span class="op">$</span><span class="va">lambda</span></span>
<span></span>
<span><span class="co"># Ridge-Schätzung für Lambdas im langen Format </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">ridge_fit</span><span class="op">$</span><span class="va">beta</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://tibble.tidyverse.org/reference/as_tibble.html">as_tibble</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://tibble.tidyverse.org/reference/rownames.html">rownames_to_column</a></span><span class="op">(</span><span class="st">"Variable"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer</a></span><span class="op">(</span><span class="op">-</span><span class="va">Variable</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">Variable</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>lambda <span class="op">=</span> <span class="va">lambdas</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  </span>
<span>  <span class="co"># Grafik mit ggplot erzeugen</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span></span>
<span>    mapping <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span></span>
<span>      x <span class="op">=</span> <span class="va">lambda</span>, </span>
<span>      y <span class="op">=</span> <span class="va">value</span>, </span>
<span>      col <span class="op">=</span> <span class="va">Variable</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"gesch. Koeffizienten"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_continuous.html">scale_x_log10</a></span><span class="op">(</span><span class="st">"log_10(lambda)"</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display page-columns page-full">
<div id="fig-ridgesolpath" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-ridgesolpath-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="RegReg_files/figure-html/fig-ridgesolpath-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-ridgesolpath-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Abbildung&nbsp;13.2: Lösungspfad für Ridge-Schätzung
</figcaption></figure>
</div>
</div>
</div>
<p><a href="#fig-ridgesolpath" class="quarto-xref">Abbildung&nbsp;<span>13.2</span></a> zeigt den nicht-linearen Verlauf der Shrinkage auf den geschätzten Modellkoeffizienten. Die Koeffizienten werden mit zunehmendem <span class="math inline">\(\lambda\)</span> von der KQ-Lösung ausgehend (linkes Ende der Skala) in Richtung 0 gezwungen.</p>
<p>Über die Funktion <code><a href="https://glmnet.stanford.edu/reference/cv.glmnet.html">cv.glmnet()</a></code> kann ein optimales <span class="math inline">\(\lambda\)</span> mit Cross Validation (CV) ermittelt werden. Ähnlich wie bei <code><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet()</a></code> wird für die Validierung automatisch eine <span class="math inline">\(\lambda\)</span>-Sequenz erzeugt. Wir nutzen <code><a href="https://ggplot2.tidyverse.org/reference/autoplot.html">autoplot()</a></code> aus dem R-Paket <code>ggfortify</code> für die Visualisierung der Ergebnisse mit <code>ggplot2</code>.</p>
<div class="cell page-columns page-full">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/sinhrks/ggfortify">ggfortify</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Cross-validierte Bestimmung von lambda</span></span>
<span><span class="va">ridge_cvfit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/cv.glmnet.html">cv.glmnet</a></span><span class="op">(</span></span>
<span>  y <span class="op">=</span> <span class="va">Y</span>, </span>
<span>  x <span class="op">=</span> <span class="va">X</span>, </span>
<span>  intercept <span class="op">=</span> <span class="cn">F</span>,</span>
<span>  alpha <span class="op">=</span> <span class="fl">0</span></span>
<span><span class="op">)</span> </span>
<span></span>
<span><span class="co"># Ergebnisse plotten</span></span>
<span><span class="va">ridge_cvfit</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/autoplot.html">autoplot</a></span><span class="op">(</span>label.n <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display page-columns page-full">
<div id="fig-ridgecvplot" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-ridgecvplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="RegReg_files/figure-html/fig-ridgecvplot-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-ridgecvplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Abbildung&nbsp;13.3: Lösungspfad für Ridge-Schätzung
</figcaption></figure>
</div>
</div>
</div>
<p><a href="#fig-ridgecvplot" class="quarto-xref">Abbildung&nbsp;<span>13.3</span></a> zeigt <code>ridge_cvfit$lambda.min</code>, das optimale <span class="math inline">\(\lambda\)</span> mit dem geringsten CV Mean-Squarred-Error (linke gestrichelte Linie) und <code>ridge_cvfit$lambda.1se</code>, das größte <span class="math inline">\(\lambda\)</span>, welches innerhalb einer Standardabweichung entfernt ist (rechte gestrichelte Linie).<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> Wir berechnen die Schätzung für <code>lambda.min</code>.</p>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;Die Wahl von <code>lambda.1se</code> ist eine Heuristik, welche die Schätzunsicherheit berücksichtigt und zu einem “sparsameren” Modell tendiert.</p></div></div><div class="cell">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="op">(</span></span>
<span>  <span class="va">ridge_coefs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span></span>
<span>    object <span class="op">=</span> <span class="va">ridge_cvfit</span>, </span>
<span>    s <span class="op">=</span> <span class="va">ridge_cvfit</span><span class="op">$</span><span class="va">lambda.min</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>11 x 1 sparse Matrix of class "dgCMatrix"
                    s1
(Intercept)  .        
X1           4.1302194
X2           1.0245661
X3           0.3139297
X4           0.5697498
X5           0.2928664
X6          -4.1693524
X7          -0.7509305
X8          -0.3844761
X9          -0.3841997
X10         -0.4078514</code></pre>
</div>
</div>
<p>Wir schätzen das Modell nun mit KQ und vergleichen die Koeffizienten mit der Ridge-Schätzung.</p>
<div class="cell page-columns page-full">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># KQ-Schätzung durchführen</span></span>
<span><span class="va">KQ_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Koeffizienten auslesen und transformieren:</span></span>
<span><span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span></span>
<span>  Ridge <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">ridge_coefs</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="fl">11</span>, <span class="op">]</span>,</span>
<span>  KQ <span class="op">=</span> <span class="va">KQ_fit</span><span class="op">$</span><span class="va">coefficients</span></span>
<span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>j <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer</a></span><span class="op">(</span></span>
<span>    cols <span class="op">=</span> <span class="va">Ridge</span><span class="op">:</span><span class="va">KQ</span>, </span>
<span>    names_to <span class="op">=</span> <span class="st">"Methode"</span>, </span>
<span>    values_to <span class="op">=</span> <span class="st">"Koeffizient"</span></span>
<span>  <span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span></span>
<span><span class="co"># Bar-Plot für Koeffizientenvergleich erzeugen  </span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span></span>
<span>    mapping <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span></span>
<span>      x <span class="op">=</span> <span class="va">j</span>, </span>
<span>      y <span class="op">=</span> <span class="va">Koeffizient</span>, </span>
<span>      fill <span class="op">=</span> <span class="va">Methode</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_bar.html">geom_bar</a></span><span class="op">(</span></span>
<span>    position <span class="op">=</span> <span class="st">"dodge"</span>, </span>
<span>    stat <span class="op">=</span> <span class="st">"identity"</span>, </span>
<span>    width <span class="op">=</span> <span class="fl">.5</span></span>
<span>  <span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display page-columns page-full">
<div id="fig-KoefRidgeVsKQ" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-KoefRidgeVsKQ-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="RegReg_files/figure-html/fig-KoefRidgeVsKQ-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-KoefRidgeVsKQ-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Abbildung&nbsp;13.4: Koeffizientenvergleich: Ridge vs.&nbsp;KQ
</figcaption></figure>
</div>
</div>
</div>
<p>Der Vergleich anhand von <a href="#fig-KoefRidgeVsKQ" class="quarto-xref">Abbildung&nbsp;<span>13.4</span></a> zeigt deutlich, dass Ridge Regression im Vergleich mit KQ zu absolut kleineren Koeffizientenschätzungen tendiert. Inwiefern dies Konsequenzen für die Prognosegüte der Schätzung hat, können wir Anhand eines Testdatensatzes bestimmen. Hierzu vergleichen wir die mittleren Fehler (MSE) bei der Prognose von <span class="math inline">\(Y\)</span> für die Beobachtungen im Testdatensatz. Für die Simulation des Testdatensatzes nutzen wir erneut die Vorschrift <span class="math inline">\(\eqref{eq:ridgedgp1}\)</span> um 80 neue Beobachtungen zu erzeugen.</p>
<div class="cell">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Test-Datensatz erstellen</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">4321</span><span class="op">)</span></span>
<span><span class="co"># Regressoren</span></span>
<span><span class="va">new_X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/gendata/man/genmvnorm.html">genmvnorm</a></span><span class="op">(</span></span>
<span>    k <span class="op">=</span> <span class="va">k</span>, </span>
<span>    cor <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">.85</span>, <span class="op">(</span><span class="va">k</span><span class="op">^</span><span class="fl">2</span><span class="op">-</span><span class="va">k</span><span class="op">)</span><span class="op">/</span><span class="fl">2</span><span class="op">)</span>, </span>
<span>    n <span class="op">=</span> <span class="va">N</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="co"># Abh. Variable</span></span>
<span><span class="va">new_Y</span> <span class="op">&lt;-</span> <span class="va">new_X</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">beta</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Für beide Methoden können wir <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> für die Prognosen von <span class="math inline">\(Y\)</span> für den Testdatensatz (<code>new_Y</code>) nutzen.</p>
<div class="cell">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Ridge: Vorhersage von new_Y für Test-Datensatz</span></span>
<span><span class="va">Y_predict_ridge</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span></span>
<span>  object <span class="op">=</span> <span class="va">ridge_cvfit</span>, </span>
<span>  newx <span class="op">=</span> <span class="va">new_X</span>, </span>
<span>  s <span class="op">=</span> <span class="va">ridge_cvfit</span><span class="op">$</span><span class="va">lambda.min</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Ridge: MSE für Test-Datensatz berechnen</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">Y_predict_ridge</span> <span class="op">-</span> <span class="va">new_Y</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.288457</code></pre>
</div>
</div>
<p>Die Vorhersage für <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> benötigt dieselben Variablennamen wie im angepassten Modell, s. <code>KQ_fit$coefficients</code>.</p>
<div class="cell">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Test-Datensatz für predict.lm() formatieren</span></span>
<span><span class="va">new_X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="va">new_X</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">new_X</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"X"</span>, <span class="fl">1</span><span class="op">:</span><span class="va">k</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># KQ: Vorhersage von new_Y für Test-Datensatz</span></span>
<span><span class="va">Y_predict_KQ</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span></span>
<span>  object <span class="op">=</span> <span class="va">KQ_fit</span>, </span>
<span>  newdata <span class="op">=</span> <span class="va">new_X</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># KQ: MSE für Test-Datensatz berechnen</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">Y_predict_KQ</span> <span class="op">-</span> <span class="va">new_Y</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 29.33797</code></pre>
</div>
</div>
<p>Die Ergebnisse zeigen, dass der Ridge-Schätzer trotz seiner Verzerrung einen deutlich geringeren mittleren Vorhersagefehler für die Testdaten erzielt als der KQ-Schätzer. Diese Eigenschaft der Koeffizientenschätzung kann die Prognosegüte von Ridge Regression gegenüber der KQ-Regression verbessern.</p>
</section><section id="beispiel-vorhersage-von-abschlussnoten-in-mathe" class="level3 page-columns page-full" data-number="13.1.3"><h3 data-number="13.1.3" class="anchored" data-anchor-id="beispiel-vorhersage-von-abschlussnoten-in-mathe">
<span class="header-section-number">13.1.3</span> Beispiel: Vorhersage von Abschlussnoten in Mathe</h3>
<p>Zur Illustration von Ridge Regression nutzen wir den Datensatz <code>SP</code> aus <span class="citation" data-cites="CortezSilva2008">Cortez und Silva (<a href="Literatur.html#ref-CortezSilva2008" role="doc-biblioref">2008</a>)</span>.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> <code>SP</code> enhält Beobachtungen zu Leistungen von insgesamt 100 Schülerinnen und Schülern im Fach Mathematik in der Sekundarstufe an zwei portugiesischen Schulen. Neben der Abschlussnote in Mathe (<code>G3</code>, Skala von 0 bis 20) beinhaltet <code>SP</code> diverse demografische, soziale und schulbezogene Merkmale, die mithilfe von Schulberichten und Fragebögen erhoben wurden. Ziel ist es, ein Modell für die Prognose von <code>G3</code> anzupassen.</p>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;Wir verwenden eine Auszug aus dem Orignaldatensatz, der nebst ausführlicher Variablenbeschreibung <a href="https://archive.ics.uci.edu/dataset/320/student+performance">hier</a> verfügbar ist.</p></div></div><p>Wir lesen zunächst die Daten (im .csv-Format) ein.</p>
<div class="cell">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Daten einlesen</span></span>
<span><span class="va">SP</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://readr.tidyverse.org/reference/read_delim.html">read_csv</a></span><span class="op">(</span>file <span class="op">=</span> <span class="st">"datasets/SP.csv"</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ein Überblick zeigt, dass der Großteil der Regressoren aus kategorialen Variablen mit sozio-ökonomischen Informationen besteht.</p>
<div class="cell">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Überblick</span></span>
<span><span class="fu"><a href="https://pillar.r-lib.org/reference/glimpse.html">glimpse</a></span><span class="op">(</span><span class="va">SP</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 100
Columns: 31
$ school     &lt;chr&gt; "GP", "GP", "GP", "MS", "GP", "GP", "GP", "GP", "GP", "GP",…
$ sex        &lt;chr&gt; "M", "M", "F", "F", "M", "F", "F", "F", "F", "F", "M", "M",…
$ age        &lt;dbl&gt; 17, 18, 19, 17, 16, 16, 19, 16, 16, 16, 18, 16, 15, 17, 17,…
$ address    &lt;chr&gt; "R", "R", "U", "U", "U", "U", "U", "U", "U", "R", "U", "U",…
$ famsize    &lt;chr&gt; "GT3", "GT3", "LE3", "GT3", "LE3", "GT3", "GT3", "GT3", "GT…
$ Pstatus    &lt;chr&gt; "T", "T", "T", "T", "A", "T", "T", "T", "A", "T", "T", "T",…
$ Medu       &lt;dbl&gt; 1, 4, 3, 2, 3, 2, 0, 2, 3, 4, 4, 2, 1, 2, 2, 3, 3, 4, 4, 2,…
$ Fedu       &lt;dbl&gt; 2, 3, 2, 2, 4, 3, 1, 1, 1, 4, 4, 2, 2, 3, 2, 3, 1, 3, 4, 2,…
$ Mjob       &lt;chr&gt; "at_home", "teacher", "services", "other", "services", "oth…
$ Fjob       &lt;chr&gt; "other", "services", "other", "at_home", "other", "other", …
$ reason     &lt;chr&gt; "home", "course", "reputation", "home", "home", "reputation…
$ guardian   &lt;chr&gt; "mother", "mother", "other", "mother", "mother", "mother", …
$ traveltime &lt;dbl&gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1,…
$ studytime  &lt;dbl&gt; 2, 3, 2, 3, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 3, 1, 2,…
$ failures   &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
$ schoolsup  &lt;chr&gt; "no", "no", "no", "no", "yes", "yes", "no", "no", "no", "no…
$ famsup     &lt;chr&gt; "no", "no", "yes", "no", "yes", "yes", "yes", "no", "yes", …
$ paid       &lt;chr&gt; "no", "no", "yes", "no", "no", "yes", "no", "no", "yes", "n…
$ activities &lt;chr&gt; "no", "no", "no", "yes", "yes", "yes", "no", "no", "no", "y…
$ nursery    &lt;chr&gt; "yes", "yes", "no", "yes", "yes", "yes", "no", "yes", "yes"…
$ higher     &lt;chr&gt; "yes", "yes", "yes", "yes", "yes", "yes", "no", "yes", "yes…
$ internet   &lt;chr&gt; "no", "yes", "yes", "no", "yes", "no", "no", "yes", "yes", …
$ romantic   &lt;chr&gt; "no", "yes", "yes", "yes", "no", "no", "no", "yes", "no", "…
$ famrel     &lt;dbl&gt; 3, 5, 4, 3, 5, 4, 3, 4, 2, 2, 1, 5, 4, 5, 3, 5, 4, 4, 5, 5,…
$ freetime   &lt;dbl&gt; 1, 3, 2, 4, 3, 4, 4, 5, 3, 4, 4, 4, 3, 3, 4, 4, 5, 2, 3, 4,…
$ goout      &lt;dbl&gt; 3, 2, 2, 3, 3, 3, 2, 2, 3, 4, 2, 4, 2, 3, 4, 2, 4, 2, 3, 4,…
$ Dalc       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1,…
$ Walc       &lt;dbl&gt; 5, 2, 2, 1, 1, 3, 1, 1, 2, 3, 2, 4, 1, 3, 3, 1, 3, 2, 1, 1,…
$ health     &lt;dbl&gt; 3, 4, 1, 3, 5, 4, 5, 5, 4, 4, 1, 5, 5, 3, 5, 5, 1, 3, 5, 5,…
$ absences   &lt;dbl&gt; 4, 9, 22, 8, 4, 6, 2, 20, 5, 6, 5, 0, 2, 2, 12, 0, 17, 0, 4…
$ G3         &lt;dbl&gt; 10, 16, 11, 11, 11, 10, 9, 12, 7, 11, 16, 12, 9, 12, 12, 13…</code></pre>
</div>
</div>
<p>Um die Prognosegüte des Modells beurteilen zu können, partitionieren wir <code>SP</code> zufällig in einen Test- sowie einen Trainingsdatensatz (mit 30 und 70 Beobachtungen), jeweils für die Regressoren und die abhängige Variable.</p>
<div class="cell">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># ID für Beobachtungen im Testdatensatz zufällig erzeugen</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1234</span><span class="op">)</span></span>
<span><span class="va">ID</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">SP</span><span class="op">)</span>, size <span class="op">=</span> <span class="fl">30</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Regressoren aufteilen</span></span>
<span><span class="va">SP_test</span> <span class="op">&lt;-</span> <span class="va">SP</span><span class="op">[</span><span class="va">ID</span>,<span class="op">]</span></span>
<span><span class="va">SP_train</span> <span class="op">&lt;-</span> <span class="va">SP</span><span class="op">[</span><span class="op">-</span><span class="va">ID</span>,<span class="op">]</span></span>
<span></span>
<span><span class="co"># Abh. Variable aufteilen</span></span>
<span><span class="va">Y_test</span> <span class="op">&lt;-</span> <span class="va">SP_test</span><span class="op">$</span><span class="va">G3</span></span>
<span><span class="va">Y_train</span> <span class="op">&lt;-</span> <span class="va">SP_train</span><span class="op">$</span><span class="va">G3</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Als nächstes passen wir ein Ridge-Regressionsmodell für alle Regressoren in <code>SP_train</code> an und ermitteln ein optimales <span class="math inline">\(\lambda\)</span> mit Cross Validation. Beachte, dass <code>cv.glmnet</code> nicht für Regressoren im <code>data.frame</code>/<code>tibble</code>-Format ausgelegt ist, sondern ein <code>matrix</code>-Format erwartet. Wir transformieren <code>SP_train</code> daher mit <code><a href="https://rdrr.io/r/base/data.matrix.html">data.matrix()</a></code>.</p>
<div class="cell">
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Ridge-Regression und CV für Trainingsdaten</span></span>
<span><span class="va">SP_fit_cv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/cv.glmnet.html">cv.glmnet</a></span><span class="op">(</span></span>
<span>  x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.matrix.html">data.matrix</a></span><span class="op">(</span><span class="va">SP_train</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">G3</span><span class="op">)</span><span class="op">)</span>, </span>
<span>  y <span class="op">=</span> <span class="va">Y_train</span>, </span>
<span>  alpha <span class="op">=</span> <span class="fl">0</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># CV-Ergebnisse für lambda visualisieren</span></span>
<span><span class="va">SP_fit_cv</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/autoplot.html">autoplot</a></span><span class="op">(</span>label.n <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="RegReg_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Wie für das Beispiel mit simulierten Daten erhalten wir mit <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> Vorhersagen für die erzielte Punktzahl. Beachte, dass wir den MSE nicht für die Trainingsdaten <code>SP_train</code>, sondern für die Testdaten <code>SP_test</code> berechnen.</p>
<div class="cell">
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Prognose von G3 anhand des Ridge-Modells</span></span>
<span><span class="va">Y_predict_ridge</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span></span>
<span>  object <span class="op">=</span> <span class="va">SP_fit_cv</span>, </span>
<span>  newx <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.matrix.html">data.matrix</a></span><span class="op">(</span></span>
<span>    <span class="va">SP_test</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>      <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">G3</span><span class="op">)</span></span>
<span>    <span class="op">)</span>, </span>
<span>  s <span class="op">=</span> <span class="va">SP_fit_cv</span><span class="op">$</span><span class="va">lambda.min</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># MSE für Testdaten berechnen</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">Y_predict_ridge</span> <span class="op">-</span> <span class="va">Y_test</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 21.13249</code></pre>
</div>
</div>
<p>Auch in diesem empirischen Beispiel zeigt ein Vergleich der MSEs, dass Ridge Regression dem KQ-Schätzer hinsichtlich der Vorhersagegüte überlegen ist.</p>
<div class="cell">
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Modell mit KQ schätzen</span></span>
<span><span class="va">SP_fit_KQ</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">G3</span> <span class="op">~</span> <span class="va">.</span>, <span class="va">SP_train</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Prognose</span></span>
<span><span class="va">Y_predict_KQ</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span></span>
<span>  object <span class="op">=</span> <span class="va">SP_fit_KQ</span>, </span>
<span>  newdata <span class="op">=</span> <span class="va">SP_test</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">G3</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Testset-MSE berechnen</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">Y_predict_KQ</span> <span class="op">-</span> <span class="va">Y_test</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 29.76893</code></pre>
</div>
</div>
<p>Der MSE für Ridge ist mit <span class="math inline">\(21.13\)</span> deutlich kleiner als <span class="math inline">\(29.77\)</span>, der MSE für KQ.</p>
<p>Für die Interpretation der Ridge-Schätzung erweitern den Code für die <code>ggplot2</code>-Grafik der Koeffizienten-Pfade um eine vertikale Linie des mit CV ermittelten <span class="math inline">\(\lambda\)</span> und fügen mit dem Paket <code>ggrepel</code> Labels für die Pfade der größten Koeffizienten hinzu.</p>
<div class="cell page-columns page-full">
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggrepel.slowkow.com/">ggrepel</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Lambda-Sequenz auslesen</span></span>
<span><span class="va">lambdas</span> <span class="op">&lt;-</span> <span class="va">SP_fit_cv</span><span class="op">$</span><span class="va">lambda</span></span>
<span></span>
<span><span class="co"># Ridge-Schätzung für Lambdas im langen Format </span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">SP_fit_cv</span><span class="op">$</span><span class="va">glmnet.fit</span><span class="op">$</span><span class="va">beta</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://tibble.tidyverse.org/reference/as_tibble.html">as_tibble</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span></span>
<span>    Variable <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">rownames</a></span><span class="op">(</span><span class="va">SP_fit_cv</span><span class="op">$</span><span class="va">glmnet.fit</span><span class="op">$</span><span class="va">beta</span><span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer</a></span><span class="op">(</span><span class="op">-</span><span class="va">Variable</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">Variable</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>lambda <span class="op">=</span> <span class="va">lambdas</span><span class="op">)</span> </span>
<span></span>
<span><span class="co"># Grafik mit ggplot erzeugen</span></span>
<span><span class="va">df</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span></span>
<span>    mapping <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span></span>
<span>      x <span class="op">=</span> <span class="va">lambda</span>, </span>
<span>      y <span class="op">=</span> <span class="va">value</span>, </span>
<span>      col <span class="op">=</span> <span class="va">Variable</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggrepel.slowkow.com/reference/geom_text_repel.html">geom_label_repel</a></span><span class="op">(</span></span>
<span>    data <span class="op">=</span> <span class="va">df</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>      <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">lambda</span> <span class="op">==</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">lambdas</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    mapping <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>label <span class="op">=</span> <span class="va">Variable</span><span class="op">)</span>, </span>
<span>    seed <span class="op">=</span> <span class="fl">1234</span>,</span>
<span>    size <span class="op">=</span> <span class="fl">5</span>, </span>
<span>    max.overlaps <span class="op">=</span> <span class="fl">8</span>, </span>
<span>    nudge_x <span class="op">=</span> <span class="op">-</span><span class="fl">.5</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"gesch. Koeffizienten"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_continuous.html">scale_x_log10</a></span><span class="op">(</span><span class="st">"log_10(lambda)"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span></span>
<span>    xintercept <span class="op">=</span> <span class="va">SP_fit_cv</span><span class="op">$</span><span class="va">lambda.min</span>, </span>
<span>    col <span class="op">=</span> <span class="st">"red"</span>, </span>
<span>    lty <span class="op">=</span> <span class="fl">2</span></span>
<span>  <span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.position <span class="op">=</span> <span class="st">"none"</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display page-columns page-full">
<div id="fig-ridgAppPlot" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-ridgAppPlot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="RegReg_files/figure-html/fig-ridgAppPlot-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-ridgAppPlot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Abbildung&nbsp;13.5: Lösungspfad für Ridge-Schätzung
</figcaption></figure>
</div>
</div>
</div>
<p><a href="#fig-ridgAppPlot" class="quarto-xref">Abbildung&nbsp;<span>13.5</span></a> gibt Hinweise darauf, dass neben der Schulzugehörigkeit und Indikatoren für schulische Leistung (bspw. <code>failures</code>) sozio-ökonomische Prädiktoren wie <code>internet</code> (Internetzugang zuhause), <code>Pstatus</code> (Zusammenleben der Eltern) und <code>address</code>/<code>traveltime</code> (sozialer Status) relevante Variablen zu sein scheinen.</p>
<p>Das optimale <span class="math inline">\(\lambda_\mathrm{cv} \approx 0.21\)</span> (gestrichelte rote Linie in <a href="#fig-ridgAppPlot" class="quarto-xref">Abbildung&nbsp;<span>13.5</span></a>) führt zu deutlicher Shrinkage, was eine mögliche Erklärung für den besseren Testset-MSE von Ridge Regression ist: Die Koeffizienten von Variablen mit wenig Erklärungskraft werden durch die Regularisierung in Richtung 0 gezwungen und reduzieren so die Varianz der Vorhersage gegenüber der (idealerweise) unverzerrten KQ-Schätzung.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Facts zu Ridge Regression
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Ridge-Regression regularisiert den KQ-Schätzer mit der <span class="math inline">\(\ell_2\)</span>-Norm der Koeffizienten. Diese Form von Regularisierung ist eine Alternative für KQ in Anwendungen mit mehr Regressoren als Beobachtugen (<span class="math inline">\(k\geq n\)</span>) und/oder wenn KQ aufgrund starker Kollinearität eine hohe Varianz aufweist.</p></li>
<li><p>Der Ridge-Schätzer <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda\)</span> ist <em>nicht</em> erwartungstreu. Die geschätzten Koeffizienten sind auch für <span class="math inline">\(n\to\infty\)</span> verzerrt.</p></li>
<li><p>Aufgrund der verzerrten Schätzung ist statistische Inferenz für Koeffizienten mit <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda\)</span> problematisch. Anstatt für strukturelle Modelle oder die Schätzung kausaler Effekte wird Ridge Regression in der Praxis daher überwiegend für Prognosen verwendet.</p></li>
<li><p>Die Wahl von <span class="math inline">\(\lambda\)</span> impliziert einen Tradeoff zwischen Verzerrung und Varianz: Große <span class="math inline">\(\lambda\)</span> schrumpfen die Koeffizientenschätzer Richtung 0 (mehr Verzerrung), führen aber zu einer kleineren Varianz der Schätzung. Entsprechend können Vorhersagen mit mehr Verzerrung aber weniger Varianz als mit KQ getroffen werden.</p></li>
<li><p>Ridge Regression kann in R mit dem Paket <code>glmnet</code> berechnet werden.</p></li>
</ul>
</div>
</div>
</section></section><section id="lasso-regression" class="level2 page-columns page-full" data-number="13.2"><h2 data-number="13.2" class="anchored" data-anchor-id="lasso-regression">
<span class="header-section-number">13.2</span> Lasso Regression</h2>
<p>Least Absolute Shrinkage and Selection Operator (Lasso) ist ein von <span class="citation" data-cites="Tibshirani1996">Tibshirani (<a href="Literatur.html#ref-Tibshirani1996" role="doc-biblioref">1996</a>)</span> vorgeschlagener Schätzer, der die Verlustfunktion des KQ-Schätzers um einen Strafterm für die Summe der (absoluten) Größe der Koeffizienten <span class="math inline">\(\boldsymbol\beta = (\beta_1, \dots,\beta_k)'\)</span> erweitert. Die Verlustfunktion des Lasso-Schätzers von <span class="math inline">\(\boldsymbol{\beta}\)</span> lautet <span class="math display">\[\begin{align}
\mathrm{RSS}(\boldsymbol{\beta},p=1,\lambda) = \mathrm{RSS}(\boldsymbol{\beta}) + \lambda \lVert\boldsymbol{\beta}\rVert_1.\label{eq:lassoloss}
\end{align}\]</span> Für den Strafterm wird also die <span class="math inline">\(\ell_1\)</span>-norm <span class="math display">\[
\lVert\boldsymbol{\beta}\rVert_1 = \sum_{j=1}^k \lvert\beta_j \rvert
\]</span> verwendet. Der Lasso-Schätzer <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda\)</span> für <span class="math inline">\(\boldsymbol{\beta}\)</span> minimiert <span class="math inline">\(\eqref{eq:lassoloss}\)</span>, <span class="math display">\[\begin{align}
\boldsymbol{\beta}^{\mathrm{L}}_\lambda = \arg\min_{\boldsymbol{\beta}} \ \mathrm{RSS}(\boldsymbol{\beta},p=1,\lambda).
\end{align}\]</span> Entsprechend erhalten wir in Abhängigkeit von <span class="math inline">\(\lambda\)</span> ein Kontinuum an Lösungen <span class="math display">\[\begin{align}
  \left\{\widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda\right\}_{\lambda=0}^{\lambda=\infty},\label{eq:LassoPath}
\end{align}\]</span> der sogenannte <em>Lasso-Pfad</em>.</p>
<p>Das Optimierungsproblem <span class="math inline">\(\eqref{eq:lassoloss}\)</span> hat die äquivalente Darstellung <span class="math display">\[\begin{align}
  \begin{split}
    \widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda =&amp;\, \arg\min_{\boldsymbol{\beta}} \mathrm{RSS}(\boldsymbol{\beta}) + \lambda\left(\lVert\boldsymbol{\beta}\rVert_1 - t\right)\\
    =&amp;\, \arg\min_{\lVert\boldsymbol{\beta}\rVert_1\leq t} \mathrm{RSS}(\boldsymbol{\beta}),
  \end{split}\label{eq:lassolagrange}
\end{align}\]</span> welche über den <a href="https://de.wikipedia.org/wiki/Lagrange-Multiplikator#Beispiel_mit_Anwendungsbezug">Lagrange-Ansatz</a> unter der Nebenbedingung <span class="math inline">\(\lVert\boldsymbol{\beta}\rVert_1 \leq t\)</span> gelöst werden kann.</p>
<p>Ähnlich wie der KQ-Schätzer ist der Lasso-Schätzer <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda\)</span> durch Bedingungen 1. Ordnung bestimmt. Diese Bedingungen lassen sich komfortabel in Matrix-Schreibweise darstellen als <span class="math display">\[\begin{align}
  -2\boldsymbol{X}_j'(\boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\beta}) + \lambda\cdot\mathrm{sgn}(\beta_j) = 0, \quad j = 1,\dots,k.\label{eq:LassoFOC}
\end{align}\]</span> Aus Gleichung <span class="math inline">\(\eqref{eq:LassoFOC}\)</span> folgt, dass der Lasso-Schätzer aufgrund des Strafterms im Allgemeinen nicht algebraisch bestimmt werden kann.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn11"><p><sup>11</sup>&nbsp;Zur Bestimmung des Schätzers werden Algorithmen der nicht-linearen Optimierung genutzt.</p></div></div><p>In Abhängigkeit von <span class="math inline">\(\lambda\)</span> zwingt der Lasso-Schätzer die KQ-Schätzung von <span class="math inline">\(\beta_j\)</span> zu einem (absolut) kleineren Wert: Ähnlich wie bei Ridge Regression bewirkt der <span class="math inline">\(\ell_1\)</span>-Strafterm eine mit <span class="math inline">\(\lambda\)</span> zunehmende Schrumpfung der geschätzen Koeffizienten in Richtung 0. Charakteristisch für die Lösung des Lasso-Schätzers ist, dass <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{\mathrm{L}}_j = 0\)</span>, wenn die Bedingung <span class="math display">\[\begin{align}
  \left\lvert\boldsymbol{X}_j'(\boldsymbol{Y} - \boldsymbol{X}\widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda)\right\rvert - \lambda/2 \leq 0 \label{eq:lassoselection}
\end{align}\]</span> erfüllt ist. In Abhängigkeit von <span class="math inline">\(\lambda\)</span> kann der Lasso-Schätzer folglich geschätzte Regressionskoeffizienten nicht nur in Richtung <span class="math inline">\(0\)</span>, sondern diese auch <em>exakt</em> mit <span class="math inline">\(0\)</span> schätzen und damit <em>Variablenselektion</em> betreiben. Aufgrund der mit <span class="math inline">\(\lambda\)</span> zunehmenden Shrinkage bis die Bedingung <span class="math inline">\(\eqref{eq:lassoselection}\)</span> erfüllt und der Koeffizient gleich <span class="math inline">\(0\)</span> gesetzt wird, bezeichnet man Lasso auch als einen <em>Soft Thresholding Operator</em>. Im nächsten Abschnitt betrachten wir die Eigenschaften von Lasso-Regularisierung unter vereinfachten Annahmen bzgl. der Regressoren.</p>
<section id="lasso-ist-soft-thresholding" class="level3 page-columns page-full" data-number="13.2.1"><h3 data-number="13.2.1" class="anchored" data-anchor-id="lasso-ist-soft-thresholding">
<span class="header-section-number">13.2.1</span> Lasso ist Soft Thresholding</h3>
<p>Wir betrachten nun eine mathematische Darstellung von Selektions- und Shrinkage-Eigenschaft des Lasso-Schätzers in einem vereinfachten Modell. Wenn die Regressoren <span class="math inline">\(\boldsymbol{X}\)</span> orthonormal zueinander sind, existiert eine analytische Lösung des Lasso-Schätzers, <span class="math display">\[\begin{align}
  \widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda =
  \begin{cases}
    \widehat{\boldsymbol{\beta}}_j - \lambda/2 &amp;, \ \ \widehat{\boldsymbol{\beta}}_j &gt; \lambda/2\\
    0 &amp;, \ \ \lvert\widehat{\boldsymbol{\beta}}_j\rvert\leq\lambda/2\\
    \widehat{\boldsymbol{\beta}}_j + \lambda/2 &amp;, \ \ \widehat{\boldsymbol{\beta}}_j &lt; \lambda/2
  \end{cases},\label{eq:lassoST}
\end{align}\]</span> wobei <span class="math inline">\(\widehat{\boldsymbol{\beta}}_j\)</span> der KQ-Schätzer von <span class="math inline">\(\beta_j\)</span> ist. Anhand von <span class="math inline">\(\eqref{eq:lassoST}\)</span> können wir die Selektionseigenschaft sowie die Schrumpfung der KQ-Koeffizientenschätzung in Abhängigkeit der durch <span class="math inline">\(\lambda\)</span> regulierten <span class="math inline">\(\ell_1\)</span>-Strafe erkennen. Für eine Visualisierung implementieren wir <span class="math inline">\(\eqref{eq:lassoST}\)</span> als R-Funktion <code>lasso_st()</code> und zeichnen die resultierenden Koeffizientenschätzungen für die Parameterwerte <span class="math inline">\(\lambda\in\{0, 0.2, 0.4\}\)</span>.</p>
<p>Wir definieren zunächst die Funktion <code>lasso_st()</code>.</p>
<div class="cell">
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Funktion für Lasso soft-thresholding definieren</span></span>
<span><span class="va">lasso_st</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">KQ</span>, <span class="va">lambda</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span></span>
<span>    <span class="va">KQ</span> <span class="op">&gt;</span> <span class="va">lambda</span><span class="op">/</span><span class="fl">2</span>         <span class="op">~</span> <span class="va">KQ</span> <span class="op">-</span> <span class="va">lambda</span><span class="op">/</span><span class="fl">2</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">KQ</span><span class="op">)</span> <span class="op">&lt;=</span> <span class="va">lambda</span><span class="op">/</span><span class="fl">2</span>   <span class="op">~</span> <span class="fl">0</span>,</span>
<span>    <span class="va">KQ</span> <span class="op">&lt;</span> <span class="op">-</span><span class="va">lambda</span><span class="op">/</span><span class="fl">2</span>        <span class="op">~</span> <span class="va">KQ</span> <span class="op">+</span> <span class="va">lambda</span><span class="op">/</span><span class="fl">2</span>,</span>
<span>  <span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Im nächsten Schritt zeichnen wir <code>lasso_st()</code> für eine Sequenz von KQ-Schätzwerten gegeben <span class="math inline">\(\lambda\)</span>.</p>
<div class="cell page-columns page-full">
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Sequenz von KQ-Schätzwerten für Illustration definieren</span></span>
<span><span class="va">dat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span></span>
<span>  KQ <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">.01</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Lasso-Schätzer als Funktion des KQ-Schätzers plotten</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">dat</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_function.html">geom_function</a></span><span class="op">(</span></span>
<span>    fun <span class="op">=</span> <span class="va">lasso_st</span>, </span>
<span>    args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>lambda <span class="op">=</span> <span class="fl">0</span><span class="op">)</span>, </span>
<span>    lty <span class="op">=</span> <span class="fl">2</span></span>
<span>  <span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_function.html">geom_function</a></span><span class="op">(</span></span>
<span>    fun <span class="op">=</span> <span class="va">lasso_st</span>, </span>
<span>    args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>lambda <span class="op">=</span> <span class="fl">.2</span><span class="op">)</span>,</span>
<span>    col <span class="op">=</span> <span class="st">"red"</span></span>
<span>  <span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_function.html">geom_function</a></span><span class="op">(</span></span>
<span>    fun <span class="op">=</span> <span class="va">lasso_st</span>, </span>
<span>    args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>lambda <span class="op">=</span> <span class="fl">.4</span><span class="op">)</span>, </span>
<span>    col <span class="op">=</span> <span class="st">"blue"</span></span>
<span>  <span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/lims.html">xlim</a></span><span class="op">(</span><span class="op">-</span><span class="fl">.4</span>, <span class="fl">.4</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">"KQ-Schätzer von beta_1"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"Lasso-Schätzer von beta_1"</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display page-columns page-full">
<div id="fig-lassoST" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-lassoST-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="RegReg_files/figure-html/fig-lassoST-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-lassoST-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Abbildung&nbsp;13.6: Shrinkage und Selektion von OLS-Koeffizienten mit Lasso
</figcaption></figure>
</div>
</div>
</div>
<p><a href="#fig-lassoST" class="quarto-xref">Abbildung&nbsp;<span>13.6</span></a> zeigt, dass der <span class="math inline">\(\ell_1\)</span>-Strafterm des Lasso-Schätzers zu einem linearen Verlauf der auf den KQ-Schätzer (gezeichnet für <span class="math inline">\(\lambda = 0\)</span>, gestrichelte Linie) applizierten Shrinkage führt: Der Lasso-Schätzer ist eine abschnittsweise-lineare Funktion des KQ-Schätzers in <span class="math inline">\(\lambda\)</span>: Je größer der Parameter <span class="math inline">\(\lambda\)</span>, desto größer ist das Intervall von KQ-Schätzwerten <span class="math inline">\([-\lambda/2,\lambda/2]\)</span>, wo der Lasso-Schätzer zu Variablenselektion führt, d.h. hier den Koeffizienten <span class="math inline">\(\beta_j\)</span> als <span class="math inline">\(0\)</span> schätzt (rote bzw. blaue Linie).</p>
<p>Anhand von <a href="#fig-lassoST" class="quarto-xref">Abbildung&nbsp;<span>13.6</span></a> kann abgeleitet werden, dass der Lasso-Schätzer nicht invariant gegenüber der Skalierung der Regressoren ist: Die Stärke der Regularisierung durch <span class="math inline">\(\lambda\)</span> ist hängt von der Magnitude des KQ-Schätzers ab. Daher müssen die Regressoren vor Berechnung der Schätzung standardsiert werden. Üblich ist hierbei eine Normierung auf einen Mittelwert von <span class="math inline">\(0\)</span> und eine Varianz von <span class="math inline">\(1\)</span>.</p>
<p>Die nachstehende interaktive Grafik illustriert das Lasso-Optimierungsproblem <span class="math inline">\(\eqref{eq:lassolagrange}\)</span> sowie den resultierenden Schätzer der Koeffizienten <span class="math inline">\((\beta_1, \beta_2)\)</span> in einem multiplen Regressionsmodell mit korrelierten Regressoren <span class="math inline">\(X_1\)</span> und <span class="math inline">\(X_2\)</span>.</p>
<ul>
<li><p>Die blaue Ellipse ist die Menge aller Schätzwerte <span class="math inline">\(\left(\widehat\beta_{1},\, \widehat\beta_{2}\right)\)</span> für den angegebenen Wert von <span class="math inline">\(\mathrm{RSS}\)</span>. Im Zentrum der Ellipse liegt der KQ-Schätzer, welcher <span class="math inline">\(\mathrm{RSS}\)</span> minimiert.</p></li>
<li><p>Das graue Quadrat ist die Menge aller Koeffizienten-Paare <span class="math inline">\((\beta_1, \beta_2)\)</span>, welche die Restriktion <span class="math inline">\(\lvert\beta_1\rvert+\lvert\beta_2\rvert\leq t\)</span> erfüllen. Beachte, dass die Größe dieser Region nur durch den Parameter <span class="math inline">\(t\)</span> bestimmt wird.</p></li>
<li><p>Der blaue Punkt ist der Lasso-Schätzer <span class="math inline">\((\widehat{\boldsymbol{\beta}}^L_{1,t},\, \widehat{\boldsymbol{\beta}}^L_{2,t})\)</span>. Dieser ergibt sich als Schnittpunkt zwischen der blauen <span class="math inline">\(\mathrm{RSS}\)</span>-Ellipse und der Restriktionsregion und variiert mit <span class="math inline">\(t\)</span>. Die gestrichelte rote Linie zeigt den Lasso-Lösungspfad.</p></li>
<li><p>Für kleine Werte, erhalten wir starke Shrinkage auf <span class="math inline">\(\widehat\beta_{1,t}\)</span> bis zum Wertebereich <span class="math inline">\(t\leq50\)</span>, wo <span class="math inline">\(\widehat{\boldsymbol{\beta}}^L_{1,t}=0\)</span>. Hier erfolgt Variablenselektion: Die Regularisierung führt zu einem geschätzten Modell, das lediglich <span class="math inline">\(X_2\)</span> als erklärende Variable enthält. In diesem Bereich von <span class="math inline">\(t\)</span> bewirkt die Shrinkage, dass <span class="math inline">\(\widehat{\boldsymbol{\beta}}^L_{2,t}\to0\)</span> für <span class="math inline">\(t\to0\)</span>.</p></li>
</ul>
<iframe width="100%" height="567" frameborder="0" scroll="false" src="https://observablehq.com/embed/2e11f2b535e23c25@16?cells=viewof+s%2Cchart115">
</iframe>
<p>Beachte, dass der rote Lasso-Pfad (die Menge aller Lasso-Lösungen) äquivalent als Funktion von <span class="math inline">\(\lambda\)</span> im Optimierungsproblem <span class="math inline">\(\eqref{eq:lassoloss}\)</span> dargestellt werden kann. Implementierungen mit statistischer Software berechnen die Lasso-Lösung häufig in Abhängigkeit von <span class="math inline">\(\lambda\)</span>. Ein Algorithmus hierfür ist LARS.</p>
</section><section id="berechnung-der-lasso-lösung-mit-dem-lars-algorithmus" class="level3 page-columns page-full" data-number="13.2.2"><h3 data-number="13.2.2" class="anchored" data-anchor-id="berechnung-der-lasso-lösung-mit-dem-lars-algorithmus">
<span class="header-section-number">13.2.2</span> Berechnung der Lasso-Lösung mit dem LARS-Algorithmus</h3>
<p>Für die Berechnung des Lasso-Lösungspfads kann der <a href="https://en.wikipedia.org/wiki/Least-angle_regression">LARS-Algorithmus</a> von <span class="citation" data-cites="Efronetal2004">Efron u.&nbsp;a. (<a href="Literatur.html#ref-Efronetal2004" role="doc-biblioref">2004</a>)</span> im Lasso-Modus genutzt werden.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> Der Lasso-Lösungspfad beinhaltet geschätzte Koeffizienten über ein Intervall für <span class="math inline">\(\lambda\)</span>, welches sämtliche Modellkomplexitäten zwischen der (trivialen) Lösung mit maximaler Shrinkage auf allen Koeffizienten (<span class="math inline">\(\lambda\)</span> groß, alle gesch. Koeffizienten sind <span class="math inline">\(0\)</span>) und der unregularisierten Lösung (<span class="math inline">\(\lambda = 0\)</span>, KQ-Schätzung) abbildet. Der LARS-Algorithmus erzeugt den Lösungspfad sequentiell, sodass die Schätzung als Funktion von <span class="math inline">\(\lambda\)</span> veranschaulicht werden kann, ähnlich wie bei Ridge Regression.</p>
<div class="no-row-height column-margin column-container"><div id="fn12"><p><sup>12</sup>&nbsp;LARS steht für <em>Least Angle Regression</em>.</p></div></div><p>Wir zeigen nun anhand simulierter Daten, wie Lasso-Lösungen mit dem R-Paket <code>lars</code> berechnet werden können. Hierfür erzeugen wir Daten gemäß der Vorschrift <span class="math display">\[\begin{align}
  \begin{split}
  Y_i =&amp;\, \boldsymbol{X}_i' \boldsymbol{\beta}_v + u_i\\
  \\
  \boldsymbol{\beta}_v =&amp;\, (-1.25, -.75, 0, 0, 0, 0, 0, .75, 1.25)'\\
  \\
  \boldsymbol{X}_i \sim&amp;\, N(\boldsymbol{0}, \boldsymbol{I}_{9\times9}), \quad u_i \overset{u.i.v.}{\sim} N(0, 1), \quad i = 1,\dots,25.
  \end{split}\label{eq:larsdgp}
\end{align}\]</span></p>
<div class="cell">
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://doi.org/10.1214/009053604000000067">lars</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1234</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Parameter definieren</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">25</span></span>
<span><span class="va">beta_v</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1.25</span>, <span class="op">-</span><span class="fl">.75</span>, <span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">.75</span>, <span class="fl">1.25</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Beobachtungen simulieren</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span> <span class="op">*</span> <span class="fl">9</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">9</span><span class="op">)</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="va">X</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">beta_v</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Entsprechend des DGP passen wir ein Modell ohne Konstante an. Damit <code><a href="https://rdrr.io/pkg/lars/man/lars.html">lars::lars()</a></code> den Lösungspfad des Lasso-Schätzers berechnet, muss <code>type = "lasso"</code> gewählt werden.<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn13"><p><sup>13</sup>&nbsp;<code><a href="https://rdrr.io/pkg/lars/man/lars.html">lars()</a></code> standardisiert die Regressoren standardmäßig (aufgrund des DGPs hier nicht nötig).</p></div></div><div class="cell">
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Lösungen des Lasso-Schätzers mit LARS berechnen</span></span>
<span><span class="op">(</span></span>
<span>  <span class="va">fit_lars</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lars/man/lars.html">lars</a></span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="va">X</span>, </span>
<span>    y <span class="op">=</span> <span class="va">Y</span>, </span>
<span>    intercept <span class="op">=</span> <span class="cn">F</span>,</span>
<span>    type <span class="op">=</span> <span class="st">"lasso"</span> <span class="co"># Wichtig: Lasso-Modus</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lars(x = X, y = Y, type = "lasso", intercept = F)
R-squared: 0.858 
Sequence of LASSO moves:
                      
Var  9 2 8 1 3 5 4 7 6
Step 1 2 3 4 5 6 7 8 9</code></pre>
</div>
</div>
<p>Die Zusammenfassung zeigt, dass der LARS-Algorithmus als erstes die (relevante) Variable <span class="math inline">\(X_9\)</span> aktiviert.<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> Mit abnehmender Regularisierung (kleinere <span class="math inline">\(\lambda\)</span>) werden in den nächsten 3 Schritten die übrigen relevanten Variablen <span class="math inline">\(X_2\)</span>, <span class="math inline">\(X_8\)</span> und <span class="math inline">\(X_1\)</span> aktiviert. Über die weiteren Schritte nähert der Algorithmus die Lösung an die <em>saturierte</em> Schätzung (das Modell mit allen neun Regressoren) an und aktiviert schrittweise die übrigen, irrelevanten Variablen.</p>
<div class="no-row-height column-margin column-container"><div id="fn14"><p><sup>14</sup>&nbsp;Aktivierung meint die Aufnahme einer Variable in der Modell gegeben eines hinreichend kleinen <span class="math inline">\(\lambda\)</span>.</p></div></div><p>Wir visualisieren die geschätzen Koeffizienten an jedem Schritt des Lösungspfads als Funktion von <span class="math inline">\(\lambda\)</span>. In der Praxis wird der Regularisierungsparameter häufig auf der natürlichen log-Skala dargestellt.</p>
<div class="cell page-columns page-full">
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Transformation in ein weites Format</span></span>
<span><span class="va">fit_lars</span><span class="op">$</span><span class="va">beta</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://tibble.tidyverse.org/reference/as_tibble.html">as_tibble</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span></span>
<span>    lambda <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">fit_lars</span><span class="op">$</span><span class="va">lambda</span>, <span class="fl">1e-2</span><span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer</a></span><span class="op">(</span></span>
<span>    cols <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">9</span>, </span>
<span>    names_to <span class="op">=</span> <span class="st">"Variable"</span>, </span>
<span>    values_to <span class="op">=</span> <span class="st">"gesch. Koeffizient"</span></span>
<span>  <span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  </span>
<span><span class="co"># Visualisierung mit ggplot  </span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span></span>
<span>    mapping <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span></span>
<span>      x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">lambda</span><span class="op">)</span>, </span>
<span>      y <span class="op">=</span> <span class="va">`gesch. Koeffizient`</span>, </span>
<span>      color <span class="op">=</span> <span class="va">Variable</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="op">)</span> </span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display page-columns page-full">
<div id="fig-larssolpath" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-larssolpath-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="RegReg_files/figure-html/fig-larssolpath-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-larssolpath-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Abbildung&nbsp;13.7: LARS-Lösungspfad für Lasso-Schätzung
</figcaption></figure>
</div>
</div>
</div>
<p><a href="#fig-larssolpath" class="quarto-xref">Abbildung&nbsp;<span>13.7</span></a> zeigt, dass die Shrinkage der geschätzten Koeffizienten nach der Aktivierung rasch abnimmt und sich für kleine Werte von <span class="math inline">\(\lambda\)</span> der KQ-Lösung annähert. Wir sehen auch, dass es einen Bereich von <span class="math inline">\(\lambda\)</span>-Werten gibt, für die das wahre Modell mit den Variablen <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, <span class="math inline">\(X_8\)</span> und <span class="math inline">\(X_9\)</span> selektiert werden kann. Je nach Ziel der Analyse kann es sinnvoll sein, ein <span class="math inline">\(\lambda\)</span> in diesem Intervall zu schätzen.</p>
</section><section id="wahl-des-regularisierungsparameters-lambda-für-den-lasso-schätzer" class="level3 page-columns page-full" data-number="13.2.3"><h3 data-number="13.2.3" class="anchored" data-anchor-id="wahl-des-regularisierungsparameters-lambda-für-den-lasso-schätzer">
<span class="header-section-number">13.2.3</span> Wahl des Regularisierungsparameters <span class="math inline">\(\lambda\)</span> für den Lasso-Schätzer</h3>
<p>Wie zuvor bei Ridge Regression muss in empirischen Anwendungen ein Wert für den Tuning-Parameter <span class="math inline">\(\lambda\)</span> gewählt werden. Hierbei besteht die Herausforderung darin, einen geeigneten Wert zu finden, der zu wünschenswerten Eigenschaften des resultierenden Modells führt. So ist für gute Vorhersagen wichtig, dass das Modell nicht zu sehr an die Daten angepasst ist (<em>Overfitting</em>), um eine gute Generalisierung auf neue Daten zu ermöglichen. Gleichzeitig muss das Modell flexibel genug sein, um wesentliche Eigenschaften des datenerzeugenden Prozesses hinreichend gut zu erfassen. In der Regel wird hierbei eine sparsame Modellierung angestrebt, die nur eine Teilmenge der Prädiktoren nutzt.</p>
<p>In der Praxis werden verschiedene Verfahren verwendet, um den Wert für den Tuning-Parameter <span class="math inline">\(\lambda\)</span> zu bestimmen. Gängige Methoden sind <em>Cross Validation</em> (CV) und Informationskriterien. In Abhängigkeit der Methode und der Daten ergeben sich ober- oder unterparameterisierte Modelle. Aufgrund der Implementierung im R-Paket <code>lars</code> betrachten wir CV.<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> Wir zeigen nachfolgend anhand der simulierten Daten aus dem letzten Abschnitt, wie für die LARS-Schätzung ein optimales <span class="math inline">\(\lambda\)</span> mit leave-one-out CV (LOO-CV) bestimmt werden kann. Hierzu nutzen wir <code><a href="https://rdrr.io/pkg/lars/man/cv.lars.html">lars::cv.lars()</a></code> unter Verwendung derselben Argumente wie zuvor im Aufruf von <code><a href="https://rdrr.io/pkg/lars/man/lars.html">lars()</a></code>.</p>
<div class="no-row-height column-margin column-container"><div id="fn15"><p><sup>15</sup>&nbsp;Chetverikov, Liao, and Chernozhukov (2020) zeigen, dass CV zu konsistenter Modellselektion führen kann.</p></div></div><div class="cell">
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># LARS-Lösungen mit CV evaluieren</span></span>
<span><span class="va">fit_lars_cv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lars/man/cv.lars.html">cv.lars</a></span><span class="op">(</span></span>
<span>  x <span class="op">=</span> <span class="va">X</span>, </span>
<span>  y <span class="op">=</span> <span class="va">Y</span>, </span>
<span>  intercept <span class="op">=</span> <span class="cn">F</span>,</span>
<span>  normalize <span class="op">=</span> <span class="cn">T</span>,</span>
<span>  type <span class="op">=</span> <span class="st">"lasso"</span>, </span>
<span>  plot.it <span class="op">=</span> <span class="cn">F</span>, </span>
<span>  K <span class="op">=</span> <span class="va">N</span> <span class="co"># für LOO-CV</span></span>
<span><span class="op">)</span> </span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Das Objekt <code>fit_lars_cv</code> ist eine Liste mit den CV-Ergebnissen. Wir können diese einfach mit <code>ggplot</code> visualisieren. <code>index</code> ist hierbei das Verhältnis der <span class="math inline">\(\ell_1\)</span>-Norm des Lasso-Schätzers für einen spezifischen Wert von <span class="math inline">\(\lambda\)</span> und der <span class="math inline">\(\ell_1\)</span>-Norm des KQ-Schätzers. Das optimale <span class="math inline">\(\lambda\)</span> wird so implizit geschätzt. <code>cv.error</code> ist der mit CV geschätzte MSE.</p>
<div class="cell page-columns page-full">
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># CV-MSE</span></span>
<span><span class="va">fit_lars_cv</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://tibble.tidyverse.org/reference/as_tibble.html">as_tibble</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span></span>
<span>    mapping <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span></span>
<span>      x <span class="op">=</span> <span class="va">index</span>, </span>
<span>      y <span class="op">=</span> <span class="va">cv.error</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">"|beta_lambda| / |beta|"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"CV-MSE"</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display page-columns page-full">
<div id="fig-larscv" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-larscv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="RegReg_files/figure-html/fig-larscv-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-larscv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Abbildung&nbsp;13.8: CV-MSE und relative Position von <span class="math inline">\(\lambda\)</span> auf dem Lassopfad
</figcaption></figure>
</div>
</div>
</div>
<p>In der Grafik erkennen wir ein Minimum des CV-MSEs bei etwa 0.73.</p>
<div class="cell">
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># CV-MSE-minimierendes Lambda bestimmen</span></span>
<span><span class="va">ID</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/which.min.html">which.min</a></span><span class="op">(</span><span class="va">fit_lars_cv</span><span class="op">$</span><span class="va">cv.error</span><span class="op">)</span> <span class="co"># Index</span></span>
<span></span>
<span><span class="op">(</span></span>
<span>  <span class="va">fraction_opt</span> <span class="op">&lt;-</span> <span class="va">fit_lars_cv</span><span class="op">$</span><span class="va">index</span><span class="op">[</span><span class="va">ID</span><span class="op">]</span></span>
<span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.7272727</code></pre>
</div>
</div>
<p>Die geschätzten Koeffizienten für die optimale Regularisierung können mit <code><a href="https://rdrr.io/r/stats/coef.html">coef()</a></code> ausgelesen werden.</p>
<div class="cell">
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># LARS-Lasso-Fit für optimales lambda bestimmen</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span></span>
<span>  object <span class="op">=</span> <span class="va">fit_lars</span>, </span>
<span>  s <span class="op">=</span> <span class="va">fraction_opt</span>, </span>
<span>  mode <span class="op">=</span> <span class="st">"fraction"</span></span>
<span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -0.6513191 -0.6060906 -0.1946089  0.0000000  0.0000000  0.0000000  0.0000000
[8]  0.4977908  1.3122407</code></pre>
</div>
</div>
<p>Das Ergebnis veranschaulicht die Selektionseigenschaft von Lasso: Gemäß DGP <span class="math inline">\(\eqref{eq:larsdgp}\)</span> sind die Variablen <span class="math inline">\(X_3\)</span> bis <span class="math inline">\(X_7\)</span> <em>irrelevante</em> Prädiktoren für <span class="math inline">\(Y\)</span>; ihre wahren Koeffizienten sind <span class="math inline">\(0\)</span>. In der kreuzvalidierten Lasso-Schätzung erreicht die Regularisierung, dass die Koeffizienten der Variablen <span class="math inline">\(X_4\)</span> bis <span class="math inline">\(X_7\)</span> tatsächlich mit 0 geschätzt werden. Wir schätzen für das mit CV bestimmte <span class="math inline">\(\lambda\)</span> also ein leicht überspezifiziertes Modell mit den Regressoren <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, <span class="math inline">\(X_3\)</span>, <span class="math inline">\(X_8\)</span> und <span class="math inline">\(X_9\)</span>. Beachte, dass die Lasso-Schätzung einen Kompromiss impliziert: Die Varianz der Schätzung ist geringer als die des KQ-Schätzers im Modell mit allen Variablen.<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> Aufgrund der Regularisierung sind die mit Lasso geschätzten Koeffizienten der relevanten Variablen jedoch in Richtung <span class="math inline">\(0\)</span> verzerrt.</p>
<div class="no-row-height column-margin column-container"><div id="fn16"><p><sup>16</sup>&nbsp;Wegen <span class="math inline">\(N=25\)</span> verbleiben bei der KQ-Schätzung mit 9 Regressoren nur 16 Freiheitsgrade.</p></div></div><p>Einen positiven Effekt dieses Kompromisses beobachten wir anhand des mittleren Vorhersagefehlers für Daten, die <em>nicht</em> zur Berechnung des Schätzers verwendet wurden. Wir vergleichen den Vorhersagefehler nachfolgend anhand eines solchen simulierten Test-Datensatzes mit 25 neuen Beobachtungen. Den Vorhersagefehler bestimmen wir als MSE zwischen den vorhergesagten und den tatsächlichen Ausprägungen für <span class="math inline">\(Y\)</span>.</p>
<div class="cell">
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Test-Datensatz erstellen</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">4321</span><span class="op">)</span></span>
<span><span class="va">new_X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span> <span class="op">*</span> <span class="fl">9</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">9</span><span class="op">)</span></span>
<span><span class="va">new_Y</span> <span class="op">&lt;-</span> <span class="va">new_X</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">beta_v</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Lasso: Vorhersage von new_Y für Test-Datensatz</span></span>
<span><span class="va">Y_predict_lars</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span></span>
<span>  object <span class="op">=</span> <span class="va">fit_lars</span>, </span>
<span>  s <span class="op">=</span> <span class="va">fraction_opt</span>, </span>
<span>  type <span class="op">=</span> <span class="st">"fit"</span>, </span>
<span>  mode <span class="op">=</span> <span class="st">"fraction"</span>, </span>
<span>  newx <span class="op">=</span> <span class="va">new_X</span></span>
<span><span class="op">)</span><span class="op">$</span><span class="va">fit</span></span>
<span></span>
<span><span class="co"># Lasso: MSE für Test-Datensatz berechnen</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">Y_predict_lars</span> <span class="op">-</span> <span class="va">new_Y</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.419817</code></pre>
</div>
</div>
<p>Wir schätzen nun das große Modell mit allen 9 Variablen mit KQ und berechnen ebenfalls den MSE der Prognosen für den Test-Datensatz.</p>
<div class="cell">
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># KQ-Schätzung des großen Modells durchführen</span></span>
<span><span class="va">KQ_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Test-Datensatz für predict.lm() formatieren</span></span>
<span><span class="va">new_X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="va">new_X</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">new_X</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"X"</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">9</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># KQ: Vorhersage von new_Y für Test-Datensatz</span></span>
<span><span class="va">Y_predict_KQ</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span></span>
<span>  object <span class="op">=</span> <span class="va">KQ_fit</span>, </span>
<span>  newdata <span class="op">=</span> <span class="va">new_X</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># KQ: MSE für Test-Datensatz berechnen</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">Y_predict_KQ</span> <span class="op">-</span> <span class="va">new_Y</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 9.851932</code></pre>
</div>
</div>
<p>Offenbar führt die Lasso-Schätzung zu einem deutlich geringeren MSE der Vorhersage von <code>Y</code> für den Test-Datensatz als die KQ-Schätzung und damit zu einer höheren Vorhersagegüte. Das “sparsame” mit Lasso-Regression geschätzte Modell ist dem “großen” mit KQ geschätztem Modell in dieser Hinsicht also überlegen.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Facts zu Lasso-Regression
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Lasso-Regression bestraft die Verlustfunktion des KQ-Schätzers mit der <span class="math inline">\(\ell_1\)</span>-Norm der Koeffizienten.</p></li>
<li><p>Neben Koeffizientenschätzung mit Shrinkage in Richtung <span class="math inline">\(0\)</span> kann der Lasso-Schätzer Variablenselektion durchführen: Regressionskoeffizienten können exakt mit <span class="math inline">\(0\)</span> geschätzt und so ein “sparsames”, leichter zu interpretierendes Modell gewählt werden.</p></li>
<li><p>Wie bei Ridge Regression impliziert die Wahl von <span class="math inline">\(\lambda\)</span> einen Bias-Variance-Tradeoff, der für Vorhersagen nützlich ist: Für größere <span class="math inline">\(\lambda\)</span> wird mehr Verzerrung induziert und möglicherweise relevante Variablen mit kleinen Koeffizienten aus dem Modell entfernt. Ein solches sparsames Modell kann eine höhere Prognosegüte haben als ein komplexes, unregularisiertes Modell.</p></li>
<li><p>Der Lasso-Schätzer <span class="math inline">\(\widehat{\boldsymbol{\beta}}_\lambda^L\)</span> ist <em>nicht</em> erwartungstreu.</p></li>
<li><p>Lasso Regression kann bspw. mit dem LARS-Algorithmus (Paket <code>lars</code>) oder mit <code>glmnet</code> berechnet werden.</p></li>
</ul>
</div>
</div>
</section></section><section id="vergleich-von-lasso--und-ridge-regression-mit-simulation" class="level2 page-columns page-full" data-number="13.3"><h2 data-number="13.3" class="anchored" data-anchor-id="vergleich-von-lasso--und-ridge-regression-mit-simulation">
<span class="header-section-number">13.3</span> Vergleich von Lasso- und Ridge-Regression mit Simulation</h2>
<p>In diesem Kapitel illustrieren wir Vor- und Nachteile von Lasso- und Ridge-Regression in Prognose-Anwendungen anhand von Monte-Carlo-Simulationen. Wir betrachten hierbei datenerzeugende Prozesse, die sich hinsichtlich der Anzahl relevanter Variablen sowie der Korrelation dieser Variablen unterscheiden.</p>
<p>Die grundlegende Vorschrift für die Simulationen ist <span class="math display">\[\begin{align*}
  Y_i = \sum_{j=1}^{k=40} \beta_j X_{i,j} + u_i, \quad u_i \overset{u.i.v.}{\sim} N(0,1), \quad i=1,\dots,100,
\end{align*}\]</span> wobei die Regressoren <span class="math inline">\(X_j\)</span> eine Varianz von <span class="math inline">\(1\)</span> haben und aus einer multivariaten Normalverteilung mit Korrelation <span class="math display">\[\rho\in(0,0.5,0.8)\]</span> gezogen werden.</p>
<p>Für die Koeffizienten <span class="math inline">\(\boldsymbol{\beta}\)</span> unterscheiden wir zwei Szenarien. In Szenario A ist <span class="math display">\[\boldsymbol{\beta} = (1,\dots,1)',\]</span> d.h. alle Variablen sind relevant und haben denselben Einfluss auf <span class="math inline">\(Y\)</span>. In Szenario B erzeugen wir <span class="math inline">\(\boldsymbol{\beta}\)</span> einmalig vorab so, dass <span class="math display">\[\beta_j = \begin{cases}1,\quad \text{mit Wsk.  }p\\ 0,\quad \text{mit Wsk.  }1-p, \end{cases}\]</span> d.h. nur eine Teilmenge der Variablen beeinflusst <span class="math inline">\(Y\)</span> jeweils mit demselben Effekt <span class="math inline">\(\beta_j = 1\)</span>. Die übrigen Variablen sind irrelevant.</p>
<p>Wir schätzen und validieren die Modelle mit <code><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet()</a></code>.</p>
<section id="sec-pdz" class="level3 page-columns page-full" data-number="13.3.1"><h3 data-number="13.3.1" class="anchored" data-anchor-id="sec-pdz">
<span class="header-section-number">13.3.1</span> Prognosegüte in diversen Szenarien</h3>
<div class="cell">
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Simulationsparameter definieren</span></span>
<span><span class="va">rho</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0.5</span>, <span class="fl">0.8</span><span class="op">)</span>   <span class="co"># Korrelation</span></span>
<span><span class="va">k</span> <span class="op">&lt;-</span> <span class="fl">40</span>                 <span class="co"># Anz. Regressoren</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">100</span>                <span class="co"># Anz. Beobachtungen</span></span>
<span><span class="va">n_sim</span> <span class="op">&lt;-</span> <span class="fl">100</span>            <span class="co"># Anz. Simulationen</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Damit der Code für die Simulation möglichst wenig repetitiv ist, definieren wir eine Funktion <code>cv.glmnet_MSE()</code>, die unter Angabe der Daten <code>X</code> und <code>Y</code>, des Trainingssets <code>train</code> sowie des Parameters <code>alpha</code> den gewünschten regularisierten Schätzer under Verwendung von Cross Validation anpasst und den Testset-MSE zurückgibt.</p>
<div class="cell">
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># allg. Funktion für Testset-MSE nach CV</span></span>
<span><span class="va">cv.glmnet_MSE</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">X</span>, <span class="va">Y</span>, <span class="va">train</span>, <span class="va">alpha</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># Modell mit glmnet schätzen; lambda per CV bestimmen</span></span>
<span>  <span class="va">fit_cv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/cv.glmnet.html">cv.glmnet</a></span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="va">X</span><span class="op">[</span><span class="va">train</span>,<span class="op">]</span>,</span>
<span>    y <span class="op">=</span><span class="va">Y</span><span class="op">[</span><span class="va">train</span><span class="op">]</span>,</span>
<span>    alpha <span class="op">=</span> <span class="va">alpha</span></span>
<span>  <span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Vorhersagen treffen</span></span>
<span>  <span class="va">Y_pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span></span>
<span>    object <span class="op">=</span> <span class="va">fit_cv</span>, </span>
<span>    s <span class="op">=</span> <span class="va">fit_cv</span><span class="op">$</span><span class="va">lambda.min</span>, </span>
<span>    newx <span class="op">=</span> <span class="va">X</span><span class="op">[</span><span class="op">-</span><span class="va">train</span>,<span class="op">]</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span></span>
<span>    <span class="co"># Testset-MSE berechnen</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span></span>
<span>      <span class="op">(</span><span class="va">Y</span><span class="op">[</span><span class="op">-</span><span class="va">train</span><span class="op">]</span> <span class="op">-</span> <span class="va">Y_pred</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span>
<span>      <span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Wir initialisieren zunächst Matrizen, in welche die MSEs aus den 100 Simulationsdurchläufen reihenweise geschrieben werden. <code>lasso_mse</code> und <code>ridge_mse</code> haben je eine Spalte für jede Korrelation in <code>rho</code></p>
<div class="cell">
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Matrizen für simulierte MSEs initialisieren...</span></span>
<span><span class="va">lasso_mse</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="cn">NA</span>, </span>
<span>  nrow <span class="op">=</span> <span class="va">n_sim</span>, </span>
<span>  ncol <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">rho</span><span class="op">)</span></span>
<span><span class="op">)</span> </span>
<span><span class="va">ridge_mse</span> <span class="op">&lt;-</span> <span class="va">lasso_mse</span></span>
<span></span>
<span><span class="co"># ... und benennen</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">lasso_mse</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"Kor="</span>, <span class="va">rho</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">ridge_mse</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">lasso_mse</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Für die Simulation iterieren wir mit <code><a href="https://purrr.tidyverse.org/reference/map.html">purrr::walk</a></code> über den Vektor <code>rho</code> sowie über die Laufvariable <code>1:n_sim</code>. Beide Schleifen nutzen den Syntax für anonyme Funktionen:</p>
<div class="cell">
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Die anonyme Funktion</span></span>
<span><span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="co"># ist äquivalent definiert als</span></span>
<span>\<span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In jeden Simulationsdurchlauf erzeugen wir den Datensatz entsprechend der obigen Vorschrift, teilen die Daten auf und berechnen MSEs für Lasso- und Ridge-Regression mit <code>cv.glmnet_MSE()</code>.</p>
<p><strong>Szenario A</strong></p>
<div class="cell">
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Koeffizienten-Vektor definieren</span></span>
<span><span class="va">beta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span>, <span class="va">k</span><span class="op">)</span> </span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://mvtnorm.R-forge.R-project.org">mvtnorm</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1234</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulation durchführen</span></span>
<span><span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">walk</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">rho</span><span class="op">)</span>, \<span class="op">(</span><span class="va">j</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># Korrelationsmatrix definieren</span></span>
<span>  <span class="va">Sigma</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span></span>
<span>    data <span class="op">=</span> <span class="va">rho</span><span class="op">[</span><span class="va">j</span><span class="op">]</span>, </span>
<span>    nrow <span class="op">=</span> <span class="va">k</span>, </span>
<span>    ncol <span class="op">=</span> <span class="va">k</span></span>
<span>  <span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">Sigma</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span>  </span>
<span>  <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">walk</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">n_sim</span>, \<span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">{</span></span>
<span>    </span>
<span>  <span class="co"># Daten simulieren</span></span>
<span>  <span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html">rmvnorm</a></span><span class="op">(</span></span>
<span>    n <span class="op">=</span> <span class="va">N</span>, </span>
<span>    mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>, <span class="va">k</span><span class="op">)</span>, </span>
<span>    sigma <span class="op">=</span> <span class="va">Sigma</span></span>
<span>  <span class="op">)</span></span>
<span>  <span class="va">Y</span> <span class="op">&lt;-</span> <span class="va">X</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">beta</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span></span>
<span>    </span>
<span>  <span class="co"># Trainingsdaten definieren</span></span>
<span>  <span class="va">ID_train</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">N</span><span class="op">)</span>, </span>
<span>    size <span class="op">=</span> <span class="va">N</span><span class="op">/</span><span class="fl">2</span></span>
<span>  <span class="op">)</span></span>
<span>    </span>
<span>  <span class="co"># Modelle mit CV schätzen und MSEs berechnen</span></span>
<span>  <span class="co"># Ridge-Regression</span></span>
<span>  <span class="va">ridge_mse</span><span class="op">[</span><span class="va">i</span>, <span class="va">j</span><span class="op">]</span> <span class="op">&lt;&lt;-</span> <span class="fu">cv.glmnet_MSE</span><span class="op">(</span></span>
<span>    X <span class="op">=</span> <span class="va">X</span>, </span>
<span>    Y <span class="op">=</span> <span class="va">Y</span>, </span>
<span>    train <span class="op">=</span> <span class="va">ID_train</span>, </span>
<span>    alpha <span class="op">=</span> <span class="fl">0</span></span>
<span>  <span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Lasso-Regression</span></span>
<span>  <span class="va">lasso_mse</span><span class="op">[</span><span class="va">i</span>, <span class="va">j</span><span class="op">]</span> <span class="op">&lt;&lt;-</span> <span class="fu">cv.glmnet_MSE</span><span class="op">(</span></span>
<span>    X <span class="op">=</span> <span class="va">X</span>, </span>
<span>    Y <span class="op">=</span> <span class="va">Y</span>, </span>
<span>    train <span class="op">=</span> <span class="va">ID_train</span>, </span>
<span>    alpha <span class="op">=</span> <span class="fl">1</span></span>
<span>  <span class="op">)</span></span>
<span>  </span>
<span>  <span class="op">}</span><span class="op">)</span></span>
<span>  </span>
<span><span class="op">}</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Beachte, dass hier der Super-Assignment-Operator <code>&lt;&lt;-</code> genutzt wird, damit <code>walk</code> die Matrizen <code>ridge_mse</code> und <code>lasso_mse</code> in der globalen Umgebung überschreibt.<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn17"><p><sup>17</sup>&nbsp;Dies folgt aus der Definition von <code>walk</code>. <code>&lt;-</code> bewirkt hier lediglich Assignment in der Funktionsumgebung.</p></div></div><p>Wir berechnen jeweils den mittleren MSEs, sammeln die Ergebnisse in einer <code><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble()</a></code> und nutzen <code><a href="https://gt.rstudio.com/reference/gt.html">gt()</a></code> für die tabellarische Darstellung.</p>
<div class="cell page-columns page-full">
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://gt.rstudio.com">gt</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Ergebnisse tabellarisch darstellen</span></span>
<span><span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span></span>
<span>  Methode <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>    <span class="st">"Lasso-Regression"</span>, </span>
<span>    <span class="st">"Ridge-Regression"</span></span>
<span>  <span class="op">)</span>,</span>
<span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/bind_cols.html">bind_cols</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/bind_rows.html">bind_rows</a></span><span class="op">(</span></span>
<span>      <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/colSums.html">colMeans</a></span><span class="op">(</span><span class="va">lasso_mse</span><span class="op">)</span>,</span>
<span>      <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/colSums.html">colMeans</a></span><span class="op">(</span><span class="va">ridge_mse</span><span class="op">)</span>  </span>
<span>    <span class="op">)</span>    </span>
<span>  <span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://gt.rstudio.com/reference/gt.html">gt</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="va">tabopts</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="tbl-lrsimA" class="cell quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-tbl figure page-columns page-full"><div aria-describedby="tbl-lrsimA-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div id="vvecpmnxyr" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#vvecpmnxyr table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#vvecpmnxyr thead, #vvecpmnxyr tbody, #vvecpmnxyr tfoot, #vvecpmnxyr tr, #vvecpmnxyr td, #vvecpmnxyr th {
  border-style: none;
}

#vvecpmnxyr p {
  margin: 0;
  padding: 0;
}

#vvecpmnxyr .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #000000;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #000000;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#vvecpmnxyr .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#vvecpmnxyr .gt_title {
  color: #000000;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#vvecpmnxyr .gt_subtitle {
  color: #000000;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#vvecpmnxyr .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#vvecpmnxyr .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#vvecpmnxyr .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #000000;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #000000;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#vvecpmnxyr .gt_col_heading {
  color: #000000;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: bold;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#vvecpmnxyr .gt_column_spanner_outer {
  color: #000000;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: bold;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#vvecpmnxyr .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#vvecpmnxyr .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#vvecpmnxyr .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #000000;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#vvecpmnxyr .gt_spanner_row {
  border-bottom-style: hidden;
}

#vvecpmnxyr .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #000000;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#vvecpmnxyr .gt_empty_group_heading {
  padding: 0.5px;
  color: #000000;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#vvecpmnxyr .gt_from_md > :first-child {
  margin-top: 0;
}

#vvecpmnxyr .gt_from_md > :last-child {
  margin-bottom: 0;
}

#vvecpmnxyr .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#vvecpmnxyr .gt_stub {
  color: #000000;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#vvecpmnxyr .gt_stub_row_group {
  color: #000000;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#vvecpmnxyr .gt_row_group_first td {
  border-top-width: 2px;
}

#vvecpmnxyr .gt_row_group_first th {
  border-top-width: 2px;
}

#vvecpmnxyr .gt_summary_row {
  color: #000000;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#vvecpmnxyr .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#vvecpmnxyr .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#vvecpmnxyr .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#vvecpmnxyr .gt_grand_summary_row {
  color: #000000;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#vvecpmnxyr .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#vvecpmnxyr .gt_last_grand_summary_row_top {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#vvecpmnxyr .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#vvecpmnxyr .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #000000;
}

#vvecpmnxyr .gt_footnotes {
  color: #000000;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#vvecpmnxyr .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#vvecpmnxyr .gt_sourcenotes {
  color: #000000;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#vvecpmnxyr .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#vvecpmnxyr .gt_left {
  text-align: left;
}

#vvecpmnxyr .gt_center {
  text-align: center;
}

#vvecpmnxyr .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#vvecpmnxyr .gt_font_normal {
  font-weight: normal;
}

#vvecpmnxyr .gt_font_bold {
  font-weight: bold;
}

#vvecpmnxyr .gt_font_italic {
  font-style: italic;
}

#vvecpmnxyr .gt_super {
  font-size: 65%;
}

#vvecpmnxyr .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#vvecpmnxyr .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#vvecpmnxyr .gt_indent_1 {
  text-indent: 5px;
}

#vvecpmnxyr .gt_indent_2 {
  text-indent: 10px;
}

#vvecpmnxyr .gt_indent_3 {
  text-indent: 15px;
}

#vvecpmnxyr .gt_indent_4 {
  text-indent: 20px;
}

#vvecpmnxyr .gt_indent_5 {
  text-indent: 25px;
}

#vvecpmnxyr .katex-display {
  display: inline-flex !important;
  margin-bottom: 0.75em !important;
}

#vvecpmnxyr div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {
  height: 0px !important;
}
</style>
<table class="gt_table do-not-create-environment cell table table-sm table-striped small" data-quarto-postprocess="true" data-quarto-disable-processing="false" data-quarto-bootstrap="false">
<thead><tr class="header gt_col_headings">
<th id="Methode" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" scope="col">Methode</th>
<th id="Kor=0" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Kor=0</th>
<th id="Kor=0.5" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Kor=0.5</th>
<th id="Kor=0.8" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Kor=0.8</th>
</tr></thead>
<tbody class="gt_table_body">
<tr class="odd">
<td class="gt_row gt_left" headers="Methode">Lasso-Regression</td>
<td class="gt_row gt_right" headers="Kor=0">7.17</td>
<td class="gt_row gt_right" headers="Kor=0.5">10.398</td>
<td class="gt_row gt_right" headers="Kor=0.8">7.581</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="Methode">Ridge-Regression</td>
<td class="gt_row gt_right" headers="Kor=0">4.841</td>
<td class="gt_row gt_right" headers="Kor=0.5">1.615</td>
<td class="gt_row gt_right" headers="Kor=0.8">1.517</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-tbl margin-caption" id="tbl-lrsimA-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabelle&nbsp;13.1: Durchschnittliche Testset-MSEs für Setting A
</figcaption></figure>
</div>
</div>
<p><a href="#tbl-lrsimA" class="quarto-xref">Tabelle&nbsp;<span>13.1</span></a> zeigt, dass Ridge-Regression gegenüber Lasso-Regression für jede der drei betrachteten Korrelationen überlegen ist. Insbesondere bei stärker korrelierten Regressoren ist Ridge vorteilhaft.</p>
<p>Für Szenario B überschreiben wir <code>beta</code> nach Multiplikation mit einem zufälligen binären Vektor, sodass einige der Koeffizienten <span class="math inline">\(0\)</span> und die zugehörigen Variablen irrelevant für <span class="math inline">\(Y\)</span> sind.</p>
<p><strong>Szenario B</strong></p>
<div class="cell">
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Wsk. für Relevanz einer Variable</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">.3</span></span>
<span></span>
<span><span class="co"># Koeffizienten-Vektor definieren</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">beta</span> <span class="op">&lt;-</span> <span class="va">beta</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span></span>
<span>  x <span class="op">=</span> <span class="fl">0</span><span class="op">:</span><span class="fl">1</span>, </span>
<span>  size <span class="op">=</span> <span class="va">k</span>, </span>
<span>  replace <span class="op">=</span> <span class="cn">T</span>, </span>
<span>  prob <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">p</span>, <span class="va">p</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Koeffizienten prüfen</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">beta</span>, n <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 0 1 0 1 1 0 0 1 0 0</code></pre>
</div>
</div>
<p>Eine wiederholung der Simulation für die modifizierten Koeffizienten <code>beta</code> und liefert folgende tabellarische Auswertung.</p>
<div class="cell page-columns page-full">
<div id="tbl-lrsimB" class="cell quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-tbl figure page-columns page-full"><div aria-describedby="tbl-lrsimB-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div id="vvecpmnxyr" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#vvecpmnxyr table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#vvecpmnxyr thead, #vvecpmnxyr tbody, #vvecpmnxyr tfoot, #vvecpmnxyr tr, #vvecpmnxyr td, #vvecpmnxyr th {
  border-style: none;
}

#vvecpmnxyr p {
  margin: 0;
  padding: 0;
}

#vvecpmnxyr .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #000000;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #000000;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#vvecpmnxyr .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#vvecpmnxyr .gt_title {
  color: #000000;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#vvecpmnxyr .gt_subtitle {
  color: #000000;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#vvecpmnxyr .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#vvecpmnxyr .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#vvecpmnxyr .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #000000;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #000000;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#vvecpmnxyr .gt_col_heading {
  color: #000000;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: bold;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#vvecpmnxyr .gt_column_spanner_outer {
  color: #000000;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: bold;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#vvecpmnxyr .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#vvecpmnxyr .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#vvecpmnxyr .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #000000;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#vvecpmnxyr .gt_spanner_row {
  border-bottom-style: hidden;
}

#vvecpmnxyr .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #000000;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#vvecpmnxyr .gt_empty_group_heading {
  padding: 0.5px;
  color: #000000;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#vvecpmnxyr .gt_from_md > :first-child {
  margin-top: 0;
}

#vvecpmnxyr .gt_from_md > :last-child {
  margin-bottom: 0;
}

#vvecpmnxyr .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#vvecpmnxyr .gt_stub {
  color: #000000;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#vvecpmnxyr .gt_stub_row_group {
  color: #000000;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#vvecpmnxyr .gt_row_group_first td {
  border-top-width: 2px;
}

#vvecpmnxyr .gt_row_group_first th {
  border-top-width: 2px;
}

#vvecpmnxyr .gt_summary_row {
  color: #000000;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#vvecpmnxyr .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#vvecpmnxyr .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#vvecpmnxyr .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#vvecpmnxyr .gt_grand_summary_row {
  color: #000000;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#vvecpmnxyr .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#vvecpmnxyr .gt_last_grand_summary_row_top {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#vvecpmnxyr .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#vvecpmnxyr .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #000000;
}

#vvecpmnxyr .gt_footnotes {
  color: #000000;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#vvecpmnxyr .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#vvecpmnxyr .gt_sourcenotes {
  color: #000000;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#vvecpmnxyr .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#vvecpmnxyr .gt_left {
  text-align: left;
}

#vvecpmnxyr .gt_center {
  text-align: center;
}

#vvecpmnxyr .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#vvecpmnxyr .gt_font_normal {
  font-weight: normal;
}

#vvecpmnxyr .gt_font_bold {
  font-weight: bold;
}

#vvecpmnxyr .gt_font_italic {
  font-style: italic;
}

#vvecpmnxyr .gt_super {
  font-size: 65%;
}

#vvecpmnxyr .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#vvecpmnxyr .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#vvecpmnxyr .gt_indent_1 {
  text-indent: 5px;
}

#vvecpmnxyr .gt_indent_2 {
  text-indent: 10px;
}

#vvecpmnxyr .gt_indent_3 {
  text-indent: 15px;
}

#vvecpmnxyr .gt_indent_4 {
  text-indent: 20px;
}

#vvecpmnxyr .gt_indent_5 {
  text-indent: 25px;
}
</style>
<table class="gt_table do-not-create-environment cell table table-sm table-striped small" data-quarto-postprocess="true" data-quarto-disable-processing="false" data-quarto-bootstrap="false">
<thead><tr class="header gt_col_headings">
<th id="Methode" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" scope="col">Methode</th>
<th id="Kor=0" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Kor=0</th>
<th id="Kor=0.5" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Kor=0.5</th>
<th id="Kor=0.8" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Kor=0.8</th>
</tr></thead>
<tbody class="gt_table_body">
<tr class="odd">
<td class="gt_row gt_left" headers="Methode">Lasso</td>
<td class="gt_row gt_right" headers="Kor=0">2.51</td>
<td class="gt_row gt_right" headers="Kor=0.5">2.143</td>
<td class="gt_row gt_right" headers="Kor=0.8">1.923</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="Methode">Ridge</td>
<td class="gt_row gt_right" headers="Kor=0">3.331</td>
<td class="gt_row gt_right" headers="Kor=0.5">2.562</td>
<td class="gt_row gt_right" headers="Kor=0.8">2.014</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-tbl margin-caption" id="tbl-lrsimB-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabelle&nbsp;13.2: Durchschnittliche Testset-MSEs für Szenario B
</figcaption></figure>
</div>
</div>
<p>Die Ergebnisse in <a href="#tbl-lrsimB" class="quarto-xref">Tabelle&nbsp;<span>13.2</span></a> zeigen, dass Ridge-Regression in Szenario B bis auf den Fall unkorrelierter Regressoren etwas schlechter abschneidet als in Szenario A. Die hohe Anzahl irrelevanter Variablen verbessert die Leistung von Lasso deutlich: Hier ist es plausibel, dass Lasso aufgrund der Thresholding-Eigenschaft die Koeffizienten einiger irrelevanten Variablen häufig exakt <span class="math inline">\(0\)</span> setzt und damit ein sparsameres Modell schätzt als Ridge. Entsprechend erzielt Lasso in diesem Szenario insbesondere für <span class="math inline">\(\rho = 0\)</span> genauere Vorhersagen als Ridge Regression.</p>
</section><section id="visualisierung-des-bias-variance-tradeoffs-bei-prognosen" class="level3 page-columns page-full" data-number="13.3.2"><h3 data-number="13.3.2" class="anchored" data-anchor-id="visualisierung-des-bias-variance-tradeoffs-bei-prognosen">
<span class="header-section-number">13.3.2</span> Visualisierung des Bias-Variance-Tradeoffs bei Prognosen</h3>
<p>Für ein besseres Verständnis, wie sich der Regularisierungsparameter <span class="math inline">\(\lambda\)</span> auf den Bias-Variance-Tradeoff bei Prognosen mit Ridge- und Lasso-Regression auswirkt, vergleichen wir für beide Methoden nachfolgend die Abhängigkeit des MSEs der Prognose <span class="math inline">\(\widehat{Y}_0\)</span> für den Wert <span class="math inline">\(Y_0\)</span> der abhängigen Variable eines Datenpunkts anhand seiner Regressoren <span class="math inline">\(\boldsymbol{X}_0'\)</span>, wobei <span class="math display">\[\begin{align}
  \text{MSE}(\widehat{Y}_0) = \text{Bias}(\widehat{Y}_0)^2 + \text{Var}(\widehat{Y}_0) + \text{Var}(Y_0) \label{eq:pbvdecomp}
\end{align}\]</span> Beachte, dass <span class="math inline">\(\text{Var}(Y_0)\)</span> die durch den datenerzeugenden Prozess (und damit unvermeidbare) Varianz von <span class="math inline">\(Y_0\)</span> ist, wohingegen <span class="math inline">\(\text{Bias}(\widehat{Y}_0)^2\)</span> und <span class="math inline">\(\text{Var}(\widehat{Y}_0)\)</span> von dem verwendeten Schätzer für <span class="math inline">\(\widehat{Y}_0\)</span> abhängt.</p>
<p>Für die Simulation betrachten wir erneut Szenario A aus <a href="#sec-pdz" class="quarto-xref"><span>Kapitel 13.3.1</span></a> mit <span class="math inline">\(50\)</span> Beobachtungen für ein Modell mit <span class="math inline">\(40\)</span> unkorrelierten Regressoren. Wir legen zunächst die Simulationsparameter fest und erzeugen den vorherzusagenden Datenpunkt (<code>X_0</code>, <code>Y_0</code>).</p>
<div class="cell">
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Parameter festlegen</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1234</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">200</span> <span class="co"># Anz. Iterationen</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">50</span>  <span class="co"># Anz. Beobachtungen</span></span>
<span><span class="va">k</span> <span class="op">&lt;-</span> <span class="fl">40</span>  <span class="co"># Anz. Variablen</span></span>
<span></span>
<span><span class="co"># Korrelationsmatrix definieren</span></span>
<span><span class="va">Sigma</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">k</span><span class="op">)</span> <span class="co"># Diagonalmatrix</span></span>
<span><span class="va">beta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fl">1</span>, <span class="va">k</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Prognose-Ziel vorab zufällig generieren:</span></span>
<span></span>
<span><span class="co"># Regressoren</span></span>
<span><span class="va">X_0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html">rmvnorm</a></span><span class="op">(</span></span>
<span>  n <span class="op">=</span> <span class="fl">1</span>, </span>
<span>  mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fl">0</span>, <span class="va">k</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Abh. Variable</span></span>
<span><span class="va">Y_0</span> <span class="op">&lt;-</span> <span class="va">X_0</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">beta</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/vector.html">as.vector</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Anhand der Simulationsergebnisse wollen wir die von der verwendeten Schätzfunktion abhängigen Komponenten von <span class="math inline">\(\eqref{eq:pbvdecomp}\)</span> untersuchen. Wir initialisieren hierzu die Listen <code>ridge_fits</code> und <code>lasso_fits</code>, in die unsere Simulationsergebnisse geschrieben werden.</p>
<div class="cell">
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Listen für Simulationsergebnisse initialisieren</span></span>
<span><span class="va">ridge_fits</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">lasso_fits</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Weiterhin definieren wir separate <span class="math inline">\(\lambda\)</span>-Sequenzen für Lasso- und Ridge-Schätzer.<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn18"><p><sup>18</sup>&nbsp;Die Sequenzen haben wir in Abhängigkeit des DGP so gewählt, dass die Abhängigkeit der Prognosegüte von <span class="math inline">\(\lambda\)</span> gut visualisiert werden kann.</p></div></div><div class="cell">
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Lambda-Sequenzen festlegen</span></span>
<span><span class="va">lambdas_r</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">.25</span>, <span class="fl">2.5</span>, length.out <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span><span class="va">lambdas_l</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">.05</span>, <span class="fl">0.5</span>, length.out <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Für die Simulation iterieren wir mit <code><a href="https://purrr.tidyverse.org/reference/map.html">walk()</a></code> über simulierte Datensätze und schreiben jeweils den vollständigen Output von <code><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet()</a></code> in die zuvor definierten Listen <code>ridge_fits</code> und <code>lasso_fits</code>.</p>
<div class="cell">
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Simulation</span></span>
<span><span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">walk</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">n</span>, \<span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># Daten simulieren</span></span>
<span>  <span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html">rmvnorm</a></span><span class="op">(</span></span>
<span>    n <span class="op">=</span> <span class="va">N</span>, </span>
<span>    mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>, <span class="va">k</span><span class="op">)</span>, </span>
<span>    sigma <span class="op">=</span> <span class="va">Sigma</span></span>
<span>  <span class="op">)</span></span>
<span>  <span class="va">Y</span> <span class="op">&lt;-</span> <span class="va">X</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">beta</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">N</span>, sd <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Modelle mit glmnet schätzen</span></span>
<span>  <span class="co"># Ridge-Regression</span></span>
<span>  <span class="va">ridge_fits</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet</a></span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="va">X</span>, </span>
<span>    y <span class="op">=</span> <span class="va">Y</span>, </span>
<span>    alpha <span class="op">=</span> <span class="fl">0</span>, </span>
<span>    intercept <span class="op">=</span> <span class="cn">F</span></span>
<span>  <span class="op">)</span></span>
<span>  <span class="co"># Lasso-Regression</span></span>
<span>  <span class="va">lasso_fits</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet</a></span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="va">X</span>, </span>
<span>    y <span class="op">=</span> <span class="va">Y</span>, </span>
<span>    alpha <span class="op">=</span> <span class="fl">1</span>, </span>
<span>    intercept <span class="op">=</span> <span class="cn">F</span></span>
<span>  <span class="op">)</span></span>
<span>  </span>
<span><span class="op">}</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Wir nutzen Funktionen aus <code>purrr</code> und <code>dplyr</code>, um über die in den Simulationsdurchläufen angepassten Modelle zu iterieren. Mit <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> erhalten wir Punktvorhersagen für <code>Y_0</code> für jedes <span class="math inline">\(\lambda\)</span> der zuvor definierten <span class="math inline">\(\lambda\)</span>-Sequenzen. Beachte, dass <code><a href="https://purrr.tidyverse.org/reference/map.html">map()</a></code> jeweils eine Liste mit 200 Punktvorhersagen für jedes der 100 zurückgibt. Mit <code><a href="https://purrr.tidyverse.org/reference/list_c.html">list_rbind()</a></code> können wir die Ergebnisse komfortabel jeweils in einer <code>tibble</code> sammeln.</p>
<div class="cell">
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Prognosen für Ridge-Regression</span></span>
<span><span class="va">pred_r</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map</a></span><span class="op">(</span></span>
<span>  .x <span class="op">=</span> <span class="va">ridge_fits</span>, </span>
<span>  .f <span class="op">=</span> <span class="op">~</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/as_tibble.html">as_tibble</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span></span>
<span>      object <span class="op">=</span> <span class="va">.</span>, </span>
<span>      s <span class="op">=</span> <span class="va">lambdas_r</span>, </span>
<span>      newx <span class="op">=</span> <span class="va">X_0</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span> </span>
<span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://purrr.tidyverse.org/reference/list_c.html">list_rbind</a></span><span class="op">(</span><span class="op">)</span> </span>
<span></span>
<span><span class="co"># Prognosen für Lasso-Regression</span></span>
<span><span class="va">pred_l</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map</a></span><span class="op">(</span></span>
<span>  .x <span class="op">=</span> <span class="va">lasso_fits</span>, </span>
<span>  .f <span class="op">=</span> <span class="op">~</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/as_tibble.html">as_tibble</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span></span>
<span>      object <span class="op">=</span> <span class="va">.</span>, </span>
<span>      s <span class="op">=</span> <span class="va">lambdas_l</span>, </span>
<span>      newx <span class="op">=</span> <span class="va">X_0</span><span class="op">)</span></span>
<span>    <span class="op">)</span> </span>
<span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://purrr.tidyverse.org/reference/list_c.html">list_rbind</a></span><span class="op">(</span><span class="op">)</span> </span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Für die statistische Auswertung berechnen wir jeweils <span class="math inline">\(\text{MSE}(\widehat{Y}_0)\)</span>, <span class="math inline">\(\text{Bias}(\widehat{Y}_0)^2\)</span> und <span class="math inline">\(\text{Var}(\widehat{Y}_0)\)</span> und führen die Ergebnisse mit <code><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer()</a></code> in ein langes Format <code>sim_data_r</code> über. Wir berechnen weiterhin mit <code>MSE_min_r</code> das <span class="math inline">\(\lambda\)</span>, für das wir über die Simulationsdurchläufe durchschnittlich den geringsten <span class="math inline">\(\text{MSE}\)</span> beobachten.</p>
<p><strong>Ridge-Regression</strong></p>
<div class="cell">
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Ergebnisse für Ridge-Regression zusammenfassen</span></span>
<span><span class="va">sim_data_r</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span></span>
<span>  </span>
<span>  lambda <span class="op">=</span> <span class="va">lambdas_r</span>,</span>
<span>  </span>
<span>  <span class="st">"MSE"</span> <span class="op">=</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map_dbl</a></span><span class="op">(</span></span>
<span>    .x <span class="op">=</span> <span class="va">pred_r</span>,  </span>
<span>    .f <span class="op">=</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">.x</span> <span class="op">-</span> <span class="va">Y_0</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span>  <span class="op">)</span>,</span>
<span>  </span>
<span>  <span class="st">"Bias^2"</span> <span class="op">=</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map_dbl</a></span><span class="op">(</span></span>
<span>    .x <span class="op">=</span> <span class="va">pred_r</span>, </span>
<span>    .f <span class="op">=</span> <span class="op">~</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">.x</span><span class="op">)</span> <span class="op">-</span> <span class="va">Y_0</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span>
<span>  <span class="op">)</span>,</span>
<span>  </span>
<span>  <span class="st">"Varianz"</span> <span class="op">=</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map_dbl</a></span><span class="op">(</span></span>
<span>    .x <span class="op">=</span> <span class="va">pred_r</span>, </span>
<span>    .f <span class="op">=</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">.x</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer</a></span><span class="op">(</span></span>
<span>    cols <span class="op">=</span> <span class="op">-</span><span class="va">lambda</span>, </span>
<span>    values_to <span class="op">=</span> <span class="st">"Wert"</span>,</span>
<span>    names_to <span class="op">=</span> <span class="st">"Statistik"</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># Lambda bei MSE-Minimum bestimmen</span></span>
<span><span class="va">MSE_min_r</span> <span class="op">&lt;-</span> <span class="va">sim_data_r</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span></span>
<span>    <span class="va">Statistik</span> <span class="op">==</span> <span class="st">"MSE"</span>,</span>
<span>    <span class="va">Wert</span> <span class="op">==</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">Wert</span><span class="op">)</span></span>
<span>  <span class="op">)</span> </span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Lasso-Regression</strong></p>
<div class="cell">
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Ergebnisse zusammenfassen</span></span>
<span><span class="va">sim_data_l</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span></span>
<span>  </span>
<span>  lambda <span class="op">=</span> <span class="va">lambdas_l</span>,</span>
<span>  </span>
<span>  <span class="st">"MSE"</span> <span class="op">=</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map_dbl</a></span><span class="op">(</span></span>
<span>    .x <span class="op">=</span> <span class="va">pred_l</span>,  </span>
<span>    .f <span class="op">=</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">.</span> <span class="op">-</span> <span class="va">Y_0</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span>  <span class="op">)</span>,</span>
<span>  </span>
<span>  <span class="st">"Bias^2"</span> <span class="op">=</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map_dbl</a></span><span class="op">(</span></span>
<span>    .x <span class="op">=</span> <span class="va">pred_l</span>, </span>
<span>    .f <span class="op">=</span> <span class="op">~</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">.</span><span class="op">)</span> <span class="op">-</span> <span class="va">Y_0</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span>
<span>  <span class="op">)</span>,</span>
<span>  </span>
<span>  <span class="st">"Varianz"</span> <span class="op">=</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map_dbl</a></span><span class="op">(</span></span>
<span>    .x <span class="op">=</span> <span class="va">pred_l</span>, </span>
<span>    .f <span class="op">=</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">.</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer</a></span><span class="op">(</span></span>
<span>    cols <span class="op">=</span> <span class="op">-</span><span class="va">lambda</span>, </span>
<span>    values_to <span class="op">=</span> <span class="st">"Wert"</span>, </span>
<span>    names_to <span class="op">=</span> <span class="st">"Statistik"</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># Lambda bei MSE-Minimum bestimmen</span></span>
<span><span class="va">MSE_min_l</span> <span class="op">&lt;-</span> <span class="va">sim_data_l</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span></span>
<span>    <span class="va">Statistik</span> <span class="op">==</span> <span class="st">"MSE"</span>,</span>
<span>    <span class="va">Wert</span> <span class="op">==</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">Wert</span><span class="op">)</span></span>
<span>  <span class="op">)</span> </span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Die Datensätze im langen Format, <code>sim_data_r</code> und <code>sim_data_l</code>, werden nun für die Visualisierung der Ergebnisse mit <code>ggplo2</code> genutzt.</p>
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># MSE, Bias^2 und Varianz gegen Lambda plotten</span></span>
<span></span>
<span><span class="co"># Ridge-Regression</span></span>
<span><span class="va">sim_data_r</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span></span>
<span>    mapping <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span></span>
<span>      x <span class="op">=</span> <span class="va">lambda</span>, </span>
<span>      y <span class="op">=</span> <span class="va">Wert</span>, </span>
<span>      color <span class="op">=</span> <span class="va">Statistik</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">MSE_min_r</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Lasso-Regression</span></span>
<span><span class="va">sim_data_l</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span></span>
<span>    mapping <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span></span>
<span>      x <span class="op">=</span> <span class="va">lambda</span>, </span>
<span>      y <span class="op">=</span> <span class="va">Wert</span>, </span>
<span>      color <span class="op">=</span> <span class="va">Statistik</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">MSE_min_l</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-MSEBVT" class="quarto-layout-panel page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-MSEBVT-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-MSEBVT" style="flex-basis: 100.0%;justify-content: flex-start;">
<div id="fig-MSEBVT-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure"><div aria-describedby="fig-MSEBVT-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="RegReg_files/figure-html/fig-MSEBVT-1.png" class="img-fluid figure-img" data-ref-parent="fig-MSEBVT" width="672">
</div>
<figcaption class="quarto-float-caption-margin quarto-subfloat-caption quarto-subfloat-fig" id="fig-MSEBVT-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Ridge Regression
</figcaption></figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-MSEBVT" style="flex-basis: 100.0%;justify-content: flex-start;">
<div id="fig-MSEBVT-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure"><div aria-describedby="fig-MSEBVT-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="RegReg_files/figure-html/fig-MSEBVT-2.png" class="img-fluid figure-img" data-ref-parent="fig-MSEBVT" width="672">
</div>
<figcaption class="quarto-float-caption-margin quarto-subfloat-caption quarto-subfloat-fig" id="fig-MSEBVT-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Lasso Regression
</figcaption></figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-MSEBVT-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Abbildung&nbsp;13.9: Simulierte MSE-Komponenten in Abhängigkeit von Lambda
</figcaption></figure>
</div>
<p>Anhand von <a href="#fig-MSEBVT" class="quarto-xref">Abbildung&nbsp;<span>13.9</span></a> lässt sich der Bias-Variance-Tradeoff bei der Vorhersage von <span class="math inline">\(Y_0\)</span> gut erkennen: Bereits für kleine <span class="math inline">\(\lambda\)</span> erzielen beide Methode eine deutliche Reduktion des MSE. Dies wir durch etwas zusätzlichen Bias, aber eine überproportionale Verringerung der Varianz erreicht. Der erkennbare funktionale Zusammenhang zeigt, dass der MSE eine konvexe Funktion von <span class="math inline">\(\lambda\)</span> ist. Damit existieren optimale <span class="math inline">\(\lambda\)</span> mit minimalem MSE (grüne Punkte), die wir mit Cross Validation schätzen können.</p>
</section></section><section id="inferenz-für-treatment-effekt-schätzung-mit-vielen-variablen" class="level2 page-columns page-full" data-number="13.4"><h2 data-number="13.4" class="anchored" data-anchor-id="inferenz-für-treatment-effekt-schätzung-mit-vielen-variablen">
<span class="header-section-number">13.4</span> Inferenz für Treatment-Effekt-Schätzung mit vielen Variablen</h2>
<p>In empirischen Studien des Effekts einer Behandlungsvariable <span class="math inline">\(B\)</span> auf eine Outcome-Variable <span class="math inline">\(Y\)</span> steht häufig eine Vielzahl potentieller Kontrollvariablen zur Verfügung. Häufig ist unklar, welche Variablen in das Modell aufgenommen werden sollten, um das Risiko einer verzerrten Schätzung durch ausgelassene Variablen zu vermindern und gleichzeitig eine Schätzung mit geringer Varianz zu gewährleisten. Ist der Beobachtungsumfang <span class="math inline">\(N\)</span> relativ zur Variablenanzahl <span class="math inline">\(k\)</span> groß, so kann die KQ-Schätzung einer langen Regression (ein Modell mit allen <span class="math inline">\(k\)</span> Kontrollvariablen) gute Ergebnisse liefern. In der Praxis liegt diese wünschenswerte Situation jedoch oft nicht vor und es ist <span class="math inline">\(k\lesssim N\)</span> oder sogar <span class="math inline">\(k&gt;N\)</span>. Dann ist eine KQ-Schätzung des Behandlungseffekts anhand aller <span class="math inline">\(k\)</span> Variablen mit hoher Varianz behaftet bzw. gar nicht möglich.<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> Ein weiteres Szenario ist <span class="math inline">\(k(N)&gt;N\)</span>, d.h. die Anzahl der Regressoren kann mit dem Beobachtungsumfang wachsen.<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a> Lasso-Verfahren können dann hilfreich sein, um Determinanten von <span class="math inline">\(Y\)</span> <em>und</em> <span class="math inline">\(B\)</span> zu identifizieren und damit eine Menge an Kontrollvariablen zu selektieren, für die eine erwartungstreue und konsistente Schätzung des interessierenden Effekts wahrscheinlich ist.</p>
<div class="no-row-height column-margin column-container"><div id="fn19"><p><sup>19</sup>&nbsp;Beachte, dass der KQ-Schätzer bei <span class="math inline">\(k&gt;N\)</span> nicht lösbar ist.</p></div><div id="fn20"><p><sup>20</sup>&nbsp;Dieses Szenario wird unter Bedingungen bzgl. der Wachstumsrate und der Größe der Koeffizienten betrachet, s. <span class="citation" data-cites="BelloniChernozhukov2013">(<a href="Literatur.html#ref-BelloniChernozhukov2013" role="doc-biblioref">Belloni und Chernozhukov 2013</a>)</span>.</p></div><div id="fn21"><p><sup>21</sup>&nbsp;<span class="citation" data-cites="Hahnetal2018">Hahn u.&nbsp;a. (<a href="Literatur.html#ref-Hahnetal2018" role="doc-biblioref">2018</a>)</span> geben eine ausführliche Erläuterung dieser Problematik.</p></div></div><p>Betrachte zunächst das Modell mit allen Kontrollvariablen <span class="math inline">\(X_j\)</span>, <span class="math display">\[\begin{align}
  Y_i = \beta_0 + \alpha_0 B_i + \sum_{j=1}^k \beta_{j} X_{i,j} + u_i, \label{eq:lassotmt}
\end{align}\]</span> wobei einige <span class="math inline">\(\beta_{j}=0\)</span> sind und wir annehmen, dass <span class="math inline">\(B\)</span> lediglich mit ein paar der <span class="math inline">\(X_j\)</span> korrelliert. Die Shrinkage der geschätzten Koeffizienten aus einer naiven Lasso-Regression von <span class="math inline">\(\eqref{eq:lassotmt}\)</span> führt grundsätzlich zu einer verzerrten Schätzung des Behandlungseffekts <span class="math inline">\(\alpha_0\)</span> und damit zu ungültiger Inferenz.<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a></p>
<p>Die Verzerrung von geschätzten Koeffizienten kann vermieden werden, indem Lasso lediglich zur Selektion von Kontrollvariablen verwendet wird. Dabei wird mit einer Lasso-Regression von <span class="math inline">\(Y\)</span> auf die <span class="math inline">\(X_j\)</span> eine Teilmenge von Regressoren <span class="math inline">\(\mathcal{S}\)</span> selektiert und der Treatment-Effekt anschließend mit der KQ-Schätzung von <span class="math display">\[\begin{align}
  Y_i = \beta_0 + \alpha_0 B_i + \sum_{j\in\mathcal{S}} \beta_{j} X_{i,j} + e_i,
\end{align}\]</span> basierend auf der Selektion <span class="math inline">\(\mathcal{S}\)</span> berechnet wird.<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a> Ein solcher <em>Post-Lasso-Selection-Schätzer</em> <span class="citation" data-cites="BelloniChernozhukov2013">(<a href="Literatur.html#ref-BelloniChernozhukov2013" role="doc-biblioref">Belloni und Chernozhukov 2013</a>)</span> ist jedoch im Allgemeinen und insbesondere in hoch-dimensionalen Settings nicht konsistent für <span class="math inline">\(\alpha_0\)</span> und nicht asymptotisch normalverteilt, da weiterhin die Gefahr einer verzerrten Schätzung durch in <span class="math inline">\(\mathcal{S}\)</span> ausgelassene Variablen besteht, die mit <span class="math inline">\(B\)</span> korrelieren: Lasso selektiert Variablen <span class="math inline">\(X_j\)</span>, die “gut” <span class="math inline">\(Y\)</span> erklären. Dabei kann nicht ausgeschlossen werden, das ein Modell gewählt wird, dass relevante Determinanten von <span class="math inline">\(B\)</span> auslässt. Selbst wenn wir ein mit Lasso gewähltes Modell mit KQ (d.h. ohne Shrinkage) schätzen, würde <span class="math inline">\(\alpha_0\)</span> verzerrt geschätzt!</p>
<div class="no-row-height column-margin column-container"><div id="fn22"><p><sup>22</sup>&nbsp;Solche Verfahren werden <em>Post-Selection-Schätzer</em> gennant.</p></div></div><p><span class="citation" data-cites="Bellonietal2014">Belloni, Chernozhukov, und Hansen (<a href="Literatur.html#ref-Bellonietal2014" role="doc-biblioref">2014</a>)</span> schlagen ein alternatives Verfahren vor, dass auf Selektion der Determinanten <span class="math inline">\(X_j\)</span> von <span class="math inline">\(Y\)</span> und <span class="math inline">\(B\)</span> basiert. Dieses Verfahren wird als <em>Post-Double Selection</em> bezeichnet und kann wiefolgt implementiert werden:</p>
<p><strong>Post-Double-Selection-Schätzer</strong></p>
<ol type="1">
<li><p>Bestimme die Determinanten <span class="math inline">\(X_j\)</span> von <span class="math inline">\(Y\)</span> mit Lasso-Regression und bezeichne die Menge der selektierten Variablen als <span class="math inline">\(\mathcal{S}_Y\)</span>.</p></li>
<li><p>Bestimme die Determinanten <span class="math inline">\(X_j\)</span> von <span class="math inline">\(B\)</span> mit Lasso-Regression und bezeichne die Menge der selektierten Variablen als <span class="math inline">\(\mathcal{S}_B\)</span>.</p></li>
<li><p>Bestimme die Schnittmenge <span class="math inline">\(\mathcal{S}_{YB} = \mathcal{S}_Y \cap \mathcal{S}_B\)</span>. Schätze den Treatment-Effekt als <span class="math inline">\(\widehat{\alpha}_0\)</span> in der KQ-Regression <span class="math display">\[\begin{align}
  Y_i = \beta_0 + \alpha_0 B_i + \sum_{j\in\mathcal{S}_{YB}} \beta_{j} X_{i,j} + v_i.
\end{align}\]</span></p></li>
</ol>
<p><span class="citation" data-cites="Bellonietal2014">Belloni, Chernozhukov, und Hansen (<a href="Literatur.html#ref-Bellonietal2014" role="doc-biblioref">2014</a>)</span> zeigen, dass <span class="math inline">\(\widehat{\alpha}_0\)</span> aus diesem Verfahren ein asymptotisch normalverteiler Schätzer für <span class="math inline">\(\alpha_0\)</span> ist und herkömmliche t-Tests und Konfidenzintervalle gültige Inferenz erlauben.</p>
<p>Wir illustrieren die in diesem Abschnitt betrachteten Schätzer nun anhand simulierter Daten mit R. Die fiktive Problemstellung ist die Schätzung eines wahren Treatment-Effekts <span class="math inline">\(\alpha_0 = 2\)</span>, wenn so viele potenzielle Kontrollvariablen vorliegen, dass der KQ-Schätzer gerade noch berechnet werden kann, aber aufgrund hoher Varianz unzuverlässig ist. Hierzu erzeugen wir <span class="math inline">\(Y\)</span> gemäß der Vorschrift <span class="math display">\[\begin{align*}
  Y_i =&amp;\, \alpha_0 B_i + \sum_{j=1}^{k_Y} \beta_{j}^Y X_{i,j}^Y + \sum_{l=1}^{k_{YB}} \beta_{l}^{YB} X_{i,l}^{YB} + u_i,\\
  \\
  \beta_j^{YB} \overset{u.i.v}{\sim}&amp;\,N(10,1), \quad \beta_j^{Y} \overset{u.i.v}{\sim}U(0,1), \quad u_i \overset{u.i.v}{\sim}N(0,1).\\
  \\
  i=&amp;\,1,\dots,550
\end{align*}\]</span></p>
<p>Die Behandlungsvariable <span class="math inline">\(B_i\)</span> entspricht der Vorschrift <span class="math display">\[\begin{align*}
  B_i =&amp;\, \sum_{l=1}^{k_{YB}} \beta_{l}^{YB} X_{i,l}^{YB} + e_i,\\
  \\
  \beta_j^{YB} \overset{u.i.v}{\sim}&amp;\,N(2,0.2), \quad e_i \overset{u.i.v}{\sim}N(0,1).
\end{align*}\]</span> Wir wählen <span class="math inline">\(k_{YB} = k_{Y} = 25\)</span>. Zusätzlich zu <span class="math inline">\(B\)</span>, den Determinanten von <span class="math inline">\(Y\)</span> <em>und</em> <span class="math inline">\(B\)</span> (<span class="math inline">\(X^{YB}\)</span>) sowie den Variablen, die ausschließlich <span class="math inline">\(Y\)</span> beeinflussen (<span class="math inline">\(X^{Y}\)</span>) gibt es <span class="math inline">\(k_U = 499\)</span> Variablen <span class="math inline">\(X^U\)</span>, die weder <span class="math inline">\(Y\)</span> noch <span class="math inline">\(B\)</span> beeinflussen und damit irrelevant für die Schätzung des Behandlungseffekts sind. Wir haben also <span class="math inline">\(N=550\)</span> Beobachtungen und insgesamt <span class="math inline">\(k = 1+k_{Y} + k_{YB} + k_{U} = 550\)</span> potenzielle Kontrollvariablen von denen <span class="math inline">\(k_{YB} = 25\)</span> für eine unverzerrte Schätzung von <span class="math inline">\(\alpha_0\)</span> relevant sind.</p>
<p>Der nachstehende Code generiert die Daten gemäß der Vorschrift.</p>
<div class="cell">
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://mvtnorm.R-forge.R-project.org">mvtnorm</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">4321</span><span class="op">)</span></span>
<span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">550</span>      <span class="co"># Beobachtungen</span></span>
<span><span class="va">p_Y</span> <span class="op">&lt;-</span> <span class="fl">25</span>     <span class="co"># Determinanten Y</span></span>
<span><span class="va">p_B</span> <span class="op">&lt;-</span> <span class="fl">25</span>     <span class="co"># Determinanten B *und* Y</span></span>
<span><span class="va">p_U</span> <span class="op">&lt;-</span> <span class="fl">499</span>    <span class="co"># irrelevante Variablen </span></span>
<span></span>
<span><span class="co"># Variablen generieren</span></span>
<span><span class="va">XB</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html">rmvnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, sigma <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">p_B</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">XU</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html">rmvnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, sigma <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">p_U</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">XY</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html">rmvnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, sigma <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">p_Y</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Stetige Behandlungsvariable erzeugen</span></span>
<span><span class="va">B</span> <span class="op">&lt;-</span> <span class="va">XB</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">p_B</span>, <span class="fl">2</span>, sd <span class="op">=</span> <span class="fl">.2</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Abh. Variable erzeugen, Behandlungseffekt (ATE) ist 2</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">B</span> <span class="op">+</span> </span>
<span>  <span class="va">XB</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">p_B</span>, mean <span class="op">=</span> <span class="fl">10</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="va">XY</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">p_Y</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Variablen in tibble sammeln</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">B</span>, <span class="va">XB</span>, <span class="va">XU</span>, <span class="va">XY</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://tibble.tidyverse.org/reference/as_tibble.html">as_tibble</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Namen zuweisen</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>  <span class="st">"B"</span>, </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"XB"</span>, <span class="fl">1</span><span class="op">:</span><span class="va">p_B</span><span class="op">)</span>, </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"XU"</span>, <span class="fl">1</span><span class="op">:</span><span class="va">p_U</span><span class="op">)</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"XY"</span>, <span class="fl">1</span><span class="op">:</span><span class="va">p_Y</span><span class="op">)</span> </span>
<span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Wünschenswert wäre die KQ-Schätzung des wahren Modells. Diese ergibt eine Schätzung nahe des wahren Treatment-Effekts <span class="math inline">\(\alpha_0 = 2\)</span>. Unter realen Bedingungen wäre diese Regression jedoch nicht implementierbar, weil die relevanten Kovariablen <code>XB</code> unbekannt sind.</p>
<div class="cell">
<div class="sourceCode" id="cb59"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># KQ: Wahres Modell schätzen</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">B</span> <span class="op">+</span> <span class="va">XB</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="st">"B"</span><span class="op">]</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       B 
1.937031 </code></pre>
</div>
</div>
<p>Wir schätzen daher zunächst die “lange” Regression mit allen <span class="math inline">\(k\)</span> verfügbaren Variablen mit KQ. Beachte, dass der KQ-Schätzer für <span class="math inline">\(\alpha_0\)</span> zwar implementierbar und erwartungstreu ist, jedoch eine hohe Varianz aufweist. Wegen <span class="math inline">\(k=N=550\)</span> erhalten wir eine perfekte Anpassung an die Daten und können mangels Freiheitsgraden keine Hypothesentests durchführen.</p>
<div class="cell">
<div class="sourceCode" id="cb61"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># KQ: Lange Regression schätzen</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">.</span> <span class="op">-</span> <span class="fl">1</span>, data <span class="op">=</span> <span class="va">X</span><span class="op">)</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="st">"B"</span><span class="op">]</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       B 
3.079497 </code></pre>
</div>
</div>
<p>Die KQ-Schätzung von <span class="math inline">\(\alpha_0\)</span> anhand der langen Regression weicht deutlich vom wahren Wert <span class="math inline">\(\alpha_0 = 2\)</span> ab.</p>
<p>Eine “kurze” KQ-Regression nur mit der Behandlungsvariable <span class="math inline">\(B\)</span> führt wegen Korrelation mit den ausgelassenen Determinanten in <code>XB</code> zu einer deutlich verzerrten Schätzung.</p>
<div class="cell">
<div class="sourceCode" id="cb63"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># KQ: Kurze Regression</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">B</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="st">"B"</span><span class="op">]</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       B 
6.716837 </code></pre>
</div>
</div>
<p>Die Methoden von <span class="citation" data-cites="BelloniChernozhukov2013">Belloni und Chernozhukov (<a href="Literatur.html#ref-BelloniChernozhukov2013" role="doc-biblioref">2013</a>)</span> und <span class="citation" data-cites="Bellonietal2014">Belloni, Chernozhukov, und Hansen (<a href="Literatur.html#ref-Bellonietal2014" role="doc-biblioref">2014</a>)</span> sind im R-Paket <code>hdm</code> implementiert. Mit den Funktionen <code>hrm::rlasso()</code> und <code><a href="https://rdrr.io/pkg/hdm/man/rlassoEffects.html">hdm::rlassoEffect</a></code> kann Lasso-Regression sowie Post- und Double-Post-Selection durchgeführt werden.<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn23"><p><sup>23</sup>&nbsp;Diese Funktionen ermitteln ein optimales <span class="math inline">\(\lambda\)</span> mit dem in <span class="citation" data-cites="Bellonietal2012">Belloni u.&nbsp;a. (<a href="Literatur.html#ref-Bellonietal2012" role="doc-biblioref">2012</a>)</span> vorgeschlagenen Algorithmus.</p></div></div><p>Wir berechnen zunächst den naiven Lasso-Schätzer in einem Modell mit allen Variablen.</p>
<div class="cell">
<div class="sourceCode" id="cb65"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">hdm</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Naiver Post-Lasso-Schätzer</span></span>
<span><span class="va">lasso</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/hdm/man/rlasso.html">rlasso</a></span><span class="op">(</span></span>
<span>  x <span class="op">=</span> <span class="va">X</span>, </span>
<span>  y <span class="op">=</span> <span class="va">Y</span>, </span>
<span>  intercept <span class="op">=</span> <span class="cn">F</span>, </span>
<span>  post <span class="op">=</span> <span class="cn">F</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Koeffizientenschätzer auslesen</span></span>
<span><span class="va">lasso</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="st">"B"</span><span class="op">]</span> </span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       B 
6.368456 </code></pre>
</div>
</div>
<p>Auch dieser Schätzer ist deutlich verzerrt. Problematisch ist hier nicht nur die Shrinkage auf <span class="math inline">\(\widehat{\alpha}_0\)</span>, sondern die Selektion der Variablen in <code>XB</code>:</p>
<div class="cell">
<div class="sourceCode" id="cb67"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Welche Variablen in XB selektiert Lasso *nicht*?</span></span>
<span><span class="va">nselektiert</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">lasso</span><span class="op">$</span><span class="va">coef</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">26</span><span class="op">]</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span>   <span class="co"># ID</span></span>
<span></span>
<span><span class="co"># Namen auslesen</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">lasso</span><span class="op">$</span><span class="va">coef</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">26</span><span class="op">]</span><span class="op">)</span><span class="op">[</span><span class="va">nselektiert</span><span class="op">]</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "XB8"  "XB10" "XB16" "XB18" "XB20"</code></pre>
</div>
</div>
<p>Durch das Auslassen dieser Determinanten von <span class="math inline">\(Y\)</span> und <span class="math inline">\(B\)</span> leidet der Lasso-Schätzer unter OVB.</p>
<p>Als nächstes berechnen wir den Post-Lasso-Selection-Schätzer.</p>
<div class="cell">
<div class="sourceCode" id="cb69"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Post-Lasso-Selection-Schätzer berechnen</span></span>
<span><span class="va">p_lasso</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/hdm/man/rlasso.html">rlasso</a></span><span class="op">(</span></span>
<span>  x <span class="op">=</span> <span class="va">X</span>,</span>
<span>  y <span class="op">=</span> <span class="va">Y</span>, </span>
<span>  intercept <span class="op">=</span> <span class="cn">F</span>, </span>
<span>  post <span class="op">=</span> <span class="cn">T</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Schätzung für alpha_0</span></span>
<span><span class="va">p_lasso</span><span class="op">$</span><span class="va">coef</span><span class="op">[</span><span class="st">"B"</span><span class="op">]</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       B 
6.362409 </code></pre>
</div>
</div>
<p>Die Ähnlichkeit der Post-Lasso-Schätzung von <span class="math inline">\(\alpha_0\)</span> zur Lasso-Schätzung zeigt deutlich, dass die Verzerrung des Lasso-Schätzers überwiegend durch ausgelassene Variablen anstatt durch Shrinkage verursacht wird.</p>
<p>Mit <code><a href="https://rdrr.io/pkg/hdm/man/rlassoEffects.html">rlassoEffect()</a></code> können wir den Post-Double-Selection-Schätzer berechnen.</p>
<div class="cell">
<div class="sourceCode" id="cb71"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Post-Double-Selection-Schätzer</span></span>
<span><span class="va">pds_lasso</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/hdm/man/rlassoEffects.html">rlassoEffect</a></span><span class="op">(</span></span>
<span>  x <span class="op">=</span> <span class="va">X</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>    <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">B</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  y <span class="op">=</span> <span class="va">Y</span>, </span>
<span>  d <span class="op">=</span> <span class="va">B</span>, </span>
<span>  method <span class="op">=</span> <span class="st">"double selection"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Schnittmenge der selektierten Determinanten </span></span>
<span><span class="co"># von Y und B</span></span>
<span><span class="op">(</span></span>
<span>  <span class="va">S_BY</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">pds_lasso</span><span class="op">$</span><span class="va">selection.index</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "XB1"   "XB2"   "XB3"   "XB4"   "XB5"   "XB6"   "XB7"   "XB8"   "XB9"  
[10] "XB10"  "XB11"  "XB12"  "XB13"  "XB14"  "XB15"  "XB16"  "XB17"  "XB18" 
[19] "XB19"  "XB20"  "XB21"  "XB22"  "XB23"  "XB24"  "XB25"  "XU209" "XU241"
[28] "XU295" "XY3"   "XY7"   "XY8"   "XY12"  "XY13"  "XY15"  "XY16"  "XY19" 
[37] "XY23" </code></pre>
</div>
</div>
<p>Double Selection führt ebenfalls zu einem Post-Lasso-KQ-Schätzer mit allen 25 relevaten Variablen in <code>XB</code>. Wir selektieren allerdings deutlich weniger irrelevante Variablen aus <code>XU</code> als mit Single Selection und dennoch einige Determinanten von <span class="math inline">\(Y\)</span> aus <code>XY</code>. Double Selection führt also zu einer unverzerrten Schätzen mit geringerer Varianz. Mit <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> erhalten wir gültige Inferenz bzgl. des Treatment-Effekts.</p>
<div class="cell">
<div class="sourceCode" id="cb73"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">pds_lasso</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Estimates and significance testing of the effect of target variables"
   Estimate. Std. Error t value Pr(&gt;|t|)    
d1   1.94977    0.07127   27.36   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>Der Post-Double-Selection-Schätzer liefert unter den betrachteten Verfahren die beste Schätzung von <span class="math inline">\(\alpha_0\)</span> und erlaubt gülstige statistische Inferenz. Der geschätzte Effekt ist hoch-signifikant.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Facts zum Post-Double-Selection-Schätzer
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Durch die sorgfältige Auswahl von Variablen, die mit Behandlung- und Outcome-Variable zusammenhängen, ermöglicht die Double-Selection eine bessere Kontrolle über das Risiko ausgelassender Variablen in Beobachtungsstudien und ermöglicht gültige (asymptotisch normale) Inferenz.</p></li>
<li>
<p>Der Post-Double-Selection-Schätzer besteht aus drei Regressionen:</p>
<ol type="1">
<li>Es werden Variablen mit Lasso selektiert, welche die <em>Behandlungs-Variable</em> erklären.</li>
<li>Es werden Variablen mit Lasso selektiert, welche die <em>Outcome-Variable</em> erklären.</li>
<li>Der Post-Double-Selection-Schätzer ist der KQ-Schätzer in einer Regression, die für die Schnittmenge der ausgewählten Variablen kontrolliert.</li>
</ol>
</li>
<li><p>Dank der Selektion mit Lasso kann der Schätzer auch bei hoch-dimensionalen Daten (<span class="math inline">\(k&gt;n\)</span>) angewendet werden.</p></li>
<li><p>Post-Double-Selection-Schätzer für Behandlungseffekte sind im R-Paket <code>hdm</code> implementiert.</p></li>
</ul>
</div>
</div>
<section id="case-study-makroökonomisches-wachstum" class="level3 page-columns page-full" data-number="13.4.1"><h3 data-number="13.4.1" class="anchored" data-anchor-id="case-study-makroökonomisches-wachstum">
<span class="header-section-number">13.4.1</span> Case Study: Makroökonomisches Wachstum</h3>
<p>Zur Illustration des Post-Double-Selection Schätzers betrachten wir eine empirische Anwendung bzgl. der Validierung von makroökonomischer Wachstumtheorie. Aus neo-klassischen Ansätzen wie dem <a href="https://de.wikipedia.org/wiki/Solow-Modell">Solow-Swan-Modell</a> kann die Hypothese, dass Volkswirtschaften zu einem gemeinsamen Wachstumspfad hin konvergieren, abgeleitet werden. Diese Konvergenzhypothese impliziert die Existenz von Aufholeffekten: Ärmere Volkswirtschaften müssen im mittel schneller Wachsen als die Wirschaft wohlhabender Länder. Die grundlegende Spezifikation eines entsprechenden Regressionsmodells lautet <span class="math display">\[\begin{align}
  \text{WR}_{i} = \alpha_0 \text{BIP0}_i + u_i, \label{eq:growthmodel1}
\end{align}\]</span> wobei <span class="math inline">\(\text{WR}_{i}\)</span> die Wachstumsrate des Pro-Kopf-BIP in Land <span class="math inline">\(i\)</span> über einen Zeitraum (typischerweise berechnet als Log-Differenz zwischen zwei Perioden) und <span class="math inline">\(\text{BIP0}_i\)</span> das (logarithmierte) Pro-Kopf-BIP zu beginn der Referenzperiode ist. Gemäß der Konvergenzhypothese muss <span class="math inline">\(\alpha_0&lt;0\)</span> sein: Je wohlhabender eine Volkswirtschaft ist, desto geringer ist das Wirtschaftswachstum.</p>
<p>Um Verzerrung durch ausgelassene Kovariablen zu vermeiden, sollte das Modell <span class="math inline">\(\eqref{eq:growthmodel1}\)</span> um länder-spezifische Regressoren <span class="math inline">\(x_{i,j}\)</span>, die sowohl das Ausgagnsniveau <span class="math inline">\(\text{BIP0}\)</span> sowie die Wachtumsrate beinflussen, erweitert werden. Zu der großen Menge potentieller Kovariablen gehören makro- und sozio-ökonomische Maße wie bspw. die Investitionstätigkeit des Staates, Offenheit der Volkswirtschaft, das politische Umfeld, das Bildungsniveau, die Demographie usw. Eine bevorzugte Spezifikation ist daher <span class="math display">\[\begin{align}
  \text{WR}_{i} = \alpha_0 \text{BIP0}_i + \sum_{j=1}^k \beta_j x_{i,j} + u_i,\label{eq:growthmodel2}
\end{align}\]</span> wobei <span class="math inline">\(\alpha_0\)</span> als Behandlungseffekt interpretiert werden kann. Beachte, dass <span class="math inline">\(\eqref{eq:growthmodel2}\)</span> eine Regression in der Form von <span class="math inline">\(\eqref{eq:lassotmt}\)</span> ist.</p>
<p>Wir illustrieren die Schätzung von und Inferenz bzgl. <span class="math inline">\(\alpha_0\)</span> in <span class="math inline">\(\eqref{eq:growthmodel2}\)</span> mit Post-Double-Selektion für einen 90 Länder umfassenden Auszug aus dem Datensatz von <span class="citation" data-cites="BarroLee2013">Barro und Lee (<a href="Literatur.html#ref-BarroLee2013" role="doc-biblioref">2013</a>)</span>, der als Objekt <code>GrowthData</code> im R-Paket <code>hdm</code> verfügbar ist.<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn24"><p><sup>24</sup>&nbsp;Eine ausführliche Beschreibung der Variablen ist <a href="https://www2.nber.org/pub/barro.lee/readme.txt">hier</a> einsehbar.</p></div></div><div class="cell">
<div class="sourceCode" id="cb75"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Datensatz in Arbeitsumgebung verfügbar machen</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">hdm</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">GrowthData</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Anzahl Beobachtungen und Variablen</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">GrowthData</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 90 63</code></pre>
</div>
</div>
<p>Die Spalte <code>Outcome</code> ist die jeweilige Wachstumsrate des BIP zwischen den Perioden 1965-1975 und 1975-1985 und <code>gdpsh465</code> ist das reale Pro-Kopf-BIP im Jahr 1965 zu Preisen von 1980.</p>
<p>Wir führen zunächst eine graphische Analyse hinsichtlich des Modells einfachen Modells <span class="math inline">\(\eqref{eq:growthmodel1}\)</span> durch, indem wir <code>gdpsh465</code> gegen <code>Outcome</code> plotten und die geschätzte Regressionsgerade einzeichnen.</p>
<div class="cell page-columns page-full">
<div class="sourceCode" id="cb77"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Einfache grafische Analyse mit ggplot2</span></span>
<span><span class="va">GrowthData</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span></span>
<span>    mapping <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span></span>
<span>      x <span class="op">=</span> <span class="va">gdpsh465</span>, </span>
<span>      y <span class="op">=</span> <span class="va">Outcome</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, se <span class="op">=</span> <span class="cn">F</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display page-columns page-full">
<div id="fig-bipsimple" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-bipsimple-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="RegReg_files/figure-html/fig-bipsimple-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-bipsimple-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Abbildung&nbsp;13.10: BIP-Wachstum: Einfache Regression
</figcaption></figure>
</div>
</div>
</div>
<p><a href="#fig-bipsimple" class="quarto-xref">Abbildung&nbsp;<span>13.10</span></a> zeigt einen geringen positiven geschätzten Effekt <span class="math inline">\(\widehat{\alpha}_0\)</span>. Eine Auswertung mit <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> ergibt, dass der Effekt <span class="math inline">\(\alpha_0\)</span> nicht signifikant von <span class="math inline">\(0\)</span> verschieden ist.</p>
<div class="cell">
<div class="sourceCode" id="cb78"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Einfache Regression durchführen, </span></span>
<span><span class="co"># Inferenz für gdpsh465 erhalten</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Outcome</span> <span class="op">~</span> <span class="va">gdpsh465</span>, data <span class="op">=</span> <span class="va">GrowthData</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coefficients</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="va">.</span><span class="op">[</span><span class="fl">2</span>, <span class="op">]</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   Estimate  Std. Error     t value    Pr(&gt;|t|) 
0.001316713 0.006102200 0.215776701 0.829661165 </code></pre>
</div>
</div>
<p>Der positive Effekt aus der einfachen Schätzung widerspricht der Konvergenzhypothese. Dieses Ergebnis könnte allerdings durch Auslassen relevanter Kovariablen ungültig sein. Beispielsweise ist es plausibel, dass das Bildungsniveau einer Volkswirtschaft sowohl mit dem BIP korreliert ist als auch die Wachstumsrate beeinflusst. Dann wäre das Bildungsniveau eine relevante Kovariable, deren Auslassen zu einer verzerrten Schätzung von <span class="math inline">\(\alpha_0\)</span> führt.</p>
<p>Eine “lange” Regression mit allen Kovariablen ist zwar möglich, aber problematisch: Das Verhältnis von Beobachtungen (90) zu Regressoren (62) bedeutet eine hohe Unsicherheit der Schätzung.</p>
<div class="cell">
<div class="sourceCode" id="cb80"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Inferenz für alpha_0 in langer Regression</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Outcome</span> <span class="op">~</span> <span class="va">.</span> <span class="op">-</span> <span class="fl">1</span> , data <span class="op">=</span> <span class="va">GrowthData</span><span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coefficients</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="va">.</span><span class="op">[</span><span class="fl">2</span>, <span class="op">]</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    Estimate   Std. Error      t value     Pr(&gt;|t|) 
-0.009377989  0.029887726 -0.313773911  0.756018518 </code></pre>
</div>
</div>
<p>Der geschätzte Koeffizient <span class="math inline">\(\widehat{\alpha}_0\)</span> ist nun zwar negativ, liefert jedoch weiterhin keine Evidenz, dass <span class="math inline">\(\alpha_0\)</span> von 0 verschieden ist. Ein Vergleich der Standardfehler zeigt aber, dass die KQ-Schätzung aufgrund Berücksichtigung aller potentiellen Kovariablen mit deutlich größerer Varianz behaftet ist als in der einfachen KQ-Regression <span class="math inline">\(\eqref{eq:growthmodel1}\)</span></p>
<p>Post-Double-Selection erlaubt gültige Inferenz bzgl. <span class="math inline">\(\alpha_0\)</span> nach Schätzung der Menge relevanter Kovariablen. Wir weisen die entsprechenden Variablen R-Objekten zu und berechnen den Schätzer.</p>
<div class="cell">
<div class="sourceCode" id="cb82"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Variablen für Post-Double-Selection vorbereiten</span></span>
<span></span>
<span><span class="co"># abh. Variable</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">GrowthData</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">Outcome</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># "Treatment"</span></span>
<span><span class="va">d</span> <span class="op">&lt;-</span> <span class="va">GrowthData</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">gdpsh465</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># potentielle Regressoren</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="va">GrowthData</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span></span>
<span>    <span class="op">-</span><span class="va">Outcome</span>, <span class="op">-</span><span class="va">intercept</span>, <span class="op">-</span><span class="va">gdpsh465</span></span>
<span>  <span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb83"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Post-Double-Selection-Schätzer berechnen</span></span>
<span><span class="va">Growth_DS</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/hdm/man/rlassoEffects.html">rlassoEffect</a></span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="va">X</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>      <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="op">)</span>, </span>
<span>    y <span class="op">=</span> <span class="va">y</span>, </span>
<span>    d <span class="op">=</span> <span class="va">d</span>, </span>
<span>    method <span class="op">=</span> <span class="st">"double selection"</span></span>
<span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Post-Double-Selection wählt aus der Menge potentieller Kovariablen lediglich sieben Regressoren aus.</p>
<div class="cell">
<div class="sourceCode" id="cb84"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Selektierte Variablen einsehen</span></span>
<span><span class="co"># ID</span></span>
<span><span class="va">Selektion</span> <span class="op">&lt;-</span> <span class="va">Growth_DS</span><span class="op">$</span><span class="va">selection.index</span></span>
<span></span>
<span><span class="co"># Namen auslesen</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">Selektion</span> <span class="op">==</span> <span class="cn">T</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "bmp1l"    "freetar"  "hm65"     "sf65"     "lifee065" "humanf65" "pop6565" </code></pre>
</div>
</div>
<p><a href="#tbl-growthpdssek" class="quarto-xref">Tabelle&nbsp;<span>13.3</span></a> zeigt die Definitionen der ausgewählten Variablen.</p>
<div class="cell page-columns page-full">
<div id="tbl-growthpdssek" class="cell quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-tbl figure page-columns page-full"><div aria-describedby="tbl-growthpdssek-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div id="kxrqdwghhf" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#kxrqdwghhf table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#kxrqdwghhf thead, #kxrqdwghhf tbody, #kxrqdwghhf tfoot, #kxrqdwghhf tr, #kxrqdwghhf td, #kxrqdwghhf th {
  border-style: none;
}

#kxrqdwghhf p {
  margin: 0;
  padding: 0;
}

#kxrqdwghhf .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #000000;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #000000;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#kxrqdwghhf .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#kxrqdwghhf .gt_title {
  color: #000000;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#kxrqdwghhf .gt_subtitle {
  color: #000000;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#kxrqdwghhf .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#kxrqdwghhf .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#kxrqdwghhf .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #000000;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #000000;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#kxrqdwghhf .gt_col_heading {
  color: #000000;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: bold;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#kxrqdwghhf .gt_column_spanner_outer {
  color: #000000;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: bold;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#kxrqdwghhf .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#kxrqdwghhf .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#kxrqdwghhf .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #000000;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#kxrqdwghhf .gt_spanner_row {
  border-bottom-style: hidden;
}

#kxrqdwghhf .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #000000;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#kxrqdwghhf .gt_empty_group_heading {
  padding: 0.5px;
  color: #000000;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#kxrqdwghhf .gt_from_md > :first-child {
  margin-top: 0;
}

#kxrqdwghhf .gt_from_md > :last-child {
  margin-bottom: 0;
}

#kxrqdwghhf .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#kxrqdwghhf .gt_stub {
  color: #000000;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#kxrqdwghhf .gt_stub_row_group {
  color: #000000;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#kxrqdwghhf .gt_row_group_first td {
  border-top-width: 2px;
}

#kxrqdwghhf .gt_row_group_first th {
  border-top-width: 2px;
}

#kxrqdwghhf .gt_summary_row {
  color: #000000;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#kxrqdwghhf .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#kxrqdwghhf .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#kxrqdwghhf .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#kxrqdwghhf .gt_grand_summary_row {
  color: #000000;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#kxrqdwghhf .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#kxrqdwghhf .gt_last_grand_summary_row_top {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#kxrqdwghhf .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#kxrqdwghhf .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #000000;
}

#kxrqdwghhf .gt_footnotes {
  color: #000000;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#kxrqdwghhf .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#kxrqdwghhf .gt_sourcenotes {
  color: #000000;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#kxrqdwghhf .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#kxrqdwghhf .gt_left {
  text-align: left;
}

#kxrqdwghhf .gt_center {
  text-align: center;
}

#kxrqdwghhf .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#kxrqdwghhf .gt_font_normal {
  font-weight: normal;
}

#kxrqdwghhf .gt_font_bold {
  font-weight: bold;
}

#kxrqdwghhf .gt_font_italic {
  font-style: italic;
}

#kxrqdwghhf .gt_super {
  font-size: 65%;
}

#kxrqdwghhf .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#kxrqdwghhf .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#kxrqdwghhf .gt_indent_1 {
  text-indent: 5px;
}

#kxrqdwghhf .gt_indent_2 {
  text-indent: 10px;
}

#kxrqdwghhf .gt_indent_3 {
  text-indent: 15px;
}

#kxrqdwghhf .gt_indent_4 {
  text-indent: 20px;
}

#kxrqdwghhf .gt_indent_5 {
  text-indent: 25px;
}

#kxrqdwghhf .katex-display {
  display: inline-flex !important;
  margin-bottom: 0.75em !important;
}

#kxrqdwghhf div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {
  height: 0px !important;
}
</style>
<table class="gt_table do-not-create-environment cell table table-sm table-striped small" data-quarto-postprocess="true" data-quarto-disable-processing="false" data-quarto-bootstrap="false">
<thead><tr class="header gt_col_headings">
<th id="Variable" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" scope="col">Variable</th>
<th id="Beschreibung" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" scope="col">Beschreibung</th>
</tr></thead>
<tbody class="gt_table_body">
<tr class="odd">
<td class="gt_row gt_left" headers="Variable">bmp1l</td>
<td class="gt_row gt_left" headers="Beschreibung">Schwarzmarktprämie d. Währung</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="Variable">freetar</td>
<td class="gt_row gt_left" headers="Beschreibung">Maß für Zollbeschränkungen</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="Variable">hm65</td>
<td class="gt_row gt_left" headers="Beschreibung">Einschreibungsquote Uni (Männer)</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="Variable">sf65</td>
<td class="gt_row gt_left" headers="Beschreibung">Beschulungsquote Sekundarstufe (Frauen)</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="Variable">lifee065</td>
<td class="gt_row gt_left" headers="Beschreibung">Lebenserwartung bei Geburt</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="Variable">humanf65</td>
<td class="gt_row gt_left" headers="Beschreibung">Durschn. Bildung im Alter 25 (Frauen)</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="Variable">pop6565</td>
<td class="gt_row gt_left" headers="Beschreibung">Anteil Bevölkerung ü. 65 Jahre</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-tbl margin-caption" id="tbl-growthpdssek-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabelle&nbsp;13.3: Mit PDS selektierte Variablen aus <code>GrowthData</code>. Referenzjahr 1965.
</figcaption></figure>
</div>
</div>
<div class="cell">
<div class="sourceCode" id="cb86"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Gültige Inferenz mit dem Post-Double-Selection-Schätzer</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">Growth_DS</span><span class="op">)</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Estimates and significance testing of the effect of target variables"
   Estimate. Std. Error t value Pr(&gt;|t|)   
d1  -0.05001    0.01579  -3.167  0.00154 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>Das Ergebnis der Post-Double-Selection-Schätzung unterstützt die (bedingte) Konvergenzhypothese mit einer signifikanten negativen Schätzung <span class="math inline">\(\widehat{\alpha}_0\approx-0.05\)</span>.</p>


<!-- -->

<script type="ojs-module-contents">
eyJjb250ZW50cyI6W119
</script><div id="exercise-loading-indicator" class="exercise-loading-indicator d-none d-flex align-items-center gap-2">
<div id="exercise-loading-status" class="d-flex gap-2">

</div>
<div class="spinner-grow spinner-grow-sm">

</div>
</div>
<script type="vfs-file">
W10=
</script><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-BarroLee2013" class="csl-entry" role="listitem">
Barro, Robert J., und Jong Wha Lee. 2013. <span>„A new data set of educational attainment in the world, 1950–2010“</span>. <em>Journal of Development Economics</em> 104: 184–98. https://doi.org/<a href="https://doi.org/10.1016/j.jdeveco.2012.10.001">https://doi.org/10.1016/j.jdeveco.2012.10.001</a>.
</div>
<div id="ref-Bellonietal2012" class="csl-entry" role="listitem">
Belloni, Alexandre, Daniel Chen, Victor Chernozhukov, und Christian Hansen. 2012. <span>„Sparse models and methods for optimal instruments with an application to eminent domain“</span>. <em>Econometrica</em> 80 (6): 2369–429.
</div>
<div id="ref-BelloniChernozhukov2013" class="csl-entry" role="listitem">
Belloni, Alexandre, und Victor Chernozhukov. 2013. <span>„Least squares after model selection in high-dimensional sparse models“</span>. <em>Bernoulli</em>, 521–47.
</div>
<div id="ref-Bellonietal2014" class="csl-entry" role="listitem">
Belloni, Alexandre, Victor Chernozhukov, und Christian Hansen. 2014. <span>„High-dimensional methods and inference on structural and treatment effects“</span>. <em>Journal of Economic Perspectives</em> 28 (2): 29–50.
</div>
<div id="ref-CortezSilva2008" class="csl-entry" role="listitem">
Cortez, Paulo, und Alice Maria Gonçalves Silva. 2008. <span>„Using data mining to predict secondary school student performance“</span>.
</div>
<div id="ref-Efronetal2004" class="csl-entry" role="listitem">
Efron, Bradley, Trevor Hastie, Iain Johnstone, und Robert Tibshirani. 2004. <span>„Least angle regression“</span>.
</div>
<div id="ref-Hahnetal2018" class="csl-entry" role="listitem">
Hahn, P Richard, Carlos M Carvalho, David Puelz, und Jingyu He. 2018. <span>„Regularization and confounding in linear regression for treatment effect estimation“</span>.
</div>
<div id="ref-HoerlKennard1970" class="csl-entry" role="listitem">
Hoerl, Arthur E, und Robert W Kennard. 1970. <span>„<span>Ridge regression: Biased estimation for nonorthogonal problems</span>“</span>. <em>Technometrics</em> 12 (1): 55–67.
</div>
<div id="ref-Tibshirani1996" class="csl-entry" role="listitem">
Tibshirani, Robert. 1996. <span>„Regression shrinkage and selection via the lasso“</span>. <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> 58 (1): 267–88.
</div>
</div>
</section></section></main><!-- /main --><script type="ojs-module-contents">
eyJjb250ZW50cyI6W119
</script><script type="module">
if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
window._ojs.paths.runtimeToDoc = "../..";
window._ojs.paths.runtimeToRoot = "../..";
window._ojs.paths.docToRoot = "";
window._ojs.selfContained = false;
window._ojs.runtime.interpretFromScriptTags();
</script><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Kopiert");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Kopiert");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./SyntheticControl.html" class="pagination-link" aria-label="Synthetic Control">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Synthetic Control</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./svm.html" class="pagination-link" aria-label="Support Vector Machines">
        <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Quellcode</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb88" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span><span class="co"> live-html</span></span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a><span class="an">engine:</span><span class="co"> knitr</span></span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a><span class="fu"># Regularisierte Regression {#sec-regreg}</span></span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo=F, message=FALSE}</span></span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a><span class="in">library(gt)</span></span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a><span class="in">library(tidyverse)</span></span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a><span class="in"># Formatierung von gt-Tabellen</span></span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a><span class="in">tabopts &lt;- function(x) {</span></span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a><span class="in">    fmt_number(x, decimals = 3, drop_trailing_zeros = T) %&gt;%</span></span>
<span id="cb88-14"><a href="#cb88-14" aria-hidden="true" tabindex="-1"></a><span class="in">  tab_options(table_body.hlines.color = "white", </span></span>
<span id="cb88-15"><a href="#cb88-15" aria-hidden="true" tabindex="-1"></a><span class="in">              column_labels.border.bottom.color = "black", </span></span>
<span id="cb88-16"><a href="#cb88-16" aria-hidden="true" tabindex="-1"></a><span class="in">             column_labels.border.top.color = "black",</span></span>
<span id="cb88-17"><a href="#cb88-17" aria-hidden="true" tabindex="-1"></a><span class="in">             table_body.border.bottom.color = "black", </span></span>
<span id="cb88-18"><a href="#cb88-18" aria-hidden="true" tabindex="-1"></a><span class="in">             table.border.bottom.color = "black",</span></span>
<span id="cb88-19"><a href="#cb88-19" aria-hidden="true" tabindex="-1"></a><span class="in">             column_labels.font.weight = "bold", </span></span>
<span id="cb88-20"><a href="#cb88-20" aria-hidden="true" tabindex="-1"></a><span class="in">             table.font.color = "black", </span></span>
<span id="cb88-21"><a href="#cb88-21" aria-hidden="true" tabindex="-1"></a><span class="in">             table.font.size = 16)</span></span>
<span id="cb88-22"><a href="#cb88-22" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb88-23"><a href="#cb88-23" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-24"><a href="#cb88-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-25"><a href="#cb88-25" aria-hidden="true" tabindex="-1"></a>In diesem Kapitel betrachten wir Varianten von Koeffizientenschätzern im linearen Modell \begin{align}</span>
<span id="cb88-26"><a href="#cb88-26" aria-hidden="true" tabindex="-1"></a> Y_i = \beta_1 X_{1,i} + \dots + \beta_k X_{k,i} + u_i, \quad i = 1,\dots,n,\label{eq:slm}</span>
<span id="cb88-27"><a href="#cb88-27" aria-hidden="true" tabindex="-1"></a>\end{align} deren Motivation die Schätzung von $\boldsymbol{\beta} := (\beta_1, \dots,\beta_k)'$ in Anwendungen ist, in denen der KQ-Schätzer \begin{align}</span>
<span id="cb88-28"><a href="#cb88-28" aria-hidden="true" tabindex="-1"></a>  \begin{split}</span>
<span id="cb88-29"><a href="#cb88-29" aria-hidden="true" tabindex="-1"></a>  \widehat{\boldsymbol{\beta}} =&amp;\, \arg\min_{\boldsymbol{\beta}}\mathrm{RSS}(\boldsymbol{\beta})<span class="sc">\\</span> </span>
<span id="cb88-30"><a href="#cb88-30" aria-hidden="true" tabindex="-1"></a>  =&amp;\,  \arg\min_{\boldsymbol{\beta}}  \sum_{i=1}^n\left(Y_i-\beta_1 X_{1,i} + \dots + \beta_k X_{k,i}\right)^2</span>
<span id="cb88-31"><a href="#cb88-31" aria-hidden="true" tabindex="-1"></a>  \end{split}\label{eq:KQLoss}</span>
<span id="cb88-32"><a href="#cb88-32" aria-hidden="true" tabindex="-1"></a>\end{align} keine stabile Schätzung zulässt oder nicht eindeutig definiert ist, und damit gar nicht erst berechnet werden kann. Solche Szenarien ergeben sich in der empirischen Forschung, wenn die Regressoren stark korreliert sind und/oder das Modell viele Regressoren enthält ($k\lesssim n$), oder das Regressionsproblem hoch-dimensional ist ($k&gt;n$).</span>
<span id="cb88-33"><a href="#cb88-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-34"><a href="#cb88-34" aria-hidden="true" tabindex="-1"></a>Regularisierte Regressionsschätzer begegnen dieser Problematik mit einer Modifikation der Verlustfunktion $\mathrm{RSS}$ in \eqref{eq:KQLoss}, \begin{align}</span>
<span id="cb88-35"><a href="#cb88-35" aria-hidden="true" tabindex="-1"></a>  \mathrm{RSS}(\boldsymbol{\beta}, p, \lambda) := \mathrm{RSS}(\boldsymbol{\beta}) + \lambda\lVert\boldsymbol{\beta}\rVert_p.</span>
<span id="cb88-36"><a href="#cb88-36" aria-hidden="true" tabindex="-1"></a>\end{align} Hierbei ist $\lambda&gt;0$ ein Tuning-Parameter und $p\geq1$ definiert die $p$-Norm des Koeffizientenvektors, \begin{align}</span>
<span id="cb88-37"><a href="#cb88-37" aria-hidden="true" tabindex="-1"></a>  \lVert\boldsymbol{\beta}\rVert_p := \left(\sum_{j=1}^k \lvert\beta_j\rvert^{p}\right)^{1/p}&gt;0.\label{eq:pnorm}</span>
<span id="cb88-38"><a href="#cb88-38" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb88-39"><a href="#cb88-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-40"><a href="#cb88-40" aria-hidden="true" tabindex="-1"></a>Wegen $\lambda\lVert\boldsymbol{\beta}\rVert_p&gt;0$ kann die $p$-Norm des Koeffizientenvektors $\boldsymbol{\beta}$ das Optimierungsproblem $$\min_{\boldsymbol{\beta}} \mathrm{RSS}(\boldsymbol{\beta}, p, \lambda) \vert\, p,\, \lambda$$ derart restringieren, dass die geschätzten Koeffizienten \begin{align*}</span>
<span id="cb88-41"><a href="#cb88-41" aria-hidden="true" tabindex="-1"></a>  \widehat{\boldsymbol{\beta}}_{p,\,\lambda} := \arg\min_{\boldsymbol{\beta}} \mathrm{RSS}(\boldsymbol{\beta}, p, \lambda)</span>
<span id="cb88-42"><a href="#cb88-42" aria-hidden="true" tabindex="-1"></a>\end{align*} im Erwartungswert absolut kleiner ausfallen als bei der KQ-Schätzung: Der Schätzer ist in Richtung 0 verzerrt.[^regreg-1] Dieser Effekt der Regularisierung wird in der Literatur als *Shrinkage* bezeichnet.</span>
<span id="cb88-43"><a href="#cb88-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-44"><a href="#cb88-44" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-1]: </span>Beachte, dass für $\lambda=0$ die Verlustfunktion des KQ-Schätzers folgt.</span>
<span id="cb88-45"><a href="#cb88-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-46"><a href="#cb88-46" aria-hidden="true" tabindex="-1"></a>Die grundlegenden Eigenschaften des Schätzers $\widehat{\boldsymbol{\beta}}_{p,\,\lambda}$ werden maßgeblich durch den Parameter $p$ bestimmt, der hinsichtlich des zu lösenden Regressionsproblems *a priori* gewählt wird.<span class="ot">[^regreg-2]</span></span>
<span id="cb88-47"><a href="#cb88-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-48"><a href="#cb88-48" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-2]: </span>D.h. wir wählen $p$, um einen Schätzer mit für die konkrete Anwendung hilfreichen Eigenschaften zu erhalten.</span>
<span id="cb88-49"><a href="#cb88-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-50"><a href="#cb88-50" aria-hidden="true" tabindex="-1"></a>Shrinkage ist eine Motivation für die Anwendung regularisierter Schätzer in Modellen, die auch mit KQ geschätzt werden könnten. Um dies zu verstehen, nehmen wir an, dass die Gauss-Markov-Annahmen in \eqref{eq:slm} gelten. Dann hat der KQ-Schätzer die kleinste Varianz unter allen *unverzerrten* Schätzern. Aufgrund der Shrinkage fallen regularisierte Schätzer zwar nicht unter das Gauss-Markov-Theorem, können dafür aber eine geringere Varianz haben als KQ. Schätzer mit solchen Eigenschaften sind nützlich, wenn eine unverzerrte Schätzung von $\boldsymbol{\beta}$ nicht unser primäres Ziel ist: Für Vorhersagen kann es hilfreich sein, etwas Verzerrung bei der Koeffizientenschätzung in Kauf zu nehmen, um eine hinreichend große Varianzreduktion zu erreichen, sodass ein geringerer erwarteter Vorhersagefehler als für KQ resultiert. Hierbei liegt, eine Abwägung zwischen Verzerrung und Varianz (*Bias Variance Tradeoff*) vor, der durch den Regularisierungsparameter $\lambda$ beeinflusst wird.</span>
<span id="cb88-51"><a href="#cb88-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-52"><a href="#cb88-52" aria-hidden="true" tabindex="-1"></a>Für die Berechnung des Schätzers in empirischen Anwendungen wird $\lambda$ meist datengetrieben (mit <span class="co">[</span><span class="ot">Cross Validation</span><span class="co">](https://de.wikipedia.org/wiki/Kreuzvalidierungsverfahren)</span> oder einem Informationskriterium) geschätzt oder mit einer analytisch fundierten Faustregel gewählt.</span>
<span id="cb88-53"><a href="#cb88-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-54"><a href="#cb88-54" aria-hidden="true" tabindex="-1"></a>Nachfolgend betrachten wir zwei häufig verwendete regularisierte Schätzer, die sich durch die Wahl $p=1$ (Lasso Regression) bzw. $p=2$ (Ridge Regression) ergeben und illustrieren ihre Anwendung mit R.</span>
<span id="cb88-55"><a href="#cb88-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-56"><a href="#cb88-56" aria-hidden="true" tabindex="-1"></a><span class="fu">## Ridge Regression</span></span>
<span id="cb88-57"><a href="#cb88-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-58"><a href="#cb88-58" aria-hidden="true" tabindex="-1"></a>Ridge Regression wurde von @HoerlKennard1970 als Alternative zur KQ-Schätzung bei hoch-korrelierten Regressoren eingeführt. Die Verlustfunktion lautet \begin{align}</span>
<span id="cb88-59"><a href="#cb88-59" aria-hidden="true" tabindex="-1"></a>  \mathrm{RSS}(\boldsymbol{\beta},p=2,\lambda) = \mathrm{RSS}(\boldsymbol{\beta}) + \lambda \lVert\boldsymbol{\beta}\rVert_2,\label{eq:ridgeloss}</span>
<span id="cb88-60"><a href="#cb88-60" aria-hidden="true" tabindex="-1"></a>\end{align} d.h. der Parameter $\lambda$ reguliert den Einfluss eines $\ell_2$-Strafterms \begin{align*}</span>
<span id="cb88-61"><a href="#cb88-61" aria-hidden="true" tabindex="-1"></a>  \lVert\boldsymbol{\beta}\rVert_2 = \sqrt{\sum_{j=1}^k\beta_j^2}</span>
<span id="cb88-62"><a href="#cb88-62" aria-hidden="true" tabindex="-1"></a>\end{align*} auf die Verlustfunktion $\mathrm{RSS}(\boldsymbol{\beta},p=2,\lambda)$. Der Ridge-Schätzer ergibt sich als \begin{align}</span>
<span id="cb88-63"><a href="#cb88-63" aria-hidden="true" tabindex="-1"></a>  \widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda := \arg\min_{\boldsymbol{\beta}}\mathrm{RSS}(\boldsymbol{\beta}) + \lambda \lVert\boldsymbol{\beta}\rVert_2.\label{eq:ridgereg}</span>
<span id="cb88-64"><a href="#cb88-64" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb88-65"><a href="#cb88-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-66"><a href="#cb88-66" aria-hidden="true" tabindex="-1"></a>Für Das Optimierungsproblem \eqref{eq:ridgereg} kann wir aus den Bedingungen 1. Ordnung \begin{align}</span>
<span id="cb88-67"><a href="#cb88-67" aria-hidden="true" tabindex="-1"></a>  -2\boldsymbol{X}'(\boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\beta}) + 2\lambda\boldsymbol{\beta} = \boldsymbol{0}</span>
<span id="cb88-68"><a href="#cb88-68" aria-hidden="true" tabindex="-1"></a>\end{align} die analytische Lösung \begin{align}</span>
<span id="cb88-69"><a href="#cb88-69" aria-hidden="true" tabindex="-1"></a>  \widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda = (\boldsymbol{X}'\boldsymbol{X} + \lambda\boldsymbol{I}_p)^{-1}\boldsymbol{X}'\boldsymbol{Y},\label{eq:ridgecf}</span>
<span id="cb88-70"><a href="#cb88-70" aria-hidden="true" tabindex="-1"></a>\end{align} bestimmt werden, wobei $\boldsymbol{I}_k$ die $k\times k$ Einheitsmatrix ist. Aus Gleichung \eqref{eq:ridgecf} kann die Wirkungsweise des Strafterms $\lambda \lVert\boldsymbol{\beta}\rVert_2$ abgeleitet werden: Ridge Regression modifiziert die Diagonale der zu invertierenden Matrix $\boldsymbol{X}'\boldsymbol{X}$ durch Addition von $\lambda&gt;0$. Dies ist hilfreich, wenn</span>
<span id="cb88-71"><a href="#cb88-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-72"><a href="#cb88-72" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$k\geq n$ und damit $\boldsymbol{X}'\boldsymbol{X}$ nicht invertiertbar (singulär) ist. Dann kann der KQ-Schätzer nicht berechnet werden.<span class="ot">[^regreg-3]</span> Die Inverse $(\boldsymbol{X}'\boldsymbol{X} + \lambda\boldsymbol{I}_p)^{-1}$ hingegen existiert unter milden Bedingungen.</span>
<span id="cb88-73"><a href="#cb88-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-74"><a href="#cb88-74" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>hohe Kollinearität vorliegt, sodass $(\boldsymbol{X}'\boldsymbol{X})^{-1}$ zwar existiert, aber zu einer instablilen KQ-Schätzung mit hoher Varianz führt.</span>
<span id="cb88-75"><a href="#cb88-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-76"><a href="#cb88-76" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-3]: </span>Beispiel: <span class="in">`X &lt;- matrix(rnorm(100), ncol = 10)`</span>. Vergleiche <span class="in">`solve(t(X) %*% X)`</span> und <span class="in">`solve(t(X) %*% X + diag(.01, nrow = 10))`</span></span>
<span id="cb88-77"><a href="#cb88-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-78"><a href="#cb88-78" aria-hidden="true" tabindex="-1"></a>Für eine grafische Betrachtung des Optimierungskalküls \eqref{eq:ridgereg} betrachten wir die äquivalente Darstellung als Lagrange-Problem \begin{align}</span>
<span id="cb88-79"><a href="#cb88-79" aria-hidden="true" tabindex="-1"></a>  \widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda := \arg\min_{\lVert\boldsymbol{\beta}\rVert&lt;t}\mathrm{RSS}(\boldsymbol{\beta}).\label{eq:ridgeLg}</span>
<span id="cb88-80"><a href="#cb88-80" aria-hidden="true" tabindex="-1"></a>\end{align} In der folgenden interaktiven Grafik illustrieren wir das Optimierungsproblem \eqref{eq:ridgeLg} sowie den resultierenden Schätzer der Koeffizienten $(\beta_1, \beta_2)$ in einem multiplen Regressionsmodell mit den Regressoren $X_1$ und $X_2$.</span>
<span id="cb88-81"><a href="#cb88-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-82"><a href="#cb88-82" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Die blaue Ellipse ist die Menge aller Schätzwerte $\left(\widehat\beta_{1},\, \widehat\beta_{2}\right)$ für den angegebenen Wert von $\mathrm{RSS}$. Im Zentrum der Ellipse liegt der KQ-Schätzer, welcher $\mathrm{RSS}$ minimiert.</span>
<span id="cb88-83"><a href="#cb88-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-84"><a href="#cb88-84" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Der blaue Kreis ist die Menge aller Koeffizienten-Paare $(\beta_1, \beta_2)$, welche die Restriktion $\beta_1^2 + \beta_2^2\leq t$ erfüllen. Beachte, dass die Größe des Kreises nur durch den Parameter $t$ bestimmt wird, welcher für einen vorgegebenen Wertebereich variiert werden kann.</span>
<span id="cb88-85"><a href="#cb88-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-86"><a href="#cb88-86" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Der blaue Punkt ist der Ridge-Schätzer $(\widehat\beta^R_{1,t},\, \widehat\beta^R_{2,t})$. Dieser ergibt sich als Schnittpunkt zwischen der blauen $\mathrm{RSS}$-Ellipse und der Restriktionsregion und variiert mit $t$. Die gestrichelte rote Kurve zeigt den Ridge-Lösungspfad.</span>
<span id="cb88-87"><a href="#cb88-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-88"><a href="#cb88-88" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Für kleine Werte $t$ drückt die Shrinkage die geschätzten Koeffizienten Richtung 0, wobei der Lösungspfad i.d.R. nicht-linear verläuft, d.h. die Shrinkage auf den Koeffizienten ist grundsätzlich unterschiedlich. Die Lösung $(\widehat\beta^R_{1,t},\, \widehat\beta^R_{2,t}) = (0,0)$ existiert nur als Grenzwert für $t\to0$.</span>
<span id="cb88-89"><a href="#cb88-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-90"><a href="#cb88-90" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Beachte, dass der Effekt von $t$ auf die Schätzung umgekehrt für $\lambda$ verläuft: Größere $\lambda$ führen zu stärkerer Regularisierung.</span>
<span id="cb88-91"><a href="#cb88-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-92"><a href="#cb88-92" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb88-93"><a href="#cb88-93" aria-hidden="true" tabindex="-1"></a>&lt;iframe width="100%" height="500" frameborder="0" src="https://observablehq.com/embed/45a59e74a4330581?cells=viewof+m%2Cchart114"&gt;</span>
<span id="cb88-94"><a href="#cb88-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-95"><a href="#cb88-95" aria-hidden="true" tabindex="-1"></a>&lt;/iframe&gt;</span>
<span id="cb88-96"><a href="#cb88-96" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb88-97"><a href="#cb88-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-98"><a href="#cb88-98" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="pdf"}</span>
<span id="cb88-99"><a href="#cb88-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-100"><a href="#cb88-100" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb88-101"><a href="#cb88-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-102"><a href="#cb88-102" aria-hidden="true" tabindex="-1"></a>***Diese Interaktive Komponente des Buchs ist nur in der Online-Version verfügbar.***</span>
<span id="cb88-103"><a href="#cb88-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-104"><a href="#cb88-104" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb88-105"><a href="#cb88-105" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb88-106"><a href="#cb88-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-107"><a href="#cb88-107" aria-hidden="true" tabindex="-1"></a><span class="fu">### Eigenschaften des Schätzers</span></span>
<span id="cb88-108"><a href="#cb88-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-109"><a href="#cb88-109" aria-hidden="true" tabindex="-1"></a>Der Ridge-Schätzer $\widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda$ ist nicht invariant gegenüber der Skalierung der Regressoren. Für empirische Daten sollte daher vorab eine Standardisierung der erklärenden Variablen durchgeführt werden.<span class="ot">[^regreg-4]</span> Um die Eigenschaften des Ridge-Schätzers besser zu verstehen, betrachten wir hier den Fall orthonormaler Regressoren $\boldsymbol{X}_j$.<span class="ot">[^regreg-5]</span> Dann ist \begin{align}</span>
<span id="cb88-110"><a href="#cb88-110" aria-hidden="true" tabindex="-1"></a>  \widehat{\beta}^{\mathrm{R}}_{\lambda,\,j} = (1+\lambda)^{-1} \cdot\widehat{\beta}_j,\quad j = 1,\dots,k,\label{eq:ridgeortho}</span>
<span id="cb88-111"><a href="#cb88-111" aria-hidden="true" tabindex="-1"></a>\end{align} d.h. der Ridge-Schätzer skaliert die KQ-Lösung mit einem von $\lambda$ abhängigen Faktor.<span class="ot">[^regreg-6]</span></span>
<span id="cb88-112"><a href="#cb88-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-113"><a href="#cb88-113" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-4]: </span>Bspw. mit der Funktion <span class="in">`scale()`</span>.</span>
<span id="cb88-114"><a href="#cb88-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-115"><a href="#cb88-115" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-5]: </span>Orthonormalität heißt $\boldsymbol{X}_i'\boldsymbol{X}_j = 1$ für $i=j$ und $0$ sonst. Dann ist $\boldsymbol{X}$'$\boldsymbol{X} = \boldsymbol{I}_k$.</span>
<span id="cb88-116"><a href="#cb88-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-117"><a href="#cb88-117" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-6]: </span>$(1+\lambda)^{-1}$ wird auch als *Shrinkage-Faktor* bezeichnet.</span>
<span id="cb88-118"><a href="#cb88-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-119"><a href="#cb88-119" aria-hidden="true" tabindex="-1"></a>Wir illustrieren dies, indem wir den Zusammenhang zwischen KQ- und Ridge-Schätzer im orthonormalen Fall als R-Funktion <span class="in">`ridge_ortho()`</span> implementieren und für die Parameterwerte $\lambda\in<span class="sc">\{</span>0,0.5,2<span class="sc">\}</span>$ plotten.</span>
<span id="cb88-120"><a href="#cb88-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-121"><a href="#cb88-121" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, warning=F}</span></span>
<span id="cb88-122"><a href="#cb88-122" aria-hidden="true" tabindex="-1"></a><span class="in">library(tidyverse)</span></span>
<span id="cb88-123"><a href="#cb88-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-124"><a href="#cb88-124" aria-hidden="true" tabindex="-1"></a><span class="in"># Funktion für Rige Regression bei orthonormalen Regressoren</span></span>
<span id="cb88-125"><a href="#cb88-125" aria-hidden="true" tabindex="-1"></a><span class="in">ridge_ortho &lt;- function(KQ, lambda) {</span></span>
<span id="cb88-126"><a href="#cb88-126" aria-hidden="true" tabindex="-1"></a><span class="in">  1/(1 + lambda) * KQ</span></span>
<span id="cb88-127"><a href="#cb88-127" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb88-128"><a href="#cb88-128" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-129"><a href="#cb88-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-130"><a href="#cb88-130" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, warning=F}</span></span>
<span id="cb88-131"><a href="#cb88-131" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Shrinkage des OLS-Schätzers bei Ridge Regression"</span></span>
<span id="cb88-132"><a href="#cb88-132" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: "fig-ridgeortho"</span></span>
<span id="cb88-133"><a href="#cb88-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-134"><a href="#cb88-134" aria-hidden="true" tabindex="-1"></a><span class="in"># KQ-Schätzer gegen Ridge-Schätzer plotten</span></span>
<span id="cb88-135"><a href="#cb88-135" aria-hidden="true" tabindex="-1"></a><span class="in">dat &lt;- tibble(KQ = seq(-1, 1, .01))</span></span>
<span id="cb88-136"><a href="#cb88-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-137"><a href="#cb88-137" aria-hidden="true" tabindex="-1"></a><span class="in">ggplot(dat) +</span></span>
<span id="cb88-138"><a href="#cb88-138" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_function(fun = ridge_ortho, </span></span>
<span id="cb88-139"><a href="#cb88-139" aria-hidden="true" tabindex="-1"></a><span class="in">                args = list(lambda =  0), </span></span>
<span id="cb88-140"><a href="#cb88-140" aria-hidden="true" tabindex="-1"></a><span class="in">                lty = 2) + </span></span>
<span id="cb88-141"><a href="#cb88-141" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_function(fun = ridge_ortho, </span></span>
<span id="cb88-142"><a href="#cb88-142" aria-hidden="true" tabindex="-1"></a><span class="in">                args = list(lambda = .5), </span></span>
<span id="cb88-143"><a href="#cb88-143" aria-hidden="true" tabindex="-1"></a><span class="in">                col = "red") + </span></span>
<span id="cb88-144"><a href="#cb88-144" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_function(fun = ridge_ortho, </span></span>
<span id="cb88-145"><a href="#cb88-145" aria-hidden="true" tabindex="-1"></a><span class="in">                args = list(lambda = 2), </span></span>
<span id="cb88-146"><a href="#cb88-146" aria-hidden="true" tabindex="-1"></a><span class="in">                col = "blue") + </span></span>
<span id="cb88-147"><a href="#cb88-147" aria-hidden="true" tabindex="-1"></a><span class="in">  xlim(-.4, .4) +</span></span>
<span id="cb88-148"><a href="#cb88-148" aria-hidden="true" tabindex="-1"></a><span class="in">  xlab("KQ-Schätzer von beta_1") +</span></span>
<span id="cb88-149"><a href="#cb88-149" aria-hidden="true" tabindex="-1"></a><span class="in">  ylab("Ridge-Schätzer von beta_1")</span></span>
<span id="cb88-150"><a href="#cb88-150" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-151"><a href="#cb88-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-152"><a href="#cb88-152" aria-hidden="true" tabindex="-1"></a>@fig-ridgeortho zeigt, dass der Ridge-Schätzer eine lineare Transformation des KQ-Schätzers (gestrichelte Linie) ist. Größere Werte des Regularisierungsparameters $\lambda$ führen zu stärkerer Shrinkage des Koeffizientenschätzers in Richtung 0. Die $\ell_2$-Norm führt zu proportional zum Absolutwert des KQ-Schätzers verlaufender Shrinkage: Größere Koeffizienten werden stärker bestraft als kleine Koeffizienten.</span>
<span id="cb88-153"><a href="#cb88-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-154"><a href="#cb88-154" aria-hidden="true" tabindex="-1"></a>Die Eigenschaft $$\mathrm{E}\left(\widehat{\boldsymbol{\beta}}^{\mathrm{R}}_{\lambda,\,j}\right) = (1+\lambda)^{-1} \cdot \beta_j$$ zeigt, dass $\widehat{\boldsymbol{\beta}}^{\mathrm{R}}_{\lambda,\,j}$ (für fixes $\lambda&gt;0$) nicht erwartungstreu für $\beta_j$ ist. Weiterhin ist \begin{align*}</span>
<span id="cb88-155"><a href="#cb88-155" aria-hidden="true" tabindex="-1"></a>  \mathrm{Var}\left(\widehat{\beta}^{\mathrm{R}}_{\lambda,\,j}\right) =&amp;\, </span>
<span id="cb88-156"><a href="#cb88-156" aria-hidden="true" tabindex="-1"></a>  \mathrm{Var}\left(\widehat{\beta}_j\right) \cdot \left(\frac{\lambda}{1+\lambda^2}\right)<span class="sc">\\</span></span>
<span id="cb88-157"><a href="#cb88-157" aria-hidden="true" tabindex="-1"></a>    =&amp;\, \sigma^2\cdot \left(\frac{\lambda}{1+\lambda^2}\right),</span>
<span id="cb88-158"><a href="#cb88-158" aria-hidden="true" tabindex="-1"></a>\end{align*} wobei $\sigma^2$ die Varianz des Regressionsfehlers $u$ ist. Wegen $\lambda&lt;(1+\lambda)^2$ für $\lambda&gt;0$ gilt $$\mathrm{Var}\left(\widehat{\beta}^{\mathrm{R}}_{\lambda,\,j}\right)&lt;\mathrm{Var}\left(\widehat{\beta}_j\right).$$ Der Ridge-Schätzer hat also eine kleinere Varianz als der KQ-Schätzer. Diese Eigenschaften können auch für korrelierte Regressoren gezeigt werden.</span>
<span id="cb88-159"><a href="#cb88-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-160"><a href="#cb88-160" aria-hidden="true" tabindex="-1"></a><span class="fu">### Ridge Regression mit `glmnet`</span></span>
<span id="cb88-161"><a href="#cb88-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-162"><a href="#cb88-162" aria-hidden="true" tabindex="-1"></a>Wir zeigen nun anhand simulierter Daten, wie der Ridge-Lösungspfad mit dem R-Paket <span class="in">`glmnet`</span> berechnet werden kann. Wir erzeugen zunächst Daten gemäß der Vorschrift \begin{align}</span>
<span id="cb88-163"><a href="#cb88-163" aria-hidden="true" tabindex="-1"></a>  \begin{split}</span>
<span id="cb88-164"><a href="#cb88-164" aria-hidden="true" tabindex="-1"></a>  Y_i =&amp;\, \boldsymbol{X}_i' \boldsymbol{\beta} + u_i,<span class="sc">\\</span></span>
<span id="cb88-165"><a href="#cb88-165" aria-hidden="true" tabindex="-1"></a>  <span class="sc">\\</span></span>
<span id="cb88-166"><a href="#cb88-166" aria-hidden="true" tabindex="-1"></a>  \beta_j =&amp;\,  \frac{5}{j^2}, \qquad\qquad\ j=1,\dots,5,<span class="sc">\\</span> </span>
<span id="cb88-167"><a href="#cb88-167" aria-hidden="true" tabindex="-1"></a>  \beta_j =&amp;\, -\frac{5}{(j-5)^2}, \quad j=6,\dots,10,<span class="sc">\\</span></span>
<span id="cb88-168"><a href="#cb88-168" aria-hidden="true" tabindex="-1"></a>  <span class="sc">\\</span></span>
<span id="cb88-169"><a href="#cb88-169" aria-hidden="true" tabindex="-1"></a>  \boldsymbol{X}_i \sim&amp;\, N(\boldsymbol{0}, \boldsymbol{\Sigma}), \quad u_i \overset{u.i.v.}{\sim} N(0, 1), \quad i = 1,\dots,25.</span>
<span id="cb88-170"><a href="#cb88-170" aria-hidden="true" tabindex="-1"></a>  \end{split} \label{eq:ridgedgp1}</span>
<span id="cb88-171"><a href="#cb88-171" aria-hidden="true" tabindex="-1"></a>\end{align} Hierbei wird $\boldsymbol{\Sigma}$ so definiert, dass jeder Regressor $N(0,1)$-verteilt ist und eine Korrelation von $0.8$ mit allen anderen Regressoren aufweist. Mit der Vorschrift für die $\beta_j$ stellen wir sicher, dass es wenige Variablen gibt, die $Y$ stark beeinflussen, da der Absolutbetrag der Koeffizienten in $j$ abnimmt.<span class="ot">[^regreg-7]</span></span>
<span id="cb88-172"><a href="#cb88-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-173"><a href="#cb88-173" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-7]: </span>Für bessere Interpretierbarkeit der Grafischen Auswertung, wählen wir positive und negative Koeffizienten mit gleichem Bertag.</span>
<span id="cb88-174"><a href="#cb88-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-175"><a href="#cb88-175" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, warning=F}</span></span>
<span id="cb88-176"><a href="#cb88-176" aria-hidden="true" tabindex="-1"></a><span class="in">library(gendata)</span></span>
<span id="cb88-177"><a href="#cb88-177" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(1234)</span></span>
<span id="cb88-178"><a href="#cb88-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-179"><a href="#cb88-179" aria-hidden="true" tabindex="-1"></a><span class="in"># Parameter definieren</span></span>
<span id="cb88-180"><a href="#cb88-180" aria-hidden="true" tabindex="-1"></a><span class="in">N &lt;- 80</span></span>
<span id="cb88-181"><a href="#cb88-181" aria-hidden="true" tabindex="-1"></a><span class="in">k &lt;- 10</span></span>
<span id="cb88-182"><a href="#cb88-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-183"><a href="#cb88-183" aria-hidden="true" tabindex="-1"></a><span class="in">coefs &lt;- 5/(1:(k/2))^2</span></span>
<span id="cb88-184"><a href="#cb88-184" aria-hidden="true" tabindex="-1"></a><span class="in">beta &lt;- c(coefs, -coefs)</span></span>
<span id="cb88-185"><a href="#cb88-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-186"><a href="#cb88-186" aria-hidden="true" tabindex="-1"></a><span class="in"># Beobachtungen simulieren</span></span>
<span id="cb88-187"><a href="#cb88-187" aria-hidden="true" tabindex="-1"></a><span class="in">X &lt;- as.matrix(</span></span>
<span id="cb88-188"><a href="#cb88-188" aria-hidden="true" tabindex="-1"></a><span class="in">  genmvnorm(</span></span>
<span id="cb88-189"><a href="#cb88-189" aria-hidden="true" tabindex="-1"></a><span class="in">    k = k, </span></span>
<span id="cb88-190"><a href="#cb88-190" aria-hidden="true" tabindex="-1"></a><span class="in">    cor = rep(.8, (k^2-k)/2), </span></span>
<span id="cb88-191"><a href="#cb88-191" aria-hidden="true" tabindex="-1"></a><span class="in">    n = N)</span></span>
<span id="cb88-192"><a href="#cb88-192" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb88-193"><a href="#cb88-193" aria-hidden="true" tabindex="-1"></a><span class="in">Y &lt;- X %*% beta + rnorm(N)</span></span>
<span id="cb88-194"><a href="#cb88-194" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-195"><a href="#cb88-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-196"><a href="#cb88-196" aria-hidden="true" tabindex="-1"></a>Wir schätzen nun ein Modell mit allen <span class="in">`r k`</span> Regressoren mit <span class="in">`glmnet`</span>. Beachte, dass für den Ridge-Strafterm <span class="in">`alpha = 0`</span> gesetzt werden muss.<span class="ot">[^regreg-8]</span></span>
<span id="cb88-197"><a href="#cb88-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-198"><a href="#cb88-198" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-8]: </span><span class="in">`alpha`</span> ist ein Mischparameter im Algorithmus für <span class="co">[</span><span class="ot">elastic net</span><span class="co">](https://en.wikipedia.org/wiki/Elastic_net_regularization)</span>, siehe <span class="in">`?glmnet`</span>.</span>
<span id="cb88-199"><a href="#cb88-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-202"><a href="#cb88-202" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-203"><a href="#cb88-203" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb88-204"><a href="#cb88-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-205"><a href="#cb88-205" aria-hidden="true" tabindex="-1"></a><span class="co"># Ridge-Regression anpassen</span></span>
<span id="cb88-206"><a href="#cb88-206" aria-hidden="true" tabindex="-1"></a>ridge_fit <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(</span>
<span id="cb88-207"><a href="#cb88-207" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> X, </span>
<span id="cb88-208"><a href="#cb88-208" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> Y, </span>
<span id="cb88-209"><a href="#cb88-209" aria-hidden="true" tabindex="-1"></a>  <span class="at">alpha =</span> <span class="dv">0</span> <span class="co"># für Ridge-Strafterm</span></span>
<span id="cb88-210"><a href="#cb88-210" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-211"><a href="#cb88-211" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-212"><a href="#cb88-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-213"><a href="#cb88-213" aria-hidden="true" tabindex="-1"></a>Der Lösungspfad der Ridge-Schätzung kann nach Transformation der geschätzen Koeffizienten und der zugehörigen $\lambda$-Werte in ein langes Format überführt und komfortabel mit <span class="in">`ggplot2`</span> dargestellt werden.</span>
<span id="cb88-214"><a href="#cb88-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-215"><a href="#cb88-215" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, warning=F}</span></span>
<span id="cb88-216"><a href="#cb88-216" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Lösungspfad für Ridge-Schätzung"</span></span>
<span id="cb88-217"><a href="#cb88-217" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-ridgesolpath</span></span>
<span id="cb88-218"><a href="#cb88-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-219"><a href="#cb88-219" aria-hidden="true" tabindex="-1"></a><span class="in"># Lambda-Sequenz auslesen</span></span>
<span id="cb88-220"><a href="#cb88-220" aria-hidden="true" tabindex="-1"></a><span class="in">lambdas &lt;- ridge_fit$lambda</span></span>
<span id="cb88-221"><a href="#cb88-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-222"><a href="#cb88-222" aria-hidden="true" tabindex="-1"></a><span class="in"># Ridge-Schätzung für Lambdas im langen Format </span></span>
<span id="cb88-223"><a href="#cb88-223" aria-hidden="true" tabindex="-1"></a><span class="in">as.matrix(ridge_fit$beta) %&gt;% </span></span>
<span id="cb88-224"><a href="#cb88-224" aria-hidden="true" tabindex="-1"></a><span class="in">  as_tibble() %&gt;% </span></span>
<span id="cb88-225"><a href="#cb88-225" aria-hidden="true" tabindex="-1"></a><span class="in">  rownames_to_column("Variable") %&gt;%</span></span>
<span id="cb88-226"><a href="#cb88-226" aria-hidden="true" tabindex="-1"></a><span class="in">  pivot_longer(-Variable) %&gt;% </span></span>
<span id="cb88-227"><a href="#cb88-227" aria-hidden="true" tabindex="-1"></a><span class="in">  group_by(Variable) %&gt;% </span></span>
<span id="cb88-228"><a href="#cb88-228" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(lambda = lambdas) %&gt;%</span></span>
<span id="cb88-229"><a href="#cb88-229" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb88-230"><a href="#cb88-230" aria-hidden="true" tabindex="-1"></a><span class="in">  # Grafik mit ggplot erzeugen</span></span>
<span id="cb88-231"><a href="#cb88-231" aria-hidden="true" tabindex="-1"></a><span class="in">  ggplot(</span></span>
<span id="cb88-232"><a href="#cb88-232" aria-hidden="true" tabindex="-1"></a><span class="in">    mapping = aes(</span></span>
<span id="cb88-233"><a href="#cb88-233" aria-hidden="true" tabindex="-1"></a><span class="in">      x = lambda, </span></span>
<span id="cb88-234"><a href="#cb88-234" aria-hidden="true" tabindex="-1"></a><span class="in">      y = value, </span></span>
<span id="cb88-235"><a href="#cb88-235" aria-hidden="true" tabindex="-1"></a><span class="in">      col = Variable</span></span>
<span id="cb88-236"><a href="#cb88-236" aria-hidden="true" tabindex="-1"></a><span class="in">    )</span></span>
<span id="cb88-237"><a href="#cb88-237" aria-hidden="true" tabindex="-1"></a><span class="in">  ) + </span></span>
<span id="cb88-238"><a href="#cb88-238" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_line() +</span></span>
<span id="cb88-239"><a href="#cb88-239" aria-hidden="true" tabindex="-1"></a><span class="in">  ylab("gesch. Koeffizienten") +</span></span>
<span id="cb88-240"><a href="#cb88-240" aria-hidden="true" tabindex="-1"></a><span class="in">  scale_x_log10("log_10(lambda)")</span></span>
<span id="cb88-241"><a href="#cb88-241" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-242"><a href="#cb88-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-243"><a href="#cb88-243" aria-hidden="true" tabindex="-1"></a>@fig-ridgesolpath zeigt den nicht-linearen Verlauf der Shrinkage auf den geschätzten Modellkoeffizienten. Die Koeffizienten werden mit zunehmendem $\lambda$ von der KQ-Lösung ausgehend (linkes Ende der Skala) in Richtung 0 gezwungen.</span>
<span id="cb88-244"><a href="#cb88-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-245"><a href="#cb88-245" aria-hidden="true" tabindex="-1"></a>Über die Funktion <span class="in">`cv.glmnet()`</span> kann ein optimales $\lambda$ mit Cross Validation (CV) ermittelt werden. Ähnlich wie bei <span class="in">`glmnet()`</span> wird für die Validierung automatisch eine $\lambda$-Sequenz erzeugt. Wir nutzen <span class="in">`autoplot()`</span> aus dem R-Paket <span class="in">`ggfortify`</span> für die Visualisierung der Ergebnisse mit <span class="in">`ggplot2`</span>.</span>
<span id="cb88-246"><a href="#cb88-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-247"><a href="#cb88-247" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, warning=F}</span></span>
<span id="cb88-248"><a href="#cb88-248" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Lösungspfad für Ridge-Schätzung"</span></span>
<span id="cb88-249"><a href="#cb88-249" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-ridgecvplot</span></span>
<span id="cb88-250"><a href="#cb88-250" aria-hidden="true" tabindex="-1"></a><span class="in">library(ggfortify)</span></span>
<span id="cb88-251"><a href="#cb88-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-252"><a href="#cb88-252" aria-hidden="true" tabindex="-1"></a><span class="in"># Cross-validierte Bestimmung von lambda</span></span>
<span id="cb88-253"><a href="#cb88-253" aria-hidden="true" tabindex="-1"></a><span class="in">ridge_cvfit &lt;- cv.glmnet(</span></span>
<span id="cb88-254"><a href="#cb88-254" aria-hidden="true" tabindex="-1"></a><span class="in">  y = Y, </span></span>
<span id="cb88-255"><a href="#cb88-255" aria-hidden="true" tabindex="-1"></a><span class="in">  x = X, </span></span>
<span id="cb88-256"><a href="#cb88-256" aria-hidden="true" tabindex="-1"></a><span class="in">  intercept = F,</span></span>
<span id="cb88-257"><a href="#cb88-257" aria-hidden="true" tabindex="-1"></a><span class="in">  alpha = 0</span></span>
<span id="cb88-258"><a href="#cb88-258" aria-hidden="true" tabindex="-1"></a><span class="in">) </span></span>
<span id="cb88-259"><a href="#cb88-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-260"><a href="#cb88-260" aria-hidden="true" tabindex="-1"></a><span class="in"># Ergebnisse plotten</span></span>
<span id="cb88-261"><a href="#cb88-261" aria-hidden="true" tabindex="-1"></a><span class="in">ridge_cvfit %&gt;% </span></span>
<span id="cb88-262"><a href="#cb88-262" aria-hidden="true" tabindex="-1"></a><span class="in">  autoplot(label.n = 0)</span></span>
<span id="cb88-263"><a href="#cb88-263" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-264"><a href="#cb88-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-265"><a href="#cb88-265" aria-hidden="true" tabindex="-1"></a>@fig-ridgecvplot zeigt <span class="in">`ridge_cvfit$lambda.min`</span>, das optimale $\lambda$ mit dem geringsten CV Mean-Squarred-Error (linke gestrichelte Linie) und <span class="in">`ridge_cvfit$lambda.1se`</span>, das größte $\lambda$, welches innerhalb einer Standardabweichung entfernt ist (rechte gestrichelte Linie).<span class="ot">[^regreg-9]</span> Wir berechnen die Schätzung für <span class="in">`lambda.min`</span>.</span>
<span id="cb88-266"><a href="#cb88-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-267"><a href="#cb88-267" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-9]: </span>Die Wahl von <span class="in">`lambda.1se`</span> ist eine Heuristik, welche die Schätzunsicherheit berücksichtigt und zu einem "sparsameren" Modell tendiert.</span>
<span id="cb88-268"><a href="#cb88-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-269"><a href="#cb88-269" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, warning=F}</span></span>
<span id="cb88-270"><a href="#cb88-270" aria-hidden="true" tabindex="-1"></a><span class="in">(</span></span>
<span id="cb88-271"><a href="#cb88-271" aria-hidden="true" tabindex="-1"></a><span class="in">  ridge_coefs &lt;- coef(</span></span>
<span id="cb88-272"><a href="#cb88-272" aria-hidden="true" tabindex="-1"></a><span class="in">    object = ridge_cvfit, </span></span>
<span id="cb88-273"><a href="#cb88-273" aria-hidden="true" tabindex="-1"></a><span class="in">    s = ridge_cvfit$lambda.min</span></span>
<span id="cb88-274"><a href="#cb88-274" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb88-275"><a href="#cb88-275" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb88-276"><a href="#cb88-276" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-277"><a href="#cb88-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-278"><a href="#cb88-278" aria-hidden="true" tabindex="-1"></a>Wir schätzen das Modell nun mit KQ und vergleichen die Koeffizienten mit der Ridge-Schätzung.</span>
<span id="cb88-279"><a href="#cb88-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-282"><a href="#cb88-282" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-283"><a href="#cb88-283" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Koeffizientenvergleich: Ridge vs. KQ"</span></span>
<span id="cb88-284"><a href="#cb88-284" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-KoefRidgeVsKQ</span></span>
<span id="cb88-285"><a href="#cb88-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-286"><a href="#cb88-286" aria-hidden="true" tabindex="-1"></a><span class="co"># KQ-Schätzung durchführen</span></span>
<span id="cb88-287"><a href="#cb88-287" aria-hidden="true" tabindex="-1"></a>KQ_fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb88-288"><a href="#cb88-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-289"><a href="#cb88-289" aria-hidden="true" tabindex="-1"></a><span class="co"># Koeffizienten auslesen und transformieren:</span></span>
<span id="cb88-290"><a href="#cb88-290" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb88-291"><a href="#cb88-291" aria-hidden="true" tabindex="-1"></a>  <span class="at">Ridge =</span> <span class="fu">as.matrix</span>(ridge_coefs)[<span class="dv">2</span><span class="sc">:</span><span class="dv">11</span>, ],</span>
<span id="cb88-292"><a href="#cb88-292" aria-hidden="true" tabindex="-1"></a>  <span class="at">KQ =</span> KQ_fit<span class="sc">$</span>coefficients</span>
<span id="cb88-293"><a href="#cb88-293" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb88-294"><a href="#cb88-294" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">j =</span> <span class="fu">factor</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb88-295"><a href="#cb88-295" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(</span>
<span id="cb88-296"><a href="#cb88-296" aria-hidden="true" tabindex="-1"></a>    <span class="at">cols =</span> Ridge<span class="sc">:</span>KQ, </span>
<span id="cb88-297"><a href="#cb88-297" aria-hidden="true" tabindex="-1"></a>    <span class="at">names_to =</span> <span class="st">"Methode"</span>, </span>
<span id="cb88-298"><a href="#cb88-298" aria-hidden="true" tabindex="-1"></a>    <span class="at">values_to =</span> <span class="st">"Koeffizient"</span></span>
<span id="cb88-299"><a href="#cb88-299" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb88-300"><a href="#cb88-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-301"><a href="#cb88-301" aria-hidden="true" tabindex="-1"></a><span class="co"># Bar-Plot für Koeffizientenvergleich erzeugen  </span></span>
<span id="cb88-302"><a href="#cb88-302" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(</span>
<span id="cb88-303"><a href="#cb88-303" aria-hidden="true" tabindex="-1"></a>    <span class="at">mapping =</span> <span class="fu">aes</span>(</span>
<span id="cb88-304"><a href="#cb88-304" aria-hidden="true" tabindex="-1"></a>      <span class="at">x =</span> j, </span>
<span id="cb88-305"><a href="#cb88-305" aria-hidden="true" tabindex="-1"></a>      <span class="at">y =</span> Koeffizient, </span>
<span id="cb88-306"><a href="#cb88-306" aria-hidden="true" tabindex="-1"></a>      <span class="at">fill =</span> Methode</span>
<span id="cb88-307"><a href="#cb88-307" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb88-308"><a href="#cb88-308" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb88-309"><a href="#cb88-309" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(</span>
<span id="cb88-310"><a href="#cb88-310" aria-hidden="true" tabindex="-1"></a>    <span class="at">position =</span> <span class="st">"dodge"</span>, </span>
<span id="cb88-311"><a href="#cb88-311" aria-hidden="true" tabindex="-1"></a>    <span class="at">stat =</span> <span class="st">"identity"</span>, </span>
<span id="cb88-312"><a href="#cb88-312" aria-hidden="true" tabindex="-1"></a>    <span class="at">width =</span> .<span class="dv">5</span></span>
<span id="cb88-313"><a href="#cb88-313" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb88-314"><a href="#cb88-314" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-315"><a href="#cb88-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-316"><a href="#cb88-316" aria-hidden="true" tabindex="-1"></a>Der Vergleich anhand von @fig-KoefRidgeVsKQ zeigt deutlich, dass Ridge Regression im Vergleich mit KQ zu absolut kleineren Koeffizientenschätzungen tendiert. Inwiefern dies Konsequenzen für die Prognosegüte der Schätzung hat, können wir Anhand eines Testdatensatzes bestimmen. Hierzu vergleichen wir die mittleren Fehler (MSE) bei der Prognose von $Y$ für die Beobachtungen im Testdatensatz. Für die Simulation des Testdatensatzes nutzen wir erneut die Vorschrift \eqref{eq:ridgedgp1} um <span class="in">`r N`</span> neue Beobachtungen zu erzeugen.</span>
<span id="cb88-317"><a href="#cb88-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-320"><a href="#cb88-320" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-321"><a href="#cb88-321" aria-hidden="true" tabindex="-1"></a><span class="co"># Test-Datensatz erstellen</span></span>
<span id="cb88-322"><a href="#cb88-322" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">4321</span>)</span>
<span id="cb88-323"><a href="#cb88-323" aria-hidden="true" tabindex="-1"></a><span class="co"># Regressoren</span></span>
<span id="cb88-324"><a href="#cb88-324" aria-hidden="true" tabindex="-1"></a>new_X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(</span>
<span id="cb88-325"><a href="#cb88-325" aria-hidden="true" tabindex="-1"></a>  <span class="fu">genmvnorm</span>(</span>
<span id="cb88-326"><a href="#cb88-326" aria-hidden="true" tabindex="-1"></a>    <span class="at">k =</span> k, </span>
<span id="cb88-327"><a href="#cb88-327" aria-hidden="true" tabindex="-1"></a>    <span class="at">cor =</span> <span class="fu">rep</span>(.<span class="dv">85</span>, (k<span class="sc">^</span><span class="dv">2</span><span class="sc">-</span>k)<span class="sc">/</span><span class="dv">2</span>), </span>
<span id="cb88-328"><a href="#cb88-328" aria-hidden="true" tabindex="-1"></a>    <span class="at">n =</span> N</span>
<span id="cb88-329"><a href="#cb88-329" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb88-330"><a href="#cb88-330" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-331"><a href="#cb88-331" aria-hidden="true" tabindex="-1"></a><span class="co"># Abh. Variable</span></span>
<span id="cb88-332"><a href="#cb88-332" aria-hidden="true" tabindex="-1"></a>new_Y <span class="ot">&lt;-</span> new_X <span class="sc">%*%</span> beta <span class="sc">+</span> <span class="fu">rnorm</span>(N)</span>
<span id="cb88-333"><a href="#cb88-333" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-334"><a href="#cb88-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-335"><a href="#cb88-335" aria-hidden="true" tabindex="-1"></a>Für beide Methoden können wir <span class="in">`predict()`</span> für die Prognosen von $Y$ für den Testdatensatz (<span class="in">`new_Y`</span>) nutzen.</span>
<span id="cb88-336"><a href="#cb88-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-339"><a href="#cb88-339" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-340"><a href="#cb88-340" aria-hidden="true" tabindex="-1"></a><span class="co"># Ridge: Vorhersage von new_Y für Test-Datensatz</span></span>
<span id="cb88-341"><a href="#cb88-341" aria-hidden="true" tabindex="-1"></a>Y_predict_ridge <span class="ot">&lt;-</span> <span class="fu">predict</span>(</span>
<span id="cb88-342"><a href="#cb88-342" aria-hidden="true" tabindex="-1"></a>  <span class="at">object =</span> ridge_cvfit, </span>
<span id="cb88-343"><a href="#cb88-343" aria-hidden="true" tabindex="-1"></a>  <span class="at">newx =</span> new_X, </span>
<span id="cb88-344"><a href="#cb88-344" aria-hidden="true" tabindex="-1"></a>  <span class="at">s =</span> ridge_cvfit<span class="sc">$</span>lambda.min</span>
<span id="cb88-345"><a href="#cb88-345" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-346"><a href="#cb88-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-347"><a href="#cb88-347" aria-hidden="true" tabindex="-1"></a><span class="co"># Ridge: MSE für Test-Datensatz berechnen</span></span>
<span id="cb88-348"><a href="#cb88-348" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((Y_predict_ridge <span class="sc">-</span> new_Y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb88-349"><a href="#cb88-349" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-350"><a href="#cb88-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-351"><a href="#cb88-351" aria-hidden="true" tabindex="-1"></a>Die Vorhersage für <span class="in">`lm()`</span> benötigt dieselben Variablennamen wie im angepassten Modell, s. <span class="in">`KQ_fit$coefficients`</span>.</span>
<span id="cb88-352"><a href="#cb88-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-355"><a href="#cb88-355" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-356"><a href="#cb88-356" aria-hidden="true" tabindex="-1"></a><span class="co"># Test-Datensatz für predict.lm() formatieren</span></span>
<span id="cb88-357"><a href="#cb88-357" aria-hidden="true" tabindex="-1"></a>new_X <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(new_X)</span>
<span id="cb88-358"><a href="#cb88-358" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(new_X) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">"X"</span>, <span class="dv">1</span><span class="sc">:</span>k)</span>
<span id="cb88-359"><a href="#cb88-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-360"><a href="#cb88-360" aria-hidden="true" tabindex="-1"></a><span class="co"># KQ: Vorhersage von new_Y für Test-Datensatz</span></span>
<span id="cb88-361"><a href="#cb88-361" aria-hidden="true" tabindex="-1"></a>Y_predict_KQ <span class="ot">&lt;-</span> <span class="fu">predict</span>(</span>
<span id="cb88-362"><a href="#cb88-362" aria-hidden="true" tabindex="-1"></a>  <span class="at">object =</span> KQ_fit, </span>
<span id="cb88-363"><a href="#cb88-363" aria-hidden="true" tabindex="-1"></a>  <span class="at">newdata =</span> new_X</span>
<span id="cb88-364"><a href="#cb88-364" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-365"><a href="#cb88-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-366"><a href="#cb88-366" aria-hidden="true" tabindex="-1"></a><span class="co"># KQ: MSE für Test-Datensatz berechnen</span></span>
<span id="cb88-367"><a href="#cb88-367" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((Y_predict_KQ <span class="sc">-</span> new_Y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb88-368"><a href="#cb88-368" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-369"><a href="#cb88-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-370"><a href="#cb88-370" aria-hidden="true" tabindex="-1"></a>Die Ergebnisse zeigen, dass der Ridge-Schätzer trotz seiner Verzerrung einen deutlich geringeren mittleren Vorhersagefehler für die Testdaten erzielt als der KQ-Schätzer. Diese Eigenschaft der Koeffizientenschätzung kann die Prognosegüte von Ridge Regression gegenüber der KQ-Regression verbessern.</span>
<span id="cb88-371"><a href="#cb88-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-372"><a href="#cb88-372" aria-hidden="true" tabindex="-1"></a><span class="fu">### Beispiel: Vorhersage von Abschlussnoten in Mathe</span></span>
<span id="cb88-373"><a href="#cb88-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-374"><a href="#cb88-374" aria-hidden="true" tabindex="-1"></a>Zur Illustration von Ridge Regression nutzen wir den Datensatz <span class="in">`SP`</span> aus @CortezSilva2008.<span class="ot">[^regreg-10]</span> <span class="in">`SP`</span> enhält Beobachtungen zu Leistungen von insgesamt 100 Schülerinnen und Schülern im Fach Mathematik in der Sekundarstufe an zwei portugiesischen Schulen. Neben der Abschlussnote in Mathe (<span class="in">`G3`</span>, Skala von 0 bis 20) beinhaltet <span class="in">`SP`</span> diverse demografische, soziale und schulbezogene Merkmale, die mithilfe von Schulberichten und Fragebögen erhoben wurden. Ziel ist es, ein Modell für die Prognose von <span class="in">`G3`</span> anzupassen.</span>
<span id="cb88-375"><a href="#cb88-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-376"><a href="#cb88-376" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-10]: </span>Wir verwenden eine Auszug aus dem Orignaldatensatz, der nebst ausführlicher Variablenbeschreibung <span class="co">[</span><span class="ot">hier</span><span class="co">](https://archive.ics.uci.edu/dataset/320/student+performance)</span> verfügbar ist.</span>
<span id="cb88-377"><a href="#cb88-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-378"><a href="#cb88-378" aria-hidden="true" tabindex="-1"></a>Wir lesen zunächst die Daten (im .csv-Format) ein.</span>
<span id="cb88-379"><a href="#cb88-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-382"><a href="#cb88-382" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-383"><a href="#cb88-383" aria-hidden="true" tabindex="-1"></a><span class="co"># Daten einlesen</span></span>
<span id="cb88-384"><a href="#cb88-384" aria-hidden="true" tabindex="-1"></a>SP <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="at">file =</span> <span class="st">"datasets/SP.csv"</span>)</span>
<span id="cb88-385"><a href="#cb88-385" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-386"><a href="#cb88-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-387"><a href="#cb88-387" aria-hidden="true" tabindex="-1"></a>Ein Überblick zeigt, dass der Großteil der Regressoren aus kategorialen Variablen mit sozio-ökonomischen Informationen besteht.</span>
<span id="cb88-388"><a href="#cb88-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-391"><a href="#cb88-391" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-392"><a href="#cb88-392" aria-hidden="true" tabindex="-1"></a><span class="co"># Überblick</span></span>
<span id="cb88-393"><a href="#cb88-393" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(SP)</span>
<span id="cb88-394"><a href="#cb88-394" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-395"><a href="#cb88-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-396"><a href="#cb88-396" aria-hidden="true" tabindex="-1"></a>Um die Prognosegüte des Modells beurteilen zu können, partitionieren wir <span class="in">`SP`</span> zufällig in einen Test- sowie einen Trainingsdatensatz (mit 30 und 70 Beobachtungen), jeweils für die Regressoren und die abhängige Variable.</span>
<span id="cb88-397"><a href="#cb88-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-400"><a href="#cb88-400" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-401"><a href="#cb88-401" aria-hidden="true" tabindex="-1"></a><span class="co"># ID für Beobachtungen im Testdatensatz zufällig erzeugen</span></span>
<span id="cb88-402"><a href="#cb88-402" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb88-403"><a href="#cb88-403" aria-hidden="true" tabindex="-1"></a>ID <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(SP), <span class="at">size =</span> <span class="dv">30</span>)</span>
<span id="cb88-404"><a href="#cb88-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-405"><a href="#cb88-405" aria-hidden="true" tabindex="-1"></a><span class="co"># Regressoren aufteilen</span></span>
<span id="cb88-406"><a href="#cb88-406" aria-hidden="true" tabindex="-1"></a>SP_test <span class="ot">&lt;-</span> SP[ID,]</span>
<span id="cb88-407"><a href="#cb88-407" aria-hidden="true" tabindex="-1"></a>SP_train <span class="ot">&lt;-</span> SP[<span class="sc">-</span>ID,]</span>
<span id="cb88-408"><a href="#cb88-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-409"><a href="#cb88-409" aria-hidden="true" tabindex="-1"></a><span class="co"># Abh. Variable aufteilen</span></span>
<span id="cb88-410"><a href="#cb88-410" aria-hidden="true" tabindex="-1"></a>Y_test <span class="ot">&lt;-</span> SP_test<span class="sc">$</span>G3</span>
<span id="cb88-411"><a href="#cb88-411" aria-hidden="true" tabindex="-1"></a>Y_train <span class="ot">&lt;-</span> SP_train<span class="sc">$</span>G3</span>
<span id="cb88-412"><a href="#cb88-412" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-413"><a href="#cb88-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-414"><a href="#cb88-414" aria-hidden="true" tabindex="-1"></a>Als nächstes passen wir ein Ridge-Regressionsmodell für alle Regressoren in <span class="in">`SP_train`</span> an und ermitteln ein optimales $\lambda$ mit Cross Validation. Beachte, dass <span class="in">`cv.glmnet`</span> nicht für Regressoren im <span class="in">`data.frame`</span>/<span class="in">`tibble`</span>-Format ausgelegt ist, sondern ein <span class="in">`matrix`</span>-Format erwartet. Wir transformieren <span class="in">`SP_train`</span> daher mit <span class="in">`data.matrix()`</span>.</span>
<span id="cb88-415"><a href="#cb88-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-418"><a href="#cb88-418" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-419"><a href="#cb88-419" aria-hidden="true" tabindex="-1"></a><span class="co"># Ridge-Regression und CV für Trainingsdaten</span></span>
<span id="cb88-420"><a href="#cb88-420" aria-hidden="true" tabindex="-1"></a>SP_fit_cv <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(</span>
<span id="cb88-421"><a href="#cb88-421" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">data.matrix</span>(SP_train <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>G3)), </span>
<span id="cb88-422"><a href="#cb88-422" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> Y_train, </span>
<span id="cb88-423"><a href="#cb88-423" aria-hidden="true" tabindex="-1"></a>  <span class="at">alpha =</span> <span class="dv">0</span></span>
<span id="cb88-424"><a href="#cb88-424" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-425"><a href="#cb88-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-426"><a href="#cb88-426" aria-hidden="true" tabindex="-1"></a><span class="co"># CV-Ergebnisse für lambda visualisieren</span></span>
<span id="cb88-427"><a href="#cb88-427" aria-hidden="true" tabindex="-1"></a>SP_fit_cv <span class="sc">%&gt;%</span> </span>
<span id="cb88-428"><a href="#cb88-428" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>(<span class="at">label.n =</span> <span class="dv">0</span>)</span>
<span id="cb88-429"><a href="#cb88-429" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-430"><a href="#cb88-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-431"><a href="#cb88-431" aria-hidden="true" tabindex="-1"></a>Wie für das Beispiel mit simulierten Daten erhalten wir mit <span class="in">`predict()`</span> Vorhersagen für die erzielte Punktzahl. Beachte, dass wir den MSE nicht für die Trainingsdaten <span class="in">`SP_train`</span>, sondern für die Testdaten <span class="in">`SP_test`</span> berechnen.</span>
<span id="cb88-432"><a href="#cb88-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-435"><a href="#cb88-435" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-436"><a href="#cb88-436" aria-hidden="true" tabindex="-1"></a><span class="co"># Prognose von G3 anhand des Ridge-Modells</span></span>
<span id="cb88-437"><a href="#cb88-437" aria-hidden="true" tabindex="-1"></a>Y_predict_ridge <span class="ot">&lt;-</span> <span class="fu">predict</span>(</span>
<span id="cb88-438"><a href="#cb88-438" aria-hidden="true" tabindex="-1"></a>  <span class="at">object =</span> SP_fit_cv, </span>
<span id="cb88-439"><a href="#cb88-439" aria-hidden="true" tabindex="-1"></a>  <span class="at">newx =</span> <span class="fu">data.matrix</span>(</span>
<span id="cb88-440"><a href="#cb88-440" aria-hidden="true" tabindex="-1"></a>    SP_test <span class="sc">%&gt;%</span> </span>
<span id="cb88-441"><a href="#cb88-441" aria-hidden="true" tabindex="-1"></a>      <span class="fu">select</span>(<span class="sc">-</span>G3)</span>
<span id="cb88-442"><a href="#cb88-442" aria-hidden="true" tabindex="-1"></a>    ), </span>
<span id="cb88-443"><a href="#cb88-443" aria-hidden="true" tabindex="-1"></a>  <span class="at">s =</span> SP_fit_cv<span class="sc">$</span>lambda.min</span>
<span id="cb88-444"><a href="#cb88-444" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-445"><a href="#cb88-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-446"><a href="#cb88-446" aria-hidden="true" tabindex="-1"></a><span class="co"># MSE für Testdaten berechnen</span></span>
<span id="cb88-447"><a href="#cb88-447" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((Y_predict_ridge <span class="sc">-</span> Y_test)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb88-448"><a href="#cb88-448" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-449"><a href="#cb88-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-450"><a href="#cb88-450" aria-hidden="true" tabindex="-1"></a>Auch in diesem empirischen Beispiel zeigt ein Vergleich der MSEs, dass Ridge Regression dem KQ-Schätzer hinsichtlich der Vorhersagegüte überlegen ist.</span>
<span id="cb88-451"><a href="#cb88-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-454"><a href="#cb88-454" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-455"><a href="#cb88-455" aria-hidden="true" tabindex="-1"></a><span class="co"># Modell mit KQ schätzen</span></span>
<span id="cb88-456"><a href="#cb88-456" aria-hidden="true" tabindex="-1"></a>SP_fit_KQ <span class="ot">&lt;-</span> <span class="fu">lm</span>(G3 <span class="sc">~</span> ., SP_train)</span>
<span id="cb88-457"><a href="#cb88-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-458"><a href="#cb88-458" aria-hidden="true" tabindex="-1"></a><span class="co"># Prognose</span></span>
<span id="cb88-459"><a href="#cb88-459" aria-hidden="true" tabindex="-1"></a>Y_predict_KQ <span class="ot">&lt;-</span> <span class="fu">predict</span>(</span>
<span id="cb88-460"><a href="#cb88-460" aria-hidden="true" tabindex="-1"></a>  <span class="at">object =</span> SP_fit_KQ, </span>
<span id="cb88-461"><a href="#cb88-461" aria-hidden="true" tabindex="-1"></a>  <span class="at">newdata =</span> SP_test <span class="sc">%&gt;%</span> </span>
<span id="cb88-462"><a href="#cb88-462" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(<span class="sc">-</span>G3)</span>
<span id="cb88-463"><a href="#cb88-463" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-464"><a href="#cb88-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-465"><a href="#cb88-465" aria-hidden="true" tabindex="-1"></a><span class="co"># Testset-MSE berechnen</span></span>
<span id="cb88-466"><a href="#cb88-466" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((Y_predict_KQ <span class="sc">-</span> Y_test)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb88-467"><a href="#cb88-467" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-468"><a href="#cb88-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-469"><a href="#cb88-469" aria-hidden="true" tabindex="-1"></a>Der MSE für Ridge ist mit $<span class="in">`r round(mean((Y_predict_ridge - Y_test)^2), 2)`</span>$ deutlich kleiner als $<span class="in">`r round(mean((Y_predict_KQ - Y_test)^2), 2)`</span>$, der MSE für KQ.</span>
<span id="cb88-470"><a href="#cb88-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-471"><a href="#cb88-471" aria-hidden="true" tabindex="-1"></a>Für die Interpretation der Ridge-Schätzung erweitern den Code für die <span class="in">`ggplot2`</span>-Grafik der Koeffizienten-Pfade um eine vertikale Linie des mit CV ermittelten $\lambda$ und fügen mit dem Paket <span class="in">`ggrepel`</span> Labels für die Pfade der größten Koeffizienten hinzu.</span>
<span id="cb88-472"><a href="#cb88-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-473"><a href="#cb88-473" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, warning=F}</span></span>
<span id="cb88-474"><a href="#cb88-474" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Lösungspfad für Ridge-Schätzung"</span></span>
<span id="cb88-475"><a href="#cb88-475" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-ridgAppPlot</span></span>
<span id="cb88-476"><a href="#cb88-476" aria-hidden="true" tabindex="-1"></a><span class="in">library(ggrepel)</span></span>
<span id="cb88-477"><a href="#cb88-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-478"><a href="#cb88-478" aria-hidden="true" tabindex="-1"></a><span class="in"># Lambda-Sequenz auslesen</span></span>
<span id="cb88-479"><a href="#cb88-479" aria-hidden="true" tabindex="-1"></a><span class="in">lambdas &lt;- SP_fit_cv$lambda</span></span>
<span id="cb88-480"><a href="#cb88-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-481"><a href="#cb88-481" aria-hidden="true" tabindex="-1"></a><span class="in"># Ridge-Schätzung für Lambdas im langen Format </span></span>
<span id="cb88-482"><a href="#cb88-482" aria-hidden="true" tabindex="-1"></a><span class="in">df &lt;- as.matrix(SP_fit_cv$glmnet.fit$beta) %&gt;% </span></span>
<span id="cb88-483"><a href="#cb88-483" aria-hidden="true" tabindex="-1"></a><span class="in">  as_tibble() %&gt;% </span></span>
<span id="cb88-484"><a href="#cb88-484" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(</span></span>
<span id="cb88-485"><a href="#cb88-485" aria-hidden="true" tabindex="-1"></a><span class="in">    Variable = rownames(SP_fit_cv$glmnet.fit$beta)</span></span>
<span id="cb88-486"><a href="#cb88-486" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb88-487"><a href="#cb88-487" aria-hidden="true" tabindex="-1"></a><span class="in">  pivot_longer(-Variable) %&gt;% </span></span>
<span id="cb88-488"><a href="#cb88-488" aria-hidden="true" tabindex="-1"></a><span class="in">  group_by(Variable) %&gt;% </span></span>
<span id="cb88-489"><a href="#cb88-489" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(lambda = lambdas) </span></span>
<span id="cb88-490"><a href="#cb88-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-491"><a href="#cb88-491" aria-hidden="true" tabindex="-1"></a><span class="in"># Grafik mit ggplot erzeugen</span></span>
<span id="cb88-492"><a href="#cb88-492" aria-hidden="true" tabindex="-1"></a><span class="in">df %&gt;%</span></span>
<span id="cb88-493"><a href="#cb88-493" aria-hidden="true" tabindex="-1"></a><span class="in">  ggplot(</span></span>
<span id="cb88-494"><a href="#cb88-494" aria-hidden="true" tabindex="-1"></a><span class="in">    mapping = aes(</span></span>
<span id="cb88-495"><a href="#cb88-495" aria-hidden="true" tabindex="-1"></a><span class="in">      x = lambda, </span></span>
<span id="cb88-496"><a href="#cb88-496" aria-hidden="true" tabindex="-1"></a><span class="in">      y = value, </span></span>
<span id="cb88-497"><a href="#cb88-497" aria-hidden="true" tabindex="-1"></a><span class="in">      col = Variable</span></span>
<span id="cb88-498"><a href="#cb88-498" aria-hidden="true" tabindex="-1"></a><span class="in">    )</span></span>
<span id="cb88-499"><a href="#cb88-499" aria-hidden="true" tabindex="-1"></a><span class="in">  ) + </span></span>
<span id="cb88-500"><a href="#cb88-500" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_line() +</span></span>
<span id="cb88-501"><a href="#cb88-501" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_label_repel(</span></span>
<span id="cb88-502"><a href="#cb88-502" aria-hidden="true" tabindex="-1"></a><span class="in">    data = df %&gt;% </span></span>
<span id="cb88-503"><a href="#cb88-503" aria-hidden="true" tabindex="-1"></a><span class="in">      filter(lambda == min(lambdas)),</span></span>
<span id="cb88-504"><a href="#cb88-504" aria-hidden="true" tabindex="-1"></a><span class="in">    mapping = aes(label = Variable), </span></span>
<span id="cb88-505"><a href="#cb88-505" aria-hidden="true" tabindex="-1"></a><span class="in">    seed = 1234,</span></span>
<span id="cb88-506"><a href="#cb88-506" aria-hidden="true" tabindex="-1"></a><span class="in">    size = 5, </span></span>
<span id="cb88-507"><a href="#cb88-507" aria-hidden="true" tabindex="-1"></a><span class="in">    max.overlaps = 8, </span></span>
<span id="cb88-508"><a href="#cb88-508" aria-hidden="true" tabindex="-1"></a><span class="in">    nudge_x = -.5) +</span></span>
<span id="cb88-509"><a href="#cb88-509" aria-hidden="true" tabindex="-1"></a><span class="in">  ylab("gesch. Koeffizienten") +</span></span>
<span id="cb88-510"><a href="#cb88-510" aria-hidden="true" tabindex="-1"></a><span class="in">  scale_x_log10("log_10(lambda)") +</span></span>
<span id="cb88-511"><a href="#cb88-511" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_vline(</span></span>
<span id="cb88-512"><a href="#cb88-512" aria-hidden="true" tabindex="-1"></a><span class="in">    xintercept = SP_fit_cv$lambda.min, </span></span>
<span id="cb88-513"><a href="#cb88-513" aria-hidden="true" tabindex="-1"></a><span class="in">    col = "red", </span></span>
<span id="cb88-514"><a href="#cb88-514" aria-hidden="true" tabindex="-1"></a><span class="in">    lty = 2</span></span>
<span id="cb88-515"><a href="#cb88-515" aria-hidden="true" tabindex="-1"></a><span class="in">  ) +</span></span>
<span id="cb88-516"><a href="#cb88-516" aria-hidden="true" tabindex="-1"></a><span class="in">  theme(legend.position = "none")</span></span>
<span id="cb88-517"><a href="#cb88-517" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-518"><a href="#cb88-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-519"><a href="#cb88-519" aria-hidden="true" tabindex="-1"></a>@fig-ridgAppPlot gibt Hinweise darauf, dass neben der Schulzugehörigkeit und Indikatoren für schulische Leistung (bspw. <span class="in">`failures`</span>) sozio-ökonomische Prädiktoren wie <span class="in">`internet`</span> (Internetzugang zuhause), <span class="in">`Pstatus`</span> (Zusammenleben der Eltern) und <span class="in">`address`</span>/<span class="in">`traveltime`</span> (sozialer Status) relevante Variablen zu sein scheinen.</span>
<span id="cb88-520"><a href="#cb88-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-521"><a href="#cb88-521" aria-hidden="true" tabindex="-1"></a>Das optimale $\lambda_\mathrm{cv} \approx <span class="in">`r round(min(lambdas), 2)`</span>$ (gestrichelte rote Linie in @fig-ridgAppPlot) führt zu deutlicher Shrinkage, was eine mögliche Erklärung für den besseren Testset-MSE von Ridge Regression ist: Die Koeffizienten von Variablen mit wenig Erklärungskraft werden durch die Regularisierung in Richtung 0 gezwungen und reduzieren so die Varianz der Vorhersage gegenüber der (idealerweise) unverzerrten KQ-Schätzung.</span>
<span id="cb88-522"><a href="#cb88-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-523"><a href="#cb88-523" aria-hidden="true" tabindex="-1"></a>::: callout-note</span>
<span id="cb88-524"><a href="#cb88-524" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Key Facts zu Ridge Regression</span></span>
<span id="cb88-525"><a href="#cb88-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-526"><a href="#cb88-526" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Ridge-Regression regularisiert den KQ-Schätzer mit der $\ell_2$-Norm der Koeffizienten. Diese Form von Regularisierung ist eine Alternative für KQ in Anwendungen mit mehr Regressoren als Beobachtugen ($k\geq n$) und/oder wenn KQ aufgrund starker Kollinearität eine hohe Varianz aufweist.</span>
<span id="cb88-527"><a href="#cb88-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-528"><a href="#cb88-528" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Der Ridge-Schätzer $\widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda$ ist *nicht* erwartungstreu. Die geschätzten Koeffizienten sind auch für $n\to\infty$ verzerrt.</span>
<span id="cb88-529"><a href="#cb88-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-530"><a href="#cb88-530" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Aufgrund der verzerrten Schätzung ist statistische Inferenz für Koeffizienten mit $\widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda$ problematisch. Anstatt für strukturelle Modelle oder die Schätzung kausaler Effekte wird Ridge Regression in der Praxis daher überwiegend für Prognosen verwendet.</span>
<span id="cb88-531"><a href="#cb88-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-532"><a href="#cb88-532" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Die Wahl von $\lambda$ impliziert einen Tradeoff zwischen Verzerrung und Varianz: Große $\lambda$ schrumpfen die Koeffizientenschätzer Richtung 0 (mehr Verzerrung), führen aber zu einer kleineren Varianz der Schätzung. Entsprechend können Vorhersagen mit mehr Verzerrung aber weniger Varianz als mit KQ getroffen werden.</span>
<span id="cb88-533"><a href="#cb88-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-534"><a href="#cb88-534" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Ridge Regression kann in R mit dem Paket <span class="in">`glmnet`</span> berechnet werden.</span>
<span id="cb88-535"><a href="#cb88-535" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb88-536"><a href="#cb88-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-537"><a href="#cb88-537" aria-hidden="true" tabindex="-1"></a><span class="fu">## Lasso Regression</span></span>
<span id="cb88-538"><a href="#cb88-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-539"><a href="#cb88-539" aria-hidden="true" tabindex="-1"></a>Least Absolute Shrinkage and Selection Operator (Lasso) ist ein von @Tibshirani1996 vorgeschlagener Schätzer, der die Verlustfunktion des KQ-Schätzers um einen Strafterm für die Summe der (absoluten) Größe der Koeffizienten $\boldsymbol\beta = (\beta_1, \dots,\beta_k)'$ erweitert. Die Verlustfunktion des Lasso-Schätzers von $\boldsymbol{\beta}$ lautet \begin{align}</span>
<span id="cb88-540"><a href="#cb88-540" aria-hidden="true" tabindex="-1"></a>\mathrm{RSS}(\boldsymbol{\beta},p=1,\lambda) = \mathrm{RSS}(\boldsymbol{\beta}) + \lambda \lVert\boldsymbol{\beta}\rVert_1.\label{eq:lassoloss}</span>
<span id="cb88-541"><a href="#cb88-541" aria-hidden="true" tabindex="-1"></a>\end{align} Für den Strafterm wird also die $\ell_1$-norm $$</span>
<span id="cb88-542"><a href="#cb88-542" aria-hidden="true" tabindex="-1"></a>\lVert\boldsymbol{\beta}\rVert_1 = \sum_{j=1}^k \lvert\beta_j \rvert</span>
<span id="cb88-543"><a href="#cb88-543" aria-hidden="true" tabindex="-1"></a>$$ verwendet. Der Lasso-Schätzer $\widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda$ für $\boldsymbol{\beta}$ minimiert \eqref{eq:lassoloss}, \begin{align}</span>
<span id="cb88-544"><a href="#cb88-544" aria-hidden="true" tabindex="-1"></a>\boldsymbol{\beta}^{\mathrm{L}}_\lambda = \arg\min_{\boldsymbol{\beta}} \ \mathrm{RSS}(\boldsymbol{\beta},p=1,\lambda).</span>
<span id="cb88-545"><a href="#cb88-545" aria-hidden="true" tabindex="-1"></a>\end{align} Entsprechend erhalten wir in Abhängigkeit von $\lambda$ ein Kontinuum an Lösungen \begin{align}</span>
<span id="cb88-546"><a href="#cb88-546" aria-hidden="true" tabindex="-1"></a>  \left<span class="sc">\{</span>\widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda\right\}_{\lambda=0}^{\lambda=\infty},\label{eq:LassoPath}</span>
<span id="cb88-547"><a href="#cb88-547" aria-hidden="true" tabindex="-1"></a>\end{align} der sogenannte *Lasso-Pfad*.</span>
<span id="cb88-548"><a href="#cb88-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-549"><a href="#cb88-549" aria-hidden="true" tabindex="-1"></a>Das Optimierungsproblem \eqref{eq:lassoloss} hat die äquivalente Darstellung \begin{align}</span>
<span id="cb88-550"><a href="#cb88-550" aria-hidden="true" tabindex="-1"></a>  \begin{split}</span>
<span id="cb88-551"><a href="#cb88-551" aria-hidden="true" tabindex="-1"></a>    \widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda =&amp;\, \arg\min_{\boldsymbol{\beta}} \mathrm{RSS}(\boldsymbol{\beta}) + \lambda\left(\lVert\boldsymbol{\beta}\rVert_1 - t\right)<span class="sc">\\</span></span>
<span id="cb88-552"><a href="#cb88-552" aria-hidden="true" tabindex="-1"></a>    =&amp;\, \arg\min_{\lVert\boldsymbol{\beta}\rVert_1\leq t} \mathrm{RSS}(\boldsymbol{\beta}), </span>
<span id="cb88-553"><a href="#cb88-553" aria-hidden="true" tabindex="-1"></a>  \end{split}\label{eq:lassolagrange}</span>
<span id="cb88-554"><a href="#cb88-554" aria-hidden="true" tabindex="-1"></a>\end{align} welche über den <span class="co">[</span><span class="ot">Lagrange-Ansatz</span><span class="co">](https://de.wikipedia.org/wiki/Lagrange-Multiplikator#Beispiel_mit_Anwendungsbezug)</span> unter der Nebenbedingung $\lVert\boldsymbol{\beta}\rVert_1 \leq t$ gelöst werden kann.</span>
<span id="cb88-555"><a href="#cb88-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-556"><a href="#cb88-556" aria-hidden="true" tabindex="-1"></a>Ähnlich wie der KQ-Schätzer ist der Lasso-Schätzer $\widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda$ durch Bedingungen 1. Ordnung bestimmt. Diese Bedingungen lassen sich komfortabel in Matrix-Schreibweise darstellen als \begin{align}</span>
<span id="cb88-557"><a href="#cb88-557" aria-hidden="true" tabindex="-1"></a>  -2\boldsymbol{X}_j'(\boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\beta}) + \lambda\cdot\mathrm{sgn}(\beta_j) = 0, \quad j = 1,\dots,k.\label{eq:LassoFOC}</span>
<span id="cb88-558"><a href="#cb88-558" aria-hidden="true" tabindex="-1"></a>\end{align} Aus Gleichung \eqref{eq:LassoFOC} folgt, dass der Lasso-Schätzer aufgrund des Strafterms im Allgemeinen nicht algebraisch bestimmt werden kann.<span class="ot">[^regreg-11]</span></span>
<span id="cb88-559"><a href="#cb88-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-560"><a href="#cb88-560" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-11]: </span>Zur Bestimmung des Schätzers werden Algorithmen der nicht-linearen Optimierung genutzt.</span>
<span id="cb88-561"><a href="#cb88-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-562"><a href="#cb88-562" aria-hidden="true" tabindex="-1"></a>In Abhängigkeit von $\lambda$ zwingt der Lasso-Schätzer die KQ-Schätzung von $\beta_j$ zu einem (absolut) kleineren Wert: Ähnlich wie bei Ridge Regression bewirkt der $\ell_1$-Strafterm eine mit $\lambda$ zunehmende Schrumpfung der geschätzen Koeffizienten in Richtung 0. Charakteristisch für die Lösung des Lasso-Schätzers ist, dass $\widehat{\boldsymbol{\beta}}^{\mathrm{L}}_j = 0$, wenn die Bedingung \begin{align}</span>
<span id="cb88-563"><a href="#cb88-563" aria-hidden="true" tabindex="-1"></a>  \left\lvert\boldsymbol{X}_j'(\boldsymbol{Y} - \boldsymbol{X}\widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda)\right\rvert - \lambda/2 \leq 0 \label{eq:lassoselection}</span>
<span id="cb88-564"><a href="#cb88-564" aria-hidden="true" tabindex="-1"></a>\end{align} erfüllt ist. In Abhängigkeit von $\lambda$ kann der Lasso-Schätzer folglich geschätzte Regressionskoeffizienten nicht nur in Richtung $0$, sondern diese auch *exakt* mit $0$ schätzen und damit *Variablenselektion* betreiben. Aufgrund der mit $\lambda$ zunehmenden Shrinkage bis die Bedingung \eqref{eq:lassoselection} erfüllt und der Koeffizient gleich $0$ gesetzt wird, bezeichnet man Lasso auch als einen *Soft Thresholding Operator*. Im nächsten Abschnitt betrachten wir die Eigenschaften von Lasso-Regularisierung unter vereinfachten Annahmen bzgl. der Regressoren.</span>
<span id="cb88-565"><a href="#cb88-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-566"><a href="#cb88-566" aria-hidden="true" tabindex="-1"></a><span class="fu">### Lasso ist Soft Thresholding</span></span>
<span id="cb88-567"><a href="#cb88-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-568"><a href="#cb88-568" aria-hidden="true" tabindex="-1"></a>Wir betrachten nun eine mathematische Darstellung von Selektions- und Shrinkage-Eigenschaft des Lasso-Schätzers in einem vereinfachten Modell. Wenn die Regressoren $\boldsymbol{X}$ orthonormal zueinander sind, existiert eine analytische Lösung des Lasso-Schätzers, \begin{align}</span>
<span id="cb88-569"><a href="#cb88-569" aria-hidden="true" tabindex="-1"></a>  \widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda =</span>
<span id="cb88-570"><a href="#cb88-570" aria-hidden="true" tabindex="-1"></a>  \begin{cases}</span>
<span id="cb88-571"><a href="#cb88-571" aria-hidden="true" tabindex="-1"></a>    \widehat{\boldsymbol{\beta}}_j - \lambda/2 &amp;, \ \ \widehat{\boldsymbol{\beta}}_j &gt; \lambda/2<span class="sc">\\</span></span>
<span id="cb88-572"><a href="#cb88-572" aria-hidden="true" tabindex="-1"></a>    0 &amp;, \ \ \lvert\widehat{\boldsymbol{\beta}}_j\rvert\leq\lambda/2<span class="sc">\\</span></span>
<span id="cb88-573"><a href="#cb88-573" aria-hidden="true" tabindex="-1"></a>    \widehat{\boldsymbol{\beta}}_j + \lambda/2 &amp;, \ \ \widehat{\boldsymbol{\beta}}_j &lt; \lambda/2</span>
<span id="cb88-574"><a href="#cb88-574" aria-hidden="true" tabindex="-1"></a>  \end{cases},\label{eq:lassoST}</span>
<span id="cb88-575"><a href="#cb88-575" aria-hidden="true" tabindex="-1"></a>\end{align} wobei $\widehat{\boldsymbol{\beta}}_j$ der KQ-Schätzer von $\beta_j$ ist. Anhand von \eqref{eq:lassoST} können wir die Selektionseigenschaft sowie die Schrumpfung der KQ-Koeffizientenschätzung in Abhängigkeit der durch $\lambda$ regulierten $\ell_1$-Strafe erkennen. Für eine Visualisierung implementieren wir \eqref{eq:lassoST} als R-Funktion <span class="in">`lasso_st()`</span> und zeichnen die resultierenden Koeffizientenschätzungen für die Parameterwerte $\lambda\in<span class="sc">\{</span>0, 0.2, 0.4<span class="sc">\}</span>$.</span>
<span id="cb88-576"><a href="#cb88-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-577"><a href="#cb88-577" aria-hidden="true" tabindex="-1"></a>Wir definieren zunächst die Funktion <span class="in">`lasso_st()`</span>.</span>
<span id="cb88-578"><a href="#cb88-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-579"><a href="#cb88-579" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, warning=F}</span></span>
<span id="cb88-580"><a href="#cb88-580" aria-hidden="true" tabindex="-1"></a><span class="in">library(tidyverse)</span></span>
<span id="cb88-581"><a href="#cb88-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-582"><a href="#cb88-582" aria-hidden="true" tabindex="-1"></a><span class="in"># Funktion für Lasso soft-thresholding definieren</span></span>
<span id="cb88-583"><a href="#cb88-583" aria-hidden="true" tabindex="-1"></a><span class="in">lasso_st &lt;- function(KQ, lambda) {</span></span>
<span id="cb88-584"><a href="#cb88-584" aria-hidden="true" tabindex="-1"></a><span class="in">  case_when(</span></span>
<span id="cb88-585"><a href="#cb88-585" aria-hidden="true" tabindex="-1"></a><span class="in">    KQ &gt; lambda/2         ~ KQ - lambda/2,</span></span>
<span id="cb88-586"><a href="#cb88-586" aria-hidden="true" tabindex="-1"></a><span class="in">    abs(KQ) &lt;= lambda/2   ~ 0,</span></span>
<span id="cb88-587"><a href="#cb88-587" aria-hidden="true" tabindex="-1"></a><span class="in">    KQ &lt; -lambda/2        ~ KQ + lambda/2,</span></span>
<span id="cb88-588"><a href="#cb88-588" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb88-589"><a href="#cb88-589" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb88-590"><a href="#cb88-590" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-591"><a href="#cb88-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-592"><a href="#cb88-592" aria-hidden="true" tabindex="-1"></a>Im nächsten Schritt zeichnen wir <span class="in">`lasso_st()`</span> für eine Sequenz von KQ-Schätzwerten gegeben $\lambda$.</span>
<span id="cb88-593"><a href="#cb88-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-594"><a href="#cb88-594" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, warning=F}</span></span>
<span id="cb88-595"><a href="#cb88-595" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Shrinkage und Selektion von OLS-Koeffizienten mit Lasso"</span></span>
<span id="cb88-596"><a href="#cb88-596" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: "fig-lassoST"</span></span>
<span id="cb88-597"><a href="#cb88-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-598"><a href="#cb88-598" aria-hidden="true" tabindex="-1"></a><span class="in"># Sequenz von KQ-Schätzwerten für Illustration definieren</span></span>
<span id="cb88-599"><a href="#cb88-599" aria-hidden="true" tabindex="-1"></a><span class="in">dat &lt;- tibble(</span></span>
<span id="cb88-600"><a href="#cb88-600" aria-hidden="true" tabindex="-1"></a><span class="in">  KQ = seq(-1, 1, .01)</span></span>
<span id="cb88-601"><a href="#cb88-601" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb88-602"><a href="#cb88-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-603"><a href="#cb88-603" aria-hidden="true" tabindex="-1"></a><span class="in"># Lasso-Schätzer als Funktion des KQ-Schätzers plotten</span></span>
<span id="cb88-604"><a href="#cb88-604" aria-hidden="true" tabindex="-1"></a><span class="in">ggplot(dat) +</span></span>
<span id="cb88-605"><a href="#cb88-605" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_function(</span></span>
<span id="cb88-606"><a href="#cb88-606" aria-hidden="true" tabindex="-1"></a><span class="in">    fun = lasso_st, </span></span>
<span id="cb88-607"><a href="#cb88-607" aria-hidden="true" tabindex="-1"></a><span class="in">    args = list(lambda = 0), </span></span>
<span id="cb88-608"><a href="#cb88-608" aria-hidden="true" tabindex="-1"></a><span class="in">    lty = 2</span></span>
<span id="cb88-609"><a href="#cb88-609" aria-hidden="true" tabindex="-1"></a><span class="in">  ) + </span></span>
<span id="cb88-610"><a href="#cb88-610" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_function(</span></span>
<span id="cb88-611"><a href="#cb88-611" aria-hidden="true" tabindex="-1"></a><span class="in">    fun = lasso_st, </span></span>
<span id="cb88-612"><a href="#cb88-612" aria-hidden="true" tabindex="-1"></a><span class="in">    args = list(lambda = .2),</span></span>
<span id="cb88-613"><a href="#cb88-613" aria-hidden="true" tabindex="-1"></a><span class="in">    col = "red"</span></span>
<span id="cb88-614"><a href="#cb88-614" aria-hidden="true" tabindex="-1"></a><span class="in">  ) + </span></span>
<span id="cb88-615"><a href="#cb88-615" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_function(</span></span>
<span id="cb88-616"><a href="#cb88-616" aria-hidden="true" tabindex="-1"></a><span class="in">    fun = lasso_st, </span></span>
<span id="cb88-617"><a href="#cb88-617" aria-hidden="true" tabindex="-1"></a><span class="in">    args = list(lambda = .4), </span></span>
<span id="cb88-618"><a href="#cb88-618" aria-hidden="true" tabindex="-1"></a><span class="in">    col = "blue"</span></span>
<span id="cb88-619"><a href="#cb88-619" aria-hidden="true" tabindex="-1"></a><span class="in">  ) + </span></span>
<span id="cb88-620"><a href="#cb88-620" aria-hidden="true" tabindex="-1"></a><span class="in">  xlim(-.4, .4) +</span></span>
<span id="cb88-621"><a href="#cb88-621" aria-hidden="true" tabindex="-1"></a><span class="in">  xlab("KQ-Schätzer von beta_1") +</span></span>
<span id="cb88-622"><a href="#cb88-622" aria-hidden="true" tabindex="-1"></a><span class="in">  ylab("Lasso-Schätzer von beta_1")</span></span>
<span id="cb88-623"><a href="#cb88-623" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-624"><a href="#cb88-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-625"><a href="#cb88-625" aria-hidden="true" tabindex="-1"></a>@fig-lassoST zeigt, dass der $\ell_1$-Strafterm des Lasso-Schätzers zu einem linearen Verlauf der auf den KQ-Schätzer (gezeichnet für $\lambda = 0$, gestrichelte Linie) applizierten Shrinkage führt: Der Lasso-Schätzer ist eine abschnittsweise-lineare Funktion des KQ-Schätzers in $\lambda$: Je größer der Parameter $\lambda$, desto größer ist das Intervall von KQ-Schätzwerten $<span class="co">[</span><span class="ot">-\lambda/2,\lambda/2</span><span class="co">]</span>$, wo der Lasso-Schätzer zu Variablenselektion führt, d.h. hier den Koeffizienten $\beta_j$ als $0$ schätzt (rote bzw. blaue Linie).</span>
<span id="cb88-626"><a href="#cb88-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-627"><a href="#cb88-627" aria-hidden="true" tabindex="-1"></a>Anhand von @fig-lassoST kann abgeleitet werden, dass der Lasso-Schätzer nicht invariant gegenüber der Skalierung der Regressoren ist: Die Stärke der Regularisierung durch $\lambda$ ist hängt von der Magnitude des KQ-Schätzers ab. Daher müssen die Regressoren vor Berechnung der Schätzung standardsiert werden. Üblich ist hierbei eine Normierung auf einen Mittelwert von $0$ und eine Varianz von $1$.</span>
<span id="cb88-628"><a href="#cb88-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-629"><a href="#cb88-629" aria-hidden="true" tabindex="-1"></a>Die nachstehende interaktive Grafik illustriert das Lasso-Optimierungsproblem \eqref{eq:lassolagrange} sowie den resultierenden Schätzer der Koeffizienten $(\beta_1, \beta_2)$ in einem multiplen Regressionsmodell mit korrelierten Regressoren $X_1$ und $X_2$.</span>
<span id="cb88-630"><a href="#cb88-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-631"><a href="#cb88-631" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Die blaue Ellipse ist die Menge aller Schätzwerte $\left(\widehat\beta_{1},\, \widehat\beta_{2}\right)$ für den angegebenen Wert von $\mathrm{RSS}$. Im Zentrum der Ellipse liegt der KQ-Schätzer, welcher $\mathrm{RSS}$ minimiert.</span>
<span id="cb88-632"><a href="#cb88-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-633"><a href="#cb88-633" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Das graue Quadrat ist die Menge aller Koeffizienten-Paare $(\beta_1, \beta_2)$, welche die Restriktion $\lvert\beta_1\rvert+\lvert\beta_2\rvert\leq t$ erfüllen. Beachte, dass die Größe dieser Region nur durch den Parameter $t$ bestimmt wird.</span>
<span id="cb88-634"><a href="#cb88-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-635"><a href="#cb88-635" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Der blaue Punkt ist der Lasso-Schätzer $(\widehat{\boldsymbol{\beta}}^L_{1,t},\, \widehat{\boldsymbol{\beta}}^L_{2,t})$. Dieser ergibt sich als Schnittpunkt zwischen der blauen $\mathrm{RSS}$-Ellipse und der Restriktionsregion und variiert mit $t$. Die gestrichelte rote Linie zeigt den Lasso-Lösungspfad.</span>
<span id="cb88-636"><a href="#cb88-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-637"><a href="#cb88-637" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Für kleine Werte, erhalten wir starke Shrinkage auf $\widehat\beta_{1,t}$ bis zum Wertebereich $t\leq50$, wo $\widehat{\boldsymbol{\beta}}^L_{1,t}=0$. Hier erfolgt Variablenselektion: Die Regularisierung führt zu einem geschätzten Modell, das lediglich $X_2$ als erklärende Variable enthält. In diesem Bereich von $t$ bewirkt die Shrinkage, dass $\widehat{\boldsymbol{\beta}}^L_{2,t}\to0$ für $t\to0$.</span>
<span id="cb88-638"><a href="#cb88-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-639"><a href="#cb88-639" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb88-640"><a href="#cb88-640" aria-hidden="true" tabindex="-1"></a>&lt;iframe width="100%" height="567" frameborder="0" scroll="false" src="https://observablehq.com/embed/2e11f2b535e23c25@16?cells=viewof+s%2Cchart115"&gt;</span>
<span id="cb88-641"><a href="#cb88-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-642"><a href="#cb88-642" aria-hidden="true" tabindex="-1"></a>&lt;/iframe&gt;</span>
<span id="cb88-643"><a href="#cb88-643" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb88-644"><a href="#cb88-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-645"><a href="#cb88-645" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="pdf"}</span>
<span id="cb88-646"><a href="#cb88-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-647"><a href="#cb88-647" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb88-648"><a href="#cb88-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-649"><a href="#cb88-649" aria-hidden="true" tabindex="-1"></a>***Diese Interaktive Komponente des Buchs ist nur in der Online-Version verfügbar.***</span>
<span id="cb88-650"><a href="#cb88-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-651"><a href="#cb88-651" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb88-652"><a href="#cb88-652" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb88-653"><a href="#cb88-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-654"><a href="#cb88-654" aria-hidden="true" tabindex="-1"></a>Beachte, dass der rote Lasso-Pfad (die Menge aller Lasso-Lösungen) äquivalent als Funktion von $\lambda$ im Optimierungsproblem \eqref{eq:lassoloss} dargestellt werden kann. Implementierungen mit statistischer Software berechnen die Lasso-Lösung häufig in Abhängigkeit von $\lambda$. Ein Algorithmus hierfür ist LARS.</span>
<span id="cb88-655"><a href="#cb88-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-656"><a href="#cb88-656" aria-hidden="true" tabindex="-1"></a><span class="fu">### Berechnung der Lasso-Lösung mit dem LARS-Algorithmus</span></span>
<span id="cb88-657"><a href="#cb88-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-658"><a href="#cb88-658" aria-hidden="true" tabindex="-1"></a>Für die Berechnung des Lasso-Lösungspfads kann der <span class="co">[</span><span class="ot">LARS-Algorithmus</span><span class="co">](https://en.wikipedia.org/wiki/Least-angle_regression)</span> von @Efronetal2004 im Lasso-Modus genutzt werden.<span class="ot">[^regreg-12]</span> Der Lasso-Lösungspfad beinhaltet geschätzte Koeffizienten über ein Intervall für $\lambda$, welches sämtliche Modellkomplexitäten zwischen der (trivialen) Lösung mit maximaler Shrinkage auf allen Koeffizienten ($\lambda$ groß, alle gesch. Koeffizienten sind $0$) und der unregularisierten Lösung ($\lambda = 0$, KQ-Schätzung) abbildet. Der LARS-Algorithmus erzeugt den Lösungspfad sequentiell, sodass die Schätzung als Funktion von $\lambda$ veranschaulicht werden kann, ähnlich wie bei Ridge Regression.</span>
<span id="cb88-659"><a href="#cb88-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-660"><a href="#cb88-660" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-12]: </span>LARS steht für *Least Angle Regression*.</span>
<span id="cb88-661"><a href="#cb88-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-662"><a href="#cb88-662" aria-hidden="true" tabindex="-1"></a>Wir zeigen nun anhand simulierter Daten, wie Lasso-Lösungen mit dem R-Paket <span class="in">`lars`</span> berechnet werden können. Hierfür erzeugen wir Daten gemäß der Vorschrift \begin{align}</span>
<span id="cb88-663"><a href="#cb88-663" aria-hidden="true" tabindex="-1"></a>  \begin{split}</span>
<span id="cb88-664"><a href="#cb88-664" aria-hidden="true" tabindex="-1"></a>  Y_i =&amp;\, \boldsymbol{X}_i' \boldsymbol{\beta}_v + u_i<span class="sc">\\</span></span>
<span id="cb88-665"><a href="#cb88-665" aria-hidden="true" tabindex="-1"></a>  <span class="sc">\\</span></span>
<span id="cb88-666"><a href="#cb88-666" aria-hidden="true" tabindex="-1"></a>  \boldsymbol{\beta}_v =&amp;\, (-1.25, -.75, 0, 0, 0, 0, 0, .75, 1.25)'<span class="sc">\\</span></span>
<span id="cb88-667"><a href="#cb88-667" aria-hidden="true" tabindex="-1"></a>  <span class="sc">\\</span></span>
<span id="cb88-668"><a href="#cb88-668" aria-hidden="true" tabindex="-1"></a>  \boldsymbol{X}_i \sim&amp;\, N(\boldsymbol{0}, \boldsymbol{I}_{9\times9}), \quad u_i \overset{u.i.v.}{\sim} N(0, 1), \quad i = 1,\dots,25.</span>
<span id="cb88-669"><a href="#cb88-669" aria-hidden="true" tabindex="-1"></a>  \end{split}\label{eq:larsdgp}</span>
<span id="cb88-670"><a href="#cb88-670" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb88-671"><a href="#cb88-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-674"><a href="#cb88-674" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-675"><a href="#cb88-675" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lars)</span>
<span id="cb88-676"><a href="#cb88-676" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb88-677"><a href="#cb88-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-678"><a href="#cb88-678" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameter definieren</span></span>
<span id="cb88-679"><a href="#cb88-679" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">25</span></span>
<span id="cb88-680"><a href="#cb88-680" aria-hidden="true" tabindex="-1"></a>beta_v <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">1.25</span>, <span class="sc">-</span>.<span class="dv">75</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, .<span class="dv">75</span>, <span class="fl">1.25</span>)</span>
<span id="cb88-681"><a href="#cb88-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-682"><a href="#cb88-682" aria-hidden="true" tabindex="-1"></a><span class="co"># Beobachtungen simulieren</span></span>
<span id="cb88-683"><a href="#cb88-683" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(N <span class="sc">*</span> <span class="dv">9</span>), <span class="at">ncol =</span> <span class="dv">9</span>)</span>
<span id="cb88-684"><a href="#cb88-684" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta_v <span class="sc">+</span> <span class="fu">rnorm</span>(N)</span>
<span id="cb88-685"><a href="#cb88-685" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-686"><a href="#cb88-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-687"><a href="#cb88-687" aria-hidden="true" tabindex="-1"></a>Entsprechend des DGP passen wir ein Modell ohne Konstante an. Damit <span class="in">`lars::lars()`</span> den Lösungspfad des Lasso-Schätzers berechnet, muss <span class="in">`type = "lasso"`</span> gewählt werden.<span class="ot">[^regreg-13]</span></span>
<span id="cb88-688"><a href="#cb88-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-689"><a href="#cb88-689" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-13]: </span><span class="in">`lars()`</span> standardisiert die Regressoren standardmäßig (aufgrund des DGPs hier nicht nötig).</span>
<span id="cb88-690"><a href="#cb88-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-693"><a href="#cb88-693" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-694"><a href="#cb88-694" aria-hidden="true" tabindex="-1"></a><span class="co"># Lösungen des Lasso-Schätzers mit LARS berechnen</span></span>
<span id="cb88-695"><a href="#cb88-695" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb88-696"><a href="#cb88-696" aria-hidden="true" tabindex="-1"></a>  fit_lars <span class="ot">&lt;-</span> <span class="fu">lars</span>(</span>
<span id="cb88-697"><a href="#cb88-697" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> X, </span>
<span id="cb88-698"><a href="#cb88-698" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> Y, </span>
<span id="cb88-699"><a href="#cb88-699" aria-hidden="true" tabindex="-1"></a>    <span class="at">intercept =</span> F,</span>
<span id="cb88-700"><a href="#cb88-700" aria-hidden="true" tabindex="-1"></a>    <span class="at">type =</span> <span class="st">"lasso"</span> <span class="co"># Wichtig: Lasso-Modus</span></span>
<span id="cb88-701"><a href="#cb88-701" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb88-702"><a href="#cb88-702" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-703"><a href="#cb88-703" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-704"><a href="#cb88-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-705"><a href="#cb88-705" aria-hidden="true" tabindex="-1"></a>Die Zusammenfassung zeigt, dass der LARS-Algorithmus als erstes die (relevante) Variable $X_9$ aktiviert.<span class="ot">[^regreg-14]</span> Mit abnehmender Regularisierung (kleinere $\lambda$) werden in den nächsten 3 Schritten die übrigen relevanten Variablen $X_2$, $X_8$ und $X_1$ aktiviert. Über die weiteren Schritte nähert der Algorithmus die Lösung an die *saturierte* Schätzung (das Modell mit allen neun Regressoren) an und aktiviert schrittweise die übrigen, irrelevanten Variablen.</span>
<span id="cb88-706"><a href="#cb88-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-707"><a href="#cb88-707" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-14]: </span>Aktivierung meint die Aufnahme einer Variable in der Modell gegeben eines hinreichend kleinen $\lambda$.</span>
<span id="cb88-708"><a href="#cb88-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-709"><a href="#cb88-709" aria-hidden="true" tabindex="-1"></a>Wir visualisieren die geschätzen Koeffizienten an jedem Schritt des Lösungspfads als Funktion von $\lambda$. In der Praxis wird der Regularisierungsparameter häufig auf der natürlichen log-Skala dargestellt.</span>
<span id="cb88-710"><a href="#cb88-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-713"><a href="#cb88-713" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-714"><a href="#cb88-714" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "LARS-Lösungspfad für Lasso-Schätzung"</span></span>
<span id="cb88-715"><a href="#cb88-715" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-larssolpath</span></span>
<span id="cb88-716"><a href="#cb88-716" aria-hidden="true" tabindex="-1"></a><span class="co"># Transformation in ein weites Format</span></span>
<span id="cb88-717"><a href="#cb88-717" aria-hidden="true" tabindex="-1"></a>fit_lars<span class="sc">$</span>beta <span class="sc">%&gt;%</span> </span>
<span id="cb88-718"><a href="#cb88-718" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb88-719"><a href="#cb88-719" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb88-720"><a href="#cb88-720" aria-hidden="true" tabindex="-1"></a>    <span class="at">lambda =</span> <span class="fu">c</span>(fit_lars<span class="sc">$</span>lambda, <span class="fl">1e-2</span>)</span>
<span id="cb88-721"><a href="#cb88-721" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb88-722"><a href="#cb88-722" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(</span>
<span id="cb88-723"><a href="#cb88-723" aria-hidden="true" tabindex="-1"></a>    <span class="at">cols =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>, </span>
<span id="cb88-724"><a href="#cb88-724" aria-hidden="true" tabindex="-1"></a>    <span class="at">names_to =</span> <span class="st">"Variable"</span>, </span>
<span id="cb88-725"><a href="#cb88-725" aria-hidden="true" tabindex="-1"></a>    <span class="at">values_to =</span> <span class="st">"gesch. Koeffizient"</span></span>
<span id="cb88-726"><a href="#cb88-726" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb88-727"><a href="#cb88-727" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb88-728"><a href="#cb88-728" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualisierung mit ggplot  </span></span>
<span id="cb88-729"><a href="#cb88-729" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(</span>
<span id="cb88-730"><a href="#cb88-730" aria-hidden="true" tabindex="-1"></a>    <span class="at">mapping =</span> <span class="fu">aes</span>(</span>
<span id="cb88-731"><a href="#cb88-731" aria-hidden="true" tabindex="-1"></a>      <span class="at">x =</span> <span class="fu">log</span>(lambda), </span>
<span id="cb88-732"><a href="#cb88-732" aria-hidden="true" tabindex="-1"></a>      <span class="at">y =</span> <span class="st">`</span><span class="at">gesch. Koeffizient</span><span class="st">`</span>, </span>
<span id="cb88-733"><a href="#cb88-733" aria-hidden="true" tabindex="-1"></a>      <span class="at">color =</span> Variable</span>
<span id="cb88-734"><a href="#cb88-734" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb88-735"><a href="#cb88-735" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span> </span>
<span id="cb88-736"><a href="#cb88-736" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() </span>
<span id="cb88-737"><a href="#cb88-737" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-738"><a href="#cb88-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-739"><a href="#cb88-739" aria-hidden="true" tabindex="-1"></a>@fig-larssolpath zeigt, dass die Shrinkage der geschätzten Koeffizienten nach der Aktivierung rasch abnimmt und sich für kleine Werte von $\lambda$ der KQ-Lösung annähert. Wir sehen auch, dass es einen Bereich von $\lambda$-Werten gibt, für die das wahre Modell mit den Variablen $X_1$, $X_2$, $X_8$ und $X_9$ selektiert werden kann. Je nach Ziel der Analyse kann es sinnvoll sein, ein $\lambda$ in diesem Intervall zu schätzen.</span>
<span id="cb88-740"><a href="#cb88-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-741"><a href="#cb88-741" aria-hidden="true" tabindex="-1"></a><span class="fu">### Wahl des Regularisierungsparameters $\lambda$ für den Lasso-Schätzer</span></span>
<span id="cb88-742"><a href="#cb88-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-743"><a href="#cb88-743" aria-hidden="true" tabindex="-1"></a>Wie zuvor bei Ridge Regression muss in empirischen Anwendungen ein Wert für den Tuning-Parameter $\lambda$ gewählt werden. Hierbei besteht die Herausforderung darin, einen geeigneten Wert zu finden, der zu wünschenswerten Eigenschaften des resultierenden Modells führt. So ist für gute Vorhersagen wichtig, dass das Modell nicht zu sehr an die Daten angepasst ist (*Overfitting*), um eine gute Generalisierung auf neue Daten zu ermöglichen. Gleichzeitig muss das Modell flexibel genug sein, um wesentliche Eigenschaften des datenerzeugenden Prozesses hinreichend gut zu erfassen. In der Regel wird hierbei eine sparsame Modellierung angestrebt, die nur eine Teilmenge der Prädiktoren nutzt.</span>
<span id="cb88-744"><a href="#cb88-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-745"><a href="#cb88-745" aria-hidden="true" tabindex="-1"></a>In der Praxis werden verschiedene Verfahren verwendet, um den Wert für den Tuning-Parameter $\lambda$ zu bestimmen. Gängige Methoden sind *Cross Validation* (CV) und Informationskriterien. In Abhängigkeit der Methode und der Daten ergeben sich ober- oder unterparameterisierte Modelle. Aufgrund der Implementierung im R-Paket <span class="in">`lars`</span> betrachten wir CV.<span class="ot">[^regreg-15]</span> Wir zeigen nachfolgend anhand der simulierten Daten aus dem letzten Abschnitt, wie für die LARS-Schätzung ein optimales $\lambda$ mit leave-one-out CV (LOO-CV) bestimmt werden kann. Hierzu nutzen wir <span class="in">`lars::cv.lars()`</span> unter Verwendung derselben Argumente wie zuvor im Aufruf von <span class="in">`lars()`</span>.</span>
<span id="cb88-746"><a href="#cb88-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-747"><a href="#cb88-747" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-15]: </span>Chetverikov, Liao, and Chernozhukov (2020) zeigen, dass CV zu konsistenter Modellselektion führen kann.</span>
<span id="cb88-748"><a href="#cb88-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-751"><a href="#cb88-751" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-752"><a href="#cb88-752" aria-hidden="true" tabindex="-1"></a><span class="co"># LARS-Lösungen mit CV evaluieren</span></span>
<span id="cb88-753"><a href="#cb88-753" aria-hidden="true" tabindex="-1"></a>fit_lars_cv <span class="ot">&lt;-</span> <span class="fu">cv.lars</span>(</span>
<span id="cb88-754"><a href="#cb88-754" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> X, </span>
<span id="cb88-755"><a href="#cb88-755" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> Y, </span>
<span id="cb88-756"><a href="#cb88-756" aria-hidden="true" tabindex="-1"></a>  <span class="at">intercept =</span> F,</span>
<span id="cb88-757"><a href="#cb88-757" aria-hidden="true" tabindex="-1"></a>  <span class="at">normalize =</span> T,</span>
<span id="cb88-758"><a href="#cb88-758" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">"lasso"</span>, </span>
<span id="cb88-759"><a href="#cb88-759" aria-hidden="true" tabindex="-1"></a>  <span class="at">plot.it =</span> F, </span>
<span id="cb88-760"><a href="#cb88-760" aria-hidden="true" tabindex="-1"></a>  <span class="at">K =</span> N <span class="co"># für LOO-CV</span></span>
<span id="cb88-761"><a href="#cb88-761" aria-hidden="true" tabindex="-1"></a>) </span>
<span id="cb88-762"><a href="#cb88-762" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-763"><a href="#cb88-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-764"><a href="#cb88-764" aria-hidden="true" tabindex="-1"></a>Das Objekt <span class="in">`fit_lars_cv`</span> ist eine Liste mit den CV-Ergebnissen. Wir können diese einfach mit <span class="in">`ggplot`</span> visualisieren. <span class="in">`index`</span> ist hierbei das Verhältnis der $\ell_1$-Norm des Lasso-Schätzers für einen spezifischen Wert von $\lambda$ und der $\ell_1$-Norm des KQ-Schätzers. Das optimale $\lambda$ wird so implizit geschätzt. <span class="in">`cv.error`</span> ist der mit CV geschätzte MSE.</span>
<span id="cb88-765"><a href="#cb88-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-768"><a href="#cb88-768" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-769"><a href="#cb88-769" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "CV-MSE und relative Position von $\\lambda$ auf dem Lassopfad"</span></span>
<span id="cb88-770"><a href="#cb88-770" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-larscv</span></span>
<span id="cb88-771"><a href="#cb88-771" aria-hidden="true" tabindex="-1"></a><span class="co"># CV-MSE</span></span>
<span id="cb88-772"><a href="#cb88-772" aria-hidden="true" tabindex="-1"></a>fit_lars_cv <span class="sc">%&gt;%</span> </span>
<span id="cb88-773"><a href="#cb88-773" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span></span>
<span id="cb88-774"><a href="#cb88-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-775"><a href="#cb88-775" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(</span>
<span id="cb88-776"><a href="#cb88-776" aria-hidden="true" tabindex="-1"></a>    <span class="at">mapping =</span> <span class="fu">aes</span>(</span>
<span id="cb88-777"><a href="#cb88-777" aria-hidden="true" tabindex="-1"></a>      <span class="at">x =</span> index, </span>
<span id="cb88-778"><a href="#cb88-778" aria-hidden="true" tabindex="-1"></a>      <span class="at">y =</span> cv.error</span>
<span id="cb88-779"><a href="#cb88-779" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb88-780"><a href="#cb88-780" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span> </span>
<span id="cb88-781"><a href="#cb88-781" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb88-782"><a href="#cb88-782" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"|beta_lambda| / |beta|"</span>) <span class="sc">+</span></span>
<span id="cb88-783"><a href="#cb88-783" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"CV-MSE"</span>)</span>
<span id="cb88-784"><a href="#cb88-784" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-785"><a href="#cb88-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-786"><a href="#cb88-786" aria-hidden="true" tabindex="-1"></a>In der Grafik erkennen wir ein Minimum des CV-MSEs bei etwa <span class="in">`r round(fit_lars_cv$index[which.min(fit_lars_cv$cv.error)], 2)`</span>.</span>
<span id="cb88-787"><a href="#cb88-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-790"><a href="#cb88-790" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-791"><a href="#cb88-791" aria-hidden="true" tabindex="-1"></a><span class="co"># CV-MSE-minimierendes Lambda bestimmen</span></span>
<span id="cb88-792"><a href="#cb88-792" aria-hidden="true" tabindex="-1"></a>ID <span class="ot">&lt;-</span> <span class="fu">which.min</span>(fit_lars_cv<span class="sc">$</span>cv.error) <span class="co"># Index</span></span>
<span id="cb88-793"><a href="#cb88-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-794"><a href="#cb88-794" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb88-795"><a href="#cb88-795" aria-hidden="true" tabindex="-1"></a>  fraction_opt <span class="ot">&lt;-</span> fit_lars_cv<span class="sc">$</span>index[ID]</span>
<span id="cb88-796"><a href="#cb88-796" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-797"><a href="#cb88-797" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-798"><a href="#cb88-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-799"><a href="#cb88-799" aria-hidden="true" tabindex="-1"></a>Die geschätzten Koeffizienten für die optimale Regularisierung können mit <span class="in">`coef()`</span> ausgelesen werden.</span>
<span id="cb88-800"><a href="#cb88-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-803"><a href="#cb88-803" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-804"><a href="#cb88-804" aria-hidden="true" tabindex="-1"></a><span class="co"># LARS-Lasso-Fit für optimales lambda bestimmen</span></span>
<span id="cb88-805"><a href="#cb88-805" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(</span>
<span id="cb88-806"><a href="#cb88-806" aria-hidden="true" tabindex="-1"></a>  <span class="at">object =</span> fit_lars, </span>
<span id="cb88-807"><a href="#cb88-807" aria-hidden="true" tabindex="-1"></a>  <span class="at">s =</span> fraction_opt, </span>
<span id="cb88-808"><a href="#cb88-808" aria-hidden="true" tabindex="-1"></a>  <span class="at">mode =</span> <span class="st">"fraction"</span></span>
<span id="cb88-809"><a href="#cb88-809" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-810"><a href="#cb88-810" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-811"><a href="#cb88-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-812"><a href="#cb88-812" aria-hidden="true" tabindex="-1"></a>Das Ergebnis veranschaulicht die Selektionseigenschaft von Lasso: Gemäß DGP \eqref{eq:larsdgp} sind die Variablen $X_3$ bis $X_7$ *irrelevante* Prädiktoren für $Y$; ihre wahren Koeffizienten sind $0$. In der kreuzvalidierten Lasso-Schätzung erreicht die Regularisierung, dass die Koeffizienten der Variablen $X_4$ bis $X_7$ tatsächlich mit 0 geschätzt werden. Wir schätzen für das mit CV bestimmte $\lambda$ also ein leicht überspezifiziertes Modell mit den Regressoren $X_1$, $X_2$, $X_3$, $X_8$ und $X_9$. Beachte, dass die Lasso-Schätzung einen Kompromiss impliziert: Die Varianz der Schätzung ist geringer als die des KQ-Schätzers im Modell mit allen Variablen.<span class="ot">[^regreg-16]</span> Aufgrund der Regularisierung sind die mit Lasso geschätzten Koeffizienten der relevanten Variablen jedoch in Richtung $0$ verzerrt.</span>
<span id="cb88-813"><a href="#cb88-813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-814"><a href="#cb88-814" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-16]: </span>Wegen $N=25$ verbleiben bei der KQ-Schätzung mit 9 Regressoren nur 16 Freiheitsgrade.</span>
<span id="cb88-815"><a href="#cb88-815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-816"><a href="#cb88-816" aria-hidden="true" tabindex="-1"></a>Einen positiven Effekt dieses Kompromisses beobachten wir anhand des mittleren Vorhersagefehlers für Daten, die *nicht* zur Berechnung des Schätzers verwendet wurden. Wir vergleichen den Vorhersagefehler nachfolgend anhand eines solchen simulierten Test-Datensatzes mit 25 neuen Beobachtungen. Den Vorhersagefehler bestimmen wir als MSE zwischen den vorhergesagten und den tatsächlichen Ausprägungen für $Y$.</span>
<span id="cb88-817"><a href="#cb88-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-820"><a href="#cb88-820" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-821"><a href="#cb88-821" aria-hidden="true" tabindex="-1"></a><span class="co"># Test-Datensatz erstellen</span></span>
<span id="cb88-822"><a href="#cb88-822" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">4321</span>)</span>
<span id="cb88-823"><a href="#cb88-823" aria-hidden="true" tabindex="-1"></a>new_X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(N <span class="sc">*</span> <span class="dv">9</span>), <span class="at">ncol =</span> <span class="dv">9</span>)</span>
<span id="cb88-824"><a href="#cb88-824" aria-hidden="true" tabindex="-1"></a>new_Y <span class="ot">&lt;-</span> new_X <span class="sc">%*%</span> beta_v <span class="sc">+</span> <span class="fu">rnorm</span>(N)</span>
<span id="cb88-825"><a href="#cb88-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-826"><a href="#cb88-826" aria-hidden="true" tabindex="-1"></a><span class="co"># Lasso: Vorhersage von new_Y für Test-Datensatz</span></span>
<span id="cb88-827"><a href="#cb88-827" aria-hidden="true" tabindex="-1"></a>Y_predict_lars <span class="ot">&lt;-</span> <span class="fu">predict</span>(</span>
<span id="cb88-828"><a href="#cb88-828" aria-hidden="true" tabindex="-1"></a>  <span class="at">object =</span> fit_lars, </span>
<span id="cb88-829"><a href="#cb88-829" aria-hidden="true" tabindex="-1"></a>  <span class="at">s =</span> fraction_opt, </span>
<span id="cb88-830"><a href="#cb88-830" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">"fit"</span>, </span>
<span id="cb88-831"><a href="#cb88-831" aria-hidden="true" tabindex="-1"></a>  <span class="at">mode =</span> <span class="st">"fraction"</span>, </span>
<span id="cb88-832"><a href="#cb88-832" aria-hidden="true" tabindex="-1"></a>  <span class="at">newx =</span> new_X</span>
<span id="cb88-833"><a href="#cb88-833" aria-hidden="true" tabindex="-1"></a>)<span class="sc">$</span>fit</span>
<span id="cb88-834"><a href="#cb88-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-835"><a href="#cb88-835" aria-hidden="true" tabindex="-1"></a><span class="co"># Lasso: MSE für Test-Datensatz berechnen</span></span>
<span id="cb88-836"><a href="#cb88-836" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((Y_predict_lars <span class="sc">-</span> new_Y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb88-837"><a href="#cb88-837" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-838"><a href="#cb88-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-839"><a href="#cb88-839" aria-hidden="true" tabindex="-1"></a>Wir schätzen nun das große Modell mit allen 9 Variablen mit KQ und berechnen ebenfalls den MSE der Prognosen für den Test-Datensatz.</span>
<span id="cb88-840"><a href="#cb88-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-843"><a href="#cb88-843" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-844"><a href="#cb88-844" aria-hidden="true" tabindex="-1"></a><span class="co"># KQ-Schätzung des großen Modells durchführen</span></span>
<span id="cb88-845"><a href="#cb88-845" aria-hidden="true" tabindex="-1"></a>KQ_fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb88-846"><a href="#cb88-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-847"><a href="#cb88-847" aria-hidden="true" tabindex="-1"></a><span class="co"># Test-Datensatz für predict.lm() formatieren</span></span>
<span id="cb88-848"><a href="#cb88-848" aria-hidden="true" tabindex="-1"></a>new_X <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(new_X)</span>
<span id="cb88-849"><a href="#cb88-849" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(new_X) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">"X"</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>)</span>
<span id="cb88-850"><a href="#cb88-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-851"><a href="#cb88-851" aria-hidden="true" tabindex="-1"></a><span class="co"># KQ: Vorhersage von new_Y für Test-Datensatz</span></span>
<span id="cb88-852"><a href="#cb88-852" aria-hidden="true" tabindex="-1"></a>Y_predict_KQ <span class="ot">&lt;-</span> <span class="fu">predict</span>(</span>
<span id="cb88-853"><a href="#cb88-853" aria-hidden="true" tabindex="-1"></a>  <span class="at">object =</span> KQ_fit, </span>
<span id="cb88-854"><a href="#cb88-854" aria-hidden="true" tabindex="-1"></a>  <span class="at">newdata =</span> new_X</span>
<span id="cb88-855"><a href="#cb88-855" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-856"><a href="#cb88-856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-857"><a href="#cb88-857" aria-hidden="true" tabindex="-1"></a><span class="co"># KQ: MSE für Test-Datensatz berechnen</span></span>
<span id="cb88-858"><a href="#cb88-858" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((Y_predict_KQ <span class="sc">-</span> new_Y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb88-859"><a href="#cb88-859" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-860"><a href="#cb88-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-861"><a href="#cb88-861" aria-hidden="true" tabindex="-1"></a>Offenbar führt die Lasso-Schätzung zu einem deutlich geringeren MSE der Vorhersage von <span class="in">`Y`</span> für den Test-Datensatz als die KQ-Schätzung und damit zu einer höheren Vorhersagegüte. Das "sparsame" mit Lasso-Regression geschätzte Modell ist dem "großen" mit KQ geschätztem Modell in dieser Hinsicht also überlegen.</span>
<span id="cb88-862"><a href="#cb88-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-863"><a href="#cb88-863" aria-hidden="true" tabindex="-1"></a>::: callout-note</span>
<span id="cb88-864"><a href="#cb88-864" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Key Facts zu Lasso-Regression</span></span>
<span id="cb88-865"><a href="#cb88-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-866"><a href="#cb88-866" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Lasso-Regression bestraft die Verlustfunktion des KQ-Schätzers mit der $\ell_1$-Norm der Koeffizienten.</span>
<span id="cb88-867"><a href="#cb88-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-868"><a href="#cb88-868" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Neben Koeffizientenschätzung mit Shrinkage in Richtung $0$ kann der Lasso-Schätzer Variablenselektion durchführen: Regressionskoeffizienten können exakt mit $0$ geschätzt und so ein "sparsames", leichter zu interpretierendes Modell gewählt werden.</span>
<span id="cb88-869"><a href="#cb88-869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-870"><a href="#cb88-870" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Wie bei Ridge Regression impliziert die Wahl von $\lambda$ einen Bias-Variance-Tradeoff, der für Vorhersagen nützlich ist: Für größere $\lambda$ wird mehr Verzerrung induziert und möglicherweise relevante Variablen mit kleinen Koeffizienten aus dem Modell entfernt. Ein solches sparsames Modell kann eine höhere Prognosegüte haben als ein komplexes, unregularisiertes Modell.</span>
<span id="cb88-871"><a href="#cb88-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-872"><a href="#cb88-872" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Der Lasso-Schätzer $\widehat{\boldsymbol{\beta}}_\lambda^L$ ist *nicht* erwartungstreu.</span>
<span id="cb88-873"><a href="#cb88-873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-874"><a href="#cb88-874" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Lasso Regression kann bspw. mit dem LARS-Algorithmus (Paket <span class="in">`lars`</span>) oder mit <span class="in">`glmnet`</span> berechnet werden.</span>
<span id="cb88-875"><a href="#cb88-875" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb88-876"><a href="#cb88-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-877"><a href="#cb88-877" aria-hidden="true" tabindex="-1"></a><span class="fu">## Vergleich von Lasso- und Ridge-Regression mit Simulation</span></span>
<span id="cb88-878"><a href="#cb88-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-879"><a href="#cb88-879" aria-hidden="true" tabindex="-1"></a>In diesem Kapitel illustrieren wir Vor- und Nachteile von Lasso- und Ridge-Regression in Prognose-Anwendungen anhand von Monte-Carlo-Simulationen. Wir betrachten hierbei datenerzeugende Prozesse, die sich hinsichtlich der Anzahl relevanter Variablen sowie der Korrelation dieser Variablen unterscheiden.</span>
<span id="cb88-880"><a href="#cb88-880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-881"><a href="#cb88-881" aria-hidden="true" tabindex="-1"></a>Die grundlegende Vorschrift für die Simulationen ist \begin{align*}</span>
<span id="cb88-882"><a href="#cb88-882" aria-hidden="true" tabindex="-1"></a>  Y_i = \sum_{j=1}^{k=40} \beta_j X_{i,j} + u_i, \quad u_i \overset{u.i.v.}{\sim} N(0,1), \quad i=1,\dots,100,</span>
<span id="cb88-883"><a href="#cb88-883" aria-hidden="true" tabindex="-1"></a>\end{align*} wobei die Regressoren $X_j$ eine Varianz von $1$ haben und aus einer multivariaten Normalverteilung mit Korrelation $$\rho\in(0,0.5,0.8)$$ gezogen werden.</span>
<span id="cb88-884"><a href="#cb88-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-885"><a href="#cb88-885" aria-hidden="true" tabindex="-1"></a>Für die Koeffizienten $\boldsymbol{\beta}$ unterscheiden wir zwei Szenarien. In Szenario A ist $$\boldsymbol{\beta} = (1,\dots,1)',$$ d.h. alle Variablen sind relevant und haben denselben Einfluss auf $Y$. In Szenario B erzeugen wir $\boldsymbol{\beta}$ einmalig vorab so, dass $$\beta_j = \begin{cases}1,\quad \text{mit Wsk.  }p<span class="sc">\\</span> 0,\quad \text{mit Wsk.  }1-p, \end{cases}$$ d.h. nur eine Teilmenge der Variablen beeinflusst $Y$ jeweils mit demselben Effekt $\beta_j = 1$. Die übrigen Variablen sind irrelevant.</span>
<span id="cb88-886"><a href="#cb88-886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-887"><a href="#cb88-887" aria-hidden="true" tabindex="-1"></a>Wir schätzen und validieren die Modelle mit <span class="in">`glmnet()`</span>.</span>
<span id="cb88-888"><a href="#cb88-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-889"><a href="#cb88-889" aria-hidden="true" tabindex="-1"></a><span class="fu">### Prognosegüte in diversen Szenarien {#sec-pdz}</span></span>
<span id="cb88-890"><a href="#cb88-890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-893"><a href="#cb88-893" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-894"><a href="#cb88-894" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulationsparameter definieren</span></span>
<span id="cb88-895"><a href="#cb88-895" aria-hidden="true" tabindex="-1"></a>rho <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.5</span>, <span class="fl">0.8</span>)   <span class="co"># Korrelation</span></span>
<span id="cb88-896"><a href="#cb88-896" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">40</span>                 <span class="co"># Anz. Regressoren</span></span>
<span id="cb88-897"><a href="#cb88-897" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">100</span>                <span class="co"># Anz. Beobachtungen</span></span>
<span id="cb88-898"><a href="#cb88-898" aria-hidden="true" tabindex="-1"></a>n_sim <span class="ot">&lt;-</span> <span class="dv">100</span>            <span class="co"># Anz. Simulationen</span></span>
<span id="cb88-899"><a href="#cb88-899" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-900"><a href="#cb88-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-901"><a href="#cb88-901" aria-hidden="true" tabindex="-1"></a>Damit der Code für die Simulation möglichst wenig repetitiv ist, definieren wir eine Funktion <span class="in">`cv.glmnet_MSE()`</span>, die unter Angabe der Daten <span class="in">`X`</span> und <span class="in">`Y`</span>, des Trainingssets <span class="in">`train`</span> sowie des Parameters <span class="in">`alpha`</span> den gewünschten regularisierten Schätzer under Verwendung von Cross Validation anpasst und den Testset-MSE zurückgibt.</span>
<span id="cb88-902"><a href="#cb88-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-905"><a href="#cb88-905" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-906"><a href="#cb88-906" aria-hidden="true" tabindex="-1"></a><span class="co"># allg. Funktion für Testset-MSE nach CV</span></span>
<span id="cb88-907"><a href="#cb88-907" aria-hidden="true" tabindex="-1"></a>cv.glmnet_MSE <span class="ot">&lt;-</span> <span class="cf">function</span>(X, Y, train, alpha) {</span>
<span id="cb88-908"><a href="#cb88-908" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb88-909"><a href="#cb88-909" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Modell mit glmnet schätzen; lambda per CV bestimmen</span></span>
<span id="cb88-910"><a href="#cb88-910" aria-hidden="true" tabindex="-1"></a>  fit_cv <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(</span>
<span id="cb88-911"><a href="#cb88-911" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> X[train,],</span>
<span id="cb88-912"><a href="#cb88-912" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span>Y[train],</span>
<span id="cb88-913"><a href="#cb88-913" aria-hidden="true" tabindex="-1"></a>    <span class="at">alpha =</span> alpha</span>
<span id="cb88-914"><a href="#cb88-914" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb88-915"><a href="#cb88-915" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb88-916"><a href="#cb88-916" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Vorhersagen treffen</span></span>
<span id="cb88-917"><a href="#cb88-917" aria-hidden="true" tabindex="-1"></a>  Y_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(</span>
<span id="cb88-918"><a href="#cb88-918" aria-hidden="true" tabindex="-1"></a>    <span class="at">object =</span> fit_cv, </span>
<span id="cb88-919"><a href="#cb88-919" aria-hidden="true" tabindex="-1"></a>    <span class="at">s =</span> fit_cv<span class="sc">$</span>lambda.min, </span>
<span id="cb88-920"><a href="#cb88-920" aria-hidden="true" tabindex="-1"></a>    <span class="at">newx =</span> X[<span class="sc">-</span>train,])</span>
<span id="cb88-921"><a href="#cb88-921" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb88-922"><a href="#cb88-922" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(</span>
<span id="cb88-923"><a href="#cb88-923" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Testset-MSE berechnen</span></span>
<span id="cb88-924"><a href="#cb88-924" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mean</span>(</span>
<span id="cb88-925"><a href="#cb88-925" aria-hidden="true" tabindex="-1"></a>      (Y[<span class="sc">-</span>train] <span class="sc">-</span> Y_pred)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb88-926"><a href="#cb88-926" aria-hidden="true" tabindex="-1"></a>      )</span>
<span id="cb88-927"><a href="#cb88-927" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb88-928"><a href="#cb88-928" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb88-929"><a href="#cb88-929" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-930"><a href="#cb88-930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-931"><a href="#cb88-931" aria-hidden="true" tabindex="-1"></a>Wir initialisieren zunächst Matrizen, in welche die MSEs aus den <span class="in">`r n_sim`</span> Simulationsdurchläufen reihenweise geschrieben werden. <span class="in">`lasso_mse`</span> und <span class="in">`ridge_mse`</span> haben je eine Spalte für jede Korrelation in <span class="in">`rho`</span></span>
<span id="cb88-932"><a href="#cb88-932" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-935"><a href="#cb88-935" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-936"><a href="#cb88-936" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrizen für simulierte MSEs initialisieren...</span></span>
<span id="cb88-937"><a href="#cb88-937" aria-hidden="true" tabindex="-1"></a>lasso_mse <span class="ot">&lt;-</span> <span class="fu">matrix</span>(</span>
<span id="cb88-938"><a href="#cb88-938" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> <span class="cn">NA</span>, </span>
<span id="cb88-939"><a href="#cb88-939" aria-hidden="true" tabindex="-1"></a>  <span class="at">nrow =</span> n_sim, </span>
<span id="cb88-940"><a href="#cb88-940" aria-hidden="true" tabindex="-1"></a>  <span class="at">ncol =</span> <span class="fu">length</span>(rho)</span>
<span id="cb88-941"><a href="#cb88-941" aria-hidden="true" tabindex="-1"></a>) </span>
<span id="cb88-942"><a href="#cb88-942" aria-hidden="true" tabindex="-1"></a>ridge_mse <span class="ot">&lt;-</span> lasso_mse</span>
<span id="cb88-943"><a href="#cb88-943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-944"><a href="#cb88-944" aria-hidden="true" tabindex="-1"></a><span class="co"># ... und benennen</span></span>
<span id="cb88-945"><a href="#cb88-945" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(lasso_mse) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">"Kor="</span>, rho)</span>
<span id="cb88-946"><a href="#cb88-946" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(ridge_mse) <span class="ot">&lt;-</span> <span class="fu">colnames</span>(lasso_mse)</span>
<span id="cb88-947"><a href="#cb88-947" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-948"><a href="#cb88-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-949"><a href="#cb88-949" aria-hidden="true" tabindex="-1"></a>Für die Simulation iterieren wir mit <span class="in">`purrr::walk`</span> über den Vektor <span class="in">`rho`</span> sowie über die Laufvariable <span class="in">`1:n_sim`</span>. Beide Schleifen nutzen den Syntax für anonyme Funktionen:</span>
<span id="cb88-950"><a href="#cb88-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-951"><a href="#cb88-951" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, eval = F}</span></span>
<span id="cb88-952"><a href="#cb88-952" aria-hidden="true" tabindex="-1"></a><span class="in"># Die anonyme Funktion</span></span>
<span id="cb88-953"><a href="#cb88-953" aria-hidden="true" tabindex="-1"></a><span class="in">function(x) return(x)</span></span>
<span id="cb88-954"><a href="#cb88-954" aria-hidden="true" tabindex="-1"></a><span class="in"># ist äquivalent definiert als</span></span>
<span id="cb88-955"><a href="#cb88-955" aria-hidden="true" tabindex="-1"></a><span class="in">\(x) return(x)</span></span>
<span id="cb88-956"><a href="#cb88-956" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-957"><a href="#cb88-957" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-958"><a href="#cb88-958" aria-hidden="true" tabindex="-1"></a>In jeden Simulationsdurchlauf erzeugen wir den Datensatz entsprechend der obigen Vorschrift, teilen die Daten auf und berechnen MSEs für Lasso- und Ridge-Regression mit <span class="in">`cv.glmnet_MSE()`</span>.</span>
<span id="cb88-959"><a href="#cb88-959" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-960"><a href="#cb88-960" aria-hidden="true" tabindex="-1"></a>**Szenario A**</span>
<span id="cb88-961"><a href="#cb88-961" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-964"><a href="#cb88-964" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-965"><a href="#cb88-965" aria-hidden="true" tabindex="-1"></a><span class="co"># Koeffizienten-Vektor definieren</span></span>
<span id="cb88-966"><a href="#cb88-966" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span>, k) </span>
<span id="cb88-967"><a href="#cb88-967" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-968"><a href="#cb88-968" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-971"><a href="#cb88-971" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-972"><a href="#cb88-972" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mvtnorm)</span>
<span id="cb88-973"><a href="#cb88-973" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb88-974"><a href="#cb88-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-975"><a href="#cb88-975" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb88-976"><a href="#cb88-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-977"><a href="#cb88-977" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulation durchführen</span></span>
<span id="cb88-978"><a href="#cb88-978" aria-hidden="true" tabindex="-1"></a><span class="fu">walk</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(rho), \(j) {</span>
<span id="cb88-979"><a href="#cb88-979" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb88-980"><a href="#cb88-980" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Korrelationsmatrix definieren</span></span>
<span id="cb88-981"><a href="#cb88-981" aria-hidden="true" tabindex="-1"></a>  Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(</span>
<span id="cb88-982"><a href="#cb88-982" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> rho[j], </span>
<span id="cb88-983"><a href="#cb88-983" aria-hidden="true" tabindex="-1"></a>    <span class="at">nrow =</span> k, </span>
<span id="cb88-984"><a href="#cb88-984" aria-hidden="true" tabindex="-1"></a>    <span class="at">ncol =</span> k</span>
<span id="cb88-985"><a href="#cb88-985" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb88-986"><a href="#cb88-986" aria-hidden="true" tabindex="-1"></a>  <span class="fu">diag</span>(Sigma) <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb88-987"><a href="#cb88-987" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb88-988"><a href="#cb88-988" aria-hidden="true" tabindex="-1"></a>  <span class="fu">walk</span>(<span class="dv">1</span><span class="sc">:</span>n_sim, \(i) {</span>
<span id="cb88-989"><a href="#cb88-989" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb88-990"><a href="#cb88-990" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Daten simulieren</span></span>
<span id="cb88-991"><a href="#cb88-991" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(</span>
<span id="cb88-992"><a href="#cb88-992" aria-hidden="true" tabindex="-1"></a>    <span class="at">n =</span> N, </span>
<span id="cb88-993"><a href="#cb88-993" aria-hidden="true" tabindex="-1"></a>    <span class="at">mean =</span> <span class="fu">rep</span>(<span class="dv">0</span>, k), </span>
<span id="cb88-994"><a href="#cb88-994" aria-hidden="true" tabindex="-1"></a>    <span class="at">sigma =</span> Sigma</span>
<span id="cb88-995"><a href="#cb88-995" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb88-996"><a href="#cb88-996" aria-hidden="true" tabindex="-1"></a>  Y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> <span class="fu">rnorm</span>(N)</span>
<span id="cb88-997"><a href="#cb88-997" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb88-998"><a href="#cb88-998" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Trainingsdaten definieren</span></span>
<span id="cb88-999"><a href="#cb88-999" aria-hidden="true" tabindex="-1"></a>  ID_train <span class="ot">&lt;-</span> <span class="fu">sample</span>(</span>
<span id="cb88-1000"><a href="#cb88-1000" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>N), </span>
<span id="cb88-1001"><a href="#cb88-1001" aria-hidden="true" tabindex="-1"></a>    <span class="at">size =</span> N<span class="sc">/</span><span class="dv">2</span></span>
<span id="cb88-1002"><a href="#cb88-1002" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb88-1003"><a href="#cb88-1003" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb88-1004"><a href="#cb88-1004" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Modelle mit CV schätzen und MSEs berechnen</span></span>
<span id="cb88-1005"><a href="#cb88-1005" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Ridge-Regression</span></span>
<span id="cb88-1006"><a href="#cb88-1006" aria-hidden="true" tabindex="-1"></a>  ridge_mse[i, j] <span class="ot">&lt;&lt;-</span> <span class="fu">cv.glmnet_MSE</span>(</span>
<span id="cb88-1007"><a href="#cb88-1007" aria-hidden="true" tabindex="-1"></a>    <span class="at">X =</span> X, </span>
<span id="cb88-1008"><a href="#cb88-1008" aria-hidden="true" tabindex="-1"></a>    <span class="at">Y =</span> Y, </span>
<span id="cb88-1009"><a href="#cb88-1009" aria-hidden="true" tabindex="-1"></a>    <span class="at">train =</span> ID_train, </span>
<span id="cb88-1010"><a href="#cb88-1010" aria-hidden="true" tabindex="-1"></a>    <span class="at">alpha =</span> <span class="dv">0</span></span>
<span id="cb88-1011"><a href="#cb88-1011" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb88-1012"><a href="#cb88-1012" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb88-1013"><a href="#cb88-1013" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Lasso-Regression</span></span>
<span id="cb88-1014"><a href="#cb88-1014" aria-hidden="true" tabindex="-1"></a>  lasso_mse[i, j] <span class="ot">&lt;&lt;-</span> <span class="fu">cv.glmnet_MSE</span>(</span>
<span id="cb88-1015"><a href="#cb88-1015" aria-hidden="true" tabindex="-1"></a>    <span class="at">X =</span> X, </span>
<span id="cb88-1016"><a href="#cb88-1016" aria-hidden="true" tabindex="-1"></a>    <span class="at">Y =</span> Y, </span>
<span id="cb88-1017"><a href="#cb88-1017" aria-hidden="true" tabindex="-1"></a>    <span class="at">train =</span> ID_train, </span>
<span id="cb88-1018"><a href="#cb88-1018" aria-hidden="true" tabindex="-1"></a>    <span class="at">alpha =</span> <span class="dv">1</span></span>
<span id="cb88-1019"><a href="#cb88-1019" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb88-1020"><a href="#cb88-1020" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb88-1021"><a href="#cb88-1021" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb88-1022"><a href="#cb88-1022" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb88-1023"><a href="#cb88-1023" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb88-1024"><a href="#cb88-1024" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1025"><a href="#cb88-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1026"><a href="#cb88-1026" aria-hidden="true" tabindex="-1"></a>Beachte, dass hier der Super-Assignment-Operator <span class="in">`&lt;&lt;-`</span> genutzt wird, damit <span class="in">`walk`</span> die Matrizen <span class="in">`ridge_mse`</span> und <span class="in">`lasso_mse`</span> in der globalen Umgebung überschreibt.<span class="ot">[^regreg-17]</span></span>
<span id="cb88-1027"><a href="#cb88-1027" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1028"><a href="#cb88-1028" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-17]: </span>Dies folgt aus der Definition von <span class="in">`walk`</span>. <span class="in">`&lt;-`</span> bewirkt hier lediglich Assignment in der Funktionsumgebung.</span>
<span id="cb88-1029"><a href="#cb88-1029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1030"><a href="#cb88-1030" aria-hidden="true" tabindex="-1"></a>Wir berechnen jeweils den mittleren MSEs, sammeln die Ergebnisse in einer <span class="in">`tibble()`</span> und nutzen <span class="in">`gt()`</span> für die tabellarische Darstellung.</span>
<span id="cb88-1031"><a href="#cb88-1031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1034"><a href="#cb88-1034" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-1035"><a href="#cb88-1035" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: Durchschnittliche Testset-MSEs für Setting A</span></span>
<span id="cb88-1036"><a href="#cb88-1036" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-lrsimA</span></span>
<span id="cb88-1037"><a href="#cb88-1037" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gt)</span>
<span id="cb88-1038"><a href="#cb88-1038" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1039"><a href="#cb88-1039" aria-hidden="true" tabindex="-1"></a><span class="co"># Ergebnisse tabellarisch darstellen</span></span>
<span id="cb88-1040"><a href="#cb88-1040" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb88-1041"><a href="#cb88-1041" aria-hidden="true" tabindex="-1"></a>  <span class="at">Methode =</span> <span class="fu">c</span>(</span>
<span id="cb88-1042"><a href="#cb88-1042" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Lasso-Regression"</span>, </span>
<span id="cb88-1043"><a href="#cb88-1043" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Ridge-Regression"</span></span>
<span id="cb88-1044"><a href="#cb88-1044" aria-hidden="true" tabindex="-1"></a>  ),</span>
<span id="cb88-1045"><a href="#cb88-1045" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb88-1046"><a href="#cb88-1046" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(</span>
<span id="cb88-1047"><a href="#cb88-1047" aria-hidden="true" tabindex="-1"></a>    <span class="fu">bind_rows</span>(</span>
<span id="cb88-1048"><a href="#cb88-1048" aria-hidden="true" tabindex="-1"></a>      <span class="fu">colMeans</span>(lasso_mse),</span>
<span id="cb88-1049"><a href="#cb88-1049" aria-hidden="true" tabindex="-1"></a>      <span class="fu">colMeans</span>(ridge_mse)  </span>
<span id="cb88-1050"><a href="#cb88-1050" aria-hidden="true" tabindex="-1"></a>    )    </span>
<span id="cb88-1051"><a href="#cb88-1051" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb88-1052"><a href="#cb88-1052" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gt</span>() <span class="sc">%&gt;%</span></span>
<span id="cb88-1053"><a href="#cb88-1053" aria-hidden="true" tabindex="-1"></a>  tabopts</span>
<span id="cb88-1054"><a href="#cb88-1054" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1055"><a href="#cb88-1055" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1056"><a href="#cb88-1056" aria-hidden="true" tabindex="-1"></a>@tbl-lrsimA zeigt, dass Ridge-Regression gegenüber Lasso-Regression für jede der drei betrachteten Korrelationen überlegen ist. Insbesondere bei stärker korrelierten Regressoren ist Ridge vorteilhaft.</span>
<span id="cb88-1057"><a href="#cb88-1057" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1058"><a href="#cb88-1058" aria-hidden="true" tabindex="-1"></a>Für Szenario B überschreiben wir <span class="in">`beta`</span> nach Multiplikation mit einem zufälligen binären Vektor, sodass einige der Koeffizienten $0$ und die zugehörigen Variablen irrelevant für $Y$ sind.</span>
<span id="cb88-1059"><a href="#cb88-1059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1060"><a href="#cb88-1060" aria-hidden="true" tabindex="-1"></a>**Szenario B**</span>
<span id="cb88-1061"><a href="#cb88-1061" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1064"><a href="#cb88-1064" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-1065"><a href="#cb88-1065" aria-hidden="true" tabindex="-1"></a><span class="co"># Wsk. für Relevanz einer Variable</span></span>
<span id="cb88-1066"><a href="#cb88-1066" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> .<span class="dv">3</span></span>
<span id="cb88-1067"><a href="#cb88-1067" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1068"><a href="#cb88-1068" aria-hidden="true" tabindex="-1"></a><span class="co"># Koeffizienten-Vektor definieren</span></span>
<span id="cb88-1069"><a href="#cb88-1069" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb88-1070"><a href="#cb88-1070" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> beta <span class="sc">*</span> <span class="fu">sample</span>(</span>
<span id="cb88-1071"><a href="#cb88-1071" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">1</span>, </span>
<span id="cb88-1072"><a href="#cb88-1072" aria-hidden="true" tabindex="-1"></a>  <span class="at">size =</span> k, </span>
<span id="cb88-1073"><a href="#cb88-1073" aria-hidden="true" tabindex="-1"></a>  <span class="at">replace =</span> T, </span>
<span id="cb88-1074"><a href="#cb88-1074" aria-hidden="true" tabindex="-1"></a>  <span class="at">prob =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">-</span>p, p)</span>
<span id="cb88-1075"><a href="#cb88-1075" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-1076"><a href="#cb88-1076" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1077"><a href="#cb88-1077" aria-hidden="true" tabindex="-1"></a><span class="co"># Koeffizienten prüfen</span></span>
<span id="cb88-1078"><a href="#cb88-1078" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(beta, <span class="at">n =</span> <span class="dv">10</span>)</span>
<span id="cb88-1079"><a href="#cb88-1079" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1080"><a href="#cb88-1080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1081"><a href="#cb88-1081" aria-hidden="true" tabindex="-1"></a>Eine wiederholung der Simulation für die modifizierten Koeffizienten <span class="in">`beta`</span> und liefert folgende tabellarische Auswertung.</span>
<span id="cb88-1082"><a href="#cb88-1082" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1083"><a href="#cb88-1083" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo = F, cache=TRUE}</span></span>
<span id="cb88-1084"><a href="#cb88-1084" aria-hidden="true" tabindex="-1"></a><span class="in">#| tbl-cap: Durchschnittliche Testset-MSEs für Szenario B</span></span>
<span id="cb88-1085"><a href="#cb88-1085" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: tbl-lrsimB</span></span>
<span id="cb88-1086"><a href="#cb88-1086" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(1234)</span></span>
<span id="cb88-1087"><a href="#cb88-1087" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1088"><a href="#cb88-1088" aria-hidden="true" tabindex="-1"></a><span class="in"># Simulation durchführen</span></span>
<span id="cb88-1089"><a href="#cb88-1089" aria-hidden="true" tabindex="-1"></a><span class="in">walk(1:length(rho), \(j) {</span></span>
<span id="cb88-1090"><a href="#cb88-1090" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb88-1091"><a href="#cb88-1091" aria-hidden="true" tabindex="-1"></a><span class="in">  # Korrelationsmatrix definieren</span></span>
<span id="cb88-1092"><a href="#cb88-1092" aria-hidden="true" tabindex="-1"></a><span class="in">  Sigma &lt;- matrix(</span></span>
<span id="cb88-1093"><a href="#cb88-1093" aria-hidden="true" tabindex="-1"></a><span class="in">    data = rho[j], # Off-Diagonalelemente</span></span>
<span id="cb88-1094"><a href="#cb88-1094" aria-hidden="true" tabindex="-1"></a><span class="in">    nrow = k, </span></span>
<span id="cb88-1095"><a href="#cb88-1095" aria-hidden="true" tabindex="-1"></a><span class="in">    ncol = k</span></span>
<span id="cb88-1096"><a href="#cb88-1096" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb88-1097"><a href="#cb88-1097" aria-hidden="true" tabindex="-1"></a><span class="in">  diag(Sigma) &lt;- 1</span></span>
<span id="cb88-1098"><a href="#cb88-1098" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb88-1099"><a href="#cb88-1099" aria-hidden="true" tabindex="-1"></a><span class="in">  walk(1:n_sim, \(i) {</span></span>
<span id="cb88-1100"><a href="#cb88-1100" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb88-1101"><a href="#cb88-1101" aria-hidden="true" tabindex="-1"></a><span class="in">    # Daten simulieren</span></span>
<span id="cb88-1102"><a href="#cb88-1102" aria-hidden="true" tabindex="-1"></a><span class="in">    X &lt;- rmvnorm(</span></span>
<span id="cb88-1103"><a href="#cb88-1103" aria-hidden="true" tabindex="-1"></a><span class="in">      n = N, </span></span>
<span id="cb88-1104"><a href="#cb88-1104" aria-hidden="true" tabindex="-1"></a><span class="in">      mean = rep(0, k), </span></span>
<span id="cb88-1105"><a href="#cb88-1105" aria-hidden="true" tabindex="-1"></a><span class="in">      sigma = Sigma</span></span>
<span id="cb88-1106"><a href="#cb88-1106" aria-hidden="true" tabindex="-1"></a><span class="in">    )</span></span>
<span id="cb88-1107"><a href="#cb88-1107" aria-hidden="true" tabindex="-1"></a><span class="in">    Y &lt;- X %*% beta + rnorm(N)</span></span>
<span id="cb88-1108"><a href="#cb88-1108" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb88-1109"><a href="#cb88-1109" aria-hidden="true" tabindex="-1"></a><span class="in">    # Trainingsdaten definieren</span></span>
<span id="cb88-1110"><a href="#cb88-1110" aria-hidden="true" tabindex="-1"></a><span class="in">    ID_train &lt;- sample(</span></span>
<span id="cb88-1111"><a href="#cb88-1111" aria-hidden="true" tabindex="-1"></a><span class="in">      x = c(1:N), size = N/2</span></span>
<span id="cb88-1112"><a href="#cb88-1112" aria-hidden="true" tabindex="-1"></a><span class="in">    )</span></span>
<span id="cb88-1113"><a href="#cb88-1113" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb88-1114"><a href="#cb88-1114" aria-hidden="true" tabindex="-1"></a><span class="in">    # Modelle mit CV schätzen und MSEs berechnen</span></span>
<span id="cb88-1115"><a href="#cb88-1115" aria-hidden="true" tabindex="-1"></a><span class="in">    ridge_mse[i, j] &lt;&lt;- cv.glmnet_MSE(</span></span>
<span id="cb88-1116"><a href="#cb88-1116" aria-hidden="true" tabindex="-1"></a><span class="in">      X = X, </span></span>
<span id="cb88-1117"><a href="#cb88-1117" aria-hidden="true" tabindex="-1"></a><span class="in">      Y = Y, </span></span>
<span id="cb88-1118"><a href="#cb88-1118" aria-hidden="true" tabindex="-1"></a><span class="in">      train = ID_train, </span></span>
<span id="cb88-1119"><a href="#cb88-1119" aria-hidden="true" tabindex="-1"></a><span class="in">      alpha = 0</span></span>
<span id="cb88-1120"><a href="#cb88-1120" aria-hidden="true" tabindex="-1"></a><span class="in">    )</span></span>
<span id="cb88-1121"><a href="#cb88-1121" aria-hidden="true" tabindex="-1"></a><span class="in">    lasso_mse[i, j] &lt;&lt;- cv.glmnet_MSE(</span></span>
<span id="cb88-1122"><a href="#cb88-1122" aria-hidden="true" tabindex="-1"></a><span class="in">      X = X, </span></span>
<span id="cb88-1123"><a href="#cb88-1123" aria-hidden="true" tabindex="-1"></a><span class="in">      Y = Y, </span></span>
<span id="cb88-1124"><a href="#cb88-1124" aria-hidden="true" tabindex="-1"></a><span class="in">      train = ID_train, </span></span>
<span id="cb88-1125"><a href="#cb88-1125" aria-hidden="true" tabindex="-1"></a><span class="in">      alpha = 1</span></span>
<span id="cb88-1126"><a href="#cb88-1126" aria-hidden="true" tabindex="-1"></a><span class="in">    )</span></span>
<span id="cb88-1127"><a href="#cb88-1127" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb88-1128"><a href="#cb88-1128" aria-hidden="true" tabindex="-1"></a><span class="in">  })</span></span>
<span id="cb88-1129"><a href="#cb88-1129" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb88-1130"><a href="#cb88-1130" aria-hidden="true" tabindex="-1"></a><span class="in">})</span></span>
<span id="cb88-1131"><a href="#cb88-1131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1132"><a href="#cb88-1132" aria-hidden="true" tabindex="-1"></a><span class="in"># Ergebnisse tabellarisch darstellen</span></span>
<span id="cb88-1133"><a href="#cb88-1133" aria-hidden="true" tabindex="-1"></a><span class="in">tibble(</span></span>
<span id="cb88-1134"><a href="#cb88-1134" aria-hidden="true" tabindex="-1"></a><span class="in">  Methode = c("Lasso", "Ridge"),</span></span>
<span id="cb88-1135"><a href="#cb88-1135" aria-hidden="true" tabindex="-1"></a><span class="in">) %&gt;%</span></span>
<span id="cb88-1136"><a href="#cb88-1136" aria-hidden="true" tabindex="-1"></a><span class="in">  bind_cols(</span></span>
<span id="cb88-1137"><a href="#cb88-1137" aria-hidden="true" tabindex="-1"></a><span class="in">    bind_rows(</span></span>
<span id="cb88-1138"><a href="#cb88-1138" aria-hidden="true" tabindex="-1"></a><span class="in">      colMeans(lasso_mse),</span></span>
<span id="cb88-1139"><a href="#cb88-1139" aria-hidden="true" tabindex="-1"></a><span class="in">      colMeans(ridge_mse)  </span></span>
<span id="cb88-1140"><a href="#cb88-1140" aria-hidden="true" tabindex="-1"></a><span class="in">    )    </span></span>
<span id="cb88-1141"><a href="#cb88-1141" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb88-1142"><a href="#cb88-1142" aria-hidden="true" tabindex="-1"></a><span class="in">  gt() %&gt;%</span></span>
<span id="cb88-1143"><a href="#cb88-1143" aria-hidden="true" tabindex="-1"></a><span class="in">  tabopts</span></span>
<span id="cb88-1144"><a href="#cb88-1144" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1145"><a href="#cb88-1145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1146"><a href="#cb88-1146" aria-hidden="true" tabindex="-1"></a>Die Ergebnisse in @tbl-lrsimB zeigen, dass Ridge-Regression in Szenario B bis auf den Fall unkorrelierter Regressoren etwas schlechter abschneidet als in Szenario A. Die hohe Anzahl irrelevanter Variablen verbessert die Leistung von Lasso deutlich: Hier ist es plausibel, dass Lasso aufgrund der Thresholding-Eigenschaft die Koeffizienten einiger irrelevanten Variablen häufig exakt $0$ setzt und damit ein sparsameres Modell schätzt als Ridge. Entsprechend erzielt Lasso in diesem Szenario insbesondere für $\rho = 0$ genauere Vorhersagen als Ridge Regression.</span>
<span id="cb88-1147"><a href="#cb88-1147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1148"><a href="#cb88-1148" aria-hidden="true" tabindex="-1"></a><span class="fu">### Visualisierung des Bias-Variance-Tradeoffs bei Prognosen</span></span>
<span id="cb88-1149"><a href="#cb88-1149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1150"><a href="#cb88-1150" aria-hidden="true" tabindex="-1"></a>Für ein besseres Verständnis, wie sich der Regularisierungsparameter $\lambda$ auf den Bias-Variance-Tradeoff bei Prognosen mit Ridge- und Lasso-Regression auswirkt, vergleichen wir für beide Methoden nachfolgend die Abhängigkeit des MSEs der Prognose $\widehat{Y}_0$ für den Wert $Y_0$ der abhängigen Variable eines Datenpunkts anhand seiner Regressoren $\boldsymbol{X}_0'$, wobei \begin{align}</span>
<span id="cb88-1151"><a href="#cb88-1151" aria-hidden="true" tabindex="-1"></a>  \text{MSE}(\widehat{Y}_0) = \text{Bias}(\widehat{Y}_0)^2 + \text{Var}(\widehat{Y}_0) + \text{Var}(Y_0) \label{eq:pbvdecomp}</span>
<span id="cb88-1152"><a href="#cb88-1152" aria-hidden="true" tabindex="-1"></a>\end{align} Beachte, dass $\text{Var}(Y_0)$ die durch den datenerzeugenden Prozess (und damit unvermeidbare) Varianz von $Y_0$ ist, wohingegen $\text{Bias}(\widehat{Y}_0)^2$ und $\text{Var}(\widehat{Y}_0)$ von dem verwendeten Schätzer für $\widehat{Y}_0$ abhängt.</span>
<span id="cb88-1153"><a href="#cb88-1153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1154"><a href="#cb88-1154" aria-hidden="true" tabindex="-1"></a>Für die Simulation betrachten wir erneut Szenario A aus @sec-pdz mit $50$ Beobachtungen für ein Modell mit $40$ unkorrelierten Regressoren. Wir legen zunächst die Simulationsparameter fest und erzeugen den vorherzusagenden Datenpunkt (<span class="in">`X_0`</span>, <span class="in">`Y_0`</span>).</span>
<span id="cb88-1155"><a href="#cb88-1155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1156"><a href="#cb88-1156" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, cache=TRUE}</span></span>
<span id="cb88-1157"><a href="#cb88-1157" aria-hidden="true" tabindex="-1"></a><span class="in"># Parameter festlegen</span></span>
<span id="cb88-1158"><a href="#cb88-1158" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(1234)</span></span>
<span id="cb88-1159"><a href="#cb88-1159" aria-hidden="true" tabindex="-1"></a><span class="in">n &lt;- 200 # Anz. Iterationen</span></span>
<span id="cb88-1160"><a href="#cb88-1160" aria-hidden="true" tabindex="-1"></a><span class="in">N &lt;- 50  # Anz. Beobachtungen</span></span>
<span id="cb88-1161"><a href="#cb88-1161" aria-hidden="true" tabindex="-1"></a><span class="in">k &lt;- 40  # Anz. Variablen</span></span>
<span id="cb88-1162"><a href="#cb88-1162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1163"><a href="#cb88-1163" aria-hidden="true" tabindex="-1"></a><span class="in"># Korrelationsmatrix definieren</span></span>
<span id="cb88-1164"><a href="#cb88-1164" aria-hidden="true" tabindex="-1"></a><span class="in">Sigma &lt;- diag(k) # Diagonalmatrix</span></span>
<span id="cb88-1165"><a href="#cb88-1165" aria-hidden="true" tabindex="-1"></a><span class="in">beta &lt;- rep(x = 1, k)</span></span>
<span id="cb88-1166"><a href="#cb88-1166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1167"><a href="#cb88-1167" aria-hidden="true" tabindex="-1"></a><span class="in"># Prognose-Ziel vorab zufällig generieren:</span></span>
<span id="cb88-1168"><a href="#cb88-1168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1169"><a href="#cb88-1169" aria-hidden="true" tabindex="-1"></a><span class="in"># Regressoren</span></span>
<span id="cb88-1170"><a href="#cb88-1170" aria-hidden="true" tabindex="-1"></a><span class="in">X_0 &lt;- rmvnorm(</span></span>
<span id="cb88-1171"><a href="#cb88-1171" aria-hidden="true" tabindex="-1"></a><span class="in">  n = 1, </span></span>
<span id="cb88-1172"><a href="#cb88-1172" aria-hidden="true" tabindex="-1"></a><span class="in">  mean = rep(x = 0, k)</span></span>
<span id="cb88-1173"><a href="#cb88-1173" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb88-1174"><a href="#cb88-1174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1175"><a href="#cb88-1175" aria-hidden="true" tabindex="-1"></a><span class="in"># Abh. Variable</span></span>
<span id="cb88-1176"><a href="#cb88-1176" aria-hidden="true" tabindex="-1"></a><span class="in">Y_0 &lt;- X_0 %*% beta + rnorm(n = 1) %&gt;% </span></span>
<span id="cb88-1177"><a href="#cb88-1177" aria-hidden="true" tabindex="-1"></a><span class="in">  as.vector()</span></span>
<span id="cb88-1178"><a href="#cb88-1178" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1179"><a href="#cb88-1179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1180"><a href="#cb88-1180" aria-hidden="true" tabindex="-1"></a>Anhand der Simulationsergebnisse wollen wir die von der verwendeten Schätzfunktion abhängigen Komponenten von \eqref{eq:pbvdecomp} untersuchen. Wir initialisieren hierzu die Listen <span class="in">`ridge_fits`</span> und <span class="in">`lasso_fits`</span>, in die unsere Simulationsergebnisse geschrieben werden.</span>
<span id="cb88-1181"><a href="#cb88-1181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1184"><a href="#cb88-1184" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-1185"><a href="#cb88-1185" aria-hidden="true" tabindex="-1"></a><span class="co"># Listen für Simulationsergebnisse initialisieren</span></span>
<span id="cb88-1186"><a href="#cb88-1186" aria-hidden="true" tabindex="-1"></a>ridge_fits <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb88-1187"><a href="#cb88-1187" aria-hidden="true" tabindex="-1"></a>lasso_fits <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb88-1188"><a href="#cb88-1188" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1189"><a href="#cb88-1189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1190"><a href="#cb88-1190" aria-hidden="true" tabindex="-1"></a>Weiterhin definieren wir separate $\lambda$-Sequenzen für Lasso- und Ridge-Schätzer.<span class="ot">[^regreg-18]</span></span>
<span id="cb88-1191"><a href="#cb88-1191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1192"><a href="#cb88-1192" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-18]: </span>Die Sequenzen haben wir in Abhängigkeit des DGP so gewählt, dass die Abhängigkeit der Prognosegüte von $\lambda$ gut visualisiert werden kann.</span>
<span id="cb88-1193"><a href="#cb88-1193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1196"><a href="#cb88-1196" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-1197"><a href="#cb88-1197" aria-hidden="true" tabindex="-1"></a><span class="co"># Lambda-Sequenzen festlegen</span></span>
<span id="cb88-1198"><a href="#cb88-1198" aria-hidden="true" tabindex="-1"></a>lambdas_r <span class="ot">&lt;-</span> <span class="fu">seq</span>(.<span class="dv">25</span>, <span class="fl">2.5</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb88-1199"><a href="#cb88-1199" aria-hidden="true" tabindex="-1"></a>lambdas_l <span class="ot">&lt;-</span> <span class="fu">seq</span>(.<span class="dv">05</span>, <span class="fl">0.5</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb88-1200"><a href="#cb88-1200" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1201"><a href="#cb88-1201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1202"><a href="#cb88-1202" aria-hidden="true" tabindex="-1"></a>Für die Simulation iterieren wir mit <span class="in">`walk()`</span> über simulierte Datensätze und schreiben jeweils den vollständigen Output von <span class="in">`glmnet()`</span> in die zuvor definierten Listen <span class="in">`ridge_fits`</span> und <span class="in">`lasso_fits`</span>.</span>
<span id="cb88-1203"><a href="#cb88-1203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1206"><a href="#cb88-1206" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-1207"><a href="#cb88-1207" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulation</span></span>
<span id="cb88-1208"><a href="#cb88-1208" aria-hidden="true" tabindex="-1"></a><span class="fu">walk</span>(<span class="dv">1</span><span class="sc">:</span>n, \(i) {</span>
<span id="cb88-1209"><a href="#cb88-1209" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb88-1210"><a href="#cb88-1210" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Daten simulieren</span></span>
<span id="cb88-1211"><a href="#cb88-1211" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(</span>
<span id="cb88-1212"><a href="#cb88-1212" aria-hidden="true" tabindex="-1"></a>    <span class="at">n =</span> N, </span>
<span id="cb88-1213"><a href="#cb88-1213" aria-hidden="true" tabindex="-1"></a>    <span class="at">mean =</span> <span class="fu">rep</span>(<span class="dv">0</span>, k), </span>
<span id="cb88-1214"><a href="#cb88-1214" aria-hidden="true" tabindex="-1"></a>    <span class="at">sigma =</span> Sigma</span>
<span id="cb88-1215"><a href="#cb88-1215" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb88-1216"><a href="#cb88-1216" aria-hidden="true" tabindex="-1"></a>  Y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="at">n =</span> N, <span class="at">sd =</span> <span class="dv">5</span>)</span>
<span id="cb88-1217"><a href="#cb88-1217" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb88-1218"><a href="#cb88-1218" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Modelle mit glmnet schätzen</span></span>
<span id="cb88-1219"><a href="#cb88-1219" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Ridge-Regression</span></span>
<span id="cb88-1220"><a href="#cb88-1220" aria-hidden="true" tabindex="-1"></a>  ridge_fits[[i]] <span class="ot">&lt;&lt;-</span> <span class="fu">glmnet</span>(</span>
<span id="cb88-1221"><a href="#cb88-1221" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> X, </span>
<span id="cb88-1222"><a href="#cb88-1222" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> Y, </span>
<span id="cb88-1223"><a href="#cb88-1223" aria-hidden="true" tabindex="-1"></a>    <span class="at">alpha =</span> <span class="dv">0</span>, </span>
<span id="cb88-1224"><a href="#cb88-1224" aria-hidden="true" tabindex="-1"></a>    <span class="at">intercept =</span> F</span>
<span id="cb88-1225"><a href="#cb88-1225" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb88-1226"><a href="#cb88-1226" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Lasso-Regression</span></span>
<span id="cb88-1227"><a href="#cb88-1227" aria-hidden="true" tabindex="-1"></a>  lasso_fits[[i]] <span class="ot">&lt;&lt;-</span> <span class="fu">glmnet</span>(</span>
<span id="cb88-1228"><a href="#cb88-1228" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> X, </span>
<span id="cb88-1229"><a href="#cb88-1229" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> Y, </span>
<span id="cb88-1230"><a href="#cb88-1230" aria-hidden="true" tabindex="-1"></a>    <span class="at">alpha =</span> <span class="dv">1</span>, </span>
<span id="cb88-1231"><a href="#cb88-1231" aria-hidden="true" tabindex="-1"></a>    <span class="at">intercept =</span> F</span>
<span id="cb88-1232"><a href="#cb88-1232" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb88-1233"><a href="#cb88-1233" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb88-1234"><a href="#cb88-1234" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb88-1235"><a href="#cb88-1235" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1236"><a href="#cb88-1236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1237"><a href="#cb88-1237" aria-hidden="true" tabindex="-1"></a>Wir nutzen Funktionen aus <span class="in">`purrr`</span> und <span class="in">`dplyr`</span>, um über die in den Simulationsdurchläufen angepassten Modelle zu iterieren. Mit <span class="in">`predict()`</span> erhalten wir Punktvorhersagen für <span class="in">`Y_0`</span> für jedes $\lambda$ der zuvor definierten $\lambda$-Sequenzen. Beachte, dass <span class="in">`map()`</span> jeweils eine Liste mit <span class="in">`r n`</span> Punktvorhersagen für jedes der <span class="in">`r length(lambdas_r)`</span> zurückgibt. Mit <span class="in">`list_rbind()`</span> können wir die Ergebnisse komfortabel jeweils in einer <span class="in">`tibble`</span> sammeln.</span>
<span id="cb88-1238"><a href="#cb88-1238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1241"><a href="#cb88-1241" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-1242"><a href="#cb88-1242" aria-hidden="true" tabindex="-1"></a><span class="co"># Prognosen für Ridge-Regression</span></span>
<span id="cb88-1243"><a href="#cb88-1243" aria-hidden="true" tabindex="-1"></a>pred_r <span class="ot">&lt;-</span> <span class="fu">map</span>(</span>
<span id="cb88-1244"><a href="#cb88-1244" aria-hidden="true" tabindex="-1"></a>  <span class="at">.x =</span> ridge_fits, </span>
<span id="cb88-1245"><a href="#cb88-1245" aria-hidden="true" tabindex="-1"></a>  <span class="at">.f =</span> <span class="sc">~</span> <span class="fu">as_tibble</span>(</span>
<span id="cb88-1246"><a href="#cb88-1246" aria-hidden="true" tabindex="-1"></a>    <span class="fu">predict</span>(</span>
<span id="cb88-1247"><a href="#cb88-1247" aria-hidden="true" tabindex="-1"></a>      <span class="at">object =</span> ., </span>
<span id="cb88-1248"><a href="#cb88-1248" aria-hidden="true" tabindex="-1"></a>      <span class="at">s =</span> lambdas_r, </span>
<span id="cb88-1249"><a href="#cb88-1249" aria-hidden="true" tabindex="-1"></a>      <span class="at">newx =</span> X_0</span>
<span id="cb88-1250"><a href="#cb88-1250" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb88-1251"><a href="#cb88-1251" aria-hidden="true" tabindex="-1"></a>  ) </span>
<span id="cb88-1252"><a href="#cb88-1252" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb88-1253"><a href="#cb88-1253" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list_rbind</span>() </span>
<span id="cb88-1254"><a href="#cb88-1254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1255"><a href="#cb88-1255" aria-hidden="true" tabindex="-1"></a><span class="co"># Prognosen für Lasso-Regression</span></span>
<span id="cb88-1256"><a href="#cb88-1256" aria-hidden="true" tabindex="-1"></a>pred_l <span class="ot">&lt;-</span> <span class="fu">map</span>(</span>
<span id="cb88-1257"><a href="#cb88-1257" aria-hidden="true" tabindex="-1"></a>  <span class="at">.x =</span> lasso_fits, </span>
<span id="cb88-1258"><a href="#cb88-1258" aria-hidden="true" tabindex="-1"></a>  <span class="at">.f =</span> <span class="sc">~</span> <span class="fu">as_tibble</span>(</span>
<span id="cb88-1259"><a href="#cb88-1259" aria-hidden="true" tabindex="-1"></a>    <span class="fu">predict</span>(</span>
<span id="cb88-1260"><a href="#cb88-1260" aria-hidden="true" tabindex="-1"></a>      <span class="at">object =</span> ., </span>
<span id="cb88-1261"><a href="#cb88-1261" aria-hidden="true" tabindex="-1"></a>      <span class="at">s =</span> lambdas_l, </span>
<span id="cb88-1262"><a href="#cb88-1262" aria-hidden="true" tabindex="-1"></a>      <span class="at">newx =</span> X_0)</span>
<span id="cb88-1263"><a href="#cb88-1263" aria-hidden="true" tabindex="-1"></a>    ) </span>
<span id="cb88-1264"><a href="#cb88-1264" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb88-1265"><a href="#cb88-1265" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list_rbind</span>() </span>
<span id="cb88-1266"><a href="#cb88-1266" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1267"><a href="#cb88-1267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1268"><a href="#cb88-1268" aria-hidden="true" tabindex="-1"></a>Für die statistische Auswertung berechnen wir jeweils $\text{MSE}(\widehat{Y}_0)$, $\text{Bias}(\widehat{Y}_0)^2$ und $\text{Var}(\widehat{Y}_0)$ und führen die Ergebnisse mit <span class="in">`pivot_longer()`</span> in ein langes Format <span class="in">`sim_data_r`</span> über. Wir berechnen weiterhin mit <span class="in">`MSE_min_r`</span> das $\lambda$, für das wir über die Simulationsdurchläufe durchschnittlich den geringsten $\text{MSE}$ beobachten.</span>
<span id="cb88-1269"><a href="#cb88-1269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1270"><a href="#cb88-1270" aria-hidden="true" tabindex="-1"></a>**Ridge-Regression**</span>
<span id="cb88-1271"><a href="#cb88-1271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1272"><a href="#cb88-1272" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, warning=F}</span></span>
<span id="cb88-1273"><a href="#cb88-1273" aria-hidden="true" tabindex="-1"></a><span class="in"># Ergebnisse für Ridge-Regression zusammenfassen</span></span>
<span id="cb88-1274"><a href="#cb88-1274" aria-hidden="true" tabindex="-1"></a><span class="in">sim_data_r &lt;- tibble(</span></span>
<span id="cb88-1275"><a href="#cb88-1275" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb88-1276"><a href="#cb88-1276" aria-hidden="true" tabindex="-1"></a><span class="in">  lambda = lambdas_r,</span></span>
<span id="cb88-1277"><a href="#cb88-1277" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb88-1278"><a href="#cb88-1278" aria-hidden="true" tabindex="-1"></a><span class="in">  "MSE" = map_dbl(</span></span>
<span id="cb88-1279"><a href="#cb88-1279" aria-hidden="true" tabindex="-1"></a><span class="in">    .x = pred_r,  </span></span>
<span id="cb88-1280"><a href="#cb88-1280" aria-hidden="true" tabindex="-1"></a><span class="in">    .f = ~ mean((.x - Y_0)^2)</span></span>
<span id="cb88-1281"><a href="#cb88-1281" aria-hidden="true" tabindex="-1"></a><span class="in">  ),</span></span>
<span id="cb88-1282"><a href="#cb88-1282" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb88-1283"><a href="#cb88-1283" aria-hidden="true" tabindex="-1"></a><span class="in">  "Bias^2" = map_dbl(</span></span>
<span id="cb88-1284"><a href="#cb88-1284" aria-hidden="true" tabindex="-1"></a><span class="in">    .x = pred_r, </span></span>
<span id="cb88-1285"><a href="#cb88-1285" aria-hidden="true" tabindex="-1"></a><span class="in">    .f = ~ (mean(.x) - Y_0)^2</span></span>
<span id="cb88-1286"><a href="#cb88-1286" aria-hidden="true" tabindex="-1"></a><span class="in">  ),</span></span>
<span id="cb88-1287"><a href="#cb88-1287" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb88-1288"><a href="#cb88-1288" aria-hidden="true" tabindex="-1"></a><span class="in">  "Varianz" = map_dbl(</span></span>
<span id="cb88-1289"><a href="#cb88-1289" aria-hidden="true" tabindex="-1"></a><span class="in">    .x = pred_r, </span></span>
<span id="cb88-1290"><a href="#cb88-1290" aria-hidden="true" tabindex="-1"></a><span class="in">    .f = ~ var(.x)</span></span>
<span id="cb88-1291"><a href="#cb88-1291" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb88-1292"><a href="#cb88-1292" aria-hidden="true" tabindex="-1"></a><span class="in">) %&gt;%</span></span>
<span id="cb88-1293"><a href="#cb88-1293" aria-hidden="true" tabindex="-1"></a><span class="in">  pivot_longer(</span></span>
<span id="cb88-1294"><a href="#cb88-1294" aria-hidden="true" tabindex="-1"></a><span class="in">    cols = -lambda, </span></span>
<span id="cb88-1295"><a href="#cb88-1295" aria-hidden="true" tabindex="-1"></a><span class="in">    values_to = "Wert",</span></span>
<span id="cb88-1296"><a href="#cb88-1296" aria-hidden="true" tabindex="-1"></a><span class="in">    names_to = "Statistik"</span></span>
<span id="cb88-1297"><a href="#cb88-1297" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb88-1298"><a href="#cb88-1298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1299"><a href="#cb88-1299" aria-hidden="true" tabindex="-1"></a><span class="in"># Lambda bei MSE-Minimum bestimmen</span></span>
<span id="cb88-1300"><a href="#cb88-1300" aria-hidden="true" tabindex="-1"></a><span class="in">MSE_min_r &lt;- sim_data_r %&gt;% </span></span>
<span id="cb88-1301"><a href="#cb88-1301" aria-hidden="true" tabindex="-1"></a><span class="in">  filter(</span></span>
<span id="cb88-1302"><a href="#cb88-1302" aria-hidden="true" tabindex="-1"></a><span class="in">    Statistik == "MSE",</span></span>
<span id="cb88-1303"><a href="#cb88-1303" aria-hidden="true" tabindex="-1"></a><span class="in">    Wert == min(Wert)</span></span>
<span id="cb88-1304"><a href="#cb88-1304" aria-hidden="true" tabindex="-1"></a><span class="in">  ) </span></span>
<span id="cb88-1305"><a href="#cb88-1305" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1306"><a href="#cb88-1306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1307"><a href="#cb88-1307" aria-hidden="true" tabindex="-1"></a>**Lasso-Regression**</span>
<span id="cb88-1308"><a href="#cb88-1308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1309"><a href="#cb88-1309" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, warning=F}</span></span>
<span id="cb88-1310"><a href="#cb88-1310" aria-hidden="true" tabindex="-1"></a><span class="in"># Ergebnisse zusammenfassen</span></span>
<span id="cb88-1311"><a href="#cb88-1311" aria-hidden="true" tabindex="-1"></a><span class="in">sim_data_l &lt;- tibble(</span></span>
<span id="cb88-1312"><a href="#cb88-1312" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb88-1313"><a href="#cb88-1313" aria-hidden="true" tabindex="-1"></a><span class="in">  lambda = lambdas_l,</span></span>
<span id="cb88-1314"><a href="#cb88-1314" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb88-1315"><a href="#cb88-1315" aria-hidden="true" tabindex="-1"></a><span class="in">  "MSE" = map_dbl(</span></span>
<span id="cb88-1316"><a href="#cb88-1316" aria-hidden="true" tabindex="-1"></a><span class="in">    .x = pred_l,  </span></span>
<span id="cb88-1317"><a href="#cb88-1317" aria-hidden="true" tabindex="-1"></a><span class="in">    .f = ~ mean((. - Y_0)^2)</span></span>
<span id="cb88-1318"><a href="#cb88-1318" aria-hidden="true" tabindex="-1"></a><span class="in">  ),</span></span>
<span id="cb88-1319"><a href="#cb88-1319" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb88-1320"><a href="#cb88-1320" aria-hidden="true" tabindex="-1"></a><span class="in">  "Bias^2" = map_dbl(</span></span>
<span id="cb88-1321"><a href="#cb88-1321" aria-hidden="true" tabindex="-1"></a><span class="in">    .x = pred_l, </span></span>
<span id="cb88-1322"><a href="#cb88-1322" aria-hidden="true" tabindex="-1"></a><span class="in">    .f = ~ (mean(.) - Y_0)^2</span></span>
<span id="cb88-1323"><a href="#cb88-1323" aria-hidden="true" tabindex="-1"></a><span class="in">  ),</span></span>
<span id="cb88-1324"><a href="#cb88-1324" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb88-1325"><a href="#cb88-1325" aria-hidden="true" tabindex="-1"></a><span class="in">  "Varianz" = map_dbl(</span></span>
<span id="cb88-1326"><a href="#cb88-1326" aria-hidden="true" tabindex="-1"></a><span class="in">    .x = pred_l, </span></span>
<span id="cb88-1327"><a href="#cb88-1327" aria-hidden="true" tabindex="-1"></a><span class="in">    .f = ~ var(.)</span></span>
<span id="cb88-1328"><a href="#cb88-1328" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb88-1329"><a href="#cb88-1329" aria-hidden="true" tabindex="-1"></a><span class="in">) %&gt;%</span></span>
<span id="cb88-1330"><a href="#cb88-1330" aria-hidden="true" tabindex="-1"></a><span class="in">  pivot_longer(</span></span>
<span id="cb88-1331"><a href="#cb88-1331" aria-hidden="true" tabindex="-1"></a><span class="in">    cols = -lambda, </span></span>
<span id="cb88-1332"><a href="#cb88-1332" aria-hidden="true" tabindex="-1"></a><span class="in">    values_to = "Wert", </span></span>
<span id="cb88-1333"><a href="#cb88-1333" aria-hidden="true" tabindex="-1"></a><span class="in">    names_to = "Statistik"</span></span>
<span id="cb88-1334"><a href="#cb88-1334" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb88-1335"><a href="#cb88-1335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1336"><a href="#cb88-1336" aria-hidden="true" tabindex="-1"></a><span class="in"># Lambda bei MSE-Minimum bestimmen</span></span>
<span id="cb88-1337"><a href="#cb88-1337" aria-hidden="true" tabindex="-1"></a><span class="in">MSE_min_l &lt;- sim_data_l %&gt;% </span></span>
<span id="cb88-1338"><a href="#cb88-1338" aria-hidden="true" tabindex="-1"></a><span class="in">  filter(</span></span>
<span id="cb88-1339"><a href="#cb88-1339" aria-hidden="true" tabindex="-1"></a><span class="in">    Statistik == "MSE",</span></span>
<span id="cb88-1340"><a href="#cb88-1340" aria-hidden="true" tabindex="-1"></a><span class="in">    Wert == min(Wert)</span></span>
<span id="cb88-1341"><a href="#cb88-1341" aria-hidden="true" tabindex="-1"></a><span class="in">  ) </span></span>
<span id="cb88-1342"><a href="#cb88-1342" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1343"><a href="#cb88-1343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1344"><a href="#cb88-1344" aria-hidden="true" tabindex="-1"></a>Die Datensätze im langen Format, <span class="in">`sim_data_r`</span> und <span class="in">`sim_data_l`</span>, werden nun für die Visualisierung der Ergebnisse mit <span class="in">`ggplo2`</span> genutzt.</span>
<span id="cb88-1345"><a href="#cb88-1345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1346"><a href="#cb88-1346" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, warning=FALSE}</span></span>
<span id="cb88-1347"><a href="#cb88-1347" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-MSEBVT</span></span>
<span id="cb88-1348"><a href="#cb88-1348" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Simulierte MSE-Komponenten in Abhängigkeit von Lambda"</span></span>
<span id="cb88-1349"><a href="#cb88-1349" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-subcap: </span></span>
<span id="cb88-1350"><a href="#cb88-1350" aria-hidden="true" tabindex="-1"></a><span class="in">#|   - "Ridge Regression"</span></span>
<span id="cb88-1351"><a href="#cb88-1351" aria-hidden="true" tabindex="-1"></a><span class="in">#|   - "Lasso Regression"</span></span>
<span id="cb88-1352"><a href="#cb88-1352" aria-hidden="true" tabindex="-1"></a><span class="in">#| layout-ncol: 1</span></span>
<span id="cb88-1353"><a href="#cb88-1353" aria-hidden="true" tabindex="-1"></a><span class="in"># MSE, Bias^2 und Varianz gegen Lambda plotten</span></span>
<span id="cb88-1354"><a href="#cb88-1354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1355"><a href="#cb88-1355" aria-hidden="true" tabindex="-1"></a><span class="in"># Ridge-Regression</span></span>
<span id="cb88-1356"><a href="#cb88-1356" aria-hidden="true" tabindex="-1"></a><span class="in">sim_data_r %&gt;%</span></span>
<span id="cb88-1357"><a href="#cb88-1357" aria-hidden="true" tabindex="-1"></a><span class="in">  ggplot(</span></span>
<span id="cb88-1358"><a href="#cb88-1358" aria-hidden="true" tabindex="-1"></a><span class="in">    mapping = aes(</span></span>
<span id="cb88-1359"><a href="#cb88-1359" aria-hidden="true" tabindex="-1"></a><span class="in">      x = lambda, </span></span>
<span id="cb88-1360"><a href="#cb88-1360" aria-hidden="true" tabindex="-1"></a><span class="in">      y = Wert, </span></span>
<span id="cb88-1361"><a href="#cb88-1361" aria-hidden="true" tabindex="-1"></a><span class="in">      color = Statistik</span></span>
<span id="cb88-1362"><a href="#cb88-1362" aria-hidden="true" tabindex="-1"></a><span class="in">    )</span></span>
<span id="cb88-1363"><a href="#cb88-1363" aria-hidden="true" tabindex="-1"></a><span class="in">  ) +</span></span>
<span id="cb88-1364"><a href="#cb88-1364" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_line() +</span></span>
<span id="cb88-1365"><a href="#cb88-1365" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_point(data = MSE_min_r)</span></span>
<span id="cb88-1366"><a href="#cb88-1366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1367"><a href="#cb88-1367" aria-hidden="true" tabindex="-1"></a><span class="in"># Lasso-Regression</span></span>
<span id="cb88-1368"><a href="#cb88-1368" aria-hidden="true" tabindex="-1"></a><span class="in">sim_data_l %&gt;%</span></span>
<span id="cb88-1369"><a href="#cb88-1369" aria-hidden="true" tabindex="-1"></a><span class="in">  ggplot(</span></span>
<span id="cb88-1370"><a href="#cb88-1370" aria-hidden="true" tabindex="-1"></a><span class="in">    mapping = aes(</span></span>
<span id="cb88-1371"><a href="#cb88-1371" aria-hidden="true" tabindex="-1"></a><span class="in">      x = lambda, </span></span>
<span id="cb88-1372"><a href="#cb88-1372" aria-hidden="true" tabindex="-1"></a><span class="in">      y = Wert, </span></span>
<span id="cb88-1373"><a href="#cb88-1373" aria-hidden="true" tabindex="-1"></a><span class="in">      color = Statistik</span></span>
<span id="cb88-1374"><a href="#cb88-1374" aria-hidden="true" tabindex="-1"></a><span class="in">    )</span></span>
<span id="cb88-1375"><a href="#cb88-1375" aria-hidden="true" tabindex="-1"></a><span class="in">  ) +</span></span>
<span id="cb88-1376"><a href="#cb88-1376" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_line() +</span></span>
<span id="cb88-1377"><a href="#cb88-1377" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_point(data = MSE_min_l)</span></span>
<span id="cb88-1378"><a href="#cb88-1378" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1379"><a href="#cb88-1379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1380"><a href="#cb88-1380" aria-hidden="true" tabindex="-1"></a>Anhand von @fig-MSEBVT lässt sich der Bias-Variance-Tradeoff bei der Vorhersage von $Y_0$ gut erkennen: Bereits für kleine $\lambda$ erzielen beide Methode eine deutliche Reduktion des MSE. Dies wir durch etwas zusätzlichen Bias, aber eine überproportionale Verringerung der Varianz erreicht. Der erkennbare funktionale Zusammenhang zeigt, dass der MSE eine konvexe Funktion von $\lambda$ ist. Damit existieren optimale $\lambda$ mit minimalem MSE (grüne Punkte), die wir mit Cross Validation schätzen können.</span>
<span id="cb88-1381"><a href="#cb88-1381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1382"><a href="#cb88-1382" aria-hidden="true" tabindex="-1"></a><span class="fu">## Inferenz für Treatment-Effekt-Schätzung mit vielen Variablen</span></span>
<span id="cb88-1383"><a href="#cb88-1383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1384"><a href="#cb88-1384" aria-hidden="true" tabindex="-1"></a>In empirischen Studien des Effekts einer Behandlungsvariable $B$ auf eine Outcome-Variable $Y$ steht häufig eine Vielzahl potentieller Kontrollvariablen zur Verfügung. Häufig ist unklar, welche Variablen in das Modell aufgenommen werden sollten, um das Risiko einer verzerrten Schätzung durch ausgelassene Variablen zu vermindern und gleichzeitig eine Schätzung mit geringer Varianz zu gewährleisten. Ist der Beobachtungsumfang $N$ relativ zur Variablenanzahl $k$ groß, so kann die KQ-Schätzung einer langen Regression (ein Modell mit allen $k$ Kontrollvariablen) gute Ergebnisse liefern. In der Praxis liegt diese wünschenswerte Situation jedoch oft nicht vor und es ist $k\lesssim N$ oder sogar $k&gt;N$. Dann ist eine KQ-Schätzung des Behandlungseffekts anhand aller $k$ Variablen mit hoher Varianz behaftet bzw. gar nicht möglich.<span class="ot">[^regreg-19]</span> Ein weiteres Szenario ist $k(N)&gt;N$, d.h. die Anzahl der Regressoren kann mit dem Beobachtungsumfang wachsen.<span class="ot">[^regreg-20]</span> Lasso-Verfahren können dann hilfreich sein, um Determinanten von $Y$ *und* $B$ zu identifizieren und damit eine Menge an Kontrollvariablen zu selektieren, für die eine erwartungstreue und konsistente Schätzung des interessierenden Effekts wahrscheinlich ist.</span>
<span id="cb88-1385"><a href="#cb88-1385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1386"><a href="#cb88-1386" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-19]: </span>Beachte, dass der KQ-Schätzer bei $k&gt;N$ nicht lösbar ist.</span>
<span id="cb88-1387"><a href="#cb88-1387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1388"><a href="#cb88-1388" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-20]: </span>Dieses Szenario wird unter Bedingungen bzgl. der Wachstumsrate und der Größe der Koeffizienten betrachet, s. <span class="co">[</span><span class="ot">@BelloniChernozhukov2013</span><span class="co">]</span>.</span>
<span id="cb88-1389"><a href="#cb88-1389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1390"><a href="#cb88-1390" aria-hidden="true" tabindex="-1"></a>Betrachte zunächst das Modell mit allen Kontrollvariablen $X_j$, \begin{align}</span>
<span id="cb88-1391"><a href="#cb88-1391" aria-hidden="true" tabindex="-1"></a>  Y_i = \beta_0 + \alpha_0 B_i + \sum_{j=1}^k \beta_{j} X_{i,j} + u_i, \label{eq:lassotmt}</span>
<span id="cb88-1392"><a href="#cb88-1392" aria-hidden="true" tabindex="-1"></a>\end{align} wobei einige $\beta_{j}=0$ sind und wir annehmen, dass $B$ lediglich mit ein paar der $X_j$ korrelliert. Die Shrinkage der geschätzten Koeffizienten aus einer naiven Lasso-Regression von \eqref{eq:lassotmt} führt grundsätzlich zu einer verzerrten Schätzung des Behandlungseffekts $\alpha_0$ und damit zu ungültiger Inferenz.<span class="ot">[^regreg-21]</span></span>
<span id="cb88-1393"><a href="#cb88-1393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1394"><a href="#cb88-1394" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-21]: </span>@Hahnetal2018 geben eine ausführliche Erläuterung dieser Problematik.</span>
<span id="cb88-1395"><a href="#cb88-1395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1396"><a href="#cb88-1396" aria-hidden="true" tabindex="-1"></a>Die Verzerrung von geschätzten Koeffizienten kann vermieden werden, indem Lasso lediglich zur Selektion von Kontrollvariablen verwendet wird. Dabei wird mit einer Lasso-Regression von $Y$ auf die $X_j$ eine Teilmenge von Regressoren $\mathcal{S}$ selektiert und der Treatment-Effekt anschließend mit der KQ-Schätzung von \begin{align}</span>
<span id="cb88-1397"><a href="#cb88-1397" aria-hidden="true" tabindex="-1"></a>  Y_i = \beta_0 + \alpha_0 B_i + \sum_{j\in\mathcal{S}} \beta_{j} X_{i,j} + e_i,</span>
<span id="cb88-1398"><a href="#cb88-1398" aria-hidden="true" tabindex="-1"></a>\end{align} basierend auf der Selektion $\mathcal{S}$ berechnet wird.<span class="ot">[^regreg-22]</span> Ein solcher *Post-Lasso-Selection-Schätzer* <span class="co">[</span><span class="ot">@BelloniChernozhukov2013</span><span class="co">]</span> ist jedoch im Allgemeinen und insbesondere in hoch-dimensionalen Settings nicht konsistent für $\alpha_0$ und nicht asymptotisch normalverteilt, da weiterhin die Gefahr einer verzerrten Schätzung durch in $\mathcal{S}$ ausgelassene Variablen besteht, die mit $B$ korrelieren: Lasso selektiert Variablen $X_j$, die "gut" $Y$ erklären. Dabei kann nicht ausgeschlossen werden, das ein Modell gewählt wird, dass relevante Determinanten von $B$ auslässt. Selbst wenn wir ein mit Lasso gewähltes Modell mit KQ (d.h. ohne Shrinkage) schätzen, würde $\alpha_0$ verzerrt geschätzt!</span>
<span id="cb88-1399"><a href="#cb88-1399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1400"><a href="#cb88-1400" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-22]: </span>Solche Verfahren werden *Post-Selection-Schätzer* gennant.</span>
<span id="cb88-1401"><a href="#cb88-1401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1402"><a href="#cb88-1402" aria-hidden="true" tabindex="-1"></a>@Bellonietal2014 schlagen ein alternatives Verfahren vor, dass auf Selektion der Determinanten $X_j$ von $Y$ und $B$ basiert. Dieses Verfahren wird als *Post-Double Selection* bezeichnet und kann wiefolgt implementiert werden:</span>
<span id="cb88-1403"><a href="#cb88-1403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1404"><a href="#cb88-1404" aria-hidden="true" tabindex="-1"></a>**Post-Double-Selection-Schätzer**</span>
<span id="cb88-1405"><a href="#cb88-1405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1406"><a href="#cb88-1406" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Bestimme die Determinanten $X_j$ von $Y$ mit Lasso-Regression und bezeichne die Menge der selektierten Variablen als $\mathcal{S}_Y$.</span>
<span id="cb88-1407"><a href="#cb88-1407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1408"><a href="#cb88-1408" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Bestimme die Determinanten $X_j$ von $B$ mit Lasso-Regression und bezeichne die Menge der selektierten Variablen als $\mathcal{S}_B$.</span>
<span id="cb88-1409"><a href="#cb88-1409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1410"><a href="#cb88-1410" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Bestimme die Schnittmenge $\mathcal{S}_{YB} = \mathcal{S}_Y \cap \mathcal{S}_B$. Schätze den Treatment-Effekt als $\widehat{\alpha}_0$ in der KQ-Regression \begin{align}</span>
<span id="cb88-1411"><a href="#cb88-1411" aria-hidden="true" tabindex="-1"></a>      Y_i = \beta_0 + \alpha_0 B_i + \sum_{j\in\mathcal{S}_{YB}} \beta_{j} X_{i,j} + v_i.</span>
<span id="cb88-1412"><a href="#cb88-1412" aria-hidden="true" tabindex="-1"></a>    \end{align}</span>
<span id="cb88-1413"><a href="#cb88-1413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1414"><a href="#cb88-1414" aria-hidden="true" tabindex="-1"></a>@Bellonietal2014 zeigen, dass $\widehat{\alpha}_0$ aus diesem Verfahren ein asymptotisch normalverteiler Schätzer für $\alpha_0$ ist und herkömmliche t-Tests und Konfidenzintervalle gültige Inferenz erlauben.</span>
<span id="cb88-1415"><a href="#cb88-1415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1416"><a href="#cb88-1416" aria-hidden="true" tabindex="-1"></a>Wir illustrieren die in diesem Abschnitt betrachteten Schätzer nun anhand simulierter Daten mit R. Die fiktive Problemstellung ist die Schätzung eines wahren Treatment-Effekts $\alpha_0 = 2$, wenn so viele potenzielle Kontrollvariablen vorliegen, dass der KQ-Schätzer gerade noch berechnet werden kann, aber aufgrund hoher Varianz unzuverlässig ist. Hierzu erzeugen wir $Y$ gemäß der Vorschrift \begin{align*}</span>
<span id="cb88-1417"><a href="#cb88-1417" aria-hidden="true" tabindex="-1"></a>  Y_i =&amp;\, \alpha_0 B_i + \sum_{j=1}^{k_Y} \beta_{j}^Y X_{i,j}^Y + \sum_{l=1}^{k_{YB}} \beta_{l}^{YB} X_{i,l}^{YB} + u_i,<span class="sc">\\</span></span>
<span id="cb88-1418"><a href="#cb88-1418" aria-hidden="true" tabindex="-1"></a>  <span class="sc">\\</span></span>
<span id="cb88-1419"><a href="#cb88-1419" aria-hidden="true" tabindex="-1"></a>  \beta_j^{YB} \overset{u.i.v}{\sim}&amp;\,N(10,1), \quad \beta_j^{Y} \overset{u.i.v}{\sim}U(0,1), \quad u_i \overset{u.i.v}{\sim}N(0,1).<span class="sc">\\</span></span>
<span id="cb88-1420"><a href="#cb88-1420" aria-hidden="true" tabindex="-1"></a>  <span class="sc">\\</span></span>
<span id="cb88-1421"><a href="#cb88-1421" aria-hidden="true" tabindex="-1"></a>  i=&amp;\,1,\dots,550</span>
<span id="cb88-1422"><a href="#cb88-1422" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb88-1423"><a href="#cb88-1423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1424"><a href="#cb88-1424" aria-hidden="true" tabindex="-1"></a>Die Behandlungsvariable $B_i$ entspricht der Vorschrift \begin{align*}</span>
<span id="cb88-1425"><a href="#cb88-1425" aria-hidden="true" tabindex="-1"></a>  B_i =&amp;\, \sum_{l=1}^{k_{YB}} \beta_{l}^{YB} X_{i,l}^{YB} + e_i,<span class="sc">\\</span></span>
<span id="cb88-1426"><a href="#cb88-1426" aria-hidden="true" tabindex="-1"></a>  <span class="sc">\\</span></span>
<span id="cb88-1427"><a href="#cb88-1427" aria-hidden="true" tabindex="-1"></a>  \beta_j^{YB} \overset{u.i.v}{\sim}&amp;\,N(2,0.2), \quad e_i \overset{u.i.v}{\sim}N(0,1).</span>
<span id="cb88-1428"><a href="#cb88-1428" aria-hidden="true" tabindex="-1"></a>\end{align*} Wir wählen $k_{YB} = k_{Y} = 25$. Zusätzlich zu $B$, den Determinanten von $Y$ *und* $B$ ($X^{YB}$) sowie den Variablen, die ausschließlich $Y$ beeinflussen ($X^{Y}$) gibt es $k_U = 499$ Variablen $X^U$, die weder $Y$ noch $B$ beeinflussen und damit irrelevant für die Schätzung des Behandlungseffekts sind. Wir haben also $N=550$ Beobachtungen und insgesamt $k = 1+k_{Y} + k_{YB} + k_{U} = 550$ potenzielle Kontrollvariablen von denen $k_{YB} = 25$ für eine unverzerrte Schätzung von $\alpha_0$ relevant sind.</span>
<span id="cb88-1429"><a href="#cb88-1429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1430"><a href="#cb88-1430" aria-hidden="true" tabindex="-1"></a>Der nachstehende Code generiert die Daten gemäß der Vorschrift.</span>
<span id="cb88-1431"><a href="#cb88-1431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1434"><a href="#cb88-1434" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-1435"><a href="#cb88-1435" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mvtnorm)</span>
<span id="cb88-1436"><a href="#cb88-1436" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb88-1437"><a href="#cb88-1437" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">4321</span>)</span>
<span id="cb88-1438"><a href="#cb88-1438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1439"><a href="#cb88-1439" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">550</span>      <span class="co"># Beobachtungen</span></span>
<span id="cb88-1440"><a href="#cb88-1440" aria-hidden="true" tabindex="-1"></a>p_Y <span class="ot">&lt;-</span> <span class="dv">25</span>     <span class="co"># Determinanten Y</span></span>
<span id="cb88-1441"><a href="#cb88-1441" aria-hidden="true" tabindex="-1"></a>p_B <span class="ot">&lt;-</span> <span class="dv">25</span>     <span class="co"># Determinanten B *und* Y</span></span>
<span id="cb88-1442"><a href="#cb88-1442" aria-hidden="true" tabindex="-1"></a>p_U <span class="ot">&lt;-</span> <span class="dv">499</span>    <span class="co"># irrelevante Variablen </span></span>
<span id="cb88-1443"><a href="#cb88-1443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1444"><a href="#cb88-1444" aria-hidden="true" tabindex="-1"></a><span class="co"># Variablen generieren</span></span>
<span id="cb88-1445"><a href="#cb88-1445" aria-hidden="true" tabindex="-1"></a>XB <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(<span class="at">n =</span> n, <span class="at">sigma =</span> <span class="fu">diag</span>(p_B))</span>
<span id="cb88-1446"><a href="#cb88-1446" aria-hidden="true" tabindex="-1"></a>XU <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(<span class="at">n =</span> n, <span class="at">sigma =</span> <span class="fu">diag</span>(p_U))</span>
<span id="cb88-1447"><a href="#cb88-1447" aria-hidden="true" tabindex="-1"></a>XY <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(<span class="at">n =</span> n, <span class="at">sigma =</span> <span class="fu">diag</span>(p_Y))</span>
<span id="cb88-1448"><a href="#cb88-1448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1449"><a href="#cb88-1449" aria-hidden="true" tabindex="-1"></a><span class="co"># Stetige Behandlungsvariable erzeugen</span></span>
<span id="cb88-1450"><a href="#cb88-1450" aria-hidden="true" tabindex="-1"></a>B <span class="ot">&lt;-</span> XB <span class="sc">%*%</span> <span class="fu">rnorm</span>(p_B, <span class="dv">2</span>, <span class="at">sd =</span> .<span class="dv">2</span>) <span class="sc">+</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb88-1451"><a href="#cb88-1451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1452"><a href="#cb88-1452" aria-hidden="true" tabindex="-1"></a><span class="co"># Abh. Variable erzeugen, Behandlungseffekt (ATE) ist 2</span></span>
<span id="cb88-1453"><a href="#cb88-1453" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> B <span class="sc">+</span> </span>
<span id="cb88-1454"><a href="#cb88-1454" aria-hidden="true" tabindex="-1"></a>  XB <span class="sc">%*%</span> <span class="fu">rnorm</span>(p_B, <span class="at">mean =</span> <span class="dv">10</span>) <span class="sc">+</span> </span>
<span id="cb88-1455"><a href="#cb88-1455" aria-hidden="true" tabindex="-1"></a>  XY <span class="sc">%*%</span> <span class="fu">runif</span>(p_Y) <span class="sc">+</span> </span>
<span id="cb88-1456"><a href="#cb88-1456" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rnorm</span>(n)</span>
<span id="cb88-1457"><a href="#cb88-1457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1458"><a href="#cb88-1458" aria-hidden="true" tabindex="-1"></a><span class="co"># Variablen in tibble sammeln</span></span>
<span id="cb88-1459"><a href="#cb88-1459" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(B, XB, XU, XY) <span class="sc">%&gt;%</span> </span>
<span id="cb88-1460"><a href="#cb88-1460" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>()</span>
<span id="cb88-1461"><a href="#cb88-1461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1462"><a href="#cb88-1462" aria-hidden="true" tabindex="-1"></a><span class="co"># Namen zuweisen</span></span>
<span id="cb88-1463"><a href="#cb88-1463" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(X) <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb88-1464"><a href="#cb88-1464" aria-hidden="true" tabindex="-1"></a>  <span class="st">"B"</span>, </span>
<span id="cb88-1465"><a href="#cb88-1465" aria-hidden="true" tabindex="-1"></a>  <span class="fu">paste0</span>(<span class="st">"XB"</span>, <span class="dv">1</span><span class="sc">:</span>p_B), </span>
<span id="cb88-1466"><a href="#cb88-1466" aria-hidden="true" tabindex="-1"></a>  <span class="fu">paste0</span>(<span class="st">"XU"</span>, <span class="dv">1</span><span class="sc">:</span>p_U),</span>
<span id="cb88-1467"><a href="#cb88-1467" aria-hidden="true" tabindex="-1"></a>  <span class="fu">paste0</span>(<span class="st">"XY"</span>, <span class="dv">1</span><span class="sc">:</span>p_Y) </span>
<span id="cb88-1468"><a href="#cb88-1468" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-1469"><a href="#cb88-1469" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1470"><a href="#cb88-1470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1471"><a href="#cb88-1471" aria-hidden="true" tabindex="-1"></a>Wünschenswert wäre die KQ-Schätzung des wahren Modells. Diese ergibt eine Schätzung nahe des wahren Treatment-Effekts $\alpha_0 = 2$. Unter realen Bedingungen wäre diese Regression jedoch nicht implementierbar, weil die relevanten Kovariablen <span class="in">`XB`</span> unbekannt sind.</span>
<span id="cb88-1472"><a href="#cb88-1472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1475"><a href="#cb88-1475" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-1476"><a href="#cb88-1476" aria-hidden="true" tabindex="-1"></a><span class="co"># KQ: Wahres Modell schätzen</span></span>
<span id="cb88-1477"><a href="#cb88-1477" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(Y <span class="sc">~</span> B <span class="sc">+</span> XB <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">$</span>coefficients[<span class="st">"B"</span>]</span>
<span id="cb88-1478"><a href="#cb88-1478" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1479"><a href="#cb88-1479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1480"><a href="#cb88-1480" aria-hidden="true" tabindex="-1"></a>Wir schätzen daher zunächst die "lange" Regression mit allen $k$ verfügbaren Variablen mit KQ. Beachte, dass der KQ-Schätzer für $\alpha_0$ zwar implementierbar und erwartungstreu ist, jedoch eine hohe Varianz aufweist. Wegen $k=N=550$ erhalten wir eine perfekte Anpassung an die Daten und können mangels Freiheitsgraden keine Hypothesentests durchführen.</span>
<span id="cb88-1481"><a href="#cb88-1481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1484"><a href="#cb88-1484" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-1485"><a href="#cb88-1485" aria-hidden="true" tabindex="-1"></a><span class="co"># KQ: Lange Regression schätzen</span></span>
<span id="cb88-1486"><a href="#cb88-1486" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(Y <span class="sc">~</span> . <span class="sc">-</span> <span class="dv">1</span>, <span class="at">data =</span> X)<span class="sc">$</span>coefficients[<span class="st">"B"</span>]</span>
<span id="cb88-1487"><a href="#cb88-1487" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1488"><a href="#cb88-1488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1489"><a href="#cb88-1489" aria-hidden="true" tabindex="-1"></a>Die KQ-Schätzung von $\alpha_0$ anhand der langen Regression weicht deutlich vom wahren Wert $\alpha_0 = 2$ ab.</span>
<span id="cb88-1490"><a href="#cb88-1490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1491"><a href="#cb88-1491" aria-hidden="true" tabindex="-1"></a>Eine "kurze" KQ-Regression nur mit der Behandlungsvariable $B$ führt wegen Korrelation mit den ausgelassenen Determinanten in <span class="in">`XB`</span> zu einer deutlich verzerrten Schätzung.</span>
<span id="cb88-1492"><a href="#cb88-1492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1495"><a href="#cb88-1495" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-1496"><a href="#cb88-1496" aria-hidden="true" tabindex="-1"></a><span class="co"># KQ: Kurze Regression</span></span>
<span id="cb88-1497"><a href="#cb88-1497" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(Y <span class="sc">~</span> B <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">$</span>coefficients[<span class="st">"B"</span>]</span>
<span id="cb88-1498"><a href="#cb88-1498" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1499"><a href="#cb88-1499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1500"><a href="#cb88-1500" aria-hidden="true" tabindex="-1"></a>Die Methoden von @BelloniChernozhukov2013 und @Bellonietal2014 sind im R-Paket <span class="in">`hdm`</span> implementiert. Mit den Funktionen <span class="in">`hrm::rlasso()`</span> und <span class="in">`hdm::rlassoEffect`</span> kann Lasso-Regression sowie Post- und Double-Post-Selection durchgeführt werden.<span class="ot">[^regreg-23]</span></span>
<span id="cb88-1501"><a href="#cb88-1501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1502"><a href="#cb88-1502" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-23]: </span>Diese Funktionen ermitteln ein optimales $\lambda$ mit dem in @Bellonietal2012 vorgeschlagenen Algorithmus.</span>
<span id="cb88-1503"><a href="#cb88-1503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1504"><a href="#cb88-1504" aria-hidden="true" tabindex="-1"></a>Wir berechnen zunächst den naiven Lasso-Schätzer in einem Modell mit allen Variablen.</span>
<span id="cb88-1505"><a href="#cb88-1505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1508"><a href="#cb88-1508" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-1509"><a href="#cb88-1509" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(hdm)</span>
<span id="cb88-1510"><a href="#cb88-1510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1511"><a href="#cb88-1511" aria-hidden="true" tabindex="-1"></a><span class="co"># Naiver Post-Lasso-Schätzer</span></span>
<span id="cb88-1512"><a href="#cb88-1512" aria-hidden="true" tabindex="-1"></a>lasso <span class="ot">&lt;-</span> <span class="fu">rlasso</span>(</span>
<span id="cb88-1513"><a href="#cb88-1513" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> X, </span>
<span id="cb88-1514"><a href="#cb88-1514" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> Y, </span>
<span id="cb88-1515"><a href="#cb88-1515" aria-hidden="true" tabindex="-1"></a>  <span class="at">intercept =</span> F, </span>
<span id="cb88-1516"><a href="#cb88-1516" aria-hidden="true" tabindex="-1"></a>  <span class="at">post =</span> F</span>
<span id="cb88-1517"><a href="#cb88-1517" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-1518"><a href="#cb88-1518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1519"><a href="#cb88-1519" aria-hidden="true" tabindex="-1"></a><span class="co"># Koeffizientenschätzer auslesen</span></span>
<span id="cb88-1520"><a href="#cb88-1520" aria-hidden="true" tabindex="-1"></a>lasso<span class="sc">$</span>coefficients[<span class="st">"B"</span>] </span>
<span id="cb88-1521"><a href="#cb88-1521" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1522"><a href="#cb88-1522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1523"><a href="#cb88-1523" aria-hidden="true" tabindex="-1"></a>Auch dieser Schätzer ist deutlich verzerrt. Problematisch ist hier nicht nur die Shrinkage auf $\widehat{\alpha}_0$, sondern die Selektion der Variablen in <span class="in">`XB`</span>:</span>
<span id="cb88-1524"><a href="#cb88-1524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1527"><a href="#cb88-1527" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-1528"><a href="#cb88-1528" aria-hidden="true" tabindex="-1"></a><span class="co"># Welche Variablen in XB selektiert Lasso *nicht*?</span></span>
<span id="cb88-1529"><a href="#cb88-1529" aria-hidden="true" tabindex="-1"></a>nselektiert <span class="ot">&lt;-</span> <span class="fu">which</span>(lasso<span class="sc">$</span>coef[<span class="dv">1</span><span class="sc">:</span><span class="dv">26</span>] <span class="sc">==</span> <span class="dv">0</span>)   <span class="co"># ID</span></span>
<span id="cb88-1530"><a href="#cb88-1530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1531"><a href="#cb88-1531" aria-hidden="true" tabindex="-1"></a><span class="co"># Namen auslesen</span></span>
<span id="cb88-1532"><a href="#cb88-1532" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(lasso<span class="sc">$</span>coef[<span class="dv">1</span><span class="sc">:</span><span class="dv">26</span>])[nselektiert]</span>
<span id="cb88-1533"><a href="#cb88-1533" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1534"><a href="#cb88-1534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1535"><a href="#cb88-1535" aria-hidden="true" tabindex="-1"></a>Durch das Auslassen dieser Determinanten von $Y$ und $B$ leidet der Lasso-Schätzer unter OVB.</span>
<span id="cb88-1536"><a href="#cb88-1536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1537"><a href="#cb88-1537" aria-hidden="true" tabindex="-1"></a>Als nächstes berechnen wir den Post-Lasso-Selection-Schätzer.</span>
<span id="cb88-1538"><a href="#cb88-1538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1541"><a href="#cb88-1541" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-1542"><a href="#cb88-1542" aria-hidden="true" tabindex="-1"></a><span class="co"># Post-Lasso-Selection-Schätzer berechnen</span></span>
<span id="cb88-1543"><a href="#cb88-1543" aria-hidden="true" tabindex="-1"></a>p_lasso <span class="ot">&lt;-</span> <span class="fu">rlasso</span>(</span>
<span id="cb88-1544"><a href="#cb88-1544" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> X,</span>
<span id="cb88-1545"><a href="#cb88-1545" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> Y, </span>
<span id="cb88-1546"><a href="#cb88-1546" aria-hidden="true" tabindex="-1"></a>  <span class="at">intercept =</span> F, </span>
<span id="cb88-1547"><a href="#cb88-1547" aria-hidden="true" tabindex="-1"></a>  <span class="at">post =</span> T</span>
<span id="cb88-1548"><a href="#cb88-1548" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-1549"><a href="#cb88-1549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1550"><a href="#cb88-1550" aria-hidden="true" tabindex="-1"></a><span class="co"># Schätzung für alpha_0</span></span>
<span id="cb88-1551"><a href="#cb88-1551" aria-hidden="true" tabindex="-1"></a>p_lasso<span class="sc">$</span>coef[<span class="st">"B"</span>]</span>
<span id="cb88-1552"><a href="#cb88-1552" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1553"><a href="#cb88-1553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1554"><a href="#cb88-1554" aria-hidden="true" tabindex="-1"></a>Die Ähnlichkeit der Post-Lasso-Schätzung von $\alpha_0$ zur Lasso-Schätzung zeigt deutlich, dass die Verzerrung des Lasso-Schätzers überwiegend durch ausgelassene Variablen anstatt durch Shrinkage verursacht wird.</span>
<span id="cb88-1555"><a href="#cb88-1555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1556"><a href="#cb88-1556" aria-hidden="true" tabindex="-1"></a>Mit <span class="in">`rlassoEffect()`</span> können wir den Post-Double-Selection-Schätzer berechnen.</span>
<span id="cb88-1557"><a href="#cb88-1557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1560"><a href="#cb88-1560" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-1561"><a href="#cb88-1561" aria-hidden="true" tabindex="-1"></a><span class="co"># Post-Double-Selection-Schätzer</span></span>
<span id="cb88-1562"><a href="#cb88-1562" aria-hidden="true" tabindex="-1"></a>pds_lasso <span class="ot">&lt;-</span> <span class="fu">rlassoEffect</span>(</span>
<span id="cb88-1563"><a href="#cb88-1563" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> X <span class="sc">%&gt;%</span> </span>
<span id="cb88-1564"><a href="#cb88-1564" aria-hidden="true" tabindex="-1"></a>    dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>B) <span class="sc">%&gt;%</span> </span>
<span id="cb88-1565"><a href="#cb88-1565" aria-hidden="true" tabindex="-1"></a>    <span class="fu">as.matrix</span>(),</span>
<span id="cb88-1566"><a href="#cb88-1566" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> Y, </span>
<span id="cb88-1567"><a href="#cb88-1567" aria-hidden="true" tabindex="-1"></a>  <span class="at">d =</span> B, </span>
<span id="cb88-1568"><a href="#cb88-1568" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">"double selection"</span></span>
<span id="cb88-1569"><a href="#cb88-1569" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-1570"><a href="#cb88-1570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1571"><a href="#cb88-1571" aria-hidden="true" tabindex="-1"></a><span class="co"># Schnittmenge der selektierten Determinanten </span></span>
<span id="cb88-1572"><a href="#cb88-1572" aria-hidden="true" tabindex="-1"></a><span class="co"># von Y und B</span></span>
<span id="cb88-1573"><a href="#cb88-1573" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb88-1574"><a href="#cb88-1574" aria-hidden="true" tabindex="-1"></a>  S_BY <span class="ot">&lt;-</span> <span class="fu">names</span>(</span>
<span id="cb88-1575"><a href="#cb88-1575" aria-hidden="true" tabindex="-1"></a>    <span class="fu">which</span>(pds_lasso<span class="sc">$</span>selection.index)</span>
<span id="cb88-1576"><a href="#cb88-1576" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb88-1577"><a href="#cb88-1577" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-1578"><a href="#cb88-1578" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1579"><a href="#cb88-1579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1580"><a href="#cb88-1580" aria-hidden="true" tabindex="-1"></a>Double Selection führt ebenfalls zu einem Post-Lasso-KQ-Schätzer mit allen 25 relevaten Variablen in <span class="in">`XB`</span>. Wir selektieren allerdings deutlich weniger irrelevante Variablen aus <span class="in">`XU`</span> als mit Single Selection und dennoch einige Determinanten von $Y$ aus <span class="in">`XY`</span>. Double Selection führt also zu einer unverzerrten Schätzen mit geringerer Varianz. Mit <span class="in">`summary()`</span> erhalten wir gültige Inferenz bzgl. des Treatment-Effekts.</span>
<span id="cb88-1581"><a href="#cb88-1581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1584"><a href="#cb88-1584" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-1585"><a href="#cb88-1585" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(pds_lasso)</span>
<span id="cb88-1586"><a href="#cb88-1586" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1587"><a href="#cb88-1587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1588"><a href="#cb88-1588" aria-hidden="true" tabindex="-1"></a>Der Post-Double-Selection-Schätzer liefert unter den betrachteten Verfahren die beste Schätzung von $\alpha_0$ und erlaubt gülstige statistische Inferenz. Der geschätzte Effekt ist hoch-signifikant.</span>
<span id="cb88-1589"><a href="#cb88-1589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1590"><a href="#cb88-1590" aria-hidden="true" tabindex="-1"></a>::: callout-note</span>
<span id="cb88-1591"><a href="#cb88-1591" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Key Facts zum Post-Double-Selection-Schätzer</span></span>
<span id="cb88-1592"><a href="#cb88-1592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1593"><a href="#cb88-1593" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Durch die sorgfältige Auswahl von Variablen, die mit Behandlung- und Outcome-Variable zusammenhängen, ermöglicht die Double-Selection eine bessere Kontrolle über das Risiko ausgelassender Variablen in Beobachtungsstudien und ermöglicht gültige (asymptotisch normale) Inferenz.</span>
<span id="cb88-1594"><a href="#cb88-1594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1595"><a href="#cb88-1595" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Der Post-Double-Selection-Schätzer besteht aus drei Regressionen:</span>
<span id="cb88-1596"><a href="#cb88-1596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1597"><a href="#cb88-1597" aria-hidden="true" tabindex="-1"></a><span class="ss">    1.  </span>Es werden Variablen mit Lasso selektiert, welche die *Behandlungs-Variable* erklären.</span>
<span id="cb88-1598"><a href="#cb88-1598" aria-hidden="true" tabindex="-1"></a><span class="ss">    2.  </span>Es werden Variablen mit Lasso selektiert, welche die *Outcome-Variable* erklären.</span>
<span id="cb88-1599"><a href="#cb88-1599" aria-hidden="true" tabindex="-1"></a><span class="ss">    3.  </span>Der Post-Double-Selection-Schätzer ist der KQ-Schätzer in einer Regression, die für die Schnittmenge der ausgewählten Variablen kontrolliert.</span>
<span id="cb88-1600"><a href="#cb88-1600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1601"><a href="#cb88-1601" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Dank der Selektion mit Lasso kann der Schätzer auch bei hoch-dimensionalen Daten ($k&gt;n$) angewendet werden.</span>
<span id="cb88-1602"><a href="#cb88-1602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1603"><a href="#cb88-1603" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Post-Double-Selection-Schätzer für Behandlungseffekte sind im R-Paket <span class="in">`hdm`</span> implementiert.</span>
<span id="cb88-1604"><a href="#cb88-1604" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb88-1605"><a href="#cb88-1605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1606"><a href="#cb88-1606" aria-hidden="true" tabindex="-1"></a><span class="fu">### Case Study: Makroökonomisches Wachstum</span></span>
<span id="cb88-1607"><a href="#cb88-1607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1608"><a href="#cb88-1608" aria-hidden="true" tabindex="-1"></a>Zur Illustration des Post-Double-Selection Schätzers betrachten wir eine empirische Anwendung bzgl. der Validierung von makroökonomischer Wachstumtheorie. Aus neo-klassischen Ansätzen wie dem <span class="co">[</span><span class="ot">Solow-Swan-Modell</span><span class="co">](https://de.wikipedia.org/wiki/Solow-Modell)</span> kann die Hypothese, dass Volkswirtschaften zu einem gemeinsamen Wachstumspfad hin konvergieren, abgeleitet werden. Diese Konvergenzhypothese impliziert die Existenz von Aufholeffekten: Ärmere Volkswirtschaften müssen im mittel schneller Wachsen als die Wirschaft wohlhabender Länder. Die grundlegende Spezifikation eines entsprechenden Regressionsmodells lautet \begin{align}</span>
<span id="cb88-1609"><a href="#cb88-1609" aria-hidden="true" tabindex="-1"></a>  \text{WR}_{i} = \alpha_0 \text{BIP0}_i + u_i, \label{eq:growthmodel1}</span>
<span id="cb88-1610"><a href="#cb88-1610" aria-hidden="true" tabindex="-1"></a>\end{align} wobei $\text{WR}_{i}$ die Wachstumsrate des Pro-Kopf-BIP in Land $i$ über einen Zeitraum (typischerweise berechnet als Log-Differenz zwischen zwei Perioden) und $\text{BIP0}_i$ das (logarithmierte) Pro-Kopf-BIP zu beginn der Referenzperiode ist. Gemäß der Konvergenzhypothese muss $\alpha_0&lt;0$ sein: Je wohlhabender eine Volkswirtschaft ist, desto geringer ist das Wirtschaftswachstum.</span>
<span id="cb88-1611"><a href="#cb88-1611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1612"><a href="#cb88-1612" aria-hidden="true" tabindex="-1"></a>Um Verzerrung durch ausgelassene Kovariablen zu vermeiden, sollte das Modell \eqref{eq:growthmodel1} um länder-spezifische Regressoren $x_{i,j}$, die sowohl das Ausgagnsniveau $\text{BIP0}$ sowie die Wachtumsrate beinflussen, erweitert werden. Zu der großen Menge potentieller Kovariablen gehören makro- und sozio-ökonomische Maße wie bspw. die Investitionstätigkeit des Staates, Offenheit der Volkswirtschaft, das politische Umfeld, das Bildungsniveau, die Demographie usw. Eine bevorzugte Spezifikation ist daher \begin{align}</span>
<span id="cb88-1613"><a href="#cb88-1613" aria-hidden="true" tabindex="-1"></a>  \text{WR}_{i} = \alpha_0 \text{BIP0}_i + \sum_{j=1}^k \beta_j x_{i,j} + u_i,\label{eq:growthmodel2}</span>
<span id="cb88-1614"><a href="#cb88-1614" aria-hidden="true" tabindex="-1"></a>\end{align} wobei $\alpha_0$ als Behandlungseffekt interpretiert werden kann. Beachte, dass \eqref{eq:growthmodel2} eine Regression in der Form von \eqref{eq:lassotmt} ist.</span>
<span id="cb88-1615"><a href="#cb88-1615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1616"><a href="#cb88-1616" aria-hidden="true" tabindex="-1"></a>Wir illustrieren die Schätzung von und Inferenz bzgl. $\alpha_0$ in \eqref{eq:growthmodel2} mit Post-Double-Selektion für einen 90 Länder umfassenden Auszug aus dem Datensatz von @BarroLee2013, der als Objekt <span class="in">`GrowthData`</span> im R-Paket <span class="in">`hdm`</span> verfügbar ist.<span class="ot">[^regreg-24]</span></span>
<span id="cb88-1617"><a href="#cb88-1617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1618"><a href="#cb88-1618" aria-hidden="true" tabindex="-1"></a><span class="ot">[^regreg-24]: </span>Eine ausführliche Beschreibung der Variablen ist <span class="co">[</span><span class="ot">hier</span><span class="co">](https://www2.nber.org/pub/barro.lee/readme.txt)</span> einsehbar.</span>
<span id="cb88-1619"><a href="#cb88-1619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1622"><a href="#cb88-1622" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-1623"><a href="#cb88-1623" aria-hidden="true" tabindex="-1"></a><span class="co"># Datensatz in Arbeitsumgebung verfügbar machen</span></span>
<span id="cb88-1624"><a href="#cb88-1624" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(hdm)</span>
<span id="cb88-1625"><a href="#cb88-1625" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(GrowthData)</span>
<span id="cb88-1626"><a href="#cb88-1626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1627"><a href="#cb88-1627" aria-hidden="true" tabindex="-1"></a><span class="co"># Anzahl Beobachtungen und Variablen</span></span>
<span id="cb88-1628"><a href="#cb88-1628" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(GrowthData)</span>
<span id="cb88-1629"><a href="#cb88-1629" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1630"><a href="#cb88-1630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1631"><a href="#cb88-1631" aria-hidden="true" tabindex="-1"></a>Die Spalte <span class="in">`Outcome`</span> ist die jeweilige Wachstumsrate des BIP zwischen den Perioden 1965-1975 und 1975-1985 und <span class="in">`gdpsh465`</span> ist das reale Pro-Kopf-BIP im Jahr 1965 zu Preisen von 1980.</span>
<span id="cb88-1632"><a href="#cb88-1632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1633"><a href="#cb88-1633" aria-hidden="true" tabindex="-1"></a>Wir führen zunächst eine graphische Analyse hinsichtlich des Modells einfachen Modells \eqref{eq:growthmodel1} durch, indem wir <span class="in">`gdpsh465`</span> gegen <span class="in">`Outcome`</span> plotten und die geschätzte Regressionsgerade einzeichnen.</span>
<span id="cb88-1634"><a href="#cb88-1634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1637"><a href="#cb88-1637" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-1638"><a href="#cb88-1638" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "BIP-Wachstum: Einfache Regression"</span></span>
<span id="cb88-1639"><a href="#cb88-1639" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-bipsimple</span></span>
<span id="cb88-1640"><a href="#cb88-1640" aria-hidden="true" tabindex="-1"></a><span class="co"># Einfache grafische Analyse mit ggplot2</span></span>
<span id="cb88-1641"><a href="#cb88-1641" aria-hidden="true" tabindex="-1"></a>GrowthData <span class="sc">%&gt;%</span></span>
<span id="cb88-1642"><a href="#cb88-1642" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(</span>
<span id="cb88-1643"><a href="#cb88-1643" aria-hidden="true" tabindex="-1"></a>    <span class="at">mapping =</span> <span class="fu">aes</span>(</span>
<span id="cb88-1644"><a href="#cb88-1644" aria-hidden="true" tabindex="-1"></a>      <span class="at">x =</span> gdpsh465, </span>
<span id="cb88-1645"><a href="#cb88-1645" aria-hidden="true" tabindex="-1"></a>      <span class="at">y =</span> Outcome</span>
<span id="cb88-1646"><a href="#cb88-1646" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb88-1647"><a href="#cb88-1647" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb88-1648"><a href="#cb88-1648" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb88-1649"><a href="#cb88-1649" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> F)</span>
<span id="cb88-1650"><a href="#cb88-1650" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1651"><a href="#cb88-1651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1652"><a href="#cb88-1652" aria-hidden="true" tabindex="-1"></a>@fig-bipsimple zeigt einen geringen positiven geschätzten Effekt $\widehat{\alpha}_0$. Eine Auswertung mit <span class="in">`lm()`</span> ergibt, dass der Effekt $\alpha_0$ nicht signifikant von $0$ verschieden ist.</span>
<span id="cb88-1653"><a href="#cb88-1653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1656"><a href="#cb88-1656" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-1657"><a href="#cb88-1657" aria-hidden="true" tabindex="-1"></a><span class="co"># Einfache Regression durchführen, </span></span>
<span id="cb88-1658"><a href="#cb88-1658" aria-hidden="true" tabindex="-1"></a><span class="co"># Inferenz für gdpsh465 erhalten</span></span>
<span id="cb88-1659"><a href="#cb88-1659" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(Outcome <span class="sc">~</span> gdpsh465, <span class="at">data =</span> GrowthData) <span class="sc">%&gt;%</span></span>
<span id="cb88-1660"><a href="#cb88-1660" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summary</span>() <span class="sc">%&gt;%</span></span>
<span id="cb88-1661"><a href="#cb88-1661" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coefficients</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb88-1662"><a href="#cb88-1662" aria-hidden="true" tabindex="-1"></a>  .[<span class="dv">2</span>, ]</span>
<span id="cb88-1663"><a href="#cb88-1663" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1664"><a href="#cb88-1664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1665"><a href="#cb88-1665" aria-hidden="true" tabindex="-1"></a>Der positive Effekt aus der einfachen Schätzung widerspricht der Konvergenzhypothese. Dieses Ergebnis könnte allerdings durch Auslassen relevanter Kovariablen ungültig sein. Beispielsweise ist es plausibel, dass das Bildungsniveau einer Volkswirtschaft sowohl mit dem BIP korreliert ist als auch die Wachstumsrate beeinflusst. Dann wäre das Bildungsniveau eine relevante Kovariable, deren Auslassen zu einer verzerrten Schätzung von $\alpha_0$ führt.</span>
<span id="cb88-1666"><a href="#cb88-1666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1667"><a href="#cb88-1667" aria-hidden="true" tabindex="-1"></a>Eine "lange" Regression mit allen Kovariablen ist zwar möglich, aber problematisch: Das Verhältnis von Beobachtungen (90) zu Regressoren (62) bedeutet eine hohe Unsicherheit der Schätzung.</span>
<span id="cb88-1668"><a href="#cb88-1668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1671"><a href="#cb88-1671" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-1672"><a href="#cb88-1672" aria-hidden="true" tabindex="-1"></a><span class="co"># Inferenz für alpha_0 in langer Regression</span></span>
<span id="cb88-1673"><a href="#cb88-1673" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(</span>
<span id="cb88-1674"><a href="#cb88-1674" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lm</span>(Outcome <span class="sc">~</span> . <span class="sc">-</span> <span class="dv">1</span> , <span class="at">data =</span> GrowthData)</span>
<span id="cb88-1675"><a href="#cb88-1675" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb88-1676"><a href="#cb88-1676" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coefficients</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb88-1677"><a href="#cb88-1677" aria-hidden="true" tabindex="-1"></a>  .[<span class="dv">2</span>, ]</span>
<span id="cb88-1678"><a href="#cb88-1678" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1679"><a href="#cb88-1679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1680"><a href="#cb88-1680" aria-hidden="true" tabindex="-1"></a>Der geschätzte Koeffizient $\widehat{\alpha}_0$ ist nun zwar negativ, liefert jedoch weiterhin keine Evidenz, dass $\alpha_0$ von 0 verschieden ist. Ein Vergleich der Standardfehler zeigt aber, dass die KQ-Schätzung aufgrund Berücksichtigung aller potentiellen Kovariablen mit deutlich größerer Varianz behaftet ist als in der einfachen KQ-Regression \eqref{eq:growthmodel1}</span>
<span id="cb88-1681"><a href="#cb88-1681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1682"><a href="#cb88-1682" aria-hidden="true" tabindex="-1"></a>Post-Double-Selection erlaubt gültige Inferenz bzgl. $\alpha_0$ nach Schätzung der Menge relevanter Kovariablen. Wir weisen die entsprechenden Variablen R-Objekten zu und berechnen den Schätzer.</span>
<span id="cb88-1683"><a href="#cb88-1683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1686"><a href="#cb88-1686" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-1687"><a href="#cb88-1687" aria-hidden="true" tabindex="-1"></a><span class="co"># Variablen für Post-Double-Selection vorbereiten</span></span>
<span id="cb88-1688"><a href="#cb88-1688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1689"><a href="#cb88-1689" aria-hidden="true" tabindex="-1"></a><span class="co"># abh. Variable</span></span>
<span id="cb88-1690"><a href="#cb88-1690" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> GrowthData <span class="sc">%&gt;%</span> </span>
<span id="cb88-1691"><a href="#cb88-1691" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pull</span>(Outcome)</span>
<span id="cb88-1692"><a href="#cb88-1692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1693"><a href="#cb88-1693" aria-hidden="true" tabindex="-1"></a><span class="co"># "Treatment"</span></span>
<span id="cb88-1694"><a href="#cb88-1694" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> GrowthData <span class="sc">%&gt;%</span> </span>
<span id="cb88-1695"><a href="#cb88-1695" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pull</span>(gdpsh465)</span>
<span id="cb88-1696"><a href="#cb88-1696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1697"><a href="#cb88-1697" aria-hidden="true" tabindex="-1"></a><span class="co"># potentielle Regressoren</span></span>
<span id="cb88-1698"><a href="#cb88-1698" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> GrowthData <span class="sc">%&gt;%</span> </span>
<span id="cb88-1699"><a href="#cb88-1699" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(</span>
<span id="cb88-1700"><a href="#cb88-1700" aria-hidden="true" tabindex="-1"></a>    <span class="sc">-</span>Outcome, <span class="sc">-</span>intercept, <span class="sc">-</span>gdpsh465</span>
<span id="cb88-1701"><a href="#cb88-1701" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb88-1702"><a href="#cb88-1702" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1703"><a href="#cb88-1703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1706"><a href="#cb88-1706" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-1707"><a href="#cb88-1707" aria-hidden="true" tabindex="-1"></a><span class="co"># Post-Double-Selection-Schätzer berechnen</span></span>
<span id="cb88-1708"><a href="#cb88-1708" aria-hidden="true" tabindex="-1"></a>Growth_DS <span class="ot">&lt;-</span> </span>
<span id="cb88-1709"><a href="#cb88-1709" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rlassoEffect</span>(</span>
<span id="cb88-1710"><a href="#cb88-1710" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> X <span class="sc">%&gt;%</span> </span>
<span id="cb88-1711"><a href="#cb88-1711" aria-hidden="true" tabindex="-1"></a>      <span class="fu">as.matrix</span>(), </span>
<span id="cb88-1712"><a href="#cb88-1712" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> y, </span>
<span id="cb88-1713"><a href="#cb88-1713" aria-hidden="true" tabindex="-1"></a>    <span class="at">d =</span> d, </span>
<span id="cb88-1714"><a href="#cb88-1714" aria-hidden="true" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">"double selection"</span></span>
<span id="cb88-1715"><a href="#cb88-1715" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-1716"><a href="#cb88-1716" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1717"><a href="#cb88-1717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1718"><a href="#cb88-1718" aria-hidden="true" tabindex="-1"></a>Post-Double-Selection wählt aus der Menge potentieller Kovariablen lediglich sieben Regressoren aus.</span>
<span id="cb88-1719"><a href="#cb88-1719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1722"><a href="#cb88-1722" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-1723"><a href="#cb88-1723" aria-hidden="true" tabindex="-1"></a><span class="co"># Selektierte Variablen einsehen</span></span>
<span id="cb88-1724"><a href="#cb88-1724" aria-hidden="true" tabindex="-1"></a><span class="co"># ID</span></span>
<span id="cb88-1725"><a href="#cb88-1725" aria-hidden="true" tabindex="-1"></a>Selektion <span class="ot">&lt;-</span> Growth_DS<span class="sc">$</span>selection.index</span>
<span id="cb88-1726"><a href="#cb88-1726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1727"><a href="#cb88-1727" aria-hidden="true" tabindex="-1"></a><span class="co"># Namen auslesen</span></span>
<span id="cb88-1728"><a href="#cb88-1728" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(</span>
<span id="cb88-1729"><a href="#cb88-1729" aria-hidden="true" tabindex="-1"></a>  <span class="fu">which</span>(Selektion <span class="sc">==</span> T)</span>
<span id="cb88-1730"><a href="#cb88-1730" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-1731"><a href="#cb88-1731" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1732"><a href="#cb88-1732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1733"><a href="#cb88-1733" aria-hidden="true" tabindex="-1"></a>@tbl-growthpdssek zeigt die Definitionen der ausgewählten Variablen.</span>
<span id="cb88-1734"><a href="#cb88-1734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1735"><a href="#cb88-1735" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo = F}</span></span>
<span id="cb88-1736"><a href="#cb88-1736" aria-hidden="true" tabindex="-1"></a><span class="in">#| tbl-cap: "Mit PDS selektierte Variablen aus `GrowthData`. Referenzjahr 1965."</span></span>
<span id="cb88-1737"><a href="#cb88-1737" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: tbl-growthpdssek</span></span>
<span id="cb88-1738"><a href="#cb88-1738" aria-hidden="true" tabindex="-1"></a><span class="in">tibble(</span></span>
<span id="cb88-1739"><a href="#cb88-1739" aria-hidden="true" tabindex="-1"></a><span class="in">  Variable = c(</span></span>
<span id="cb88-1740"><a href="#cb88-1740" aria-hidden="true" tabindex="-1"></a><span class="in">    "bmp1l", "freetar", "hm65", "sf65", </span></span>
<span id="cb88-1741"><a href="#cb88-1741" aria-hidden="true" tabindex="-1"></a><span class="in">    "lifee065", "humanf65", "pop6565"</span></span>
<span id="cb88-1742"><a href="#cb88-1742" aria-hidden="true" tabindex="-1"></a><span class="in">    ),</span></span>
<span id="cb88-1743"><a href="#cb88-1743" aria-hidden="true" tabindex="-1"></a><span class="in">  Beschreibung = c(</span></span>
<span id="cb88-1744"><a href="#cb88-1744" aria-hidden="true" tabindex="-1"></a><span class="in">    "Schwarzmarktprämie d. Währung",</span></span>
<span id="cb88-1745"><a href="#cb88-1745" aria-hidden="true" tabindex="-1"></a><span class="in">    "Maß für Zollbeschränkungen",</span></span>
<span id="cb88-1746"><a href="#cb88-1746" aria-hidden="true" tabindex="-1"></a><span class="in">    "Einschreibungsquote Uni (Männer) ",</span></span>
<span id="cb88-1747"><a href="#cb88-1747" aria-hidden="true" tabindex="-1"></a><span class="in">    "Beschulungsquote Sekundarstufe (Frauen)",</span></span>
<span id="cb88-1748"><a href="#cb88-1748" aria-hidden="true" tabindex="-1"></a><span class="in">    "Lebenserwartung bei Geburt",</span></span>
<span id="cb88-1749"><a href="#cb88-1749" aria-hidden="true" tabindex="-1"></a><span class="in">    "Durschn. Bildung im Alter 25 (Frauen)",</span></span>
<span id="cb88-1750"><a href="#cb88-1750" aria-hidden="true" tabindex="-1"></a><span class="in">    "Anteil Bevölkerung ü. 65 Jahre"</span></span>
<span id="cb88-1751"><a href="#cb88-1751" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb88-1752"><a href="#cb88-1752" aria-hidden="true" tabindex="-1"></a><span class="in">) %&gt;%</span></span>
<span id="cb88-1753"><a href="#cb88-1753" aria-hidden="true" tabindex="-1"></a><span class="in">  gt() %&gt;%</span></span>
<span id="cb88-1754"><a href="#cb88-1754" aria-hidden="true" tabindex="-1"></a><span class="in">  tabopts</span></span>
<span id="cb88-1755"><a href="#cb88-1755" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1756"><a href="#cb88-1756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1759"><a href="#cb88-1759" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-1760"><a href="#cb88-1760" aria-hidden="true" tabindex="-1"></a><span class="co"># Gültige Inferenz mit dem Post-Double-Selection-Schätzer</span></span>
<span id="cb88-1761"><a href="#cb88-1761" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(Growth_DS)</span>
<span id="cb88-1762"><a href="#cb88-1762" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-1763"><a href="#cb88-1763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-1764"><a href="#cb88-1764" aria-hidden="true" tabindex="-1"></a>Das Ergebnis der Post-Double-Selection-Schätzung unterstützt die (bedingte) Konvergenzhypothese mit einer signifikanten negativen Schätzung $\widehat{\alpha}_0\approx<span class="in">`r round(Growth_DS$alpha, 3)`</span>$.</span>
</code><button title="In die Zwischenablage kopieren" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>