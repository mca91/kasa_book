---
format: 
  live-html:
    webr: 
      chunk-options:
        message: false
        warning: false
      packages: [
            'broom',
            'cowplot',
            'dplyr',
            'ggplot2',
            'ggfortify',
            'ggrepel',
            'gendata',
            'glmnet',
            'gt',
            'hdm',
            'lars',
            'mvtnorm',
            'purrr',
            'sandwich',
            'tidyr',
            'readr'
            ]
      repos:
      - https://rstudio.r-universe.dev
---

{{< include ./_extensions/r-wasm/live/_knitr.qmd >}}

# Regularisierte Regression {#sec-regreg}

```{webr}
#| echo: false
#| output: false
#| edit: false
#| message: false
#| warning: false

# Create dataset directory
dir.create("datasets")
# Download the datasets
download.file(
    "https://raw.githubusercontent.com/mca91/kausal_data/main/SP.csv",
    'datasets/SP.csv',
)

# Formatierung von gt-Tabellen
tabopts <- function(x) {
  fmt_number(x, decimals = 3, drop_trailing_zeros = T) %>%
    tab_options(
      # general
      table.width = "80%",
      # fonts
      table.font.color = "black", 
      table.font.size = 16,
      column_labels.font.weight = "bold", 
      # table header
      column_labels.padding = 15,
      column_labels.border.bottom.color = "black", 
      column_labels.border.bottom.width = "1px", 
      column_labels.border.top.color = "black",
      # group labels
      row_group.border.top.width = 0,
      row_group.border.bottom.width = 0,
      row_group.font.weight = "bold",
      row_group.padding = 20,
      # body
      table_body.hlines.width = 0, 
      table_body.border.bottom.color = "black",
      table_body.border.bottom.width = "1px",
      table_body.border.top.width = "0px",
      table.border.bottom.color = "black"
    )
}
```


```{r, echo=F, message=FALSE}
library(gt)
library(tidyverse)
library(cowplot)
library(glmnet)
library(ggfortify)
# Formatierung von gt-Tabellen
tabopts <- function(x) {
    fmt_number(x, decimals = 3, drop_trailing_zeros = T) %>%
  tab_options(table_body.hlines.color = "white", 
              column_labels.border.bottom.color = "black", 
             column_labels.border.top.color = "black",
             table_body.border.bottom.color = "black", 
             table.border.bottom.color = "black",
             column_labels.font.weight = "bold", 
             table.font.color = "black", 
             table.font.size = 16)
}
```

In diesem Kapitel betrachten wir Varianten von Koeffizientenschätzern im linearen Modell 
\begin{align}
  Y_i = \beta_1 X_{1,i} + \dots + \beta_k X_{k,i} + u_i, \quad i = 1,\dots,n,\label{eq:slm}
\end{align}
deren Motivation die Schätzung von $\boldsymbol{\beta} := (\beta_1, \dots,\beta_k)'$ in Anwendungen ist, in denen der KQ-Schätzer 
\begin{align}
  \begin{split}
  \widehat{\boldsymbol{\beta}} =&\, \arg\min_{\boldsymbol{\beta}}\mathrm{RSS}(\boldsymbol{\beta})\\ 
  =&\,  \arg\min_{\boldsymbol{\beta}}  \sum_{i=1}^n\left(Y_i-\beta_1 X_{1,i} + \dots + \beta_k X_{k,i}\right)^2
  \end{split}\label{eq:KQLoss}
\end{align} 
keine stabile Schätzung zulässt oder nicht eindeutig definiert ist, und damit gar nicht erst berechnet werden kann. Solche Szenarien ergeben sich in der empirischen Forschung, wenn die Regressoren stark korreliert sind und/oder das Modell viele Regressoren enthält ($k\lesssim n$), oder das Regressionsproblem hoch-dimensional ist ($k>n$).

Regularisierte Regressionsschätzer begegnen dieser Problematik mit einer Modifikation der Verlustfunktion $\mathrm{RSS}$ in \eqref{eq:KQLoss}, 
\begin{align}
  \mathrm{RSS}(\boldsymbol{\beta}, p, \lambda) := \mathrm{RSS}(\boldsymbol{\beta}) + \lambda\lVert\boldsymbol{\beta}\rVert_p.
\end{align} 
Hierbei ist $\lambda>0$ ein Tuning-Parameter und $p\geq1$ definiert die $p$-Norm des Koeffizientenvektors,
\begin{align}
  \lVert\boldsymbol{\beta}\rVert_p := \left(\sum_{j=1}^k \lvert\beta_j\rvert^{p}\right)^{1/p}>0.\label{eq:pnorm}
\end{align}

Wegen $\lambda\lVert\boldsymbol{\beta}\rVert_p>0$ kann die $p$-Norm des Koeffizientenvektors $\boldsymbol{\beta}$ das Optimierungsproblem $$\min_{\boldsymbol{\beta}} \mathrm{RSS}(\boldsymbol{\beta}, p, \lambda) \vert\, p,\, \lambda$$ derart restringieren, dass die geschätzten Koeffizienten 
\begin{align*}
  \widehat{\boldsymbol{\beta}}_{p,\,\lambda} := \arg\min_{\boldsymbol{\beta}} \mathrm{RSS}(\boldsymbol{\beta}, p, \lambda)
\end{align*} 
im Erwartungswert absolut kleiner ausfallen als bei der KQ-Schätzung: Der Schätzer ist in Richtung 0 verzerrt.^[ Beachte, dass für $\lambda=0$ die Verlustfunktion des KQ-Schätzers folgt.] Dieser Effekt der Regularisierung wird in der Literatur als *Shrinkage* bezeichnet.

Die grundlegenden Eigenschaften des Schätzers $\widehat{\boldsymbol{\beta}}_{p,\,\lambda}$ werden maßgeblich durch den Parameter $p$ bestimmt, der hinsichtlich des zu lösenden Regressionsproblems *a priori* gewählt wird.^[D.h. wir wählen $p$, um einen Schätzer mit für die konkrete Anwendung hilfreichen Eigenschaften zu erhalten.]

Shrinkage ist eine Motivation für die Anwendung regularisierter Schätzer in Modellen, die auch mit KQ geschätzt werden könnten. Um dies zu verstehen, nehmen wir an, dass die Gauss-Markov-Annahmen in \eqref{eq:slm} gelten. Dann hat der KQ-Schätzer die kleinste Varianz unter allen *unverzerrten* Schätzern. Aufgrund der Shrinkage fallen regularisierte Schätzer zwar nicht unter das Gauss-Markov-Theorem, können dafür aber eine geringere Varianz haben als KQ. Schätzer mit solchen Eigenschaften sind nützlich, wenn eine unverzerrte Schätzung von $\boldsymbol{\beta}$ nicht unser primäres Ziel ist: Für Vorhersagen kann es hilfreich sein, etwas Verzerrung bei der Koeffizientenschätzung in Kauf zu nehmen, um eine hinreichend große Varianzreduktion zu erreichen, sodass ein geringerer erwarteter Vorhersagefehler als für KQ resultiert. Hierbei liegt, eine Abwägung zwischen Verzerrung und Varianz (*Bias Variance Tradeoff*) vor, der durch den Regularisierungsparameter $\lambda$ beeinflusst wird.

Für die Berechnung des Schätzers in empirischen Anwendungen wird $\lambda$ meist datengetrieben (mit [Cross Validation](https://de.wikipedia.org/wiki/Kreuzvalidierungsverfahren) oder einem Informationskriterium) geschätzt oder mit einer analytisch fundierten Faustregel gewählt.

Nachfolgend betrachten wir zwei häufig verwendete regularisierte Schätzer, die sich durch die Wahl $p=1$ (Lasso Regression) bzw. $p=2$ (Ridge Regression) ergeben und illustrieren ihre Anwendung mit R.

## Ridge Regression

Ridge Regression wurde von @HoerlKennard1970 als Alternative zur KQ-Schätzung bei hoch-korrelierten Regressoren eingeführt. Die Verlustfunktion lautet \begin{align}
  \mathrm{RSS}(\boldsymbol{\beta},p=2,\lambda) = \mathrm{RSS}(\boldsymbol{\beta}) + \lambda \lVert\boldsymbol{\beta}\rVert_2,\label{eq:ridgeloss}
\end{align} d.h. der Parameter $\lambda$ reguliert den Einfluss eines $\ell_2$-Strafterms \begin{align*}
  \lVert\boldsymbol{\beta}\rVert_2 = \sqrt{\sum_{j=1}^k\beta_j^2}
\end{align*} auf die Verlustfunktion $\mathrm{RSS}(\boldsymbol{\beta},p=2,\lambda)$. Der Ridge-Schätzer ergibt sich als \begin{align}
  \widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda := \arg\min_{\boldsymbol{\beta}}\mathrm{RSS}(\boldsymbol{\beta}) + \lambda \lVert\boldsymbol{\beta}\rVert_2.\label{eq:ridgereg}
\end{align}

Für Das Optimierungsproblem \eqref{eq:ridgereg} kann wir aus den Bedingungen 1. Ordnung 
\begin{align}
  -2\boldsymbol{X}'(\boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\beta}) + 2\lambda\boldsymbol{\beta} = \boldsymbol{0}
\end{align} die analytische Lösung \begin{align}
  \widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda = (\boldsymbol{X}'\boldsymbol{X} + \lambda\boldsymbol{I}_p)^{-1}\boldsymbol{X}'\boldsymbol{Y},\label{eq:ridgecf}
\end{align} 
bestimmt werden, wobei $\boldsymbol{I}_k$ die $k\times k$ Einheitsmatrix ist. Aus Gleichung \eqref{eq:ridgecf} kann die Wirkungsweise des Strafterms $\lambda \lVert\boldsymbol{\beta}\rVert_2$ abgeleitet werden: Ridge Regression modifiziert die Diagonale der zu invertierenden Matrix $\boldsymbol{X}'\boldsymbol{X}$ durch Addition von $\lambda>0$. Dies ist hilfreich, wenn

- $k\geq n$ und damit $\boldsymbol{X}'\boldsymbol{X}$ nicht invertiertbar (singulär) ist. Dann kann der KQ-Schätzer nicht berechnet werden.^[Beispiel: `X <- matrix(rnorm(100), ncol = 10)`. Vergleiche `solve(t(X) %*% X)` und `solve(t(X) %*% X + diag(.01, nrow = 10))`] Die Inverse $(\boldsymbol{X}'\boldsymbol{X} + \lambda\boldsymbol{I}_p)^{-1}$ hingegen existiert unter milden Bedingungen.

- hohe Kollinearität vorliegt, sodass $(\boldsymbol{X}'\boldsymbol{X})^{-1}$ zwar existiert, aber zu einer instablilen KQ-Schätzung mit hoher Varianz führt.

Für eine grafische Betrachtung des Optimierungskalküls \eqref{eq:ridgereg} betrachten wir die äquivalente Darstellung als Lagrange-Problem \begin{align}
  \widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda := \arg\min_{\lVert\boldsymbol{\beta}\rVert<t}\mathrm{RSS}(\boldsymbol{\beta}).\label{eq:ridgeLg}
\end{align} In der folgenden interaktiven Grafik illustrieren wir das Optimierungsproblem \eqref{eq:ridgeLg} sowie den resultierenden Schätzer der Koeffizienten $(\beta_1, \beta_2)$ in einem multiplen Regressionsmodell mit den Regressoren $X_1$ und $X_2$.

- Die blaue Ellipse ist die Menge aller Schätzwerte $\left(\widehat\beta_{1},\, \widehat\beta_{2}\right)$ für den angegebenen Wert von $\mathrm{RSS}$. Im Zentrum der Ellipse liegt der KQ-Schätzer, welcher $\mathrm{RSS}$ minimiert.

- Der blaue Kreis ist die Menge aller Koeffizienten-Paare $(\beta_1, \beta_2)$, welche die Restriktion $\beta_1^2 + \beta_2^2\leq t$ erfüllen. Beachte, dass die Größe des Kreises nur durch den Parameter $t$ bestimmt wird, welcher für einen vorgegebenen Wertebereich variiert werden kann.

- Der blaue Punkt ist der Ridge-Schätzer $(\widehat\beta^R_{1,t},\, \widehat\beta^R_{2,t})$. Dieser ergibt sich als Schnittpunkt zwischen der blauen $\mathrm{RSS}$-Ellipse und der Restriktionsregion und variiert mit $t$. Die gestrichelte rote Kurve zeigt den Ridge-Lösungspfad.

- Für kleine Werte $t$ drückt die Shrinkage die geschätzten Koeffizienten Richtung 0, wobei der Lösungspfad i.d.R. nicht-linear verläuft, d.h. die Shrinkage auf den Koeffizienten ist grundsätzlich unterschiedlich. Die Lösung $(\widehat\beta^R_{1,t},\, \widehat\beta^R_{2,t}) = (0,0)$ existiert nur als Grenzwert für $t\to0$.

- Beachte, dass der Effekt von $t$ auf die Schätzung umgekehrt für $\lambda$ verläuft: Größere $\lambda$ führen zu stärkerer Regularisierung.

::: {.content-visible when-format="html"}
<iframe width="100%" height="500" frameborder="0" class="box-shadow" src="https://observablehq.com/embed/45a59e74a4330581?cells=viewof+m%2Cchart114"></iframe>
:::

::: {.content-visible when-format="pdf"}

------------------------------------------------------------------------

***Diese Interaktive Komponente des Buchs ist nur in der Online-Version verfügbar.***

------------------------------------------------------------------------
:::

### Eigenschaften des Schätzers

Der Ridge-Schätzer $\widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda$ ist nicht invariant gegenüber der Skalierung der Regressoren. Für empirische Daten sollte daher vorab eine Standardisierung der erklärenden Variablen durchgeführt werden.^[Bspw. mit der Funktion `scale()`.] Um die Eigenschaften des Ridge-Schätzers besser zu verstehen, betrachten wir hier den Fall orthonormaler Regressoren $\boldsymbol{X}_j$.^[Orthonormalität heißt $\boldsymbol{X}_i'\boldsymbol{X}_j = 1$ für $i=j$ und $0$ sonst. Dann ist $\boldsymbol{X}$'$\boldsymbol{X} = \boldsymbol{I}_k$.] Dann ist 
\begin{align}
  \widehat{\beta}^{\mathrm{R}}_{\lambda,\,j} = (1+\lambda)^{-1} \cdot\widehat{\beta}_j,\quad j = 1,\dots,k,\label{eq:ridgeortho}
\end{align} 
d.h. der Ridge-Schätzer skaliert die KQ-Lösung mit einem von $\lambda$ abhängigen Faktor.^[$(1+\lambda)^{-1}$ wird auch als *Shrinkage-Faktor* bezeichnet.]

Wir illustrieren dies, indem wir den Zusammenhang zwischen KQ- und Ridge-Schätzer im orthonormalen Fall als R-Funktion `ridge_ortho()` implementieren und für die Parameterwerte $\lambda\in\{0,0.5,2\}$ plotten.

```{webr}
library(tibble)
library(dplyr)

# Funktion für Rige Regression bei orthonormalen Regressoren
ridge_ortho <- function(KQ, lambda) {
  1/(1 + lambda) * KQ
}

# Prüfen:
ridge_ortho
```

```{webr}
# Sequenz von KQ-Schätzungen erzeugen
dat <- tibble(
  KQ = seq(-1, 1, .01)
)

# Prüfen:
dat
```


```{webr}
# KQ-Schätzer gegen Ridge-Schätzer plotten
ggplot(dat) +
  geom_function(
    fun = ridge_ortho, 
    args = list(lambda =  0), 
    lty = 2) + 
  geom_function(
    fun = ridge_ortho, 
    args = list(lambda = .5), 
    col = "red") + 
  geom_function(
    fun = ridge_ortho, 
    args = list(lambda = 2), 
    col = "blue") + 
  xlim(-.4, .4) +
  labs(
    x = "KQ-Schätzer von beta_1",
    y = "Ridge-Schätzer von beta_1",
    title = "Shrinkage des OLS-Schätzers bei Ridge Regression"
  ) +
  cowplot()
```

Die obige Grafik zeigt, dass der Ridge-Schätzer eine lineare Transformation des KQ-Schätzers (gestrichelte Linie) ist. Größere Werte des Regularisierungsparameters $\lambda$ führen zu stärkerer Shrinkage des Koeffizientenschätzers in Richtung 0. Die $\ell_2$-Norm führt zu proportional zum Absolutwert des KQ-Schätzers verlaufender Shrinkage: Größere Koeffizienten werden stärker bestraft als kleine Koeffizienten.

Die Eigenschaft $$\mathrm{E}\left(\widehat{\boldsymbol{\beta}}^{\mathrm{R}}_{\lambda,\,j}\right) = (1+\lambda)^{-1} \cdot \beta_j$$ zeigt, dass $\widehat{\boldsymbol{\beta}}^{\mathrm{R}}_{\lambda,\,j}$ (für fixes $\lambda>0$) nicht erwartungstreu für $\beta_j$ ist. Weiterhin ist 
\begin{align*}
  \mathrm{Var}\left(\widehat{\beta}^{\mathrm{R}}_{\lambda,\,j}\right) =&\, 
  \mathrm{Var}\left(\widehat{\beta}_j\right) \cdot \left(\frac{\lambda}{1+\lambda^2}\right)\\
    =&\, \sigma^2\cdot \left(\frac{\lambda}{1+\lambda^2}\right),
\end{align*} 
wobei $\sigma^2$ die Varianz des Regressionsfehlers $u$ ist. Wegen $\lambda<(1+\lambda)^2$ für $\lambda>0$ gilt $$\mathrm{Var}\left(\widehat{\beta}^{\mathrm{R}}_{\lambda,\,j}\right)<\mathrm{Var}\left(\widehat{\beta}_j\right).$$ Der Ridge-Schätzer hat also eine kleinere Varianz als der KQ-Schätzer. Diese Eigenschaften können auch für korrelierte Regressoren gezeigt werden.

### Ridge Regression mit `glmnet`

Wir zeigen nun anhand simulierter Daten, wie der Ridge-Lösungspfad mit dem R-Paket `glmnet` berechnet werden kann. Wir erzeugen zunächst Daten gemäß der Vorschrift 
\begin{align}
  \begin{split}
  Y_i =&\, \boldsymbol{X}_i' \boldsymbol{\beta} + u_i,\\
  \\
  \beta_j =&\,  \frac{5}{j^2}, \qquad\qquad\ j=1,\dots,5,\\ 
  \beta_j =&\, -\frac{5}{(j-5)^2}, \quad j=6,\dots,10,\\
  \\
  \boldsymbol{X}_i \sim&\, N(\boldsymbol{0}, \boldsymbol{\Sigma}), \quad u_i \overset{u.i.v.}{\sim} N(0, 1), \quad i = 1,\dots,25.
  \end{split} \label{eq:ridgedgp1}
\end{align} 
Hierbei wird $\boldsymbol{\Sigma}$ so definiert, dass jeder Regressor $N(0,1)$-verteilt ist und eine Korrelation von $0.8$ mit allen anderen Regressoren aufweist. Mit der Vorschrift für die $\beta_j$ stellen wir sicher, dass es wenige Variablen gibt, die $Y$ stark beeinflussen, da der Absolutbetrag der Koeffizienten in $j$ abnimmt.^[Für bessere Interpretierbarkeit der Grafischen Auswertung, wählen wir positive und negative Koeffizienten mit gleichem Betrag.]

```{webr}
library(gendata)

set.seed(1234)

# Parameter definieren
N <- 80
k <- 10

coefs <- 5/(1:(k/2))^2
beta <- c(coefs, -coefs)

# Beobachtungen simulieren:
# Regressoren
X <- as.matrix(
  genmvnorm(
    k = k, 
    cor = rep(.8, (k^2-k)/2), 
    n = N)
  )

# Abh. Variable
Y <- X %*% beta + rnorm(N)
```

Wir schätzen nun ein Modell mit allen `k` Regressoren mit `glmnet`. Beachte, dass für den Ridge-Strafterm `alpha = 0` gesetzt werden muss.^[`alpha` ist ein Mischparameter im Algorithmus für [elastic net](https://en.wikipedia.org/wiki/Elastic_net_regularization), siehe `?glmnet`.]

```{webr}
library(glmnet)

# Ridge-Regression anpassen
ridge_fit <- glmnet(
  x = X, 
  y = Y, 
  alpha = 0 # für Ridge-Strafterm
)
```

Der Lösungspfad der Ridge-Schätzung kann nach Transformation der geschätzen Koeffizienten und der zugehörigen $\lambda$-Werte in ein langes Format überführt und komfortabel mit `ggplot2` dargestellt werden.

```{webr}
# Lambda-Sequenz auslesen
lambdas <- ridge_fit$lambda

# Ridge-Schätzung für Lambdas im langen Format 
as.matrix(ridge_fit$beta) %>% 
  as_tibble() %>% 
  rownames_to_column("Variable") %>%
  pivot_longer(-Variable) %>% 
  group_by(Variable) %>% 
  mutate(lambda = lambdas) %>%
  
  # Grafik mit ggplot erzeugen
  ggplot(
    mapping = aes(
      x = lambda, 
      y = value, 
      col = Variable
    )
  ) + 
  geom_line() +
  labs(
    y = "gesch. Koeffizienten",
    title = "Simulierte Daten: Lösungspfad für Ridge-Schätzung"
  ) +
  scale_x_log10("log_10(lambda)") +
  theme_cowplot()
```

Wir sehen den nicht-linearen Verlauf der Shrinkage auf den geschätzten Modellkoeffizienten in der Abbildung. Die Koeffizienten werden mit zunehmendem $\lambda$ von der KQ-Lösung ausgehend (linkes Ende der Skala) in Richtung 0 gezwungen.

Über die Funktion `cv.glmnet()` kann ein optimales $\lambda$ mit Cross Validation (CV) ermittelt werden. Ähnlich wie bei `glmnet()` wird für die Validierung automatisch eine $\lambda$-Sequenz erzeugt. Wir nutzen `autoplot()` aus dem R-Paket `ggfortify` für die Visualisierung der Ergebnisse mit `ggplot2`.

```{webr}
library(ggfortify)

# Cross-validierte Bestimmung von lambda
ridge_cvfit <- cv.glmnet(
  y = Y, 
  x = X, 
  intercept = F,
  alpha = 0
) 

# Ergebnisse plotten
ridge_cvfit %>% 
  autoplot(label.n = 0) +
  labs(
    title = "CV: MSE und Lambda für Ridge-Regression"
  ) +
  theme_cowplot()
```

Der Plot zeigt `ridge_cvfit$lambda.min`, das optimale $\lambda$ mit dem geringsten CV Mean-Squarred-Error (linke gestrichelte Linie) und `ridge_cvfit$lambda.1se`, das größte $\lambda$, welches innerhalb einer Standardabweichung entfernt ist (rechte gestrichelte Linie).^[Die Wahl von `lambda.1se` ist eine Heuristik, welche die Schätzunsicherheit berücksichtigt und zu einem "sparsameren" Modell tendiert.] Wir berechnen die Schätzung für `lambda.min`.

```{webr}
# geschätzte Koeffizienten beim
# optimalen Lambda auslesen
(
  ridge_coefs <- coef(
    object = ridge_cvfit, 
    s = ridge_cvfit$lambda.min
  )
)
```

Wir schätzen das Modell nun mit KQ und vergleichen die Koeffizienten mit der Ridge-Schätzung.

```{webr}
# KQ-Schätzung durchführen
KQ_fit <- lm(Y ~ X - 1)

# Koeffizienten auslesen und transformieren:
tibble(
  Ridge = as.matrix(ridge_coefs)[2:11, ],
  KQ = KQ_fit$coefficients
) %>% 
  mutate(j = factor(1:10)) %>%
  pivot_longer(
    cols = Ridge:KQ, 
    names_to = "Methode", 
    values_to = "Koeffizient"
  ) %>%

# Bar-Plot für Koeffizientenvergleich erzeugen  
  ggplot(
    mapping = aes(
      x = j, 
      y = Koeffizient, 
      fill = Methode
    )
  ) +
  geom_bar(
    position = "dodge", 
    stat = "identity", 
    width = .5
  ) +
  labs(
    title = "Koeffizientenvergleich: Ridge- vs. KQ-Regression"
  ) +
  theme_cowplot() +
  theme(
    legend.position = "inside",
    legend.position.inside = c(0.15, .9)
  )
```

Der Vergleich in der obigen Abbildung zeigt deutlich, dass Ridge Regression im Vergleich mit KQ zu absolut kleineren Koeffizientenschätzungen tendiert. Inwiefern dies Konsequenzen für die Prognosegüte der Schätzung hat, können wir Anhand eines Testdatensatzes bestimmen. Hierzu vergleichen wir die mittleren Fehler (MSE) bei der Prognose von $Y$ für die Beobachtungen im Testdatensatz. Für die Simulation des Testdatensatzes nutzen wir erneut die Vorschrift \eqref{eq:ridgedgp1} um `N` neue Beobachtungen zu erzeugen.

```{webr}
# Test-Datensatz erstellen
set.seed(4321)

# Regressoren generieren
new_X <- as.matrix(
  genmvnorm(
    k = k, 
    cor = rep(.85, (k^2-k)/2), 
    n = N
  )
)

# Abh. Variable generieren
new_Y <- new_X %*% beta + rnorm(N)
```

Für beide Methoden können wir `predict()` für die Prognosen von $Y$ für den Testdatensatz (`new_Y`) nutzen.

```{webr}
# Ridge: Vorhersage von new_Y für Test-Datensatz
Y_predict_ridge <- predict(
  object = ridge_cvfit, 
  newx = new_X, 
  s = ridge_cvfit$lambda.min
)

# Ridge: MSE für Test-Datensatz berechnen
mean((Y_predict_ridge - new_Y)^2)
```

Die Vorhersage für `lm()` benötigt dieselben Variablennamen wie im angepassten Modell, s. `KQ_fit$coefficients`.

```{webr}
# Test-Datensatz für predict.lm() formatieren
new_X <- as.data.frame(new_X)
colnames(new_X) <- paste0("X", 1:k)

# KQ: Vorhersage von new_Y für Test-Datensatz
Y_predict_KQ <- predict(
  object = KQ_fit, 
  newdata = new_X
)

# KQ: MSE für Test-Datensatz berechnen
mean((Y_predict_KQ - new_Y)^2)
```

Die Ergebnisse zeigen, dass der Ridge-Schätzer trotz seiner Verzerrung einen deutlich geringeren mittleren Vorhersagefehler für die Testdaten erzielt als der KQ-Schätzer. Diese Eigenschaft der Koeffizientenschätzung kann die Prognosegüte von Ridge Regression gegenüber der KQ-Regression verbessern.

### Beispiel: Vorhersage von Abschlussnoten in Mathe

Zur Illustration von Ridge Regression nutzen wir den Datensatz `SP` aus @CortezSilva2008.^[Wir verwenden eine Auszug aus dem Orignaldatensatz, der nebst ausführlicher Variablenbeschreibung [hier](https://archive.ics.uci.edu/dataset/320/student+performance) verfügbar ist.] `SP` enhält Beobachtungen zu Leistungen von insgesamt 100 Schülerinnen und Schülern im Fach Mathematik in der Sekundarstufe an zwei portugiesischen Schulen. Neben der Abschlussnote in Mathe (`G3`, Skala von 0 bis 20) beinhaltet `SP` diverse demografische, soziale und schulbezogene Merkmale, die mithilfe von Schulberichten und Fragebögen erhoben wurden. Ziel ist es, ein Modell für die Prognose von `G3` anzupassen.

Wir lesen zunächst die Daten (im .csv-Format) ein.

```{webr}
# Daten einlesen
SP <- read_csv(
  file = "datasets/SP.csv"
)
```

Ein Überblick zeigt, dass der Großteil der Regressoren aus kategorialen Variablen mit sozio-ökonomischen Informationen besteht.

```{webr}
# Überblick
glimpse(SP)
```

Um die Prognosegüte des Modells beurteilen zu können, partitionieren wir `SP` zufällig in einen Test- sowie einen Trainingsdatensatz (mit 30 und 70 Beobachtungen), jeweils für die Regressoren und die abhängige Variable.

```{webr}
set.seed(1234)

# ID für Beobachtungen im Testdatensatz zufällig erzeugen
ID <- sample(1:nrow(SP), size = 30)

# Regressoren aufteilen
SP_test <- SP[ID, ]
SP_train <- SP[-ID, ]

# Abh. Variable aufteilen
Y_test <- SP_test$G3
Y_train <- SP_train$G3
```

Als nächstes passen wir ein Ridge-Regressionsmodell für alle Regressoren in `SP_train` an und ermitteln ein optimales $\lambda$ mit Cross Validation. Beachte, dass `cv.glmnet` nicht für Regressoren im `data.frame`/`tibble`-Format ausgelegt ist, sondern ein `matrix`-Format erwartet. Wir transformieren `SP_train` daher mit `data.matrix()`.

```{webr}
# Ridge-Regression und CV für Trainingsdaten
SP_fit_cv <- cv.glmnet(
  x = data.matrix(SP_train %>% select(-G3)), 
  y = Y_train, 
  alpha = 0
)

# CV-Ergebnisse für lambda visualisieren
SP_fit_cv %>% 
  autoplot(label.n = 0) +
  theme_cowplot()
```

Wie für das Beispiel mit simulierten Daten erhalten wir mit `predict()` Vorhersagen für die erzielte Punktzahl. Beachte, dass wir den MSE nicht für die Trainingsdaten `SP_train`, sondern für die Testdaten `SP_test` berechnen.

```{webr}
# Prognose von G3 anhand des Ridge-Modells
Y_predict_ridge <- predict(
  object = SP_fit_cv, 
  newx = data.matrix(
    SP_test %>% 
      select(-G3)
    ), 
  s = SP_fit_cv$lambda.min
)

# Testset-MSE für Ridge berechnen
mean((Y_predict_ridge - Y_test)^2)
```

Auch in diesem empirischen Beispiel zeigt ein Vergleich der MSEs, dass Ridge Regression dem KQ-Schätzer hinsichtlich der Vorhersagegüte überlegen ist.

```{webr}
# Modell mit KQ schätzen
SP_fit_KQ <- lm(G3 ~ ., SP_train)

# Prognose
Y_predict_KQ <- predict(
  object = SP_fit_KQ, 
  newdata = SP_test %>% 
    select(-G3)
)

# Testset-MSE für KQ berechnen
mean((Y_predict_KQ - Y_test)^2)
```

Der MSE für Ridge ist mit deutlich kleiner als der MSE für KQ.

Für die Interpretation der Ridge-Schätzung erweitern den Code für die `ggplot2`-Grafik der Koeffizienten-Pfade um eine vertikale Linie des mit CV ermittelten $\lambda$ und fügen mit dem Paket `ggrepel` Labels für die Pfade der größten Koeffizienten hinzu.

```{webr}
#| fig-height: 7
library(ggrepel)

# Lambda-Sequenz auslesen
lambdas <- SP_fit_cv$lambda

# Ridge-Schätzung für Lambdas im langen Format 
df <- as.matrix(SP_fit_cv$glmnet.fit$beta) %>% 
  as_tibble() %>% 
  mutate(
    Variable = rownames(SP_fit_cv$glmnet.fit$beta)
  ) %>%
  pivot_longer(-Variable) %>% 
  group_by(Variable) %>% 
  mutate(lambda = lambdas) 

# Grafik mit ggplot erzeugen
df %>%
  ggplot(
    mapping = aes(
      x = lambda, 
      y = value, 
      col = Variable
    )
  ) + 
  geom_line() +
  geom_label_repel(
    data = df %>% 
      filter(lambda == min(lambdas)),
    mapping = aes(label = Variable), 
    seed = 1234,
    size = 5, 
    max.overlaps = 8, 
    nudge_x = -.5) +
  labs(
    ylab = "gesch. Koeffizienten",
    title = "Lösungspfad für Ridge-Schätzung"
  ) +
  scale_x_log10("log_10(lambda)") +
  geom_vline(
    xintercept = SP_fit_cv$lambda.min, 
    col = "red", 
    lty = 2
  ) +
  theme_cowplot() +
  theme(legend.position = "none")
```

Der Plot gibt Hinweise darauf, dass neben der Schulzugehörigkeit und Indikatoren für schulische Leistung (bspw. `failures`) sozio-ökonomische Prädiktoren wie `internet` (Internetzugang zuhause), `Pstatus` (Zusammenleben der Eltern) und `address`/`traveltime` (sozialer Status) relevante Variablen zu sein scheinen.

Das optimale $\lambda_\mathrm{cv}$ (gestrichelte rote Linie im letzten Plot) führt zu deutlicher Shrinkage, was eine mögliche Erklärung für den besseren Testset-MSE von Ridge Regression ist: Die Koeffizienten von Variablen mit wenig Erklärungskraft werden durch die Regularisierung in Richtung 0 gezwungen und reduzieren so die Varianz der Vorhersage gegenüber der (idealerweise) unverzerrten KQ-Schätzung.

::: callout-note
#### Key Facts zu Ridge Regression

- Ridge-Regression regularisiert den KQ-Schätzer mit der $\ell_2$-Norm der Koeffizienten. Diese Form von Regularisierung ist eine Alternative für KQ in Anwendungen mit mehr Regressoren als Beobachtugen ($k\geq n$) und/oder wenn KQ aufgrund starker Kollinearität eine hohe Varianz aufweist.

- Der Ridge-Schätzer $\widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda$ ist *nicht* erwartungstreu. Die geschätzten Koeffizienten sind auch für $n\to\infty$ verzerrt.

- Aufgrund der verzerrten Schätzung ist statistische Inferenz für Koeffizienten mit $\widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda$ problematisch. Anstatt für strukturelle Modelle oder die Schätzung kausaler Effekte wird Ridge Regression in der Praxis daher überwiegend für Prognosen verwendet.

- Die Wahl von $\lambda$ impliziert einen Tradeoff zwischen Verzerrung und Varianz: Große $\lambda$ schrumpfen die Koeffizientenschätzer Richtung 0 (mehr Verzerrung), führen aber zu einer kleineren Varianz der Schätzung. Entsprechend können Vorhersagen mit mehr Verzerrung aber weniger Varianz als mit KQ getroffen werden.

- Ridge Regression kann in R mit dem Paket `glmnet` berechnet werden.
:::

## Lasso Regression

Least Absolute Shrinkage and Selection Operator (Lasso) ist ein von @Tibshirani1996 vorgeschlagener Schätzer, der die Verlustfunktion des KQ-Schätzers um einen Strafterm für die Summe der (absoluten) Größe der Koeffizienten $\boldsymbol\beta = (\beta_1, \dots,\beta_k)'$ erweitert. Die Verlustfunktion des Lasso-Schätzers von $\boldsymbol{\beta}$ lautet \begin{align}
  \mathrm{RSS}(\boldsymbol{\beta},p=1,\lambda) = \mathrm{RSS}(\boldsymbol{\beta}) + \lambda \lVert\boldsymbol{\beta}\rVert_1.
  \label{eq:lassoloss}
\end{align} 
Für den Strafterm wird also die $\ell_1$-norm $$
\lVert\boldsymbol{\beta}\rVert_1 = \sum_{j=1}^k \lvert\beta_j \rvert
$$ verwendet. Der Lasso-Schätzer $\widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda$ für $\boldsymbol{\beta}$ minimiert \eqref{eq:lassoloss},
\begin{align}
  \boldsymbol{\beta}^{\mathrm{L}}_\lambda = \arg\min_{\boldsymbol{\beta}} \ \mathrm{RSS}(\boldsymbol{\beta},p=1,\lambda).
\end{align} 
Entsprechend erhalten wir in Abhängigkeit von $\lambda$ ein Kontinuum an Lösungen 
\begin{align}
  \left\{\widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda\right\}_{\lambda=0}^{\lambda=\infty},
  \label{eq:LassoPath}
\end{align} 
der sogenannte *Lasso-Pfad*.

Das Optimierungsproblem \eqref{eq:lassoloss} hat die äquivalente Darstellung 
\begin{align}
  \begin{split}
    \widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda =&\, \arg\min_{\boldsymbol{\beta}} \mathrm{RSS}(\boldsymbol{\beta}) + \lambda\left(\lVert\boldsymbol{\beta}\rVert_1 - t\right)\\
    =&\, \arg\min_{\lVert\boldsymbol{\beta}\rVert_1\leq t} \mathrm{RSS}(\boldsymbol{\beta}), 
  \end{split}\label{eq:lassolagrange}
\end{align} 
welche über den [Lagrange-Ansatz](https://de.wikipedia.org/wiki/Lagrange-Multiplikator#Beispiel_mit_Anwendungsbezug) unter der Nebenbedingung $\lVert\boldsymbol{\beta}\rVert_1 \leq t$ gelöst werden kann.

Ähnlich wie der KQ-Schätzer ist der Lasso-Schätzer $\widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda$ durch Bedingungen 1. Ordnung bestimmt. Diese Bedingungen lassen sich komfortabel in Matrix-Schreibweise darstellen als 
\begin{align}
  -2\boldsymbol{X}_j'(\boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\beta}) + \lambda\cdot\mathrm{sgn}(\beta_j) = 0, \quad j = 1,\dots,k.\label{eq:LassoFOC}
\end{align} 
Aus Gleichung \eqref{eq:LassoFOC} folgt, dass der Lasso-Schätzer aufgrund des Strafterms im Allgemeinen nicht algebraisch bestimmt werden kann.^[Zur Bestimmung des Schätzers werden Algorithmen der nicht-linearen Optimierung genutzt.]

In Abhängigkeit von $\lambda$ zwingt der Lasso-Schätzer die KQ-Schätzung von $\beta_j$ zu einem (absolut) kleineren Wert: Ähnlich wie bei Ridge Regression bewirkt der $\ell_1$-Strafterm eine mit $\lambda$ zunehmende Schrumpfung der geschätzen Koeffizienten in Richtung 0. Charakteristisch für die Lösung des Lasso-Schätzers ist, dass $\widehat{\boldsymbol{\beta}}^{\mathrm{L}}_j = 0$, wenn die Bedingung 
\begin{align}
  \left\lvert\boldsymbol{X}_j'(\boldsymbol{Y} - \boldsymbol{X}\widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda)\right\rvert - \lambda/2 \leq 0 \label{eq:lassoselection}
\end{align} 
erfüllt ist. In Abhängigkeit von $\lambda$ kann der Lasso-Schätzer folglich geschätzte Regressionskoeffizienten nicht nur in Richtung $0$, sondern diese auch *exakt* mit $0$ schätzen und damit *Variablenselektion* betreiben. Aufgrund der mit $\lambda$ zunehmenden Shrinkage bis die Bedingung \eqref{eq:lassoselection} erfüllt und der Koeffizient gleich $0$ gesetzt wird, bezeichnet man Lasso auch als einen *Soft Thresholding Operator*. Im nächsten Abschnitt betrachten wir die Eigenschaften von Lasso-Regularisierung unter vereinfachten Annahmen bzgl. der Regressoren.

### Lasso ist Soft Thresholding

Wir betrachten nun eine mathematische Darstellung von Selektions- und Shrinkage-Eigenschaft des Lasso-Schätzers in einem vereinfachten Modell. Wenn die Regressoren $\boldsymbol{X}$ orthonormal zueinander sind, existiert eine analytische Lösung des Lasso-Schätzers, \begin{align}
  \widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda =
  \begin{cases}
    \widehat{\boldsymbol{\beta}}_j - \lambda/2 &, \ \ \widehat{\boldsymbol{\beta}}_j > \lambda/2\\
    0 &, \ \ \lvert\widehat{\boldsymbol{\beta}}_j\rvert\leq\lambda/2\\
    \widehat{\boldsymbol{\beta}}_j + \lambda/2 &, \ \ \widehat{\boldsymbol{\beta}}_j < \lambda/2
  \end{cases},\label{eq:lassoST}
\end{align} wobei $\widehat{\boldsymbol{\beta}}_j$ der KQ-Schätzer von $\beta_j$ ist. Anhand von \eqref{eq:lassoST} können wir die Selektionseigenschaft sowie die Schrumpfung der KQ-Koeffizientenschätzung in Abhängigkeit der durch $\lambda$ regulierten $\ell_1$-Strafe erkennen. Für eine Visualisierung implementieren wir \eqref{eq:lassoST} als R-Funktion `lasso_st()` und zeichnen die resultierenden Koeffizientenschätzungen für die Parameterwerte $\lambda\in\{0, 0.2, 0.4\}$.

Wir definieren zunächst die Funktion `lasso_st()`.

```{webr}
# Funktion für Lasso soft-thresholding definieren
lasso_st <- function(KQ, lambda) {
  case_when(
    KQ > lambda/2         ~ KQ - lambda/2,
    abs(KQ) <= lambda/2   ~ 0,
    KQ < -lambda/2        ~ KQ + lambda/2,
  )
}
```

Im nächsten Schritt zeichnen wir `lasso_st()` für eine Sequenz von KQ-Schätzwerten gegeben $\lambda$.

```{webr}
# Sequenz von KQ-Schätzwerten für Illustration definieren
dat <- tibble(
  KQ = seq(-1, 1, .01)
)

# Lasso-Schätzer als Funktion des KQ-Schätzers plotten
ggplot(dat) +
  geom_function(
    fun = lasso_st, 
    args = list(lambda = 0), 
    lty = 2
  ) + 
  geom_function(
    fun = lasso_st, 
    args = list(lambda = .2),
    col = "red"
  ) + 
  geom_function(
    fun = lasso_st, 
    args = list(lambda = .4), 
    col = "blue"
  ) + 
  xlim(-.4, .4) +
  labs(
    x = "KQ-Schätzung von beta_1",
    y = "Lasso-Schätzung von beta_1",
    title = "Shrinkage und Selektion von Koeffizienten mit Lasso"
  ) +
  theme_cowplot()
```

Der Plot zeigt, dass der $\ell_1$-Strafterm des Lasso-Schätzers zu einem linearen Verlauf der auf den KQ-Schätzer (gezeichnet für $\lambda = 0$, gestrichelte Linie) applizierten Shrinkage führt: Der Lasso-Schätzer ist eine abschnittsweise-lineare Funktion des KQ-Schätzers in $\lambda$: Je größer der Parameter $\lambda$, desto größer ist das Intervall von KQ-Schätzwerten $[-\lambda/2,\lambda/2]$, wo der Lasso-Schätzer zu Variablenselektion führt, d.h. hier den Koeffizienten $\beta_j$ als $0$ schätzt (rote bzw. blaue Linie).

Anhand des Plots kann weiterhin abgeleitet werden, dass der Lasso-Schätzer nicht invariant gegenüber der Skalierung der Regressoren ist: Die Stärke der Regularisierung durch $\lambda$ ist hängt von der Magnitude des KQ-Schätzers ab. Daher müssen die Regressoren vor Berechnung der Schätzung standardsiert werden. Üblich ist hierbei eine Normierung auf einen Mittelwert von $0$ und eine Varianz von $1$.

Die nachstehende interaktive Grafik illustriert das Lasso-Optimierungsproblem \eqref{eq:lassolagrange} sowie den resultierenden Schätzer der Koeffizienten $(\beta_1, \beta_2)$ in einem multiplen Regressionsmodell mit korrelierten Regressoren $X_1$ und $X_2$.

- Die blaue Ellipse ist die Menge aller Schätzwerte $\left(\widehat\beta_{1},\, \widehat\beta_{2}\right)$ für den angegebenen Wert von $\mathrm{RSS}$. Im Zentrum der Ellipse liegt der KQ-Schätzer, welcher $\mathrm{RSS}$ minimiert.

- Das graue Quadrat ist die Menge aller Koeffizienten-Paare $(\beta_1, \beta_2)$, welche die Restriktion $\lvert\beta_1\rvert+\lvert\beta_2\rvert\leq t$ erfüllen. Beachte, dass die Größe dieser Region nur durch den Parameter $t$ bestimmt wird.

- Der blaue Punkt ist der Lasso-Schätzer $(\widehat{\boldsymbol{\beta}}^L_{1,t},\, \widehat{\boldsymbol{\beta}}^L_{2,t})$. Dieser ergibt sich als Schnittpunkt zwischen der blauen $\mathrm{RSS}$-Ellipse und der Restriktionsregion und variiert mit $t$. Die gestrichelte rote Linie zeigt den Lasso-Lösungspfad.

- Für kleine Werte, erhalten wir starke Shrinkage auf $\widehat\beta_{1,t}$ bis zum Wertebereich $t\leq50$, wo $\widehat{\boldsymbol{\beta}}^L_{1,t}=0$. Hier erfolgt Variablenselektion: Die Regularisierung führt zu einem geschätzten Modell, das lediglich $X_2$ als erklärende Variable enthält. In diesem Bereich von $t$ bewirkt die Shrinkage, dass $\widehat{\boldsymbol{\beta}}^L_{2,t}\to0$ für $t\to0$.

::: {.content-visible when-format="html"}
<iframe width="100%" height="567" frameborder="0" scroll="false" class="box-shadow" src="https://observablehq.com/embed/2e11f2b535e23c25@16?cells=viewof+s%2Cchart115">

</iframe>
:::

::: {.content-visible when-format="pdf"}

------------------------------------------------------------------------

***Diese Interaktive Komponente des Buchs ist nur in der Online-Version verfügbar.***

------------------------------------------------------------------------
:::

Beachte, dass der rote Lasso-Pfad (die Menge aller Lasso-Lösungen) äquivalent als Funktion von $\lambda$ im Optimierungsproblem \eqref{eq:lassoloss} dargestellt werden kann. Implementierungen mit statistischer Software berechnen die Lasso-Lösung häufig in Abhängigkeit von $\lambda$. Ein Algorithmus hierfür ist LARS.

### Berechnung der Lasso-Lösung mit dem LARS-Algorithmus

Für die Berechnung des Lasso-Lösungspfads kann der [LARS-Algorithmus](https://en.wikipedia.org/wiki/Least-angle_regression) von @Efronetal2004 im Lasso-Modus genutzt werden.^[LARS steht für *Least Angle Regression and Selection*.] Der Lasso-Lösungspfad beinhaltet geschätzte Koeffizienten über einem Intervall für $\lambda$, welches sämtliche Modellkomplexitäten zwischen der (trivialen) Lösung mit maximaler Shrinkage auf allen Koeffizienten ($\lambda$ groß, alle gesch. Koeffizienten sind $0$) und der unregularisierten Lösung ($\lambda = 0$, KQ-Schätzung) abbildet. Der LARS-Algorithmus erzeugt den Lösungspfad sequentiell, sodass die Schätzung als Funktion von $\lambda$ veranschaulicht werden kann, ähnlich wie bei Ridge Regression.

Wir zeigen nun anhand simulierter Daten, wie Lasso-Lösungen mit dem R-Paket `lars` berechnet werden können. Hierfür erzeugen wir Daten gemäß der Vorschrift \begin{align}
  \begin{split}
  Y_i =&\, \boldsymbol{X}_i' \boldsymbol{\beta}_v + u_i\\
  \\
  \boldsymbol{\beta}_v =&\, (-1.25, -.75, 0, 0, 0, 0, 0, .75, 1.25)'\\
  \\
  \boldsymbol{X}_i \sim&\, N(\boldsymbol{0}, \boldsymbol{I}_{9\times9}), \quad u_i \overset{u.i.v.}{\sim} N(0, 1), \quad i = 1,\dots,25.
  \end{split}\label{eq:larsdgp}
\end{align}

```{webr}
library(lars)

set.seed(1234)

# Parameter definieren
N <- 25
beta_v <- c(-1.25, -.75, 0, 0, 0, 0, 0, .75, 1.25)

# Beobachtungen simulieren
X <- matrix(rnorm(N * 9), ncol = 9)
Y <- X %*% beta_v + rnorm(N)
```

Entsprechend des DGP passen wir ein Modell ohne Konstante an. Damit `lars::lars()` den Lösungspfad des Lasso-Schätzers berechnet, muss `type = "lasso"` gewählt werden.^[`lars()` standardisiert die Regressoren standardmäßig (aufgrund des DGPs hier nicht nötig).]

```{webr}
# Lösungen des Lasso-Schätzers mit LARS berechnen
(
  fit_lars <- lars(
    x = X, 
    y = Y, 
    intercept = F,
    type = "lasso" # Wichtig: Lasso-Modus
  )
)
```

Die Zusammenfassung zeigt, dass der LARS-Algorithmus als erstes die (relevante) Variable $X_9$ aktiviert.^[Aktivierung meint die Aufnahme einer Variable in der Modell gegeben eines hinreichend kleinen $\lambda$.] Mit abnehmender Regularisierung (kleinere $\lambda$) werden in den nächsten 3 Schritten die übrigen relevanten Variablen $X_2$, $X_8$ und $X_1$ aktiviert. Über die weiteren Schritte nähert der Algorithmus die Lösung an die *saturierte* Schätzung (das Modell mit allen neun Regressoren) an und aktiviert schrittweise die übrigen, irrelevanten Variablen.

Wir visualisieren die geschätzen Koeffizienten an jedem Schritt des Lösungspfads als Funktion von $\lambda$. In der Praxis wird der Regularisierungsparameter häufig auf der natürlichen log-Skala dargestellt.

```{webr}
# Transformation in ein weites Format
fit_lars$beta %>% 
  as_tibble() %>% 
  mutate(
    lambda = c(fit_lars$lambda, 1e-2)
  ) %>% 
  pivot_longer(
    cols = 1:9, 
    names_to = "Variable", 
    values_to = "gesch. Koeffizient"
  ) %>% 
  
# Visualisierung mit ggplot  
  ggplot(
    mapping = aes(
      x = log(lambda), 
      y = `gesch. Koeffizient`, 
      color = Variable
    )
  ) + 
  geom_line() +
  labs(
    title = "LARS-Lösungspfad für Lasso-Schätzung"
  ) +
  theme_cowplot()
```

In der Abbildung erkennen wir, dass die Shrinkage der geschätzten Koeffizienten nach der Aktivierung rasch abnimmt und sich für kleine Werte von $\lambda$ der KQ-Lösung annähert. Wir sehen auch, dass es einen Bereich von $\lambda$-Werten gibt, für die das wahre Modell mit den Variablen $X_1$, $X_2$, $X_8$ und $X_9$ selektiert werden kann. Je nach Ziel der Analyse kann es sinnvoll sein, ein $\lambda$ in diesem Intervall zu schätzen.

### Wahl des Regularisierungsparameters $\lambda$ für den Lasso-Schätzer

Wie zuvor bei Ridge Regression muss in empirischen Anwendungen ein Wert für den Tuning-Parameter $\lambda$ gewählt werden. Hierbei besteht die Herausforderung darin, einen geeigneten Wert zu finden, der zu wünschenswerten Eigenschaften des resultierenden Modells führt. So ist für gute Vorhersagen wichtig, dass das Modell nicht zu sehr an die Daten angepasst ist (*Overfitting*), um eine gute Generalisierung auf neue Daten zu ermöglichen. Gleichzeitig muss das Modell flexibel genug sein, um wesentliche Eigenschaften des daten-erzeugenden Prozesses hinreichend gut zu erfassen. In der Regel wird hierbei eine sparsame Modellierung angestrebt, die nur eine Teilmenge der Prädiktoren nutzt.

In der Praxis werden verschiedene Verfahren verwendet, um den Wert für den Tuning-Parameter $\lambda$ zu bestimmen. Gängige Methoden sind *Cross Validation* (CV) und Informationskriterien. In Abhängigkeit der Methode und der Daten ergeben sich über- oder unterparameterisierte Modelle. Aufgrund der Implementierung im R-Paket `lars` betrachten wir nachfolgend CV.^[Chetverikov, Liao, and Chernozhukov (2020) zeigen, dass CV zu konsistenter Modellselektion führen kann.] Wir zeigen nun anhand der simulierten Daten aus dem letzten Abschnitt, wie für die LARS-Schätzung ein optimales $\lambda$ mit leave-one-out CV (LOO-CV) bestimmt werden kann. Hierzu nutzen wir `lars::cv.lars()` unter Verwendung derselben Argumente wie zuvor im Aufruf von `lars()`.

```{webr}
# LARS-Lösungen mit CV evaluieren
fit_lars_cv <- cv.lars(
  x = X, 
  y = Y, 
  intercept = F,
  normalize = T,
  type = "lasso", 
  plot.it = F, 
  K = N # für LOO-CV
) 
```

Das Objekt `fit_lars_cv` ist eine Liste mit den CV-Ergebnissen. Wir können diese einfach mit `ggplot` visualisieren. `index` ist hierbei das *Verhältnis* der $\ell_1$-Norm des Lasso-Schätzers für einen spezifischen Wert von $\lambda$ und der $\ell_1$-Norm des KQ-Schätzers. Das optimale $\lambda$ wird so implizit geschätzt. `cv.error` ist der mit CV geschätzte MSE.

```{webr}
# CV-MSE
fit_lars_cv %>% 
  as_tibble() %>%

  ggplot(
    mapping = aes(
      x = index, 
      y = cv.error
    )
  ) + 
  geom_line() +
  labs(
    x = "|beta_lambda| / |beta|",
    y = "CV-MSE",
    title = "CV-MSE vs. rel. Position von Lambda auf dem Lassopfad"
  ) +
  theme_cowplot()
```

In der Grafik erkennen wir ein Minimum des CV-MSEs bei etwa 0.73.

```{webr}
# CV-MSE-minimierendes Lambda bestimmen
ID <- which.min(fit_lars_cv$cv.error) # Index

(
  fraction_opt <- fit_lars_cv$index[ID]
)
```

Die geschätzten Koeffizienten für die optimale Regularisierung können mit `coef()` ausgelesen werden.

```{webr}
# LARS-Lasso-Fit für optimales lambda bestimmen
coef(
  object = fit_lars, 
  s = fraction_opt, 
  mode = "fraction"
)
```

Das Ergebnis veranschaulicht die Selektionseigenschaft von Lasso: Gemäß DGP \eqref{eq:larsdgp} sind die Variablen $X_3$ bis $X_7$ *irrelevante* Prädiktoren für $Y$; ihre wahren Koeffizienten sind $0$. In der kreuzvalidierten Lasso-Schätzung erreicht die Regularisierung, dass die Koeffizienten der Variablen $X_4$ bis $X_7$ tatsächlich mit 0 geschätzt werden. Wir schätzen für das mit CV bestimmte $\lambda$ also ein leicht überspezifiziertes Modell mit den Regressoren $X_1$, $X_2$, $X_3$, $X_8$ und $X_9$. Beachte, dass die Lasso-Schätzung einen Kompromiss impliziert: Die Varianz der Schätzung ist geringer als die des KQ-Schätzers im Modell mit allen Variablen.^[Wegen $N=25$ verbleiben bei der KQ-Schätzung mit 9 Regressoren nur 16 Freiheitsgrade.] Aufgrund der Regularisierung sind die mit Lasso geschätzten Koeffizienten der relevanten Variablen jedoch in Richtung $0$ verzerrt.

Einen positiven Effekt dieses Kompromisses beobachten wir anhand des mittleren Vorhersagefehlers für Daten, die *nicht* zur Berechnung des Schätzers verwendet wurden. Wir vergleichen den Vorhersagefehler nachfolgend anhand eines solchen simulierten Test-Datensatzes mit 25 neuen Beobachtungen. Den Vorhersagefehler bestimmen wir als MSE zwischen den vorhergesagten und den tatsächlichen Ausprägungen für $Y$.

```{webr}
# Test-Datensatz erstellen
set.seed(4321)

new_X <- matrix(rnorm(N * 9), ncol = 9)
new_Y <- new_X %*% beta_v + rnorm(N)

# Lasso: Vorhersage von new_Y für Test-Datensatz
Y_predict_lars <- predict(
  object = fit_lars, 
  s = fraction_opt, 
  type = "fit", 
  mode = "fraction", 
  newx = new_X
)$fit

# Lasso: MSE für Test-Datensatz berechnen
mean((Y_predict_lars - new_Y)^2)
```

Wir schätzen nun das große Modell mit allen 9 Variablen mit KQ und berechnen ebenfalls den MSE der Prognosen für den Test-Datensatz.

```{webr}
# KQ-Schätzung des großen Modells durchführen
KQ_fit <- lm(Y ~ X - 1)

# Test-Datensatz für predict.lm() formatieren
new_X <- as.data.frame(new_X)
colnames(new_X) <- paste0("X", 1:9)

# KQ: Vorhersage von new_Y für Test-Datensatz
Y_predict_KQ <- predict(
  object = KQ_fit, 
  newdata = new_X
)

# KQ: MSE für Test-Datensatz berechnen
mean((Y_predict_KQ - new_Y)^2)
```

Offenbar führt die Lasso-Schätzung zu einem deutlich geringeren MSE der Vorhersage von `Y` für den Test-Datensatz als die KQ-Schätzung und damit zu einer höheren Vorhersagegüte. Das "sparsame" mit Lasso-Regression geschätzte Modell ist dem "großen" mit KQ geschätztem Modell in dieser Hinsicht also überlegen.

::: callout-note
#### Key Facts zu Lasso-Regression

- Lasso-Regression bestraft die Verlustfunktion des KQ-Schätzers mit der $\ell_1$-Norm der Koeffizienten.

- Neben Koeffizientenschätzung mit Shrinkage in Richtung $0$ kann der Lasso-Schätzer Variablenselektion durchführen: Regressionskoeffizienten können exakt mit $0$ geschätzt und so ein "sparsames", leichter zu interpretierendes Modell gewählt werden.

- Wie bei Ridge Regression impliziert die Wahl von $\lambda$ einen Bias-Variance-Tradeoff, der für Vorhersagen nützlich ist: Für größere $\lambda$ wird mehr Verzerrung induziert und möglicherweise relevante Variablen mit kleinen Koeffizienten aus dem Modell entfernt. Ein solches sparsames Modell kann eine höhere Prognosegüte haben als ein komplexes, unregularisiertes Modell.

- Der Lasso-Schätzer $\widehat{\boldsymbol{\beta}}_\lambda^L$ ist *nicht* erwartungstreu.

- Lasso Regression kann bspw. mit dem LARS-Algorithmus (Paket `lars`) oder mit `glmnet` berechnet werden.
:::

## Vergleich von Lasso- und Ridge-Regression mit Simulation

In diesem Kapitel illustrieren wir Vor- und Nachteile von Lasso- und Ridge-Regression in Prognose-Anwendungen anhand von Monte-Carlo-Simulationen. Wir betrachten hierbei datenerzeugende Prozesse, die sich hinsichtlich der Anzahl relevanter Variablen sowie der Korrelation dieser Variablen unterscheiden.^[Aufgrund des hohren Rechenaufwands der Simulationen verzichten wir in diesem Abschnitt und im nächsten Abschnitt auf die Einbindung der WebR-Konsole.]

Die grundlegende Vorschrift für die Simulationen ist \begin{align*}
  Y_i = \sum_{j=1}^{k=40} \beta_j X_{i,j} + u_i, \quad u_i \overset{u.i.v.}{\sim} N(0,1), \quad i=1,\dots,100,
\end{align*} 
wobei die Regressoren $X_j$ eine Varianz von $1$ haben und aus einer multivariaten Normalverteilung mit Korrelation $$\rho\in(0,0.5,0.8)$$ gezogen werden.

Für die Koeffizienten $\boldsymbol{\beta}$ unterscheiden wir zwei Szenarien. In Szenario A ist $$\boldsymbol{\beta} = (1,\dots,1)',$$ d.h. alle Variablen sind relevant und haben denselben Einfluss auf $Y$. In Szenario B erzeugen wir $\boldsymbol{\beta}$ einmalig vorab so, dass $$\beta_j = \begin{cases}1,\quad \text{mit Wsk.  }p\\ 0,\quad \text{mit Wsk.  }1-p, \end{cases}$$ d.h. nur eine Teilmenge der Variablen beeinflusst $Y$ jeweils mit demselben Effekt $\beta_j = 1$. Die übrigen Variablen sind irrelevant.

Als nächstes schätzen und validieren wir die Modelle mit `glmnet()`.

### Prognosegüte in diversen Szenarien {#sec-pdz}

```{r}
# Simulationsparameter definieren
rho <- c(0, 0.5, 0.8)   # Korrelation
k <- 40                 # Anz. Regressoren
N <- 100                # Anz. Beobachtungen
n_sim <- 100            # Anz. Simulationen
```

Damit der Code für die Simulation möglichst wenig repetitiv ist, definieren wir eine Funktion `cv.glmnet_MSE()`, die unter Angabe der Daten `X` und `Y`, des Trainingssets `train` sowie des Parameters `alpha` den gewünschten regularisierten Schätzer under Verwendung von Cross Validation anpasst und den Testset-MSE zurückgibt.

```{r}
# allg. Funktion für Testset-MSE nach CV
cv.glmnet_MSE <- function(X, Y, train, alpha) {
  
  # Modell mit glmnet schätzen; lambda per CV bestimmen
  fit_cv <- cv.glmnet(
    x = X[train,],
    y =Y[train],
    alpha = alpha
  )
  
  # Vorhersagen treffen
  Y_pred <- predict(
    object = fit_cv, 
    s = fit_cv$lambda.min, 
    newx = X[-train,])
  
  return(
    # Testset-MSE berechnen
    mean(
      (Y[-train] - Y_pred)^2
      )
  )
}
```

Wir initialisieren zunächst Matrizen, in welche die MSEs aus den `n_sim` Simulationsdurchläufen reihenweise geschrieben werden. `lasso_mse` und `ridge_mse` haben je eine Spalte für jede Korrelation in `rho`

```{r}
# Matrizen für simulierte MSEs initialisieren...
lasso_mse <- matrix(
  data = NA, 
  nrow = n_sim, 
  ncol = length(rho)
) 
ridge_mse <- lasso_mse

# ... und benennen
colnames(lasso_mse) <- paste0("Kor=", rho)
colnames(ridge_mse) <- colnames(lasso_mse)
```

Für die Simulation iterieren wir mit `purrr::walk()` über den Vektor `rho` sowie über die Laufvariable `1:n_sim`. Beide Schleifen nutzen den Syntax für anonyme Funktionen:

```{r, eval = F}
# Die anonyme Funktion
function(x) return(x)
# ist äquivalent definiert als
\(x) return(x)
```

In jedem Simulationsdurchlauf erzeugen wir den Datensatz entsprechend der obigen Vorschrift, teilen die Daten auf und berechnen MSEs für Lasso- und Ridge-Regression mit `cv.glmnet_MSE()`.

**Szenario A**

```{r}
# Koeffizienten-Vektor definieren
beta <- rep(1, k) 
```

```{r, cache=TRUE}
library(mvtnorm)
library(tidyverse)

set.seed(1234)

# Simulation durchführen
walk(1:length(rho), \(j) {
  
  # Korrelationsmatrix definieren
  Sigma <- matrix(
    data = rho[j], 
    nrow = k, 
    ncol = k
  )
  diag(Sigma) <- 1
  
  walk(1:n_sim, \(i) {
    
  # Daten simulieren
  X <- rmvnorm(
    n = N, 
    mean = rep(0, k), 
    sigma = Sigma
  )
  Y <- X %*% beta + rnorm(N)
    
  # Trainingsdaten definieren
  ID_train <- sample(
    x = c(1:N), 
    size = N/2
  )
    
  # Modelle mit CV schätzen und MSEs berechnen
  # Ridge-Regression
  ridge_mse[i, j] <<- cv.glmnet_MSE(
    X = X, 
    Y = Y, 
    train = ID_train, 
    alpha = 0
  )
  
  # Lasso-Regression
  lasso_mse[i, j] <<- cv.glmnet_MSE(
    X = X, 
    Y = Y, 
    train = ID_train, 
    alpha = 1
  )
  
  })
  
})
```

Beachte, dass hier der Super-Assignment-Operator `<<-` genutzt wird, damit `walk` die Matrizen `ridge_mse` und `lasso_mse` in der globalen Umgebung überschreibt.^[Dies folgt aus der Definition von `walk`. `<-` bewirkt hier lediglich Assignment in der Funktionsumgebung.]

Wir berechnen jeweils den mittleren MSEs, sammeln die Ergebnisse in einer `tibble()` und nutzen `gt()` für die tabellarische Darstellung.

```{r}
#| tbl-cap: "Durchschnittliche Testset-MSEs für Setting A"
#| label: tbl-lrsimA
library(gt)

# Ergebnisse tabellarisch darstellen
tibble(
  Methode = c(
    "Lasso-Regression", 
    "Ridge-Regression"
  ),
) %>%
  bind_cols(
    bind_rows(
      colMeans(lasso_mse),
      colMeans(ridge_mse)  
    )    
  ) %>%
  gt() %>%
  tabopts()
```

Die Tabelle zeigt, dass Ridge-Regression gegenüber Lasso-Regression für jede der drei betrachteten Korrelationen überlegen ist. Insbesondere bei stärker korrelierten Regressoren ist Ridge vorteilhaft.

Für Szenario B überschreiben wir `beta` nach Multiplikation mit einem zufälligen binären Vektor, sodass einige der Koeffizienten $0$ und die zugehörigen Variablen irrelevant für $Y$ sind.

**Szenario B**

```{r}
# Wsk. für Relevanz einer Variable
p <- .3

set.seed(123)

# Koeffizienten-Vektor definieren
beta <- beta * sample(
  x = 0:1, 
  size = k, 
  replace = T, 
  prob = c(1-p, p)
)

# Koeffizienten prüfen
head(beta, n = 10)
```

Eine wiederholung der Simulation für die modifizierten Koeffizienten `beta` und liefert folgende tabellarische Auswertung.

```{r, echo = F, cache=TRUE}
#| tbl-cap: Durchschnittliche Testset-MSEs für Szenario B
#| label: tbl-lrsimB
set.seed(1234)

# Simulation durchführen
walk(1:length(rho), \(j) {
  
  # Korrelationsmatrix definieren
  Sigma <- matrix(
    data = rho[j], # Off-Diagonalelemente
    nrow = k, 
    ncol = k
  )
  diag(Sigma) <- 1
  
  walk(1:n_sim, \(i) {
    
    # Daten simulieren
    X <- rmvnorm(
      n = N, 
      mean = rep(0, k), 
      sigma = Sigma
    )
    Y <- X %*% beta + rnorm(N)
    
    # Trainingsdaten definieren
    ID_train <- sample(
      x = c(1:N), size = N/2
    )
    
    # Modelle mit CV schätzen und MSEs berechnen
    ridge_mse[i, j] <<- cv.glmnet_MSE(
      X = X, 
      Y = Y, 
      train = ID_train, 
      alpha = 0
    )
    lasso_mse[i, j] <<- cv.glmnet_MSE(
      X = X, 
      Y = Y, 
      train = ID_train, 
      alpha = 1
    )
    
  })
  
})

# Ergebnisse tabellarisch darstellen
tibble(
  Methode = c("Lasso", "Ridge"),
) %>%
  bind_cols(
    bind_rows(
      colMeans(lasso_mse),
      colMeans(ridge_mse)  
    )    
  ) %>%
  gt() %>%
  tabopts
```

Die Ergebnisse in @tbl-lrsimB zeigen, dass Ridge-Regression in Szenario B bis auf den Fall unkorrelierter Regressoren etwas schlechter abschneidet als in Szenario A. Die hohe Anzahl irrelevanter Variablen verbessert die Leistung von Lasso deutlich: Hier ist es plausibel, dass Lasso aufgrund der Thresholding-Eigenschaft die Koeffizienten einiger irrelevanten Variablen häufig exakt $0$ setzt und damit ein sparsameres Modell schätzt als Ridge. Entsprechend erzielt Lasso in diesem Szenario insbesondere für $\rho = 0$ genauere Vorhersagen als Ridge Regression.

### Visualisierung des Bias-Variance-Tradeoffs bei Prognosen

Für ein besseres Verständnis, wie sich der Regularisierungsparameter $\lambda$ auf den Bias-Variance-Tradeoff bei Prognosen mit Ridge- und Lasso-Regression auswirkt, vergleichen wir für beide Methoden nachfolgend die Abhängigkeit des MSEs der Prognose $\widehat{Y}_0$ für den Wert $Y_0$ der abhängigen Variable eines Datenpunkts anhand seiner Regressoren $\boldsymbol{X}_0'$, wobei \begin{align}
  \text{MSE}(\widehat{Y}_0) = \text{Bias}(\widehat{Y}_0)^2 + \text{Var}(\widehat{Y}_0) + \text{Var}(Y_0) \label{eq:pbvdecomp}
\end{align} Beachte, dass $\text{Var}(Y_0)$ die durch den datenerzeugenden Prozess (und damit unvermeidbare) Varianz von $Y_0$ ist, wohingegen $\text{Bias}(\widehat{Y}_0)^2$ und $\text{Var}(\widehat{Y}_0)$ von dem verwendeten Schätzer für $\widehat{Y}_0$ abhängt.

Für die Simulation betrachten wir erneut Szenario A aus @sec-pdz mit $50$ Beobachtungen für ein Modell mit $40$ unkorrelierten Regressoren. Wir legen zunächst die Simulationsparameter fest und erzeugen den vorherzusagenden Datenpunkt (`X_0`, `Y_0`).

```{r, cache=TRUE}
# Parameter festlegen
set.seed(1234)
n <- 200 # Anz. Iterationen
N <- 50  # Anz. Beobachtungen
k <- 40  # Anz. Variablen

# Korrelationsmatrix definieren
Sigma <- diag(k) # Diagonalmatrix
beta <- rep(x = 1, k)

# Prognose-Ziel vorab zufällig generieren:

# Regressoren
X_0 <- rmvnorm(
  n = 1, 
  mean = rep(x = 0, k)
)

# Abh. Variable
Y_0 <- X_0 %*% beta + rnorm(n = 1) %>% 
  as.vector()
```

Anhand der Simulationsergebnisse wollen wir die von der verwendeten Schätzfunktion abhängigen Komponenten von \eqref{eq:pbvdecomp} untersuchen. Wir initialisieren hierzu die Listen `ridge_fits` und `lasso_fits`, in die unsere Simulationsergebnisse geschrieben werden.

```{r}
# Listen für Simulationsergebnisse initialisieren
ridge_fits <- list()
lasso_fits <- list()
```

Weiterhin definieren wir separate $\lambda$-Sequenzen für Lasso- und Ridge-Schätzer.^[Die Sequenzen haben wir in Abhängigkeit des DGP so gewählt, dass die Abhängigkeit der Prognosegüte von $\lambda$ gut visualisiert werden kann.]

```{r}
# Lambda-Sequenzen festlegen
lambdas_r <- seq(.25, 2.5, length.out = 100)
lambdas_l <- seq(.05, 0.5, length.out = 100)
```

Für die Simulation iterieren wir mit `walk()` über simulierte Datensätze und schreiben jeweils den vollständigen Output von `glmnet()` in die zuvor definierten Listen `ridge_fits` und `lasso_fits`.

```{r}
library(mvtnorm)

# Simulation
walk(1:n, \(i) {
  
  # Daten simulieren
  X <- rmvnorm(
    n = N, 
    mean = rep(0, k), 
    sigma = Sigma
  )
  Y <- X %*% beta + rnorm(n = N, sd = 5)
  
  # Modelle mit glmnet schätzen
  # Ridge-Regression
  ridge_fits[[i]] <<- glmnet(
    x = X, 
    y = Y, 
    alpha = 0, 
    intercept = F
  )
  # Lasso-Regression
  lasso_fits[[i]] <<- glmnet(
    x = X, 
    y = Y, 
    alpha = 1, 
    intercept = F
  )
  
})
```

Wir nutzen Funktionen aus `purrr` und `dplyr`, um über die in den Simulationsdurchläufen angepassten Modelle zu iterieren. Mit `predict()` erhalten wir Punktvorhersagen für `Y_0` für jedes $\lambda$ der zuvor definierten $\lambda$-Sequenzen. Beachte, dass `map()` jeweils eine Liste mit `r n` Punktvorhersagen für jedes der `r length(lambdas_r)` zurückgibt. Mit `list_rbind()` können wir die Ergebnisse komfortabel jeweils in einer `tibble` sammeln.

```{r}
# Prognosen für Ridge-Regression
pred_r <- map(
  .x = ridge_fits, 
  .f = ~ as_tibble(
    predict(
      object = ., 
      s = lambdas_r, 
      newx = X_0
    )
  ) 
) %>%
  list_rbind() 

# Prognosen für Lasso-Regression
pred_l <- map(
  .x = lasso_fits, 
  .f = ~ as_tibble(
    predict(
      object = ., 
      s = lambdas_l, 
      newx = X_0)
    ) 
) %>%
  list_rbind() 
```

Für die statistische Auswertung berechnen wir jeweils $\text{MSE}(\widehat{Y}_0)$, $\text{Bias}(\widehat{Y}_0)^2$ und $\text{Var}(\widehat{Y}_0)$ und führen die Ergebnisse mit `pivot_longer()` in ein langes Format `sim_data_r` über. Wir berechnen weiterhin mit `MSE_min_r` das $\lambda$, für das wir über die Simulationsdurchläufe durchschnittlich den geringsten $\text{MSE}$ beobachten.

**Ridge-Regression**

```{r, warning=F}
# Ergebnisse für Ridge-Regression zusammenfassen
sim_data_r <- tibble(
  
  lambda = lambdas_r,
  
  "MSE" = map_dbl(
    .x = pred_r,  
    .f = ~ mean((.x - Y_0)^2)
  ),
  
  "Bias^2" = map_dbl(
    .x = pred_r, 
    .f = ~ (mean(.x) - Y_0)^2
  ),
  
  "Varianz" = map_dbl(
    .x = pred_r, 
    .f = ~ var(.x)
  )
) %>%
  pivot_longer(
    cols = -lambda, 
    values_to = "Wert",
    names_to = "Statistik"
  )

# Lambda bei MSE-Minimum bestimmen
MSE_min_r <- sim_data_r %>% 
  filter(
    Statistik == "MSE",
    Wert == min(Wert)
  ) 
```

**Lasso-Regression**

```{r, warning=F}
# Ergebnisse zusammenfassen
sim_data_l <- tibble(
  
  lambda = lambdas_l,
  
  "MSE" = map_dbl(
    .x = pred_l,  
    .f = ~ mean((. - Y_0)^2)
  ),
  
  "Bias^2" = map_dbl(
    .x = pred_l, 
    .f = ~ (mean(.) - Y_0)^2
  ),
  
  "Varianz" = map_dbl(
    .x = pred_l, 
    .f = ~ var(.)
  )
) %>%
  pivot_longer(
    cols = -lambda, 
    values_to = "Wert", 
    names_to = "Statistik"
  )

# Lambda bei MSE-Minimum bestimmen
MSE_min_l <- sim_data_l %>% 
  filter(
    Statistik == "MSE",
    Wert == min(Wert)
  ) 
```

Die Datensätze im langen Format, `sim_data_r` und `sim_data_l`, werden nun für die Visualisierung der Ergebnisse mit `ggplo2` genutzt.

```{r, warning=FALSE}
#| label: fig-MSEBVT
#| fig-cap: "Simulierte MSE-Komponenten in Abhängigkeit von Lambda"
#| fig-subcap: 
#|   - "Ridge Regression"
#|   - "Lasso Regression"
#| layout-ncol: 1
# MSE, Bias^2 und Varianz gegen Lambda plotten

# Ridge-Regression
sim_data_r %>%
  ggplot(
    mapping = aes(
      x = lambda, 
      y = Wert, 
      color = Statistik
    )
  ) +
  geom_line() +
  geom_point(data = MSE_min_r) +
  theme_cowplot()

# Lasso-Regression
sim_data_l %>%
  ggplot(
    mapping = aes(
      x = lambda, 
      y = Wert, 
      color = Statistik
    )
  ) +
  geom_line() +
  geom_point(data = MSE_min_l) +
  theme_cowplot()
```

Anhand von @fig-MSEBVT lässt sich der Bias-Variance-Tradeoff bei der Vorhersage von $Y_0$ gut erkennen: Bereits für kleine $\lambda$ erzielen beide Methode eine deutliche Reduktion des MSE. Dies wir durch etwas zusätzlichen Bias, aber eine überproportionale Verringerung der Varianz erreicht. Der erkennbare funktionale Zusammenhang zeigt, dass der MSE eine konvexe Funktion von $\lambda$ ist. Damit existieren optimale $\lambda$ mit minimalem MSE (grüne Punkte), die wir mit Cross Validation schätzen können.

## Inferenz für Treatment-Effekt-Schätzung mit vielen Variablen

In empirischen Studien des Effekts einer Behandlungsvariable $B$ auf eine Outcome-Variable $Y$ steht häufig eine Vielzahl potentieller Kontrollvariablen zur Verfügung. Häufig ist unklar, welche Variablen in das Modell aufgenommen werden sollten, um das Risiko einer verzerrten Schätzung durch ausgelassene Variablen zu vermindern und gleichzeitig eine Schätzung mit geringer Varianz zu gewährleisten. Ist der Beobachtungsumfang $N$ relativ zur Variablenanzahl $k$ groß, so kann die KQ-Schätzung einer langen Regression (ein Modell mit allen $k$ Kontrollvariablen) gute Ergebnisse liefern. In der Praxis liegt diese wünschenswerte Situation jedoch oft nicht vor und es ist $k\lesssim N$ oder sogar $k>N$. Dann ist eine KQ-Schätzung des Behandlungseffekts anhand aller $k$ Variablen mit hoher Varianz behaftet bzw. gar nicht möglich.^[Beachte, dass der KQ-Schätzer bei $k>N$ nicht lösbar ist.] Ein weiteres Szenario ist $k(N)>N$, d.h. die Anzahl der Regressoren kann mit dem Beobachtungsumfang wachsen.^[Dieses Szenario wird unter Bedingungen bzgl. der Wachstumsrate und der Größe der Koeffizienten betrachet, siehe [@BelloniChernozhukov2013].] Lasso-Verfahren können dann hilfreich sein, um Determinanten von $Y$ *und* $B$ zu identifizieren und damit eine Menge an Kontrollvariablen zu selektieren, für die eine erwartungstreue und konsistente Schätzung des interessierenden Effekts wahrscheinlich ist.

Betrachte zunächst das Modell mit allen Kontrollvariablen $X_j$,
\begin{align}
  Y_i = \beta_0 + \alpha_0 B_i + \sum_{j=1}^k \beta_{j} X_{i,j} + u_i, \label{eq:lassotmt}
\end{align} 
wobei einige $\beta_{j}=0$ sind und wir annehmen, dass $B$ lediglich mit ein paar der $X_j$ korrelliert. Die Shrinkage der geschätzten Koeffizienten aus einer naiven Lasso-Regression von \eqref{eq:lassotmt} führt grundsätzlich zu einer verzerrten Schätzung des Behandlungseffekts $\alpha_0$ und damit zu ungültiger Inferenz.^[@Hahnetal2018 geben eine ausführliche Erläuterung dieser Problematik.]

Die Verzerrung von geschätzten Koeffizienten kann vermieden werden, indem Lasso lediglich zur Selektion von Kontrollvariablen verwendet wird. Dabei wird mit einer Lasso-Regression von $Y$ auf die $X_j$ eine Teilmenge von Regressoren $\mathcal{S}$ selektiert und der Treatment-Effekt anschließend mit der KQ-Schätzung von 
\begin{align}
  Y_i = \beta_0 + \alpha_0 B_i + \sum_{j\in\mathcal{S}} \beta_{j} X_{i,j} + e_i,
\end{align}
basierend auf der Selektion $\mathcal{S}$ berechnet wird.^[Solche Verfahren werden *Post-Selection-Schätzer* gennant.] Ein solcher *Post-Lasso-Selection-Schätzer* [@BelloniChernozhukov2013] ist jedoch im Allgemeinen und insbesondere in hoch-dimensionalen Settings nicht konsistent für $\alpha_0$ und nicht asymptotisch normalverteilt, da weiterhin die Gefahr einer verzerrten Schätzung durch in $\mathcal{S}$ ausgelassene Variablen besteht, die mit $B$ korrelieren: Lasso selektiert Variablen $X_j$, die "gut" $Y$ erklären. Dabei kann nicht ausgeschlossen werden, das ein Modell gewählt wird, dass relevante Determinanten von $B$ auslässt. Selbst wenn wir ein mit Lasso gewähltes Modell mit KQ (d.h. ohne Shrinkage) schätzen, würde $\alpha_0$ verzerrt geschätzt!

@Bellonietal2014 schlagen ein alternatives Verfahren vor, dass auf Selektion der Determinanten $X_j$ von $Y$ und $B$ basiert. Dieses Verfahren wird als *Post-Double Selection* bezeichnet und kann wie folgt implementiert werden:

**Post-Double-Selection-Schätzer**

1. Bestimme die Determinanten $X_j$ von $Y$ mit Lasso-Regression und bezeichne die Menge der selektierten Variablen als $\mathcal{S}_Y$.

2. Bestimme die Determinanten $X_j$ von $B$ mit Lasso-Regression und bezeichne die Menge der selektierten Variablen als $\mathcal{S}_B$.

3. Bestimme die Schnittmenge $\mathcal{S}_{YB} = \mathcal{S}_Y \cap \mathcal{S}_B$. Schätze den Treatment-Effekt als $\widehat{\alpha}_0$ in der KQ-Regression \begin{align}
      Y_i = \beta_0 + \alpha_0 B_i + \sum_{j\in\mathcal{S}_{YB}} \beta_{j} X_{i,j} + v_i.
    \end{align}

@Bellonietal2014 zeigen, dass $\widehat{\alpha}_0$ aus diesem Verfahren ein asymptotisch normalverteiler Schätzer für $\alpha_0$ ist und herkömmliche t-Tests und Konfidenzintervalle gültige Inferenz erlauben.

Wir illustrieren die in diesem Abschnitt betrachteten Schätzer nun anhand simulierter Daten mit R. Die fiktive Problemstellung ist die Schätzung eines wahren Treatment-Effekts $\alpha_0 = 2$, wenn so viele potenzielle Kontrollvariablen vorliegen, dass der KQ-Schätzer gerade noch berechnet werden kann, aber aufgrund hoher Varianz unzuverlässig ist. Hierzu erzeugen wir $Y$ gemäß der Vorschrift \begin{align*}
  Y_i =&\, \alpha_0 B_i + \sum_{j=1}^{k_Y} \beta_{j}^Y X_{i,j}^Y + \sum_{l=1}^{k_{YB}} \beta_{l}^{YB} X_{i,l}^{YB} + u_i,\\
  \\
  \beta_j^{YB} \overset{u.i.v}{\sim}&\,N(10,1), \quad \beta_j^{Y} \overset{u.i.v}{\sim}U(0,1), \quad u_i \overset{u.i.v}{\sim}N(0,1).\\
  \\
  i=&\,1,\dots,550
\end{align*}

Die Behandlungsvariable $B_i$ entspricht der Vorschrift \begin{align*}
  B_i =&\, \sum_{l=1}^{k_{YB}} \beta_{l}^{YB} X_{i,l}^{YB} + e_i,\\
  \\
  \beta_j^{YB} \overset{u.i.v}{\sim}&\,N(2,0.2), \quad e_i \overset{u.i.v}{\sim}N(0,1).
\end{align*} Wir wählen $k_{YB} = k_{Y} = 25$. Zusätzlich zu $B$, den Determinanten von $Y$ *und* $B$ ($X^{YB}$) sowie den Variablen, die ausschließlich $Y$ beeinflussen ($X^{Y}$) gibt es $k_U = 499$ Variablen $X^U$, die weder $Y$ noch $B$ beeinflussen und damit irrelevant für die Schätzung des Behandlungseffekts sind. Wir haben also $N=550$ Beobachtungen und insgesamt $k = 1+k_{Y} + k_{YB} + k_{U} = 550$ potenzielle Kontrollvariablen von denen $k_{YB} = 25$ für eine unverzerrte Schätzung von $\alpha_0$ relevant sind.

Der nachstehende Code generiert die Daten gemäß der Vorschrift.

```{webr}
library(mvtnorm)

set.seed(4321)

n <- 550      # Beobachtungen
p_Y <- 25     # Determinanten Y
p_B <- 25     # Determinanten B *und* Y
p_U <- 499    # irrelevante Variablen 

# Variablen generieren
XB <- rmvnorm(n = n, sigma = diag(p_B))
XU <- rmvnorm(n = n, sigma = diag(p_U))
XY <- rmvnorm(n = n, sigma = diag(p_Y))

# Stetige Behandlungsvariable erzeugen
B <- XB %*% rnorm(p_B, 2, sd = .2) + rnorm(n)

# Abh. Variable erzeugen, Behandlungseffekt (ATE) ist 2
Y <- 2 * B + 
  XB %*% rnorm(p_B, mean = 10) + 
  XY %*% runif(p_Y) + 
  rnorm(n)

# Variablen in tibble sammeln
X <- cbind(B, XB, XU, XY) %>% 
  as_tibble()

# Namen zuweisen
colnames(X) <- c(
  "B", 
  paste0("XB", 1:p_B), 
  paste0("XU", 1:p_U),
  paste0("XY", 1:p_Y) 
)
```

Wünschenswert wäre die KQ-Schätzung des wahren Modells. Diese ergibt eine Schätzung nahe des wahren Treatment-Effekts $\alpha_0 = 2$. Unter realen Bedingungen wäre diese Regression jedoch nicht implementierbar, weil die relevanten Kovariablen `XB` unbekannt sind.

```{webr}
# KQ: Wahres Modell schätzen
lm(
  formula = Y ~ B + XB - 1
)$coefficients["B"]
```

Wir schätzen daher zunächst die "lange" Regression mit allen $k$ verfügbaren Variablen mit KQ. Beachte, dass der KQ-Schätzer für $\alpha_0$ zwar implementierbar und erwartungstreu ist, jedoch eine hohe Varianz aufweist. Wegen $k=N=550$ erhalten wir eine perfekte Anpassung an die Daten und können mangels Freiheitsgraden keine Hypothesentests durchführen.

```{webr}
# KQ: Lange Regression schätzen
lm(
  formula = Y ~ . - 1, 
  data = X
)$coefficients["B"]
```

Die KQ-Schätzung von $\alpha_0$ anhand der langen Regression weicht deutlich vom wahren Wert $\alpha_0 = 2$ ab.

Eine "kurze" KQ-Regression nur mit der Behandlungsvariable $B$ führt wegen Korrelation mit den ausgelassenen Determinanten in `XB` zu einer deutlich verzerrten Schätzung.

```{webr}
# KQ: Kurze Regression
lm(
  formula = Y ~ B - 1
)$coefficients["B"]
```

Die Methoden von @BelloniChernozhukov2013 und @Bellonietal2014 sind im R-Paket `hdm` implementiert. Mit den Funktionen `hrm::rlasso()` und `hdm::rlassoEffect` kann Lasso-Regression sowie Post- und Double-Post-Selection durchgeführt werden.^[Diese Funktionen ermitteln ein optimales $\lambda$ mit dem in @Bellonietal2012 vorgeschlagenen Algorithmus.]

Wir berechnen zunächst den naiven Lasso-Schätzer in einem Modell mit allen Variablen.

```{webr}
library(hdm)

# Naiver Post-Lasso-Schätzer
lasso <- rlasso(
  x = X, 
  y = Y, 
  intercept = F, 
  post = F
)

# Koeffizientenschätzer auslesen
lasso$coefficients["B"] 
```

Auch dieser Schätzer ist deutlich verzerrt. Problematisch ist hier nicht nur die Shrinkage auf $\widehat{\alpha}_0$, sondern die Selektion der Variablen in `XB`:

```{webr}
# Welche Variablen in XB selektiert Lasso *nicht*?
nselektiert <- which(lasso$coef[1:26] == 0) # ID

# Namen auslesen
names(lasso$coef[1:26])[nselektiert]
```

Durch das Auslassen dieser Determinanten von $Y$ und $B$ leidet der Lasso-Schätzer unter OVB.

Als nächstes berechnen wir den Post-Lasso-Selection-Schätzer.

```{webr}
# Post-Lasso-Selection-Schätzer berechnen
p_lasso <- rlasso(
  x = X,
  y = Y, 
  intercept = F, 
  post = T
)

# Schätzung für alpha_0
p_lasso$coef["B"]
```

Die Ähnlichkeit der Post-Lasso-Schätzung von $\alpha_0$ zur Lasso-Schätzung zeigt deutlich, dass die Verzerrung des Lasso-Schätzers überwiegend durch ausgelassene Variablen anstatt durch Shrinkage verursacht wird.

Mit `rlassoEffect()` können wir den Post-Double-Selection-Schätzer berechnen.

```{webr}
# Post-Double-Selection-Schätzer
pds_lasso <- rlassoEffect(
  x = X %>% 
    dplyr::select(-B) %>% 
    as.matrix(),
  y = Y, 
  d = B, 
  method = "double selection"
)

# Schnittmenge der selektierten Determinanten 
# von Y und B
(
  S_BY <- names(
    which(pds_lasso$selection.index)
  )
)
```

Double Selection führt ebenfalls zu einem Post-Lasso-KQ-Schätzer mit allen 25 relevaten Variablen in `XB`. Wir selektieren allerdings deutlich weniger irrelevante Variablen aus `XU` als mit Single Selection und dennoch einige Determinanten von $Y$ aus `XY`. Double Selection führt also zu einer unverzerrten Schätzen mit geringerer Varianz. Mit `summary()` erhalten wir gültige Inferenz bzgl. des Treatment-Effekts.

```{webr}
summary(pds_lasso)
```

Der Post-Double-Selection-Schätzer liefert unter den betrachteten Verfahren die beste Schätzung von $\alpha_0$ und erlaubt gültige statistische Inferenz mit den von `summary()` berechneten Inferenzstatistiken. Der geschätzte Effekt ist hoch-signifikant.

::: callout-note
#### Key Facts zum Post-Double-Selection-Schätzer

-   Durch die sorgfältige Auswahl von Variablen, die mit Behandlung- und Outcome-Variable zusammenhängen, ermöglicht die Double-Selection eine bessere Kontrolle über das Risiko ausgelassender Variablen in Beobachtungsstudien und ermöglicht gültige (asymptotisch normale) Inferenz.

-   Der Post-Double-Selection-Schätzer besteht aus drei Regressionen:

    1.  Es werden Variablen mit Lasso selektiert, welche die *Behandlungsvariable* erklären.
    2.  Es werden Variablen mit Lasso selektiert, welche die *Outcome-Variable* erklären.
    3.  Der Post-Double-Selection-Schätzer ist der KQ-Schätzer in einer Regression, die für die Schnittmenge der ausgewählten Variablen kontrolliert.

-   Dank der Selektion mit Lasso kann der Schätzer auch bei hoch-dimensionalen Daten ($k>n$) angewendet werden.

-   Post-Double-Selection-Schätzer für Behandlungseffekte sind im R-Paket `hdm` implementiert.
:::

### Case Study: Makroökonomisches Wachstum

Zur Illustration des Post-Double-Selection Schätzers betrachten wir eine empirische Anwendung bzgl. der Validierung von makroökonomischer Wachstumtheorie. Aus neo-klassischen Ansätzen wie dem [Solow-Swan-Modell](https://de.wikipedia.org/wiki/Solow-Modell) kann die Hypothese, dass Volkswirtschaften zu einem gemeinsamen Wachstumspfad hin konvergieren, abgeleitet werden. Diese Konvergenzhypothese impliziert die Existenz von Aufholeffekten: Ärmere Volkswirtschaften müssen im mittel schneller Wachsen als die Wirtschaft wohlhabender Länder. Die grundlegende Spezifikation eines entsprechenden Regressionsmodells lautet
\begin{align}
  \text{WR}_{i} = \alpha_0 \text{BIP0}_i + u_i, \label{eq:growthmodel1}
\end{align} 
wobei $\text{WR}_{i}$ die Wachstumsrate des Pro-Kopf-BIP in Land $i$ über einen Zeitraum (typischerweise berechnet als Log-Differenz zwischen zwei Perioden) und $\text{BIP0}_i$ das (logarithmierte) Pro-Kopf-BIP zu beginn der Referenzperiode ist. Gemäß der Konvergenzhypothese muss $\alpha_0<0$ sein: Je wohlhabender eine Volkswirtschaft ist, desto geringer ist das erwartete Wirtschaftswachstum.

Um eine Verzerrung durch ausgelassene Kovariablen zu vermeiden, sollte das Modell \eqref{eq:growthmodel1} um länder-spezifische Regressoren $x_{i,j}$, die sowohl das Ausgangsniveau $\text{BIP0}$ als auch die Wachtumsrate beeinflussen, erweitert werden. Zu der großen Menge potentieller Kovariablen gehören makro- und sozio-ökonomische Maße wie bspw. die Investitionstätigkeit des Staates, die Offenheit der Volkswirtschaft, das politische Umfeld, das Bildungsniveau, die Demographie, usw. Eine bevorzugte Spezifikation ist daher 
\begin{align}
  \text{WR}_{i} = \alpha_0 \text{BIP0}_i + \sum_{j=1}^k \beta_j x_{i,j} + u_i,\label{eq:growthmodel2}
\end{align}
wobei $\alpha_0$ als Behandlungseffekt interpretiert werden kann. Beachte, dass \eqref{eq:growthmodel2} eine Regression in der Form von \eqref{eq:lassotmt} ist.

Wir illustrieren die Schätzung von und Inferenz bzgl. $\alpha_0$ in \eqref{eq:growthmodel2} mit Post-Double-Selektion für einen 90 Länder umfassenden Auszug aus dem Datensatz von @BarroLee2013, der als Objekt `GrowthData` im R-Paket `hdm` verfügbar ist.^[Eine ausführliche Beschreibung der Variablen ist [hier](https://www2.nber.org/pub/barro.lee/readme.txt) einsehbar.]

#### Analyse mit R

```{r}
# Datensatz in Arbeitsumgebung verfügbar machen
library(hdm)
data(GrowthData)

# Anzahl Beobachtungen und Variablen
dim(GrowthData)
```

Die Spalte `Outcome` ist die jeweilige Wachstumsrate des BIP zwischen den Perioden 1965-1975 und 1975-1985 und `gdpsh465` ist das reale Pro-Kopf-BIP im Jahr 1965 zu Preisen von 1980.

Wir führen zunächst eine grafische Analyse hinsichtlich des einfachen Modells \eqref{eq:growthmodel1} durch, indem wir `gdpsh465` gegen `Outcome` plotten und die geschätzte Regressionsgerade einzeichnen.

```{r}
# Einfache grafische Analyse mit ggplot2
GrowthData %>%
  ggplot(
    mapping = aes(
      x = gdpsh465, 
      y = Outcome
    )
  ) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  labs(
    title = "BIP-Wachstum: Einfache Regression"
  ) +
  theme_cowplot()
```

Die Grafik zeigt einen geringen positiven geschätzten Effekt $\widehat{\alpha}_0$. Eine Auswertung mit `lm()` ergibt, dass der Effekt $\alpha_0$ nicht signifikant von $0$ verschieden ist.

```{r}
# Einfache Regression durchführen, 
# Inferenz für gdpsh465 erhalten
lm(Outcome ~ gdpsh465, data = GrowthData) %>%
  summary() %>%
  coefficients() %>% 
  .[2, ]
```

Der positive Effekt aus der einfachen Schätzung widerspricht der Konvergenzhypothese. Dieses Ergebnis könnte allerdings durch Auslassen relevanter Kovariablen ungültig sein. Beispielsweise ist es plausibel, dass das Bildungsniveau einer Volkswirtschaft sowohl mit dem BIP korreliert ist als auch die Wachstumsrate beeinflusst. Dann wäre das Bildungsniveau eine relevante Kovariable, deren Auslassen zu einer verzerrten Schätzung von $\alpha_0$ führt.

Eine "lange" Regression mit allen Kovariablen ist zwar möglich, aber problematisch: Das Verhältnis von Beobachtungen (90) zu Regressoren (62) bedeutet eine hohe Unsicherheit der Schätzung.

```{r}
# Inferenz für alpha_0 in langer Regression
summary(
  lm(Outcome ~ . - 1 , data = GrowthData)
  ) %>% 
  coefficients() %>% 
  .[2, ]
```

Der geschätzte Koeffizient $\widehat{\alpha}_0$ ist nun zwar negativ, liefert jedoch weiterhin keine Evidenz, dass $\alpha_0$ von 0 verschieden ist. Ein Vergleich der Standardfehler zeigt aber, dass die KQ-Schätzung aufgrund Berücksichtigung aller potentiellen Kovariablen mit deutlich größerer Varianz behaftet ist als in der einfachen KQ-Regression \eqref{eq:growthmodel1}

Post-Double-Selection erlaubt gültige Inferenz bzgl. $\alpha_0$ nach Schätzung der Menge relevanter Kovariablen. Wir weisen die entsprechenden Variablen R-Objekten zu und berechnen den Schätzer.

```{r}
# Variablen für Post-Double-Selection vorbereiten

# abh. Variable
y <- GrowthData %>% 
  pull(Outcome)

# "Treatment"
d <- GrowthData %>% 
  pull(gdpsh465)

# potentielle Regressoren
X <- GrowthData %>% 
  dplyr::select(
    -Outcome, -intercept, -gdpsh465
  )
```

```{r}
# Post-Double-Selection-Schätzer berechnen
Growth_DS <- 
  rlassoEffect(
    x = X %>% 
      as.matrix(), 
    y = y, 
    d = d, 
    method = "double selection"
)
```

Post-Double-Selection wählt aus der Menge potentieller Kovariablen lediglich sieben Regressoren aus.

```{r}
# Selektierte Variablen einsehen
# ID
Selektion <- Growth_DS$selection.index

# Namen auslesen
names(
  which(Selektion == T)
)
```

@tbl-growthpdssek zeigt die Definitionen der ausgewählten Variablen.

```{r, echo = F}
#| tbl-cap: "Mit PDS selektierte Variablen aus `GrowthData`. Referenzjahr 1965."
#| label: tbl-growthpdssek
tibble(
  Variable = c(
    "bmp1l", "freetar", "hm65", "sf65", 
    "lifee065", "humanf65", "pop6565"
    ),
  Beschreibung = c(
    "Schwarzmarktprämie d. Währung",
    "Maß für Zollbeschränkungen",
    "Einschreibungsquote Uni (Männer) ",
    "Beschulungsquote Sekundarstufe (Frauen)",
    "Lebenserwartung bei Geburt",
    "Durchschn. Bildung im Alter 25 (Frauen)",
    "Anteil Bevölkerung ü. 65 Jahre"
  )
) %>%
  gt() %>%
  tabopts()
```

```{r}
# Gültige Inferenz mit dem Post-Double-Selection-Schätzer
summary(Growth_DS)
```

Das Ergebnis der Post-Double-Selection-Schätzung unterstützt die (bedingte) Konvergenzhypothese mit einer signifikanten negativen Schätzung.

## Zusammenfassung

Regularisierte Regression erweitert klassische KQ-Schätzung durch Hinzufügen von Straftermen zur Verlustfunktion, um Schätzungen auch in Situationen mit stark korrelierten Regressoren sowie in hoch-dimensionalen Settings zu ermöglichen. Die zwei wichtigsten Varianten sind Ridge- und Lasso-Regression. Der methodische Kern hierbei ist die Regulierung der Koeffizientenschätzung durch einen Tuning-Parameter, der einen Kompromiss zwischen Verzerrung und Varianz der Schätzung steuert. Ridge-Regression führt zu einer proportionalen Schrumpfung der Koeffizienten (*Shrinkage*), während Lasso-Regression zusätzlich Variablenselektion durch exakte Null-Schätzungen einzelner Koeffizienten ermöglicht (*Soft Thresholding*). Regularisierte Regression eignet sich besonders für Prognoseaufgaben, wo eine gewisse Verzerrung zugunsten geringerer Varianz in Kauf genommen werden kann. Die Anwendung dieser Schätzer in R haben wir mit den Paketen `glmnet` und `lars` demonstriert.

Für Schätzung von Behandlungseffekten bei vielen potentiellen Kontrollvariablen wurde der Post-Double-Selection-Schätzer vorgestellt, der Lasso zur Variablenselektion nutzt und anschließend unverzerrte Schätzungen mit klassischen Standardfehlern ermöglicht. Diese Methode kann in R mit dem Paket `hdm` berechnet werden.
