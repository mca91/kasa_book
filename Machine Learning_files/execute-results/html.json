{
  "hash": "b6faf8283b4dd802d769bd95efa6aab7",
  "result": {
    "engine": "knitr",
    "markdown": "---\nformat: live-html\nengine: knitr\n---\n\n\n\n\n\n\n# Neuronale Netzwerke\n\nNeuronale Netze (NN) sind leistungsstarke Modelle, die darauf spezialisiert sind, komplexe Muster in Daten zu erkennen und sind damit insbesondere ein hilfeiches Tool für Prognosen. Ein Nachteil neuronaler Netze ist die mangelnde Fähigkeit, kausale Zusammenhänge zu identifizieren und abzuleiten. Diese Limitation stellt eine signifikante Einschränkung dar, insbesondere für den Einsatz in empirischen Disziplinen, in denen das Verständnis kausaler Beziehungen von entscheidender Bedeutung ist. Während NN effektiv komplizierte Strukturen abbilden können, sind sie nicht mit den notwendigen Mechanismen ausgestattet, um Kausalität zu modellieren oder gar zu identifizieren. Grund hierfür ist die fehlende explizite Berücksichtigung kausaler Beziehungen und des zugrunde liegenden datenerzeugenden Prozesses: NN lernen lediglich funktionale Zusammenhänge in den Trainingsdaten. Auch wenn hierdurch komplexeste Relationen abgebildet werden können, erlaubt ein angepasstes Netz keine Differenzierung zwischen einer Korrelation und einer tatsächlichen kausalen Beziehung zwischen Variablen.\n\nWeiterhin ist statistische Inferenz, etwa mit Konfidenzintervallen oder p-Werten, bei neuronalen Netzen grundsätzlich nicht wie gewohnt anwendbar: Das liegt insbesondere an der komplexen nicht-linearen Struktur und der Interaktion vieler Parameter, die schwer isoliert (und interpretiert) werden können.\n\nIn diesem Kapitel erläutern wir die Funktionsweise und Anpassung von NN und diskutieren deren Anwendung zur Prognose von Zielvariablen in Datensätzen mit vielen Variablen und Beobachtungen in R mit `keras`. Die hier erläuterten Grundlagen basieren auf den einleitenden Kapiteln in @Bishop2007 und @Goodfellowetal2016. Für ausführliche Erläuterungen der R-API `keras` für die gleichnamige Python-Bibliothek empfehlen wir @Allaire2018.\n\n## Grundlagen und Vokabeln {#sec-nn-basics}\n\nNN bestehen aus einer (often großen) Anzahl so genannter *künstlicher Neuronen*. Ein Neuron ist eine mathematische Funktion, die mehrere Eingaben empfängt, diese unter Verwendung von Gewichten linear kombiniert und eine Ausgabe durch Verwendung einer Aktivierungsfunktion generiert.\n\nDie Neuronen eines NN sind in Schichten (*Layers*) organisiert. Jedes Layer verarbeitet die Eingabedaten und gibt die Ergebnisse an das nächste Layer weiter, wobei die Neuronen verschiedener Layer miteinander verknüpft werden. Während das Eingabe-Layer (*Input*) die \"Rohdaten\" (bspw. beobachtete Regressorwerte) aufnimmt und sie an die erste versteckte Schicht (*Hidden Layer*) weiterleitet, ist die Hauptaufgabe der Neuronen in den Hidden Layers, komplexe Muster und Merkmale in den Daten zu erkennen und zu verarbeiten. Jedes Hidden Layer transformiert die empfangenen Daten anhand seiner Neuronen, bevor diese an das nächste Layer weitergeleitet werden. Das letzte Layer in einem neuronalen Netzwerk ist das Ausgabe-Layer (*Output Layer*), das die endgültige Vorhersage für die Outcome-Variable basierend auf den verarbeiteten Daten liefert.\n\nDie Stärke der Verknüpfungen zwischen den Neuronen wird durch die Gewichte $w$ bestimmt, welche während des Trainingsprozesses angepasst werden, um das Modell hinsichtlich der (Vorhersage) einer Zielvariable zu optimieren. Die $w$ bestimmen, wie stark die Aktivierung eines Neurons in einer Schicht die Aktivierung der Neuronen in der nächsten Schicht beeinflusst. Das Netzwerk kann so tiefe und abstrakte Strukturen eines Datensatzes abbilden. \n\n\n\n\n\n\n```{dot}\n//| fig-width: 6\n//| fig-height: 4\n//| fig-align: 'center'\n//| label: fig-nnex\n//| fig-cap: \"Neuronales Netzwerk mit einem Hidden Layer\"\ndigraph NNEX {\n    layout=neato;\n    fontname=\"Helvetica,Arial,sans-serif\";\n    node [fontname=\"Helvetica,Arial,sans-serif\", shape=\"circle\", style=filled, fontsize=16];\n    edge [fontname=\"Helvetica,Arial,sans-serif\", fontsize=12];\n\n    // Eingabeschicht\n    X1 [label=\"X1\", pos=\"0,1!\", fillcolor=lightblue];\n    X2 [label=\"X2\", pos=\"0,-1!\", fillcolor=lightblue];\n\n    // Versteckte Schicht\n    V1 [label=\"V1\\n(A)\", pos=\"3,2!\", fillcolor=lightyellow];\n    V2 [label=\"V2\\n(A)\", pos=\"3,0!\", fillcolor=lightyellow];\n    V3 [label=\"V3\\n(A)\", pos=\"3,-2!\", fillcolor=lightyellow];\n\n    // Ausgabeneuron\n    Y [label=\"Y\\n(A)\", pos=\"6,0!\", fillcolor=lightgreen];\n\n    // Kanten von Eingabeschicht zur versteckten Schicht\n    X1 -> V1 [label=\"w11\"];\n    X1 -> V2 [label=\"w12\"];\n    X1 -> V3 [label=\"w13\"];\n    X2 -> V1 [label=\"w21\"];\n    X2 -> V2 [label=\"w22\"];\n    X2 -> V3 [label=\"w23\"];\n\n    // Kanten von der versteckten Schicht zur Ausgabeschicht\n    V1 -> Y [label=\"w31\"];\n    V2 -> Y [label=\"w32\"];\n    V3 -> Y [label=\"w33\"];\n}\n```\n\n\n\n\n\n\nAngenommen wir interessieren uns für die Vorhersage einer Outcome-Variable $Y$ mit den Regressoren $X_1$ und $X_2$. @fig-nnex zeigt ein mögliches NN mit 3 Neuronen $V_1$, $V_2$, $V_3$ in einem Hidden Layer. Die Neuronen im Hidden Layer empfangen Eingaben aus dem Input Layer, bestehend aus Beobachtungen der Variablen $X_1$ und $X_2$, und gewichten diese Informationen gemäß der Vorschrift\n\n\\begin{align*}\n  h_i = A\\left(\\sum_{j=1}^{2} w_{ji} \\cdot x_j + b_i\\right) \\quad \\text{für } i = 1, 2, 3.\n\\end{align*}\n\nHierbei sind $w_{ji}$ die Gewichte der Verbindung von Input $j$ zu Neuron $i$ und $b_i$ ist ein *Bias*.^[Der Bias ist analog zur Konstante in einer Regression.] $A(\\cdot)$ ist eine Aktivierungsfunktion, die in Abhängigkeit der zu modellierenden Daten gewählt wird.\n\nDas Ausgabe-Neuron für $Y$ verarbeitet die Informationen aus dem Hidden Layer ebenfalls anhand einer Linearkombination, die mit einer Aktivierungsfunktion transformiert wird,\n\n\\begin{align*}\n  y = A\\left(\\sum_{i=1}^{3} w_{i} \\cdot h_i + b_y\\right).\n\\end{align*}\n\nEin solches NN \"lernt\" Relationen zwischen $Y$ und den Regressoren $X_1$ und $X_2$, indem die Gewichte anhand eines Algorithmus derart gewählt werden, dass der Fehler zwischen den vorhergesagten und den tatsächlichen Werten von $Y$ --- gemessen mit einer Verlustfunktion (*Loss-Funktion*) --- minimiert wird. Dieser Lernprozess erfolgt unter Verwendung numerischer Optimierungsverfahren wie *Gradientenabstieg* (*Gradient Descent*).\n\n\n### Training Neuronaler Netze\n\nDer Anpassungsprozess eines NN an einen Datensatz (*Training*) wird grob durch folgende Schritte bestimmt:\n\n1. Das Netz (Gewichte) wird initialisiert. \n\n2. Die Inputs jeder Beobachtung im Trainingsdatensatz werden durch das NN geleitet (*Forward Pass*): Jedes Layer transformiert die Daten mit Hilfe von Gewichten und Aktivierungsfunktionen, um eine Vorhersage von $Y$ zu erzeugen.\n\n3. Der Loss wird berechnet, indem die Vorhersage von $Y$ mit dem tatsächlichen Wert verglichen wird. Die Verlustfunktion wird entsprechend der Definition von $Y$ gewählt. Typische Verlustfunktionen sind *Quadratic Loss* (analog zur Schätzung von linearen Regressionsmodellen mit KQ) oder *Logistic Loss* (analog zu logistischer Regression).\n\n4. Zur Anpassung der Gewichte wird der Gradient^[Der Gradient einer Funktion $f(\\boldsymbol{x}) = f(x_1, x_2, \\ldots, x_k)$ ist der Vektor der partiellen Ableitungen: $\\nabla f = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_k} \\right)$. $\\nabla f(\\boldsymbol{x})$ zeigt die Richtung und Stärke der steilsten Änderung von $f$ am Punkt $\\boldsymbol{x}$ an.] der Verlustfunktion hinsichtlich der Gewichte des NN ermittelt.^[$\\nabla f$ ist in NN grundsätzlich unbekannt. Gradient-Desenct-Algorithmen verwenden numerische Verfahren, um den Gradienten anhand von $f$ zu approximieren.] Ein Gradient-Descent-Algorithmus bestimmt, in welche Richtung die Gewichte verändert werden müssen, um den Vorhersagefehler zu verringern.\n\n    Für diese Berechnung wird ein *Backward Pass* (auch *Backpropagation* genannt) genutzt. Hierbei wird der anhand des Ausgabelayers ermittelte Loss rückwärts durch das Netzwerk propagiert, um die Gewichte so anzupassen, dass der Fehler bei der Vorhersage von $Y$ minimiert wird.\n\n5. Die Gewichte werden in kleinen Schritten, die durch die so genannte *Lernrate* bestimmt werden, in Richtung des negativen Gradienten angepasst. Dies bewirkt, dass die Gewichte so verändert werden, dass der Loss im Vergleich zur letzten Iteration verringert wird.\n\n    Um den Lernprozess effizienter und stabiler zu machen, nutzen moderne Algorithmen weitere Schritte, bspw. eine Kombination von Gradientenabstieg mit *Momentum*. Dies beschleunigt die Anpassung der Gewichte und stabilisiert den Lernprozess. Fortgeschrittene Methoden verwenden adaptiven Lernraten, die die Schrittgröße für jedes Gewicht individuell anpassen können.\n\nDie Schritte 4 und 5 werden wiederholt, bis ein Abbruchkriterium erfüllt ist: Der Fehler ist ausreichend klein, oder weitere Iterationen bewirken keine signifikante Änderung des Gradienten. \n\n**Epochen und Iterationen**\n\nDer Gesamte Prozess wird für mehrere Epochen (*Epocs*) durchlaufen, in denen jeweils der gesamte Trainingsdatensatz durch das NN geleitet wird. Um das Training auch für große Datensätze durchführen zu können, werden die Trainingsdaten hierbei üblicherweise in zufällig zusammengesetzen, kleineren Datensätzen (*Batches*) gruppiert. In jeder Epoche erfolgt die Anpassung der Gewichte für jedes durch das Netz geleitete Batch (jede *Iteration*):\n\n1. **Epoche**\n\n    1. **Batch**\n\n        *Forward Pass* $\\rightarrow$ *Loss-Berechnung* $\\rightarrow$ *Backpropagation* $\\rightarrow$ *Gradient-Descent-Update*\n    \n    2. **Batch**\n  \n        *Forward Pass* $\\rightarrow$ *Loss-Berechnung* $\\rightarrow$ *Backpropagation* $\\rightarrow$ *Gradient-Descent-Update*\n  \n        ...\n    \n2. **Epoche**\n\n            ...\n    \n    ...\n\nFür das Training eines NN sind mehrere Epochen notwendig, weil ein einzelner Durchlauf der Daten oft nicht ausreicht, um die zugrundeliegenden Muster zu lernen. Durch Anpassung über mehrere Epochen können die Gewichte des Modells verfeinert werden, was insbesondere die Fähigkeit zur Generalisierung für ungesehene Daten verbessert. Die zufällige Einteilung der Daten in Batches zu Beginn jeder Epoche verhindert unter anderem, dass das NN lediglich die Reihenfolge der durchgeleiteten Datenpunkte lernt. \n\nDie Anzahl an zu durchlaufender Epochen ist ein Tuning-Parameter: Zu wenige Epochen führen zu einer schlechten Anpassung an die Daten, während zu viele Epochen das Risiko von Overfitting erhöhen. Um den Vorhersagefehler für ungesehene Daten einzuschätzen, wird ein Testdatensatz vorbehalten. Dieser Datensatz wird während des Trainings nicht zum Anpassen der Gewichte genutzt, sondern erst nach Abschluss einer Epoche für die Berechnung der Vorhersagequalität herangezogen. So kann jeweils nach dem Durchlauf einer Epoche beurteilt werden, wie gut das Modell auf neue, unbekannte Daten generalisiert. Hierbei können ein hoher Vorhersagefehler für den Testdatensatz und ein (viel) geringerer Fehler für den Trainingsdatensatz nach mehreren Epochen auf Overfitting hinweisen. Im empirischen Teil dieses Kapitels diskutieren wir (grafische) Methoden zur Beurteilung der Anpassung des Modells.\n\nBeim Training von NN können sogenannte *Callback-Funktionen* eingesetzt werden, um den Anpassungsprozess unter Einbezug von Zwischenergebnissen zu bestimmten Zeitpunkten während des Trainingsprozesses, z. B. am Ende jeder Epoche oder nach einer bestimmten Anzahl von Iterationen, zu evaluieren. Callbacks werden verwendet, um bestimmte Aktionen auszuführen, wie das Anpassen der Lernrate oder das Überwachen der Trainingsleistung: Ein Callback kann das Training automatisch stoppen (*Early Stopping*), wenn Anzeichen von Overfitting erkannt werden, beispielsweise wenn die Vorhersagegüte auf dem Test-Datensatz über mehrere Epochen hinweg stagniert. Dadurch wird ein unnötiges Fortsetzen des Trainings vermieden und ein Verlust der Generalisierungsfähigkeit auf neuen Daten verhindert.\n\nWir fassen die wichtigsten Begriffe für die Beschreibung von NN nachfolgend kurz zusammen.\n\n**Wesentliche Definitionen**\n\n- **Layer**: Eine Ebene von Neuronen im neuronalen Netzwerk. Es gibt das Eingabe-Layer, versteckte Layers (Hidden Layers) und das Ausgabe-Layer. Jedes Layer verarbeitet Informationen aus dem vorangegangenen Layer und gibt die Ergebnisse an das nächste Layer weiter.\n\n- **Input**: Die Eingangsdaten oder Merkmale, die in das NN eingespeist werden. Jeder Input wird durch ein oder mehrere Neuronen im Eingabe-Layer repräsentiert.\n\n- **Output**: Das Ergebnis, welches das NN nach der Verarbeitung der Inputs liefert. Der Output wird durch die Neuronen im Output-Layer des NN erstellt.\n\n- **Neuron**: Die kleinste Komponente eines NN. Jedes Neuron empfängt Inputs, multipliziert sie mit Gewichten, addiert einen Bias und gibt das Ergebnis nach Anwendung einer Aktivierungsfunktion weiter: Ein Neuron ist also eine *mathematische Funktion*, die Inputs aus dem vorherigen Layer mit einer transformierten Linearkombination verarbeitet und das Ergebnis das nächste Layers weiterleitet. \n\n- **Forward Pass**: Leitung der Trainingsdaten durch das NN und Berechnung der Vorhersage des Outcomes.\n\n- **Loss-Funktion**: Mathematische Funktion, welche die Güte der Vorhersage des NN für das Outcome quantifiziert. Der Loss ist eine Funktion der zu trainierenden Parameter des NN.\n\n- **Backward Pass / Backpropagation**: Ermittlung des Gradienten der Loss-Funktion durch Verkettung des Effekts der Gewichte über die Layers des NN.\n\n- **Aktivierungsfunktion**: Eine mathematische Funktion, die auf die gewichtete Summe der Eingaben eines Neurons angewendet wird. Die Aktivierungsfunktion bestimmt, ob ein Neuron aktiviert wird. Beispiele sind \n\n    \\begin{align*}\n      \\textup{ReLU}(z) =& \\max(0, z), \\\\[.5ex]\n      \\sigma(z) =&\\, \\frac{1}{1 + e^{-z}}, \\\\[.5ex]\n      \\tanh(z) =&\\, \\frac{e^z - e^{-z}}{e^z + e^{-z}}.\n    \\end{align*}\n\n- **Epoche**: Ein Trainingszyklus, bei dem der gesamte Trainingsdatensatz, aufgeteilt in Batches, das NN durchläuft.\n\n- **Batches**: Zufällig eingeteilte Teilmengen der Beobachtungen des Trainingsdatensatzes.\n\n- **Callback**: Eine Funktion, die im Zuge der Überwachung des des Trainings-Prozesses automatisch ausgeführt wird, um Aktionen wie Lernratenanpassung oder Trainingsstopp zu auszulösen.\n\nIm nächsten Abschnitt erläutern wir die Optimierung der Gewichte mit Gradient Descent beispielhaft anhand interaktiver Visualisierungen.\n\n## Optimierung mit Gradient Descent\n\nGradient Descent ist ein iteratives Optimierungsverfahren zur Minimierung einer differenzierbaren Zielfunktion $f(w)$. Ausgehend von einem Startwert $w_0$ aktualisiert der Algorithmus die Variable $w$ schrittweise gemäß einer Lernrate $\\eta$ in die entgegengesetzte Richtung des Gradienten $\\nabla f(w)$ der Funktion an der aktuellen $w$. Mit $\\nabla f(w)$ wird mathematisch die Richtung des *steilsten Anstiegs* von $f(w)$ im Punkt $w$ ermittelt. Der Algorithmus vollzieht eine Veränderung von $w$ in die entgegengesetzten Richtung -- die Richtung mit dem schnellsten *Abstieg* (Descent) der Zielfunktion.\n\nDer folgende Algorithmus zeigt die grundlegende Vorgehensweise des Gradientenabstiegsverfahrens für einen einziegen zu optimierenden Parameter $w$ unter Einbeziehung eines Momentum-Terms $v_t$.^[In der Literatur wird $v_t$ häufig auch als *Velocity* bezeichnet.] Der Momentum-Term dient dazu, das Konvergenzverhalten zu verbessern und lokale Minima effektiver zu überwinden. Die Stärke des Momentums $v_t$ wird durch den Momentum-Faktor $\\alpha \\in [0,1)$ bestimmt. \n\n\\begin{align*}\n  \\small\n  & \\textbf{Algorithmus: Gradientenabstiegsverfahren mit Momentum} \\\\\n  & \\textup{Initialisiere: }\\\\[.5ex]\n  & \\quad w_0 \\text{ (Startpunkt) }\\\\\n  & \\quad \\eta \\text{ (Lernrate) }\\\\\n  & \\quad \\alpha \\text{ (Momentum-Faktor) }\\\\ \n  & \\quad v_0 = 0 \\text{ (Anfangsmomentum) } \\\\[1em]\n  & \\text{Iteriere für } t = 0, 1, 2, \\dots \\text{ bis Konvergenz:} \\\\[.5ex]\n  & \\quad \\text{1. Berechne den Gradienten: } \\nabla f(w_t) \\\\\n  & \\quad \\text{2. Aktualisiere den Momentum-Term: } v_{t+1} = \\alpha v_t - \\eta \\nabla f(w_t) \\\\\n  & \\quad \\text{3. Aktualisiere die Position: } w_{t+1} = w_t + v_{t+1} \\\\\n  & \\quad \\text{4. Überprüfe das Abbruchkriterium } |\\nabla f(w_t)| < \\epsilon\\text{ (für ein kleines $\\epsilon>0$)} \\\\\n\\end{align*}\n\nIn der nachfolgenden interaktiven Visualisierung illustrieren wir die Minimierung einer univariaten Funktion $\\color{blue}{f(w_t)}$ über $w_t$ anhand des obigen Algorithmus mit Lernrate $\\eta = .001$ und Momentum-Faktor $\\alpha = .925$.\n\nDer <span style=\"color:orange\">Gradient</span>$\\color{orange}{\\nabla f(w_t)}$ ist hier die 1. Ableitung von $\\color{blue}{f(w_t)}$ nach $w_t$. Die Richtung der Änderung von $\\color{blue}{f(w_t)}$ in $w_t$ wird durch den <span style=\"color:orange\">orangenen Pfeil</span> angezeigt. Beachte, wie sich der Gradient bei Variation des Start-Punkts mit dem Slider ändert. Während die Animation der Optimierung mit Gradient Descent läuft, zeigt der  <span style=\"color:purple\">lilane Pfeil</span> das <span style=\"color:purple\">Momentum</span> (<span style=\"color:purple\">Velocity $v_t$</span>) für Schirtt $t$ an.^[Unterschiedliche Längen der Pfeile zeigen hier nicht Änderungen der tatsächlichen Beträge, sondern dienen lediglich der Interpretierbakeit der Grafik.] Der Algorithmus iteriert die Schritte 1. bis 3. solange, bis das Abbruchkriterium $|\\textcolor{orange}{\\nabla f(w_t)}| < \\epsilon = 0.001$ erreicht ist, die Änderung in $\\color{orange}{\\nabla f(w_t)}$ also hinreichend klein ist, dass ein Parameterwert $w_t$ mit $\\color{blue}{f(w_t)}$ nahe des (globalen) Minimums von $f$ plausibel ist.\n\nFolgende Eigenschaften der Optimierung mit Gradient Descent können anhand der Parameter geprüft werden:\n\n- Für Startpunkte mit großen Werten des Gradienten beginnt der Algorithmus mit einem starken Momentum: Der Abstieg in Richtung des negativen Gradients erfolgt also in großen Schritten, sodass die Optimierung schneller erfolgt als für Startpunkte in flachen Regionen von $\\color{blue}{f}$.\n\n    Dieser Effekt des Momentum auf den Pfad der zu optimierenden Parameter bei Gradient Descent ist vergleichbar mit dem Effekt der Schwerkraft auf eine Murmel, die auf einer hügeligen Oberfläche rollt: Anfangs gewinnt die Murmel an Geschwindigkeit und bewegt sich beschleunigt in Richtung des steilsten Gefälles. In flacheren Regionen wird die Bewegung langsamer und die Murmel kann in Tälern stecken bleiben, ähnlich wie der Optimierungsprozess in flachen Regionen von $\\color{blue}{f}$ langsamer verläuft oder gar stoppt, weil ein Abbruchkriterium erfüllt ist (geringe Änderung des Gradienten). Das Momentum hilft, auch in solchen flachen Bereichen weiter voranzukommen, indem es dem Parameterpfad eine gewisse \"Trägheit\" verleiht, die es ermöglicht, flache Stellen schneller zu durchqueren und die Optimierung effizienter zu gestalten.\n\n- Bei ungünstiger Wahl der Parameter konvergiert der Algorithmus nicht zum globalen Minimum, sondern stoppt im lokalen Minimum bei $w = -0.5$. Dies unterstreicht die Notwendigkeit, die Hyperparameter Lernrate $\\eta$ und Momentum-Faktor $\\alpha$ sorgfältig zu wählen, beispielsweise indem die Modellgüte nach erfolgter Anpassung für verschiedene Parameter-Kombinationen verglichen wird. \n\nIn empirischen Anwendungen ist es für eine hohe Modellgüte eines neuronalen Netzwerks nicht unbedingt erforderlich, das globale Minimum zu finden: Viele Optimierungsprobleme weisen zahlreiche lokale Minima auf, die eine ausreichend gute Annäherung an das Optimum bieten können. Besonders bei hochdimensionalen Optimierungsproblemen mit komplexen Loss-Funktionen können diese lokalen Minima zufriedenstellende Lösungen darstellen. In einigen Fällen existiert möglicherweise kein globales Minimum, und der Algorithmus konvergiert zwangsläufig zu einem stabilen lokalen Minimum, das dennoch eine gute Performance gewährleistet. Daher kann es sinnvoller sein, Algorithmen zu verwenden, die das Erreichen einer robusten Lösung legen, anstatt strikt nach dem globalen Minimum zu suchen.\n\nIn Software-Implementierungen für Machine und Deep Learning wie `tensorflow` und `keras` werden fortgeschrittene Techniken wie Momentum Tuning oder Stochastic [Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (SGD) eingesetzt, um die Wahrscheinlichkeit zu erhöhen, dass der Algorithmus nicht in einem (ungünstigen) lokalen Minimum endet. Ein für die Anpassung von NN häufig verwendeter Algorithmus, der SGD verwendet, ist [Adaptive Moment Estimation (Adam)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam). Wir verwenden u.a. den Adam-Optimizer in den empirischen Beispielen. \n\n<iframe class=\"obs-soft-box-shadow\" width=\"100%\" height=\"880\" frameborder=\"0\"\n  src=\"https://observablehq.com/embed/@mca91/gradient-descent-in-2d@759?cells=plot%2Clabels%2Cviewof+startAnimation%2Cviewof+startPoint%2Cviewof+alpha%2Cviewof+eta\"></iframe>\n\nIn empirischen Anwendungen sind die zu lernenden Zusammenhänge komplex und damit die Anzahl der zu optimierenden Parameter eines NN häufig groß. Der oben erläuterte Algorithmus für Gradient Descent mit Momentum kann einfach auf Optimierungsprobleme mit $k$ Parametern generalisiert werden. Dann ist $\\boldsymbol{w}_t$ ein Vektor mit $k$ Gewichten, $\\boldsymbol{v}_{t+1}$ eine vektorwertige Funktion von $\\boldsymbol{v}_t$ und $\\nabla f(\\boldsymbol{w}_t)$ mit Dimension $k$ und $f(\\boldsymbol{w}_t)$ ist eine Oberfläche in einem $k+1$-dimensionalen Raum. \n\nDie nachfolgende interaktive Grafik illustriert Gradient Descent mit Momentum für $k=2$ zu optimierende Gewichte. Statt der Parameter des Algorithmus kann hier die Form der zu optimierenden Funktion manipuliert werden, sodass bis zu 6 Extremstellen vorliegen können. Der <span style=\"color:red\">rote Punkt</span> zeigt den Verlauf der Optimierung von $\\boldsymbol{w}_t$.\n\nDie Animation verdeutlicht, dass lokale Minima insbesondere in höheren Dimensionen herausfordernd für Optimierungsalgorithmen sind: Durch Variation der Extrema lassen sich leicht Funktionen $f(w_1,w_1)$ konstruieren, für die Gradient Descent mit den voreingestellten Parametern nicht gegen das globale Minimum konvergiert, sofern vorhanden. Ein günstiger Initialwert für $\\boldsymbol{w}_t$ kann die Wahrscheinlichkeit von Stops in lokalen Minima verringern: *Grid Search Initialization* wertet die Funktion über ein gleichmäßiges Gitter von Werten für $\\boldsymbol{w}_t$ aus und wählt als Startwert $\\boldsymbol{w}_{0,\\textup{init}}$ den Punkt mit dem minimalen Funktionswert von $f$ über alle Punkte im Gitter.\n\n<iframe class=\"obs-soft-box-shadow\" width=\"100%\" height=\"1220\" frameborder=\"0\"\n  src=\"https://observablehq.com/embed/@mca91/gradient-descent-in-3d-three-js?cells=renderer%2Cviewof+restart%2Cviewof+gridinit%2Cviewof+themin%2Cviewof+themin2%2Cviewof+themin3%2Cviewof+themin4%2Cviewof+themin5%2Cviewof+themin6%2Cscene%2Ccamera\"></iframe>\n\n\n## Funktionale Zusammenhänge lernen: Regression\n\nFür einen leichten Einstieg in die Modellierung funktionaler Zusammenhänge durch NN mit statistischer Programmierung in R betrachten wir zunächst den einfachsten Zusammenhang zwischen einer Outcome-Variable $Y$ und einem Regressor $X$: Die einfache lineare Funktion\n\\begin{align*}\n  Y = w_1 X + b,\n\\end{align*}\nwobei der Regressionskoeffizient $w_1$ den Einfluss von $X$ auf $Y$ misst und $b$ eine Konstante ist. Gemäß der Definitionen in @sec-nn-basics kann dieser Funktionale Zusammenhang als NN ohne Hidden Layer dargestellt werden, wobei $X$ ein Input-Neuron ist, dessen Information mit $w_1$ gewichtet an das Output Layer mit einem einzigen Neuron für $Y$ weitergegeben wird. Die Konstante $b$ ist ein *Bias*, der als von $X$ unabhängiger Einfluss von $Y$ behandelt wird, vgl. @fig-nn-lreg.\n\n\n\n\n\n\n```{dot}\n//| fig-width: 6\n//| fig-height: 2\n//| fig-cap: \"Neuronales Netzwerk: Lineare Regression mit einer Variable und Konstante\"\n//| label: \"fig-nn-lreg\"\ndigraph NEURALNET {\n    layout=neato\n    fontname=\"Helvetica,Arial,sans-serif\"\n    node [fontname=\"Helvetica,Arial,sans-serif\", shape=\"circle\"]\n    edge [fontname=\"Helvetica,Arial,sans-serif\"]\n\n    X [label=\"X\", pos=\"0,0!\"];\n    Y [label=\"Y\", pos=\"4,0!\", style=filled, fillcolor=lightgreen];\n    B [label=\"1\", pos=\"2,2!\"];\n\n    X -> Y [headlabel = \"w1\", labeldistance=12.5, labelangle=10];\n    B -> Y [headlabel = \"Bias (b)\", labeldistance=9, labelangle=-15];\n}\n```\n\n\n\n\n\n\nFür die Illustration der Schätzung des in @sec-nn-basics dargestellten NN verwenden wir $n=1000$ simulierte Datenpunkte gemäß der Vorschrift\n\\begin{align}\n  Y = 5 + 3 \\cdot X + u\n\\end{align}\nmit $X\\sim U[0,10]$ und $u\\sim N(0,1)$.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Daten simulieren\nset.seed(1234)\n\nn <- 1000\nx <- runif(n, min = 0, max = 10)\ny <- 5 + 3 * x + rnorm(n) \n```\n:::\n\n\n\n\n\n\nFür das Training von NN verwenden wir das Python-Paket [keras](https://keras.io/). Hierzu muss lediglich eine lokale Python-Installation vorhanden sein.\n\nDie in diesem Kapitel betrachteten NN sind *sequentielle* NN. Solche Modelle können in `keras` mit der Funktion `keras_model_sequential()` definiert werden. Die Struktur des Modells kann über eine Verkettung von Funktionen für Layers (`keras::layer_dense()`) und Aktivierungen (`keras::layer_activation()`) definiert werden.\n\nFür die Implementierung des Modells in @fig-nn-lreg wählen wir mit `units = 1` und `input_shape = 1` ein Modell mit einem Neuron im Output Layer, das skalare Informationen verarbeitet. `activation = 'linear'` in `layer_dense()` führt zu der Aktivierungsfunktion $A(x) = x$, d.h. die Ausgabe des Input Layers ist die gewichtete Summe der Eingaben plus Bias, *ohne* eine zusätzliche Transformation.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(keras)\n\n# NN für einfache Regression\nmodel <- keras_model_sequential() %>%\n  layer_dense(\n    units = 1, \n    input_shape = 1, \n    activation = 'linear'\n    )\n\n# Modell-Definition prüfen\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense (Dense)                      (None, 1)                       2           \n================================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\n________________________________________________________________________________\n```\n\n\n:::\n:::\n\n\n\n\n\n\nDie Übersicht zeigt, dass `model` aus einem Layer für skalare Inputs und Outputs sowie zwei trainierbaren Parameters ($w_1$ und $b$) besteht.\n\nBevor das im Objekt `model` definierte Modell trainiert werden kann, muss der Code *kompiliert* werden. Dieser Vorgang ist notwendig, da sämtliche Berechnungen in Python durchgeführt werden. Der Python-Code wird beim kompilieren in eine Zwischendarstellung (*Bytecode*) übersetzt, die dann von der Python-Interpreter-Laufzeitumgebung ausgeführt wird.^[Im Gegensatz zu Python ist R eine *interpretierte Programmiersprache*. Kompilierung von R-Ccode ist daher nicht notwendig.]\n\nMit `keras::compile()` kompilieren wir das Modell und wählen als Optimierungsfunktion Adam mit einer Lernrate von $.01$. Die Loss-Funktion wird über das Argument `loss` festgelegt, hier der mittlere absolute Fehler,\n\\begin{align*}\n  \\textup{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\lvert y_i - \\widehat{y}_i\\rvert.\n\\end{align*}\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Modell kompilieren\nmodel %>% \n  compile(\n    optimizer = optimizer_adam(learning_rate = 0.01),\n    loss = 'mean_absolute_error'\n)\n```\n:::\n\n\n\n\n\n\nDie Kompilierung erfolgt meist innerhalb von Sekundenbruchteilen und geschieht *in-place*: Eine Zuweisung des kompilierten Modells in `model` ist *nicht* notwendig.\n\nUm das Modell zu trainieren verwenden wir `keras::fit()`. Neben den (simulierten) Daten übergeben wir die Anzahl der zudurchlaufenden Epochen `epocs`. Über das Argument `validation_split` legen wir fest, dass 20\\% der Datensatzes zufällig ausgewählt und als Test-Datensatz für die Modell-Validierung während des Trainings genutzt werden sollen.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Modell trainieren\nhistory_snn <- model %>% \n  fit(\n    x = x, \n    y = y, \n    epochs = 50, \n    validation_split = .2\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEpoch 1/50\n25/25 - 0s - loss: 23.1849 - val_loss: 21.9387 - 403ms/epoch - 16ms/step\nEpoch 2/50\n25/25 - 0s - loss: 21.6546 - val_loss: 20.4587 - 102ms/epoch - 4ms/step\nEpoch 3/50\n25/25 - 0s - loss: 20.1394 - val_loss: 18.9645 - 91ms/epoch - 4ms/step\nEpoch 4/50\n25/25 - 0s - loss: 18.6138 - val_loss: 17.4847 - 90ms/epoch - 4ms/step\nEpoch 5/50\n25/25 - 0s - loss: 17.0914 - val_loss: 16.0078 - 91ms/epoch - 4ms/step\nEpoch 6/50\n25/25 - 0s - loss: 15.5755 - val_loss: 14.5225 - 90ms/epoch - 4ms/step\nEpoch 7/50\n25/25 - 0s - loss: 14.0480 - val_loss: 13.0488 - 90ms/epoch - 4ms/step\nEpoch 8/50\n25/25 - 0s - loss: 12.5373 - val_loss: 11.5511 - 91ms/epoch - 4ms/step\nEpoch 9/50\n25/25 - 0s - loss: 11.0094 - val_loss: 10.0738 - 91ms/epoch - 4ms/step\nEpoch 10/50\n25/25 - 0s - loss: 9.4863 - val_loss: 8.6012 - 90ms/epoch - 4ms/step\nEpoch 11/50\n25/25 - 0s - loss: 7.9738 - val_loss: 7.1103 - 90ms/epoch - 4ms/step\nEpoch 12/50\n25/25 - 0s - loss: 6.4468 - val_loss: 5.6351 - 90ms/epoch - 4ms/step\nEpoch 13/50\n25/25 - 0s - loss: 4.9269 - val_loss: 4.1598 - 91ms/epoch - 4ms/step\nEpoch 14/50\n25/25 - 0s - loss: 3.4181 - val_loss: 2.6814 - 89ms/epoch - 4ms/step\nEpoch 15/50\n25/25 - 0s - loss: 1.9461 - val_loss: 1.3553 - 91ms/epoch - 4ms/step\nEpoch 16/50\n25/25 - 0s - loss: 0.9930 - val_loss: 0.9065 - 91ms/epoch - 4ms/step\nEpoch 17/50\n25/25 - 0s - loss: 0.8811 - val_loss: 0.8832 - 97ms/epoch - 4ms/step\nEpoch 18/50\n25/25 - 0s - loss: 0.8711 - val_loss: 0.8727 - 90ms/epoch - 4ms/step\nEpoch 19/50\n25/25 - 0s - loss: 0.8618 - val_loss: 0.8608 - 90ms/epoch - 4ms/step\nEpoch 20/50\n25/25 - 0s - loss: 0.8527 - val_loss: 0.8485 - 91ms/epoch - 4ms/step\nEpoch 21/50\n25/25 - 0s - loss: 0.8433 - val_loss: 0.8355 - 91ms/epoch - 4ms/step\nEpoch 22/50\n25/25 - 0s - loss: 0.8347 - val_loss: 0.8225 - 91ms/epoch - 4ms/step\nEpoch 23/50\n25/25 - 0s - loss: 0.8271 - val_loss: 0.8111 - 90ms/epoch - 4ms/step\nEpoch 24/50\n25/25 - 0s - loss: 0.8202 - val_loss: 0.8014 - 90ms/epoch - 4ms/step\nEpoch 25/50\n25/25 - 0s - loss: 0.8128 - val_loss: 0.7925 - 91ms/epoch - 4ms/step\nEpoch 26/50\n25/25 - 0s - loss: 0.8066 - val_loss: 0.7836 - 91ms/epoch - 4ms/step\nEpoch 27/50\n25/25 - 0s - loss: 0.8015 - val_loss: 0.7765 - 90ms/epoch - 4ms/step\nEpoch 28/50\n25/25 - 0s - loss: 0.7966 - val_loss: 0.7703 - 90ms/epoch - 4ms/step\nEpoch 29/50\n25/25 - 0s - loss: 0.7915 - val_loss: 0.7653 - 90ms/epoch - 4ms/step\nEpoch 30/50\n25/25 - 0s - loss: 0.7881 - val_loss: 0.7614 - 91ms/epoch - 4ms/step\nEpoch 31/50\n25/25 - 0s - loss: 0.7842 - val_loss: 0.7561 - 90ms/epoch - 4ms/step\nEpoch 32/50\n25/25 - 0s - loss: 0.7796 - val_loss: 0.7501 - 91ms/epoch - 4ms/step\nEpoch 33/50\n25/25 - 0s - loss: 0.7781 - val_loss: 0.7463 - 91ms/epoch - 4ms/step\nEpoch 34/50\n25/25 - 0s - loss: 0.7735 - val_loss: 0.7444 - 89ms/epoch - 4ms/step\nEpoch 35/50\n25/25 - 0s - loss: 0.7724 - val_loss: 0.7393 - 90ms/epoch - 4ms/step\nEpoch 36/50\n25/25 - 0s - loss: 0.7703 - val_loss: 0.7380 - 90ms/epoch - 4ms/step\nEpoch 37/50\n25/25 - 0s - loss: 0.7679 - val_loss: 0.7319 - 90ms/epoch - 4ms/step\nEpoch 38/50\n25/25 - 0s - loss: 0.7673 - val_loss: 0.7315 - 91ms/epoch - 4ms/step\nEpoch 39/50\n25/25 - 0s - loss: 0.7658 - val_loss: 0.7277 - 90ms/epoch - 4ms/step\nEpoch 40/50\n25/25 - 0s - loss: 0.7645 - val_loss: 0.7253 - 92ms/epoch - 4ms/step\nEpoch 41/50\n25/25 - 0s - loss: 0.7646 - val_loss: 0.7227 - 90ms/epoch - 4ms/step\nEpoch 42/50\n25/25 - 0s - loss: 0.7646 - val_loss: 0.7224 - 90ms/epoch - 4ms/step\nEpoch 43/50\n25/25 - 0s - loss: 0.7623 - val_loss: 0.7215 - 89ms/epoch - 4ms/step\nEpoch 44/50\n25/25 - 0s - loss: 0.7623 - val_loss: 0.7195 - 90ms/epoch - 4ms/step\nEpoch 45/50\n25/25 - 0s - loss: 0.7625 - val_loss: 0.7193 - 90ms/epoch - 4ms/step\nEpoch 46/50\n25/25 - 0s - loss: 0.7613 - val_loss: 0.7171 - 90ms/epoch - 4ms/step\nEpoch 47/50\n25/25 - 0s - loss: 0.7607 - val_loss: 0.7178 - 89ms/epoch - 4ms/step\nEpoch 48/50\n25/25 - 0s - loss: 0.7601 - val_loss: 0.7152 - 91ms/epoch - 4ms/step\nEpoch 49/50\n25/25 - 0s - loss: 0.7601 - val_loss: 0.7156 - 90ms/epoch - 4ms/step\nEpoch 50/50\n25/25 - 0s - loss: 0.7598 - val_loss: 0.7145 - 90ms/epoch - 4ms/step\n```\n\n\n:::\n:::\n\n\n\n\n\n\nDer Output zeigt die Enwicklung des Loss (MAE) für Vorhersagen des Trainingsdatensatzes (`loss`) und für den Test-Datensatz (`val_loss`) für alle 25 Epochen. Diese Informationen können mit `plot()` einfach visualisiert werden.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(cowplot)\nlibrary(purrr)\n\n# Entwicklung des Loss über Epochen plotten\nplot(history_snn) +\n  labs(\n    x = \"Epoche\",\n    y = \"Wert der Verlustfunktion\"\n  ) +\n  theme_cowplot() +\n  theme(legend.position = \"top\")\n```\n\n::: {.cell-output-display}\n![Einfaches lineares NN: Entwicklung des Loss für 25 Epochen](Machine-Learning_files/figure-html/fig-snn-loss-1.png){#fig-snn-loss width=672}\n:::\n:::\n\n\n\n\n\n\n@fig-snn-loss zeigt, dass sich sowohl die Anpassung des NN auf dem Trainingsdatenstz als auch die Generalisierung auf dem Testdatensatz innerhalb der ersten Epochen dramatisch verbessert. Jenseits der 15. Epoche hingegen bewirken weitere Trainingszyklen keine weitere Verbesserung des Loss.\n\nMit `keras::get_weights()` können wir die optimierten Parameter aus dem Modell-Objekt auslesen.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Gewichtung und Bias des trainierten NN auslesen\nmodel %>% \n  keras::get_weights() %>% \n  flatten_dbl() %>% \n  set_names(\n    c(\"w_1\", \"bias\")\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     w_1     bias \n3.006937 4.935575 \n```\n\n\n:::\n:::\n\n\n\n\n\n\nDas NN hat den funktionalen Zusammengang zwischen `x` und `y` erfolgreich gelernt: Die optimierten Parameter-Werte `bias` und `w_1` liegen nahe der wahren Parameter. Bei Parameter sind mit ihren KQ-Schätzungen vergleichbar.^[Beachte, dass die KQ-Schätzung der Einfachheit halber hier den gesamten Datensatz nutzt und daher präziser sein kann als das NN.]\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# lineares Modell\nlm_model <- lm(\n  formula = y ~ x\n  )\n\nsummary(lm_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91933 -0.62956  0.01084  0.63819  2.73178 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  5.04449    0.06028   83.69   <2e-16 ***\nx            2.98893    0.01031  290.01   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9486 on 998 degrees of freedom\nMultiple R-squared:  0.9883,\tAdjusted R-squared:  0.9883 \nF-statistic: 8.411e+04 on 1 and 998 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Koeffizienten der KQ-Schätzung auslesen\ncoef(lm_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)           x \n   5.044491    2.988928 \n```\n\n\n:::\n:::\n\n\n\n\n\n\nMit `predict()` erhalten wir Vorhersagen des NN und können so beispielsweise die Residuen für den gesamten Datensatz mit denen der KQ-Schätzung vergleichen.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Residuen vergleichen\n tibble(\n   NN = y - model %>% predict(x),\n   lm = lm_model$residuals\n ) %>%\n  \n  ggplot(mapping = aes(x = NN, y = lm)) +\n  geom_point(alpha = .5, color = \"steelblue\") +\n  theme_cowplot()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n32/32 - 0s - 75ms/epoch - 2ms/step\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Vergleich von Residuen für NN und KQ-Schätzung](Machine-Learning_files/figure-html/fig-res-nn-1.png){#fig-res-nn width=672}\n:::\n:::\n\n\n\n\n\n\n@fig-res-nn zeigt eine gute Korrespondenz zwischen der Anpassung des NN und der KQ-Schätzung des einfachen linearen Modells.\n\n## Multiple Regression\n\nEin neuronales Netz für multiple Regression kann als eine Erweiterung des Netzes für einfache Regression betrachtet werden. Das Netz enthält nun mehrere Input-Neuronen, von denen jedes eine der unabhängigen Variablen $X_1, X_2, \\dots, X_k$ repräsentiert. Diese Input-Neuronen sind mit einem einzigen Output-Neuron verbunden, das die Vorhersage für $Y$  liefert. Jede dieser Verbindungen wird mit einem Gewicht $w_i$  multipliziert, das die Stärke des Einflusses der jeweiligen unabhängigen Variable  $X_i$  auf die abhängige Variable  $Y$  repräsentiert. Wie im einfachen Modell gibt es einen Bias-Term $b$, der ähnlich wie in der einen konstanten Einfluss darstellt.\n\nDie Struktur eines NN für multiple Regression ist in @fig-nn-mlreg dargestellt. In diesem Beispiel gibt es drei unabhängige Variablen $X_1$, $X_2$ und $X_3$, die jeweils ein eigenes Input-Neuron haben und mit dem Output-Neuron $Y$ verbunden sind. $Y$ ist eine Linear-Kombination der Inputs, gewichtet mit den jeweiligen Gewichten $w_1$, $w_2$ und $w_3$, sowie dem Bias $b$.\n\n\n\n\n\n\n```{dot}\n//| fig-width: 6\n//| fig-height: 3\n//| fig-cap: \"Neuronales Netzwerk: Multiple lineare Regression\"\n//| label: \"fig-nn-mlreg\"\ndigraph NEURALNET {\n    layout=neato\n    fontname=\"Helvetica,Arial,sans-serif\"\n    node [fontname=\"Helvetica,Arial,sans-serif\", shape=\"circle\"]\n    edge [fontname=\"Helvetica,Arial,sans-serif\"]\n\n    // Eingangsneuronen\n    X1 [label=\"X1\", pos=\"0,1!\"];\n    X2 [label=\"X2\", pos=\"0,0!\"];\n    X3 [label=\"X3\", pos=\"0,-1!\"];\n    \n    // Summationsneuron\n    SUM [label=\"∑\", shape=\"circle\", pos=\"3,0!\", width=0.5, height=0.5, style=filled, fillcolor=lightgray];\n\n    // Ausgabeneuron\n    Y [label=\"Y\", pos=\"5,0!\", style=filled, fillcolor=lightgreen];\n    \n    // Bias\n    B [label=\"1\", pos=\"3,2!\", fontcolor=gray, style=filled, fillcolor=lightblue];\n    \n    // Verbindungen von Eingangsneuronen zur Summation\n    X1 -> SUM [label = \"w1\"];\n    X2 -> SUM [label = \"w2\"];\n    X3 -> SUM [label = \"w3\"];\n    \n    // Verbindung vom Bias zur Summation\n    B -> SUM [label = \"b\"];\n\n    // Verbindung von der Summation zum Ausgabeneuron\n    SUM -> Y [label = \"\"];\n}\n```\n\n\n\n\n\n\nUm die Vorgehensweise in R zu zeigen, generieren wir zunächst $n=250$ Datenpunkte gemäß der Vorschrift\n\\begin{align}\n  Y = 5 + 3 \\cdot X_1 + 2\\cdot X_2 - 1.5 \\cdot X_k + u\n\\end{align}\nmit $X_1,X_2,X_3 \\sim\\textup{u.i.v.} N(0, 1)$ und $u\\sim N(0,1)$.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Erstellen von Trainingsdaten\nset.seed(42)\n\nn <- 250\nk <- 3\n\nX <- matrix(rnorm(n = n * k), ncol = k)\nw <- c(3, 2, -1.5)\n\nY <- 5 + X %*% w + rnorm(n)\n```\n:::\n\n\n\n\n\n\nAnschließend definieren wir ein einfaches NN und fügen ein Layer hinzu. Da wir eine multiple Regression durchführen, wählen wir `input_shape = k`, wobei `k` die Anzahl der unabhängigen Variablen ist. Wie im einfachen Modell ist die Aktivierungsfunktion linear, da wir an der Anpassung von $Y$ mit einer linearen Kombination der Inputs interessiert sind.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Erstellen und Kompilieren des Modells\nmodel <- keras_model_sequential() %>%\n  layer_dense(\n    units = 1, \n    input_shape = k, \n    activation = 'linear'\n  )\n\n# Modelldefinition prüfen\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel: \"sequential_1\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_1 (Dense)                    (None, 1)                       4           \n================================================================================\nTotal params: 4\nTrainable params: 4\nNon-trainable params: 0\n________________________________________________________________________________\n```\n\n\n:::\n:::\n\n\n\n\n\n\nWir kompilieren das Modell mit dem mittleren quadratischen Fehler (mean squared error, MSE) und SGD als Loss-Funktion mit einer moderaten Lernrate.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% \n  compile(\n    loss = 'mean_squared_error',\n    optimizer = optimizer_sgd(learning_rate = 0.01)\n  )\n```\n:::\n\n\n\n\n\n\nDie Anpassung des Modells erfolgt wie bei einfacher Regression mit `keras::fit()`.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Training des Modells\nhistory_mnn <- model %>% \n  fit(\n    x = X, \n    y = Y, \n    validation_split = .2,\n    epochs = 25\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEpoch 1/25\n7/7 - 0s - loss: 38.3993 - val_loss: 32.8652 - 315ms/epoch - 45ms/step\nEpoch 2/25\n7/7 - 0s - loss: 29.4938 - val_loss: 25.0963 - 39ms/epoch - 6ms/step\nEpoch 3/25\n7/7 - 0s - loss: 22.4204 - val_loss: 19.6002 - 37ms/epoch - 5ms/step\nEpoch 4/25\n7/7 - 0s - loss: 17.4279 - val_loss: 15.4717 - 37ms/epoch - 5ms/step\nEpoch 5/25\n7/7 - 0s - loss: 13.6818 - val_loss: 12.2052 - 36ms/epoch - 5ms/step\nEpoch 6/25\n7/7 - 0s - loss: 10.7182 - val_loss: 9.5849 - 36ms/epoch - 5ms/step\nEpoch 7/25\n7/7 - 0s - loss: 8.3500 - val_loss: 7.6605 - 36ms/epoch - 5ms/step\nEpoch 8/25\n7/7 - 0s - loss: 6.6112 - val_loss: 6.2265 - 37ms/epoch - 5ms/step\nEpoch 9/25\n7/7 - 0s - loss: 5.3111 - val_loss: 5.0795 - 37ms/epoch - 5ms/step\nEpoch 10/25\n7/7 - 0s - loss: 4.2714 - val_loss: 4.2336 - 36ms/epoch - 5ms/step\nEpoch 11/25\n7/7 - 0s - loss: 3.4932 - val_loss: 3.6129 - 37ms/epoch - 5ms/step\nEpoch 12/25\n7/7 - 0s - loss: 2.9248 - val_loss: 3.1064 - 37ms/epoch - 5ms/step\nEpoch 13/25\n7/7 - 0s - loss: 2.4445 - val_loss: 2.7230 - 38ms/epoch - 5ms/step\nEpoch 14/25\n7/7 - 0s - loss: 2.0787 - val_loss: 2.4224 - 38ms/epoch - 5ms/step\nEpoch 15/25\n7/7 - 0s - loss: 1.7950 - val_loss: 2.2407 - 37ms/epoch - 5ms/step\nEpoch 16/25\n7/7 - 0s - loss: 1.6024 - val_loss: 2.0872 - 36ms/epoch - 5ms/step\nEpoch 17/25\n7/7 - 0s - loss: 1.4649 - val_loss: 1.9736 - 36ms/epoch - 5ms/step\nEpoch 18/25\n7/7 - 0s - loss: 1.3637 - val_loss: 1.9016 - 36ms/epoch - 5ms/step\nEpoch 19/25\n7/7 - 0s - loss: 1.2812 - val_loss: 1.8461 - 36ms/epoch - 5ms/step\nEpoch 20/25\n7/7 - 0s - loss: 1.2149 - val_loss: 1.8101 - 36ms/epoch - 5ms/step\nEpoch 21/25\n7/7 - 0s - loss: 1.1715 - val_loss: 1.7887 - 36ms/epoch - 5ms/step\nEpoch 22/25\n7/7 - 0s - loss: 1.1364 - val_loss: 1.7568 - 37ms/epoch - 5ms/step\nEpoch 23/25\n7/7 - 0s - loss: 1.1083 - val_loss: 1.7334 - 36ms/epoch - 5ms/step\nEpoch 24/25\n7/7 - 0s - loss: 1.0898 - val_loss: 1.7260 - 36ms/epoch - 5ms/step\nEpoch 25/25\n7/7 - 0s - loss: 1.0727 - val_loss: 1.7264 - 36ms/epoch - 5ms/step\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Entwicklung des Loss über Epochen plotten\nplot(history_mnn) +\n  labs(\n    x = \"Epoche\",\n    y = \"Wert der Verlustfunktion\"\n  ) +\n  theme_cowplot() +\n  theme(legend.position = \"top\")\n```\n\n::: {.cell-output-display}\n![NN für mult. Regression: Entwicklung des Loss für 25 Epochen](Machine-Learning_files/figure-html/fig-mnn-loss-1.png){#fig-mnn-loss width=672}\n:::\n:::\n\n\n\n\n\n\nWie bei der einfachen Regression zeigt ein Vergleich der angepassten Gewichte mit den KQ-Schätzungen eines entsprechenden linearen Regressionsmodells ähnliche Ergebnisse beider Ansätze.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Gewichtung und Bias des trainierten NN auslesen\nmodel %>% \n  keras::get_weights() %>% \n  flatten_dbl() %>% \n  set_names(\n    c(\"w_1\", \"w_2\", \"w_3\", \"bias\")\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      w_1       w_2       w_3      bias \n 2.945296  1.918664 -1.433969  4.803524 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Multiples lineares Modell mit KQ schätzen\nlm_model <- lm(\n  formula = Y ~ X\n)\n\nsummary(lm_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3129 -0.7286  0.0900  0.7439  3.4086 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  5.01583    0.06843   73.30   <2e-16 ***\nX1           2.97449    0.07028   42.32   <2e-16 ***\nX2           1.94345    0.07061   27.52   <2e-16 ***\nX3          -1.47278    0.06914  -21.30   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.079 on 246 degrees of freedom\nMultiple R-squared:  0.9271,\tAdjusted R-squared:  0.9262 \nF-statistic:  1042 on 3 and 246 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Nicht-Lineare Zusammenhänge\n\nIn diesem Abschnitt verwenden trainieren wir ein NN, um eine logistische Regression durchzuführen. Dieser Ansatz wird häufig verwendet, um eine binäre Outcome-Variablen $Y$ zu modellieren, also Variablen, die zwei mögliche Ausgänge haben (oft als 0 oder 1 dargestellt), siehe @sec-logreg für Details. Anstatt die Eingaben lediglich linear zu kombinieren, verwenden wir eine Sigmoid-Aktivierungsfunktion^[Die Sigmoid-Aktivierungsfunktion entspricht der logistischen Funktion $\\Lambda(z)$ aus @sec-logreg.], \n\\begin{align*}\n  \\sigma(z) = \\frac{1}{1 + \\exp(-z)},\n\\end{align*}\nwelche die Ausgaben auf einen Wertebereich zwischen 0 und 1 abbildet. Dadurch kann das NN Wahrscheinlichkeiten $P(Y=1\\vert \\boldsymbol{X} = \\boldsymbol{x})$ vorhersagen, die anschließend für die *Klassifikation* von Beobachtungen verwendet werden können.\n\nFür die Illustration der Schätzung mit `keras` verwenden wir den DGP aus @sec-probitreg.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Erstellen von Trainingsdaten\nset.seed(1234)\n\nn <- 500\nX <- rnorm(n = n, mean = 5, sd = 2) # Regressor\nP <- pnorm(-4 + 0.7 * X)\nY <- as.integer(runif(n) < P)\n```\n:::\n\n\n\n\n\n\n@fig-nn-log-reg zeigt ein einfaches NN für eine binäre Outcome-Variable.\n\n\n\n\n\n\n```{dot}\n//| fig-width: 6\n//| fig-height: 4\n//| fig-cap: \"Neuronales Netzwerk mit Sigmoid-Aktivierungsfunktion\"\n//| label: \"fig-nn-log-reg\"\ndigraph NNlogit {\n    layout=neato\n    fontname=\"Helvetica,Arial,sans-serif\"\n    node [fontname=\"Helvetica,Arial,sans-serif\", shape=\"circle\"]\n    edge [fontname=\"Helvetica,Arial,sans-serif\"]\n\n    // Eingangsvariablen\n    X [label=\"X1\", pos=\"0,0!\"];\n\n    // Summationsneuron\n    SUM [label=\"∑\", shape=\"circle\", pos=\"3,0!\", width=0.5, height=0.5, style=filled];\n\n    // Sigmoid-Aktivierungsfunktion\n    sigmoid [label=\"σ\", pos=\"5,0!\", style=filled, fillcolor=lightblue];\n\n    // Output\n    Y [label=\"Y\", pos=\"7,0!\", style=filled, fillcolor=lightgreen];\n\n    // Bias\n    B [label=\"1\", pos=\"3,2!\"];\n\n    // Verbindungen von Eingangsneuronen zur Summation\n    X -> SUM [label = \"w1\"];\n    B -> SUM [label = \"b\"];\n\n    // Verbindung von der Summation zur Sigmoid-Funktion\n    SUM -> sigmoid [label = \"\"];\n\n    // Verbindung von der Sigmoid-Funktion zum Output\n    sigmoid -> Y [label = \"\"];\n}\n```\n\n\n\n\n\n\nNach der Definition des NN wird das Modell mit dem Binary-Cross-Entropy-Loss (BCEL) und dem Adam-Optimierer kompiliert. BCEL ist für binäre Klassifikationsprobleme geeignet: Diese Loss-Funktion misst die die Unterschiede zwischen den vorhergesagten Wahrscheinlichkeiten $\\widehat{p}_i$ und den tatsächlichen binären Zielen $y_i$,\n\\begin{align*}\n  \\textup{BCEL} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\cdot \\log(\\widehat{p}_i) + (1 - y_i) \\cdot \\log(1 - \\widehat{p}_i) \\right].\n\\end{align*}\nAls weitere zu berechnende Metrik wählen wir $\\textup{Accuracy}$, ein geläufiges Maß zur Bewertung der Leistung von Klassifikationsmodellen. $\\textup{Accuracy}$ gibt an, wie oft das Modell korrekte Vorhersagen getroffen hat, ausgedrückt als Verhältnis der Anzahl der korrekten Vorhersagen zur Gesamtzahl der Vorhersagen,\n\\begin{align*}\n  \\text{Accuracy} = \\frac{\\textup{TP} + \\textup{TN} }{ \\textup{TP} + \\textup{TN} + \\textup{FP} + \\textup{FN} } = \\frac{\\textup{Anz. korrekte Vorhersagen}}{\\textup{Anz. alle Vorhersagen}}.\n\\end{align*}\nHierbei sind $\\textup{TP}$ und $\\textup{FP}$ die Anazhl korrekter (*true positive*) und falscher (*false positiv*) Vorhersagen für Beobachtungen mit $y_i = 1$. $\\textup{TN}$ und $\\textup{FN}$ sind analog für Beobachtungen mit tatsächlichen Werten $y_i = 0$ definiert.\n\nDie Vorhersage von $y_i$ zur Berechnung von $\\textup{Accuracy}$ erfolgt durch `keras::fit()` standardmäßig nach der Regel\n\\begin{align*}\n  \\hat{y}_i =\n  \\begin{cases}\n    1 & \\text{wenn } \\hat{p}_i \\geq 0.5, \\\\\n    0 & \\text{wenn } \\hat{p}_i < 0.5.\n  \\end{cases}\n\\end{align*}\n\nWir definieren nachchfolgend das Modell-Objekt und passen das NN über 150 Epochen an.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Erstellen und Kompilieren des Modells\nmodel_nn_logit <- keras_model_sequential() %>%\n  layer_dense(\n    units = 1, \n    input_shape = 1, \n    activation = 'sigmoid' # <= für Logit-Modell\n  )\n\nmodel_nn_logit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel: \"sequential_2\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_2 (Dense)                    (None, 1)                       2           \n================================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\n________________________________________________________________________________\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Modell kompilieren\nmodel_nn_logit %>% \n  compile(\n    loss = 'binary_crossentropy', # Für BCEL\n    optimizer = optimizer_adam(learning_rate = 0.01),\n    metrics = 'accuracy'\n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Anpassen des Modells\nhistory_nn_logit <- model_nn_logit %>% \n  fit(\n    x = X, \n    y = Y, \n    validation_split = .2,\n    epochs = 150,\n    verbose = F\n  )\n```\n:::\n\n\n\n\n\n\nDie Zusammenfassung der Anpassung für die letzte Epoche in `history_nn_logit` zeigt ergibt eine Genauigkeit von über 80\\% auf dem Validierungsdatensatz.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory_nn_logit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFinal epoch (plot to see history):\n        loss: 0.4\n    accuracy: 0.805\n    val_loss: 0.3408\nval_accuracy: 0.87 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Entwicklung des Loss über Epochen plotten\nplot(history_nn_logit) +\n  labs(\n    x = \"Epoche\",\n    y = \"\"\n  ) +\n  theme_cowplot() +\n  theme(legend.position = \"top\")\n```\n\n::: {.cell-output-display}\n![NN für Logit-Regression: Entwicklung der Metriken für 25 Epochen](Machine-Learning_files/figure-html/fig-mnn-logithist-1.png){#fig-mnn-logithist width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Gewicht und Bias extrahieren\nmodel_nn_logit %>% \n  keras::get_weights() %>% \n  flatten_dbl() %>% \n  set_names(\n    c(\"w_1\", \"bias\")\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       w_1       bias \n 0.9527357 -5.3522143 \n```\n\n\n:::\n:::\n\n\n\n\n\n\nFür einen Vergleich der Vorhersagegüter mit logistischer Regression schätzen wir zunächst ein entsprechendes GLM mit `glm()`. \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Logistisches Modell mit glm() anpassen\nglm_mod <- glm(\n  formula = Y ~ X, \n  family = binomial(link = \"logit\")\n)\n\nsummary(glm_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = Y ~ X, family = binomial(link = \"logit\"))\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -7.3289     0.6520  -11.24   <2e-16 ***\nX             1.3140     0.1191   11.04   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 673.01  on 499  degrees of freedom\nResidual deviance: 375.96  on 498  degrees of freedom\nAIC: 379.96\n\nNumber of Fisher Scoring iterations: 6\n```\n\n\n:::\n:::\n\n\n\n\n\n\nWir erzeugen nun einen Testdatensatz mit 250 Beobachtungen gemäß des oben gewählten DGP.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(4321)\n\nn <- 250\ndata_new <- tibble(\n  X = rnorm(n = n, mean = 5, sd = 2),\n  P = pnorm(-4 + 0.7 * X),\n  Y = as.integer(runif(n) < P)\n)\n```\n:::\n\n\n\n\n\n\nFür die neuen Datenpunkte `data_new` erzeugen wir Vorhersagen von $P(Y=1\\vert X=x)$ mit `model_nn_logit` und `glm_mod` und erweitern `data_new` um diese.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Vorhersagen für Trainingsdaten erstellen\npredictions_nn_logit <- model_nn_logit %>% \n  predict(data_new$X) %>% \n  c()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n8/8 - 0s - 45ms/epoch - 6ms/step\n```\n\n\n:::\n\n```{.r .cell-code}\npredictions_glm_logit <- predict(\n    glm_mod, \n    newdata = data_new, \n    type = \"response\"\n  )\n\n# Zusammenfassen\nresults <- data_new %>% \n  mutate(\n    nn_logit = predictions_nn_logit,\n    glm_logit = predictions_glm_logit\n)\n\nslice_head(results, n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 5\n       X      P     Y nn_logit glm_logit\n   <dbl>  <dbl> <int>    <dbl>     <dbl>\n 1  4.15 0.136      1    0.198    0.132 \n 2  4.55 0.208      0    0.266    0.206 \n 3  6.44 0.693      1    0.685    0.755 \n 4  6.68 0.751      1    0.734    0.810 \n 5  4.74 0.248      0    0.303    0.250 \n 6  8.22 0.960      1    0.923    0.970 \n 7  4.41 0.180      0    0.240    0.177 \n 8  5.39 0.411      0    0.446    0.439 \n 9  7.48 0.892      0    0.855    0.924 \n10  3.56 0.0660     0    0.124    0.0661\n```\n\n\n:::\n:::\n\n\n\n\n\n\nFür eine erste Beurteilung anhand der Vorhersagen auf dem Testdatensatz plotten wir die vorhergesagten Wahrscheinlichkeiten als Funktion von `X` gemeinsam mit den tatsächlichen Ausprägungen der Outcome-Variable `Y`.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(cowplot)\n\n# Plot der tatsächlichen Werte gegen die vorhergesagten Wahrscheinlichkeiten\nggplot(\n  data = results,\n  mapping =  aes(x = X, y = Y)\n  ) +\n  geom_point(\n    position = position_jitter(height = 0.05), \n    alpha = 0.5\n  ) +\n  geom_line(\n     mapping = aes(y = nn_logit),\n    col = \"darkred\"\n  ) +\n  geom_smooth(\n    method = \"glm\", \n    method.args = list(family = \"binomial\"), \n    se = FALSE\n  ) +\n  labs(\n    title = \"Logistische Regression vs. NN\",\n       x = \"x\",\n       y = \"Schätzung v. P(Y=1|X=x)\"\n  ) +\n  theme_cowplot()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Machine-Learning_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nDie geschätzten Wahrscheinlichkeitsfunktionen zeigen eine gute Übereinstimmung. Um die Vorhersagegüte von `model_nn_logit` und `model_glm_logit` genauer zu untersuchen, erstellen wir Plots der jeweilgen *Receiver Operating Characteristic* (ROC). ROC zeigt den Zusammenhang zwischen der *True Positive Rate* (TPR), dem Anteil korrekter Vorhersagen für $y_i=1$ (auch *Sensitivität* gennant) und der *False Positive Rate* (FPR), dem Anteil falscher Vorhersagen für $y_i=1$ in Abhängigkeit des Schwellenwerts von $\\widehat{p}$ für die Klassifikation des Outcomes einer Beobachtung $y_i = 1$. Es gilt\n\n\\begin{align*}\n  TPR =&\\, \\frac{TP}{TP + FN},\\\\\n  \\\\\n  FPR =&\\, \\frac{FP}{FP + TN}.\n\\end{align*}\n\nDer Schwellenwert $\\widehat{p}$ reguliert den Trade-Off zwischen $\\textup{FPR}$ und $\\textup{TPR}$: Kleine $\\widehat{p}$ führen tendenziell zu großer $\\textup{TPR}$ (gut), aber auch zu großer $\\textup{FPR}$ (schlecht). Für ein Modell, das zufällig klassifiziert, entspricht die ROC-Kurve der Winkelhalbierenden. Wünschenwert ist ein verlauf der ROC-Kurve möglichst oberhalb der Winkelhalbierenden. \n\nEine ROC-Kurve kann in R mit dem `plotROC` anhand der Vorhergesagten und tatsächlichen Werte der Outcome-Varibale berechnet und geplottet werden. Hierzu transformieren wir `results` in langes Format und verwenden `ggplot()` mit dem Layer `geom_roc()`.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(plotROC)\nlibrary(tidyr)\n\nroc_data <- results %>%\n  pivot_longer(\n    cols = glm_logit:nn_logit,\n    names_to = \"model\", \n    values_to = \"pp\"\n    )\n\n# ROC-Kurve plotten\n  ggplot(\n    data = roc_data, \n    mapping = aes(\n      m = pp, \n      d = Y, \n      colour = model\n    )\n  ) +\n  geom_roc() +\n  style_roc() + \n  facet_wrap(~ model) +\n  theme(legend.position = \"top\")\n```\n\n::: {.cell-output-display}\n![ROC-Kurve für logistische Modelle](Machine-Learning_files/figure-html/fig-nnlogit-roc-1.png){#fig-nnlogit-roc width=672}\n:::\n:::\n\n\n\n\n\n\n@fig-nnlogit-roc zeigt sehr ähnliche ROC-Kurven für `model_nn_logit` und `model_glm_logit`.^[Beide Plots enthalten Indikatoren des ROC für bestimmte Schwellenwerte $\\widehat{p}$.] Für die Quantifizierung der Vorhersageleistung wird *Area under the Curve* (AUC), die Fläche unterhalb der ROC-Kurve, herangezogen. Mit `plotROC::calc_auc()` kann AUC aus dem `geom_roc()`-Layer berechnet werden. Für ein geschätztes Modell $\\widehat{M}$ gilt $0\\leq\\textup{AUC}(\\widehat{M})\\leq1$.^[Ein geschätzes, dass nicht besser als raten ist, gilt $\\textup{AUC}(\\widehat{M})=.5$.]\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# AUC berechnen\nroc_data %>%\n  group_by(model) %>%\n  summarise(\n    AUC = calc_auc(\n      ggplot(mapping = aes(m = pp, d = Y)) +\n        geom_roc()\n    )$AUC\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  model       AUC\n  <chr>     <dbl>\n1 glm_logit 0.890\n2 nn_logit  0.890\n```\n\n\n:::\n:::\n\n\n\n\n\n\nDer Vergleich der AUC-Statistiken beider Modelle für den Testdatensatz zeigt, dass das NN ähnlich gut klassifizert, wie das GLM mit logistischer Link-Funktion.\n\n## Empirisches Beispiel: Boston Housing\n\nIn diesem Beispiel illustrieren wir die Anwendung von (NN) für den Boston Housing-Datensatz `MASS::Boston`. `Boston` enthält enthält verschiedene Charakteristika von Häusern in Boston, wie z.B. Kriminalitätsrate, Anzahl der Räume und Alter der Gebäude. Ziel ist esm den Medianpreis der Häuser (`medv`) vorherzusagen.\n\nFür die Analyse verwenden wir insbesondere die Pakete `tidymodels` und `recipes`. Das `recipes`-Paket erlaubt eine automatisierte und leicht reproduzierbare Vorbereitung  von Datensätzen für statistische Modellierung mit `tidymodels`. Es stellt Verben zur Konstruktion von Pipelines mit Schritten wie Skalierung, Kodierung und Handling fehlender Werte bereit. Diese Schritte werden mit einem Rezept (`recipe()`) definiert, vorbereitet (`prep()`) und dann auf Trainings- und/oder Testdaten angewendet (`bake()`).\n\nZunächst laden wir den Boston Housing-Datensatz und teilen `boston_tbl` in Trainings- und Testdaten auf. Hierbei verwenden wir 80% der Daten für das Training und reservieren 20% zum Testen.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n✔ broom        1.0.6      ✔ rsample      1.2.1 \n✔ dials        1.3.0      ✔ tibble       3.2.1 \n✔ infer        1.0.7      ✔ tune         1.2.1 \n✔ modeldata    1.4.0      ✔ workflows    1.1.4 \n✔ parsnip      1.2.1      ✔ workflowsets 1.1.0 \n✔ recipes      1.0.10     ✔ yardstick    1.3.1 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'broom' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'modeldata' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard()        masks purrr::discard()\n✖ dplyr::filter()          masks stats::filter()\n✖ yardstick::get_weights() masks keras::get_weights()\n✖ dplyr::lag()             masks stats::lag()\n✖ recipes::step()          masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n```\n\n\n:::\n\n```{.r .cell-code}\n# Datensatz in tibble umwandeln \nboston_tbl <- as_tibble(MASS::Boston)\n\n# Trainings- und Testdaten einteilen \n# (80% / 20%)\nset.seed(1234)\n\nsplit <- initial_split(boston_tbl, prop = 0.8)\ntrain_data <- training(split)\ntest_data <- testing(split)\n\n# Zielvariable und Prädiktoren aufteilen\ntrain_x <- select(train_data, -medv)\ntrain_y <- train_data$medv\n\ntest_x <- select(test_data, -medv)\ntest_y <- test_data$medv\n```\n:::\n\n\n\n\n\n\nDa neuronale Netze empfindlich auf unterschiedlich skalierte Eingabedaten reagieren (ähnlich wie regularisierte Schätzer, vgl. @sec-regreg), normalisieren wir sämtliche (numerischen) Prädiktoren, ausgenommen `chas` und `rad`, mit `recipes::step_normalize()`.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Daten normalisieren\n\n# Rezept\nrecipe_obj <- recipe(\n  formula = medv ~ ., \n  data = train_data\n  ) %>%\n  step_normalize(\n    all_predictors(), -chas, -rad\n  )\n\n# Vorbreiten\ndata_prep <- prep(\n  x = recipe_obj, \n  training = train_data\n)\n\n# Anwenden\ntrain_x <- bake(\n  object = data_prep, \n  new_data = train_data\n  ) %>% \n  select(-medv)\n\ntest_x <- bake(\n  object = data_prep, \n  new_data = test_data) %>% \n  select(-medv)\n```\n:::\n\n\n\n\n\n\nZunächst führen wir eine KQ-Regression durch, um den Zusammenhang zwischen sämtlichen verfügbaren Prädiktoren und dem Zielwert `medv` zu schätzen. Hierfür nutzen wir das linear_reg()-Modell aus tidymodels und passen es mit der Methode der kleinsten Quadrate (KQ) an.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Lineare Regression mit KQ\nkq_spec <- linear_reg() %>%\n  set_engine(\"lm\")\n\nkq_fit <- fit(\n  object = kq_spec, \n  formula = medv ~ ., \n  data = train_data\n)\n\n# KQ-Vorhersagen für Testdaten\nkq_predictions <- predict(\n  object = kq_fit, \n  new_data = test_data\n  ) %>%\n  bind_cols(test_data) %>%\n  rename(kq_pred = .pred)\n```\n:::\n\n\n\n\n\n\nFür das NN wählen wir eine Spezifikation mit zwei Hidden Layers, die jeweils 64 Neuronen haben. Mit `input_shape = ncol(train_x)` verarbeitet das 1. Hidden Layer die Inputs sämtlicher Prädiktoren in `boston_tbl`. Als Aktivierungsfunktion verwenden wir *Recified Linear Unit*,\n\\begin{align}\n      \\textup{ReLU}(z) \\max(0, z).\\\\\n\\end{align}\n\nDie ReLU-Aktivierungsfunktion setzt also negative Werte auf 0 und lässt positive Werte unverändert. In NN ermöglicht sie eine sparsame anpassung und ist numerisch leicht handhabbar bei der Berechnung des Gradienten: Die Verwendung von Layers mit ReLU verringert das *Vanishing-Gradient-Problem*.^[Bei nicht-linearen Aktivierungsfunktionen kann der Gradient der Loss-Funktion extrem klein werden, was das Training von NN mit vielen Layers schwierig machen kann.]\n\nDas Output Layer des Netzes besteht aus einem einzigen Neuron, dass einen numerischen Wert (den geschätzten Hauspreis) zurückgibt.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# NN mit Keras erstellen\nmodel <- keras_model_sequential() %>%\n  # Hidden Layer 1\n  layer_dense(\n    units = 64, \n    activation = \"relu\", \n    input_shape = ncol(train_x)\n  ) %>%\n  # Hidden Layer 2\n  layer_dense(\n    units = 64, \n    activation = \"relu\"\n  ) %>%\n  # Output Layer\n  layer_dense(units = 1)\n\n# Modell kompilieren\nmodel %>% compile(\n  optimizer = \"rmsprop\",\n  loss = \"mse\",\n  metrics = \"mae\"\n)\n```\n:::\n\n\n\n\n\n\nWir trainieren das NN über 100 Epochen mit einem Batch-Größe von 32 Datenpunkten.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Modell trainieren\nhistory <- model %>% \n  fit(\n    x = as.matrix(train_x), \n    y = train_y,\n    epochs = 100,\n    batch_size = 32,\n    validation_split = 0.2,\n    verbose = 0\n  )\n```\n:::\n\n\n\n\n\n\nMit dem trainierten Nachdem das NN trainiert ist, verwenden wir `model`, um Vorhersagen auf den Testdaten `test_data` zu machen.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Vorhersagen mit trainiertem NN\nnn_pred <- model %>% \n  predict(as.matrix(test_x))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n4/4 - 0s - 65ms/epoch - 16ms/step\n```\n\n\n:::\n\n```{.r .cell-code}\n# Testdaten erweitern\nnn_predictions <- test_data %>%\n  mutate(\n    nn_pred = c(nn_pred)\n  )\n```\n:::\n\n\n\n\n\n\nUm die Leistung der Modelle zu vergleichen, berechnen wir den MSE für beide Modelle.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# MSE berechnen\nols_mse <- kq_predictions %>%\n  summarise(mse = mean((medv - kq_pred)^2)) %>%\n  pull(mse)\n\nnn_mse <- nn_predictions %>%\n  summarise(mse = mean((medv - nn_pred)^2)) %>%\n  pull(mse)\n\nc(\n  \"MSE KQ\" = ols_mse,\n  \"MSE NN\" = nn_mse\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  MSE KQ   MSE NN \n28.79773 10.82384 \n```\n\n\n:::\n:::\n\n\n\n\n\n\nDa das NN ist in der Lage ist, nicht-lineare Zusammenhänge zwischen den Prädiktoren und der Zielvariablen zu modellieren, liefert hat es eine höhere Vorhersagegenauigkeit auf dem Testdatensatz.\n\nFür eine Visualisierung der Vorhersagen tragen wir den wahren und die vorhergesagten Werte von `medv` für beide Modelle in einem Punkteplot ab.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# KQ- und NN-Vorhersagen sammeln\npreds <- kq_predictions %>%\n  select(medv, kq_pred) %>%\n  mutate(nn_pred = nn_predictions$nn_pred)\n\n# Visualisierung der Ergebnisse mit ggplot2\npreds %>%\n  ggplot(aes(x = medv)) +\n  geom_point(aes(y = kq_pred, color = \"OLS\"), alpha = 0.6) +\n  geom_point(aes(y = nn_pred, color = \"Neuronales Netz\"), alpha = 0.6) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\") +\n  labs(\n    x = \"Wahre Werte (medv)\",\n    y = \"Vorhersagen\",\n    color = \"Modell\"\n  ) +\n  theme_minimal() +\n  scale_color_manual(\n    values = c(\n      \"OLS\" = \"red\", \n      \"Neuronales Netz\" = \"steelblue\"\n    )\n  )\n```\n\n::: {.cell-output-display}\n![Boston Housing: KQ vs. NN](Machine-Learning_files/figure-html/fig-bhkq-nn-1.png){#fig-bhkq-nn width=672}\n:::\n:::\n\n\n\n\n\n\n@fig-bhkq-nn zeigt, dass KQ und NN weitgehend vergleichbare Vorhersagen von `medv` auf dem Testdatensatz liefern. Das NN scheint jedoch im Mittel besser in der Vorhersage hoher Verkaufspreise zu sein -- möglicherweise weil extreme Preise auf nicht-lineare Beziehungen zwischen bestimmten Regressoren und `medv` zurückzuführen sind, die in einer linearen KQ-Regression nicht erfasst werden können.\n\n## Case Study: Vorhersage von Immobilienpreisen\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AmesHousing)\nhousing <- make_ames()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the necessary libraries\nlibrary(ggplot2)\nlibrary(sf)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tigris)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n```\n\n\n:::\n\n```{.r .cell-code}\n# Retrieve basemap for Ames, Iowa using the tigris package\nplaces_map <- places(\n  state = 'IA', \n  cb = TRUE, \n  progress = F\n) %>%\n  st_as_sf()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRetrieving data for the year 2022\n```\n\n\n:::\n\n```{.r .cell-code}\n# Filter for Ames city\names_map <- places_map %>% \n  filter(NAME == \"Ames\")\n\nhouses <- housing %>%\n    select(Latitude, Longitude, Sale_Price) %>%\n    mutate(\n      Sale_Price = cut(log(Sale_Price, base = 2), breaks = 5, labels = FALSE)\n    ) %>%\n    st_as_sf(coords = c(\"Longitude\", \"Latitude\"), \n             crs = 4326, agr = \"constant\")\n\nrainbow_colors <- rainbow(5, rev = TRUE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot the map with just the outline of Ames\nggplot() +\n  geom_sf(data = ames_map, color = \"black\", fill = alpha(\"black\", alpha = 0)) +\n  geom_sf(data = houses, mapping = aes(color = factor(Sale_Price)), size = .2) +\n  theme_map() +\n  scale_color_manual(\n    name = \"log(Verkaufspreis)\", \n    values = rainbow_colors, \n    labels = levels(factor(houses$Sale_Price))\n  ) +\n  theme(legend.position = \"bottom\", legend.direction = \"horizontal\") +\n  ggtitle(\"Verkaufte Häuser in Ames, Iowa\")\n```\n\n::: {.cell-output-display}\n![](Machine-Learning_files/figure-html/unnamed-chunk-44-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhousing %>%\n  ggplot(mapping = aes(x = Year_Built, y = Sale_Price)) +\n  geom_point(alpha = .5, fill = \"steelblue\") +\n  theme_cowplot()\n```\n\n::: {.cell-output-display}\n![](Machine-Learning_files/figure-html/unnamed-chunk-45-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split the data into training and testing sets\nset.seed(1234)\n\nlibrary(tidymodels)\n\nsplit <- initial_split(housing, prop = 0.8)\n\nhousing_train <- training(split)\nhousing_test <- testing(split)\n\n# Separate the predictors and the outcome\nhousing_train_x <- housing_train %>% select(-Sale_Price)\nhousing_train_y <- housing_train$Sale_Price\n\nhousing_test_x <- housing_test %>% select(-Sale_Price)\nhousing_test_y <- housing_test$Sale_Price\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nblueprint <- recipe(\n  Sale_Price ~ ., \n  data = housing_train) %>%\n  step_nzv(all_nominal()) %>%\n  step_other(all_nominal(), threshold = .01, other = \"other\") %>%\n  step_integer(matches(\"(Qual|Cond|QC|Qu)$\")) %>%\n  step_YeoJohnson(all_numeric(), -all_outcomes()) %>%\n  step_center(all_numeric(), -all_outcomes()) %>%\n  step_scale(all_numeric(), -all_outcomes()) %>%\n  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)\n\nprepare <- prep(blueprint, training = housing_train)\nprepare\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Recipe ──────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Inputs \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNumber of variables by role\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\noutcome:    1\npredictor: 80\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Training information \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nTraining data contained 2344 data points and no incomplete rows.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Operations \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Sparse, unbalanced variable filter removed: Street and Alley, ... | Trained\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Collapsing factor levels for: MS_SubClass and MS_Zoning, ... | Trained\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Integer encoding for: Overall_Qual and Overall_Cond, ... | Trained\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Yeo-Johnson transformation on: Lot_Frontage and Lot_Area, ... | Trained\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Centering for: Lot_Frontage, Lot_Area, Overall_Qual, ... | Trained\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Scaling for: Lot_Frontage, Lot_Area, Overall_Qual, ... | Trained\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Dummy variables from: MS_SubClass, MS_Zoning, Lot_Shape, ... | Trained\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbaked_train <- bake(prepare, new_data = housing_train)\nbaked_test <- bake(prepare, new_data = housing_test)\n\nbaked_train\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2,344 × 190\n   Lot_Frontage  Lot_Area Overall_Qual Overall_Cond Year_Built Year_Remod_Add\n          <dbl>     <dbl>        <dbl>        <dbl>      <dbl>          <dbl>\n 1       1.15    0.350         -0.0340       -0.487     0.843           0.654\n 2       0.277  -0.0336        -0.800        -0.487    -0.155          -0.839\n 3      -0.129  -0.0213        -0.800        -1.76     -1.75           -1.66 \n 4      -0.909  -3.19          -0.0340        0.479     0.277           0.365\n 5       0.407  -0.223         -1.68          1.26     -0.0554         -0.694\n 6      -1.88    0.0595         1.28         -0.487     1.01            0.847\n 7       0.407   0.000506       0.655        -0.487     0.943           0.750\n 8       0.144  -0.377         -0.0340       -0.487     0.910           0.895\n 9      -0.0461 -1.52          -0.0340       -0.487     0.111          -0.453\n10       1.98    0.501          1.28         -0.487     0.876           0.654\n# ℹ 2,334 more rows\n# ℹ 184 more variables: Mas_Vnr_Area <dbl>, Exter_Qual <dbl>, Exter_Cond <dbl>,\n#   Bsmt_Qual <dbl>, BsmtFin_SF_1 <dbl>, BsmtFin_SF_2 <dbl>, Bsmt_Unf_SF <dbl>,\n#   Total_Bsmt_SF <dbl>, Heating_QC <dbl>, First_Flr_SF <dbl>,\n#   Second_Flr_SF <dbl>, Low_Qual_Fin_SF <dbl>, Gr_Liv_Area <dbl>,\n#   Bsmt_Full_Bath <dbl>, Bsmt_Half_Bath <dbl>, Full_Bath <dbl>,\n#   Half_Bath <dbl>, Bedroom_AbvGr <dbl>, Kitchen_AbvGr <dbl>, …\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Matrix\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'Matrix'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoaded glmnet 4.1-8\n```\n\n\n:::\n\n```{.r .cell-code}\n# Prepare the data for glmnet (which requires matrices)\nx_train_glmnet <- as.matrix(baked_train %>% select(-Sale_Price))\ny_train_glmnet <- log10(baked_train$Sale_Price)\n\nx_test_glmnet <- as.matrix(baked_test %>% select(-Sale_Price))\n\n# Fit a Ridge Regression model\nridge_model <- glmnet(x_train_glmnet, y_train_glmnet, alpha = 0)\n\n# Use cross-validation to find the optimal lambda\ncv_ridge <- cv.glmnet(x_train_glmnet, y_train_glmnet, alpha = 0)\nbest_lambda <- cv_ridge$lambda.min\n\n# Predict on the test set using the best lambda\nridge_preds_log <- predict(cv_ridge, s = best_lambda, newx = x_test_glmnet)\n\n# Convert predictions back from log scale\nridge_preds <- as.numeric(10^ridge_preds_log)\n\n# Evaluate the performance\nmae_vec(\n  truth = housing_test_y, \n  estimate = ridge_preds\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 14695.86\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nx_train <- select(baked_train, -Sale_Price) %>% as.matrix()\ny_train <- baked_train %>% pull(Sale_Price)\n\nx_test <- select(baked_test, -Sale_Price) %>% as.matrix()\ny_test <- baked_test %>% pull(Sale_Price)\n\nnetwork <- keras_model_sequential() %>% \n  layer_dense(units = 512, activation = \"relu\", input_shape = ncol(x_train)) %>%\n  layer_dense(units = 512, activation = \"relu\") %>%\n  layer_dense(units = 512, activation = \"relu\") %>%\n  layer_dense(units = 512, activation = \"relu\") %>%\n  layer_dense(units = 512, activation = \"relu\") %>%\n  layer_dense(units = 1)\n\nnetwork %>%\n  compile(\n    optimizer = optimizer_rmsprop(learning_rate = 0.01),\n    loss = \"msle\",\n    metrics = c(\"mae\")\n  )\n\nset.seed(1234)\n\nhistory <- network %>% fit(\n  x_train,\n  y_train,\n  epochs = 30,\n  batch_size = 32,\n  validation_split = 0.2,\n  callbacks = list(\n        callback_early_stopping(patience = 10, restore_best_weights = TRUE),\n        callback_reduce_lr_on_plateau(factor = 0.2, patience = 4)\n    )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEpoch 1/30\n59/59 - 2s - loss: 2.6961 - mae: 65566.8516 - val_loss: 0.1204 - val_mae: 61695.5000 - lr: 0.0100 - 2s/epoch - 29ms/step\nEpoch 2/30\n59/59 - 1s - loss: 0.3329 - mae: 103204.2891 - val_loss: 0.3089 - val_mae: 77835.7578 - lr: 0.0100 - 571ms/epoch - 10ms/step\nEpoch 3/30\n59/59 - 1s - loss: 0.2295 - mae: 79307.1562 - val_loss: 0.0469 - val_mae: 31878.8457 - lr: 0.0100 - 596ms/epoch - 10ms/step\nEpoch 4/30\n59/59 - 1s - loss: 0.1750 - mae: 69237.9922 - val_loss: 0.0602 - val_mae: 37176.0469 - lr: 0.0100 - 572ms/epoch - 10ms/step\nEpoch 5/30\n59/59 - 1s - loss: 0.1273 - mae: 57111.7969 - val_loss: 0.0546 - val_mae: 39749.2852 - lr: 0.0100 - 569ms/epoch - 10ms/step\nEpoch 6/30\n59/59 - 1s - loss: 0.1003 - mae: 47636.3320 - val_loss: 0.1183 - val_mae: 55252.9570 - lr: 0.0100 - 563ms/epoch - 10ms/step\nEpoch 7/30\n59/59 - 1s - loss: 0.0913 - mae: 47773.8320 - val_loss: 0.0235 - val_mae: 17547.1543 - lr: 0.0100 - 565ms/epoch - 10ms/step\nEpoch 8/30\n59/59 - 1s - loss: 0.0786 - mae: 38930.9961 - val_loss: 0.0226 - val_mae: 18443.8047 - lr: 0.0100 - 571ms/epoch - 10ms/step\nEpoch 9/30\n59/59 - 1s - loss: 0.0717 - mae: 40277.1797 - val_loss: 0.0743 - val_mae: 41584.4727 - lr: 0.0100 - 600ms/epoch - 10ms/step\nEpoch 10/30\n59/59 - 1s - loss: 0.0630 - mae: 38954.5586 - val_loss: 0.1135 - val_mae: 63368.8555 - lr: 0.0100 - 562ms/epoch - 10ms/step\nEpoch 11/30\n59/59 - 1s - loss: 0.0592 - mae: 37092.7109 - val_loss: 0.0756 - val_mae: 43168.1914 - lr: 0.0100 - 667ms/epoch - 11ms/step\nEpoch 12/30\n59/59 - 1s - loss: 0.0564 - mae: 36305.6523 - val_loss: 0.0539 - val_mae: 36463.4414 - lr: 0.0100 - 725ms/epoch - 12ms/step\nEpoch 13/30\n59/59 - 1s - loss: 0.0131 - mae: 14490.8447 - val_loss: 0.0183 - val_mae: 15141.9609 - lr: 0.0020 - 606ms/epoch - 10ms/step\nEpoch 14/30\n59/59 - 1s - loss: 0.0108 - mae: 13287.9570 - val_loss: 0.0229 - val_mae: 18301.9277 - lr: 0.0020 - 861ms/epoch - 15ms/step\nEpoch 15/30\n59/59 - 1s - loss: 0.0093 - mae: 12609.0977 - val_loss: 0.0178 - val_mae: 14599.4482 - lr: 0.0020 - 623ms/epoch - 11ms/step\nEpoch 16/30\n59/59 - 1s - loss: 0.0093 - mae: 12647.4199 - val_loss: 0.0243 - val_mae: 19336.5898 - lr: 0.0020 - 605ms/epoch - 10ms/step\nEpoch 17/30\n59/59 - 1s - loss: 0.0085 - mae: 12090.3691 - val_loss: 0.0217 - val_mae: 16738.5273 - lr: 0.0020 - 575ms/epoch - 10ms/step\nEpoch 18/30\n59/59 - 1s - loss: 0.0086 - mae: 12272.2314 - val_loss: 0.0237 - val_mae: 19321.1289 - lr: 0.0020 - 574ms/epoch - 10ms/step\nEpoch 19/30\n59/59 - 1s - loss: 0.0077 - mae: 11642.5586 - val_loss: 0.0188 - val_mae: 14671.7715 - lr: 0.0020 - 573ms/epoch - 10ms/step\nEpoch 20/30\n59/59 - 1s - loss: 0.0057 - mae: 9591.2090 - val_loss: 0.0188 - val_mae: 14833.0439 - lr: 4.0000e-04 - 574ms/epoch - 10ms/step\nEpoch 21/30\n59/59 - 1s - loss: 0.0056 - mae: 9495.5127 - val_loss: 0.0185 - val_mae: 14587.7715 - lr: 4.0000e-04 - 569ms/epoch - 10ms/step\nEpoch 22/30\n59/59 - 1s - loss: 0.0053 - mae: 9407.4951 - val_loss: 0.0186 - val_mae: 14563.4980 - lr: 4.0000e-04 - 571ms/epoch - 10ms/step\nEpoch 23/30\n59/59 - 1s - loss: 0.0053 - mae: 9368.8047 - val_loss: 0.0187 - val_mae: 14801.7344 - lr: 4.0000e-04 - 568ms/epoch - 10ms/step\nEpoch 24/30\n59/59 - 1s - loss: 0.0050 - mae: 9056.7705 - val_loss: 0.0186 - val_mae: 14604.1611 - lr: 8.0000e-05 - 561ms/epoch - 10ms/step\nEpoch 25/30\n59/59 - 1s - loss: 0.0049 - mae: 9000.3135 - val_loss: 0.0186 - val_mae: 14576.3076 - lr: 8.0000e-05 - 567ms/epoch - 10ms/step\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFinal epoch (plot to see history):\n    loss: 0.004929\n     mae: 9,000\nval_loss: 0.01858\n val_mae: 14,576\n      lr: 0.00008 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(history) + \n  scale_y_log10() +\n  theme_cowplot() +\n  theme(legend.position = \"top\")\n```\n\n::: {.cell-output-display}\n![](Machine-Learning_files/figure-html/unnamed-chunk-52-1.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}