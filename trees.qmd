---
format: 
  live-html:
    webr: 
      packages:
        - 'baguette'
        - 'cowplot'
        - 'gbm'
        - 'dplyr'
        - 'ggplot2'
        - 'MASS'
        - 'purrr'
        - 'randomForest'
        - 'rattle'
        - 'tidymodels'
        - 'tidyr'
        - 'vip'
      cell-options:
        fig-width: 8
engine: knitr
---

{{< include ./_extensions/r-wasm/live/_knitr.qmd >}}

# Baum-basierte Methoden

Baum-basierte Methoden bieten eine vielseitige und leistungsstarke Herangehensweise für Vorhersage und Klassifikation in komplexen Datensätzen mit nicht-linearen Zusammenhängen. Ein Vorteil baum-basierter Methoden ist ihre inhärente Fähigkeit, die Bedeutung einzelner Variablen für die Vorhersage zu quantifizieren – eine Eigenschaft, die viele Machine-Learning-Modelle nicht ohne weiteres bieten und insbesondere in hoch-dimensionalen Anwendungen (mit vielen potentiellen Regressoren) nicht trivial ist. Dies ermöglicht es, tiefere Einblicke in den Einfluss einzelner Merkmale auf die Vorhersagen des Modells zu erhalten, was besonders in empirischen Anwendungen für die Entscheidungsstützung mit Machine Learning hilfreich sein kann.

*Entscheidungsbäume* stellen die Grundlage dieser Methoden dar. Sie ermöglichen die Aufteilung der Daten in immer kleinere, homogenere Gruppen, basierend auf *binären* Entscheidungsregeln, die aus den Prädiktoren abgleitet werden. Die trainierten Regeln eines solchen Modells lassen sich anhand eines Binärbaums visualisieren, was eine intuitive Interpretierbarkeit der Ergebnisse erlaubt. 

*Random Forests* ist ein Ensemble-Ansatz, bei dem viele Entscheidungsbäume kombiniert werden. Jeder Baum wird auf einer zufälligen Teilmenge der Daten trainiert (*Bagging*), und bei jedem Knoten wird zusätzlich eine zufällige Teilmenge der Merkmale berücksichtigt. Die finale Vorhersage des Random Forests basiert auf der Aggregation der Vorhersagen aller Bäume (z.B. Mehrheitsvotum für Klassifikation, Durchschnitt für Regression). Dieses Verfahren reduziert das Risiko einer Überanpassung und erhöht oft die Vorhersagegenauigkeit im Vergleich zu einzelnen Entscheidungsbäumen.

*Boosting* ist eine weitere Ensemble-Methode zur Anpassung von Modellen mit hoher Vorhersagegüte durch Kombination einfacher Modelle (*Base learner*), wobei Regressions- oder Klassifikationsbäume eingesetzt werden können. Alternativ zu Random Forests trainieren Boosting-Algorithmen sukzessiv einfache (Klassifikations- oder Regressions-)Bäume, wobei jeder nachfolgende Baum das Ziel hat, die Vorhersagefehler der vorherigen Bäume zu korrigieren. 

In diesem Kapitel erläutern wir die Anwendung baum-basierter Methoden in R anhand von Beispieldatensätzen. Wir zeigen, wie Regressionsbäume, Random Forests und Boosting-Modelle im `tidymodels`-Framework trainiert werden und wie die Vorhersageleistung durch die Wahl geeigneter Hyperparameter mit Cross-Validation und Out-of-Sample-Evaluierungsmethoden optimiert werden kann.


## Entscheidungsbäume {#sec-simpletrees}

Ein Entscheidungsbaum ist ein Modell, das auf der Basis von hierarchischen Bedingungen bzgl. der Regressoren Vorhersagen für die Outcome-Variable trifft. Jeder Baum beginnt mit einem Wurzelknoten (*root node*) und verzweigt sich binär. Jede Verzweigung (*split*) stellt eine Bedingung dar, die auf einem bestimmten Regressor basiert. Der Baum trifft Entscheidungen, indem er diese Bedingungen sukzessive überprüft, bis er zu einem Blattknoten (*leaf node* / *terminal node*) gelangt, der die finale Vorhersage liefert. Hierbei handelt es sich eine Mehrheitsentscheidung für Klassifikation und einen Mittelwert, jeweils gebildet anhand Beobachten des Trainingsdatensatzes im leaf node.

@fig-exdectree zeigt ein einfaches Beispiel eines Entscheidungsbaums zur Klassifikation der Kreditwürdigkeit einer Person. Die Klassfikation erfolgt, in dem die Beobachtung basierend auf den Merkmalen Alter, Einkommen und Eigentum durch den Baum geleitet wird. Zunächst wird geprüft, die Person 30 Jahre oder jünger ist. Fall ja, entscheidet der Baum anhand des Einkommens: Bei einem Jahreseinkommen von 40.000 oder weniger wird die Person als wenig kreditwürdig klassifiziert, bei höherem Einkommen als mäßig kreditwürdig. Für Personen älter als 30 Jahre überprüft das Modell lediglich, ob die Person eine Immobilie besitzt, um zwischen mäßiger Kreditwürdigkeit und guter Bonität zu unterscheiden.

```{dot}
//| fig-width: 8
//| fig-height: 5
//| fig-cap: "Entscheidungsbaum: Klassifikation von Kreditwürdigkeit"
//| label: fig-exdectree
digraph exdectree {
    node [shape=box];
    splines=false;
    ranksep = 1;  // Abstand zwischen den Ebenen
    nodesep = 1.75;   // Abstand zwischen den Knoten
    margin = 0.3;    // Rand um den gesamten Baum

    1 [label="Alter <= 30?"];
    2 [label="Einkommen <= 40 Tsd.?"];
    3 [label="Eigentum?"];
    4 [label="Status: Niedrig"];
    5 [label="Status: Mittel"];
    6 [label="Status: Hoch"];

    1 -> 2 [label="Ja"];
    1 -> 3 [label="Nein"];
    2 -> 4 [label="Ja"];
    2 -> 5 [label="Nein"];
    3 -> 6 [label="Ja"];
    3 -> 5 [label="Nein"];
}
```

## Training von Bäumen

Zur Konstruktion von Binär-Bäumen werden etablierte Algorithmen wie *Classification and Regression Trees* ([CART](https://de.wikipedia.org/wiki/CART_(Algorithmus)) von @Breimanetal1984 verwendet. Die wesentliche Vorgehensweise ist wie folgt:

1. **Splitting**: Beginnend am root node sucht der Algorithmus nach der ``besten'' Regel, die Daten anhand eines Merkmals in zwei Gruppen zu teilen. Die Qualität des Splits wird in Abhängigkeit der Definition der Outcome-Variable beurteilt:

    **Bei Klassifikation**: Die Reinheit (*purtity*) der Klassen in den unmittelbar nachfolgen nodes wird maximiert. Ein gängiges Kriterium hierfür ist der *Gini-Koeffizient*.
    
    **Bei Regression**: Die Fehlerquadratsumme bei Vorhersage des Outcomes durch Mittelwertbildung für Beobachtungen in den unmittelbar nachfolgenden nodes wird minimiert.

2. **Rekursion**: Der Prozess wird rekursiv fortgesetzt, bis eine maximale Baumtiefe (*tree depth*) erreicht ist oder die Klassen in den leaf nodes ``rein'' sind: alle Beobachtungen in einem lead node gehören zur gleichen Klasse.

3. **Pruning**: Um Überanpassung an die Trainingsdaten zu vermeiden, kann der Baum beschnitten werden (*pruning*). Der Grundgedanke ist, dass tief verzweigte Bäume die Trainingsdaten zwar gut modellieren können, aber schlecht auf neue, unbekannte Daten generalisieren. 

    Bei *cost complexity (CP) pruning* werden, beginnend auf Ebene der leaf nodes sukuzessive Äste entfernt, und eine Balance zwischen Komplexität des Baums und dem Anpassungsfehler zu finden. Ähnlich wie bei regularisierter KQ-Schätzung (@sec-regreg), wird die Verlustfunktion $L$ um einen Strafterm für die Komplexität erweitert. Der Effekt der Strafe wird durch den CP-Parameter $\alpha\in[0,1]$ geregelt,
    
    \begin{align*}
      L_{\alpha}(T) = L(T) + \alpha \lvert T\rvert,
    \end{align*}
  
  für einen Baum $T$ mit Komplexitätsmaß $\lvert T\rvert$ (Anzahl der leaf nodes) [@Hastieetal2013].
 
Zur Demonstation der Schätzung von Regressionsbäumen mit R betrachten wir nachfolgend den Datensatz `MASS::Bosten`. Ziel hierbei ist es, mittlere Hauswerte `medv` in Stadteilen von Boston, MA vorherzusagen. Wir verwenden hierzu Funktionen aus dem Paket `parsnip`. 

Zunächst transformieren wir den Datensatz in ein `tibble`-Objekt und definieren Trainings- und Test-Daten.

```{webr}
library(parsnip)
library(cowplot)

# Seed setzen
set.seed(1234)

# Datensatz als tibble
Boston <- as_tibble(MASS::Boston)

# Splitting in Training- und Test-Daten
Boston_split <- initial_split(
  data = Boston, 
  prop = 0.8, 
  strata = medv
  )

Boston_train <- training(Boston_split)
Boston_test <- testing(Boston_split)

slice_head(Boston_train, n = 10)
```

`parsnip` bietet eine vereinheitlichetes Framework für das Training von Modellen mit R und bietet eine flexible und konsistente API für Machine Learning. Wir definieren zunächst mit `parsnip::decision_tree()` eine Spezifikation zum Training von Entschieundgsmodellen und übergeben beispielhaft einen CP-Parameter $\alpha=.1$. Mit `parsnip::set_engine` wählen wir das Paket `raprt`. Der hier implementierte Agorithmus ist CART. Zuletzt legen wir mit ` parsnip::set_mode()` fest, dass der Algorithmus für Regression durchgeführt werden soll.

```{webr}
# Spezifikation festlegen
tree_spec <- decision_tree(
  cost_complexity = 0.1
  ) %>%
  set_engine("rpart") %>%
  set_mode("regression")

# Modell trainieren
tree_fit <- tree_spec %>%
  fit(
    formula = medv ~ ., 
    data = Boston_train, 
    model = TRUE
  )

# Trainierten Baum in Konsole ausgeben
tree_fit$fit
```

Der Output in `tree_fit$fit` zeigt, dass CP-Pruning zu einem kleinen Baum mit 3 Hierarchie-Ebenen geführt hat. Die Struktur zeigt, dass `lstat` und `rm` für Splitting-Regeln (`split`) verwendet werden, wie viele Beobachtungen den  nodes zugeordnet sind (`n`), den Wert der Verlustfunktion (`deviance`) sowie den Durchschnitt von `medv` für jede node (`yval`). Für die drei leaf nodes (gekennzeichnet mit `*`) ist `yval` die Vorhersage der Outcome-Varibale für entsprechend gruppierte Beobachtungen.

Eine besser interpretierbare Darstellung des angepassten Baums in `tree_fit$fit`  erhalten wir mit `rattle::fancyRpartPlot()`.

```{webr}
#| fig-width: 8
#| fig-height: 8
library(rattle)

# Plot the decision tree
fancyRpartPlot(
  tree_fit$fit,
  split.col = "black", 
  nn.col = "black", 
  caption = "",
  palette = "Set1",
  branch.col = "black"
)
```

Für eine datengetriebene Wahl des CP-Parameters $\alpha$ kann Cross Validation (CV) verwendet werden. Hierzu erstellen wir zunächst eine `parsnip`-Spezifikation mit `cost_complexity = tune::tune()` in `decision_tree()` und erstellen einen *workflow* mit `parsnip::workflow()`

```{webr}
# Spezifikation für CV von cost_complexity
tree_spec_cv <- decision_tree(
  cost_complexity = tune()
  ) %>%
  set_engine("rpart") %>%
  set_mode("regression")

# Workflow definieren
tree_wf_cv <- workflow() %>%
  add_model(tree_spec_cv) %>%
  add_formula(medv ~ .)

```

Mit `rsample::vfold_cv()` definieren wir den CV-Prozess: 10-fold CV mit 2 Wiederholungen. `tune::tune_grid()` führt CV anhand des in `tree_wf_cv` definierten workflows durch. Hierbei werden in `cp_grid` festgelegte Werte von `cost_complexity` berücksichtigt. Die mit `yardstick::metric_set(rmse)` festgelegte Verlustfunktion ist der mittlere quadratische Fehler (RMSE).^[Die hier verwedete Funktion ist `yardstick::rmse()`.]

```{webr}
# CV-Prozess definieren
cv_folds <- vfold_cv(Boston_train, v = 10, repeats = 2)

# CV durchführen:
set.seed(1234)

# Grid definieren
cp_grid <- tibble(
  cost_complexity = c(
    0.1, .075, 0.05, 0.01, 0.001, 0.0001
    )
  )

# Tuning mit CV
tree_fit_cv <- tree_wf_cv %>%
    tune_grid(
        resamples = cv_folds, 
        grid = cp_grid,
        metrics = metric_set(rmse)
    )

# CV-Ergebnisse
tree_fit_cv
```

Mit `workflowsets::autoplot()` kann der CV-RMSE für als Funktion des CP-Parameter leicht grafisch betrachtet dargestellt werden.

```{webr}
# CV-Ergebnisse visualisieren
autoplot(tree_fit_cv) +
  theme_cowplot()
```

Für eine tabellierte Übersicht der besten Modelle kann `tune::show_best()` verwendet werden. `tune::select_best()` liest die beste Parameter-Kombination aus. 

```{webr}
# Tabellarische Übersicht
show_best(
  x = tree_fit_cv, 
  metric = "rmse"
)

# Getunter Paremater
best_tree_fit <- select_best(
  x = tree_fit_cv, 
  metric = "rmse"
)

best_tree_fit_cv
```

Anhand `tree_fit_cv` trainieren wir die finale Spezifikation.

```{webr}
# Finales Modell schätzen
final_tree_spec <- decision_tree(
  cost_complexity = best_tree_fit$cost_complexity
  ) %>%
  set_engine("rpart") %>%
  set_mode("regression")

final_tree_fit <- final_tree_spec %>%
  fit(
    formula = medv ~ ., 
    data = Boston_train
  )

# final_tree_fit
```

Der geringe CP-Parameter führt zu einem großen Entscheidungsbaum.^[Die Dimension der Grafik wurde hier zwecks Darstellung des gesamten Baums gewählt. `print(final_tree_fit$fit)` druckt die Entscheidungsregeln in die R-Konsole (hierzu die letzte Zeile ausführen).] 

```{webr}
#| fig-width: 8
#| fig-height: 8

# CV-Fit plotten
fancyRpartPlot(
  final_tree_fit$fit,
  split.col = "black", 
  nn.col = "black", 
  caption = "",
  palette = "Set1",
  branch.col = "black"
)
```

Zur Beurteilung der Relevanz von Variablen für die Reduktion des Anpassungsfehlers (*variable importance*) kann der Eintrag `variable.importance` des `rpart`-Objekts herangezogen werden. Variable importance misst hier die Gesamtreduktion der Fehlerquadratsumem über alle Knoten, an denen die jeweilige Variable für Splits verwendet wird. 

```{webr}
# Variable-Importance auslesen
final_tree_fit$fit$variable.importance
```

Die Werte zeigen, dass der mit CV ermittelte Baum alle Regressoren in `boston_train` für Splits nutzt, wobei `lstat` und `rm` die relevantesten Variablen sind.

Anhand von Vorhersagen für `medv` mit dem Test-Datensatz `boston_test` können wir das naive Baum-Modell `tree_fit` mit dem durch CV ermittelten Modell `tree_fit_cv` hinsichtlich des Vorhersagefehlers für ungesehene Beobachtungen vergleich. `yardstick::metric()` berechnet hierzu automatisch gängige Statistiken für Regressionsprobleme. 

```{webr}
# Vorhersagegüte naives Modell
tree_pred <- predict(
  object = tree_fit, 
  new_data = Boston_test
) %>%
  bind_cols(Boston_test) %>%
  metrics(truth = medv, estimate = .pred)

# Vorhersagegüte bei CV
tree_pred_cv <- predict(
  object = final_tree_fit, 
  new_data = Boston_test
) %>%
  bind_cols(Boston_test) %>%
  metrics(truth = medv, estimate = .pred)

tree_pred
tree_pred_cv
```

Der Vergleich zeigt eine bessere Vorsageleistung des großen Baums in `tree_fit_cv`. In diesem Fall scheint CP-Pruning wenig hilfreich zu sein. Tatsächlich liefert ein Baum mit $\alpha=0$ bessere Vorhersagen als `tree_fit_cv` (überprüfe dies!). 

## Bagging und Random Forests {#sec-brf}

*Bagging* ist eine Ensemble-Modelle, die durch aus einer Kombination von vielen Entscheidungsbäumen bestehen. Bagging steht für *Bootstrap Aggregating* und nutzt einen Algorithmus, bei dem Bäume auf *zufälligen* Stichproben aus dem Trainingsdatensatz angepasst werden: Jeder Baum wird auf einer *Bootstrap-Stichprobe* (siehe @sec-sim) trainiert, die durch zufällige Züge (mit Zurücklegen) erstellt wird. Nach dem Training aggregiert Bagging die Vorhersagen aller Bäume des Ensembles.

Der Vorteil von Bagging gegenüber einem einzelnen Entscheidungsbaum ist, dass die Varianz der Vorhersage deutlich reduziert werden kann: Einzelne Entscheidungsbäume neigen dazu, Muster in den Trainingsdaten zu lernen, die sich zufällig aus der Zusammensetzung der Stichprobe ergeben und nicht repräsentativ für Zusammenhänge zwischen den Regressoren und der Outcome-Variable sind. Diese Überanpassung führt zu hoher Varianz auf von Vorhersagen für ungesehene Daten. Durch das Training vieler Bäume auf unterschiedlichen *zufälligen* Stichproben aus den Trainingsdaten und das anschließende Aggregieren kann der negative Effekt der Überanpassung auf die Unsicherheit der Vorhersage einzelner Bäume reduziert werden.

Eine Bagging-Spezifikation kann mit `parsnip::bag_tree()` festgelegt werden. Mit `times = 500` wird definiert, dass der Bagging-Algorithmus ein Ensemble mit 500 Bäumen (mit CART) anpassen soll. Das Training und die Vorhersage auf den Testdaten erfolgt analog zur Vorgehensweise in @sec-simpletrees.

```{webr}
# Spezifikation für Bagging
bagging_spec <- bag_tree() %>%
  set_engine(
    engine = "rpart",
    times = 500
  ) %>%
  set_mode("regression")


# Training durchführen
set.seed(1234)

bagging_fit <- bagging_spec %>%
  fit(
    formula = medv ~ ., 
    data = Boston_train
  )

# Auswertung
bagging_pred <- predict(
  object = bagging_fit, 
  new_data = Boston_test
  ) %>%
  bind_cols(Boston_test) %>%
  metrics(
    truth = medv,
    estimate = .pred
  )

bagging_pred
```

Die Auswertung auf den Testdatensatz ergibt eine deutliche Verbesserung der Vorhersageleistung gegenüber einem einfachen Regressionsbaum.

*Random Forests* erweitern Bagging, indem zusätzlich bei jedem Knoten innerhalb jedes Baumes eine *zufällige Teilmenge der Regressoren* als potentielle Variable für die Split-Regel ausgewählt wird. Dies führt zu einer weiteren Reduktion der Korrelation zwischen den Bäumen, was die Genauigkeit verbessert und das Risiko des Overfittings weiter verringert.

```{webr}
set.seed(1234)

# Funktion: Random Forest für mtry = m
rf_spec_mtry <- function(m) {
  rand_forest(mtry = m, trees = 500) %>%
    set_engine(
      engine = "randomForest", 
      importance = TRUE
    ) %>%
    set_mode("regression")
}

rf_mtry4_fit <- rf_spec_mtry(m = 4) %>%
  fit(medv ~ ., data = Boston_train)

rf_mtry6_fit <- rf_spec_mtry(m = 6) %>%
  fit(medv ~ ., data = Boston_train)

rf_mtry10_fit <- rf_spec_mtry(m = 10) %>%
  fit(medv ~ ., data = Boston_train)
```

```{webr}
# 6. Evaluate the random forest models
rf_mtry4_pred <- predict(
  object = rf_mtry4_fit, 
  Boston_test
  ) %>%
  bind_cols(Boston_test) %>%
  metrics(truth = medv, estimate = .pred)

rf_mtry6_pred <- predict(
  object = rf_mtry6_fit, 
  Boston_test
  ) %>%
  bind_cols(Boston_test) %>%
  metrics(
    truth = medv, 
    estimate = .pred
  )

rf_mtry10_pred <- predict(
  object = rf_mtry10_fit, 
  Boston_test
  ) %>%
  bind_cols(Boston_test) %>%
  metrics(
    truth = medv, 
    estimate = .pred
  )

rf_mtry4_pred
rf_mtry6_pred
rf_mtry10_pred
```

```{webr}
# Variable importance für mtry = 10
vip::vip(rf_mtry10_fit$fit) +
  theme_cowplot()
```

## Boosting

```{webr}
# Step 2: Fit the model using gbm
set.seed(1234)

gbm_model <- gbm(
  formula = medv ~ ., 
  data = Boston_train, 
  distribution = "gaussian", 
  n.trees = 50000,         # Number of trees
  interaction.depth = 2,   # Interaction depth
  shrinkage = 0.001,       # Learning rate
  n.minobsinnode = 2       # Minimum observations in nodes
)
```

```{webr}
# Step 3: Make predictions on the test data
gbm_predictions <- predict(
  object = gbm_model, 
  Boston_test, 
  n.trees = gbm_model$n.trees
)

# Step 4: Evaluate the model performance using tidymodels' yardstick package
results <- Boston_test %>%
  mutate(predictions = gbm_predictions) %>%
  metrics(
    truth = medv, 
    estimate = predictions
  )

results
```

```{webr}
# Step 3: Get the variable importance from the gbm model
var_importance <- summary(
  object = gbm_model, 
  plotit = FALSE # k. graphische Ausgabe
)

# Step 4: Create a ggplot2 variable importance plot
# Reorder the variables by importance for better plotting
var_importance <- var_importance %>%
  as_tibble() %>%
  arrange(
    desc(rel.inf)
  )

# Plot using ggplot2
ggplot(
  data = var_importance,
  mapping = aes(
    x = reorder(var, rel.inf), 
    y = rel.inf
  )
) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(
    title = "Variable Importance für Gradient Boosting",
    x = "Variable",
    y = "Relativer Einfluss"
  ) +
  theme_cowplot()
```


