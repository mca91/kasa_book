% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  a4paper,
  DIV=11,
  oneside]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{setspace}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[inner=3cm,outer=5cm,top=3cm,bottom=4cm,headsep=22pt,headheight=11pt,footskip=33pt,ignorehead,ignorefoot,heightrounded]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{lscape}
\newcommand{\blandscape}{\begin{landscape}}
\newcommand{\elandscape}{\end{landscape}}
\usepackage{float}
\floatplacement{table}{t}
\usepackage{amsmath, amssymb, amsthm, amstext}
\usepackage{mlmodern}
\usepackage[T1]{fontenc}
\usepackage{fvextra}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Inhaltsverzeichnis}
\else
  \newcommand\contentsname{Inhaltsverzeichnis}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{Abbildungsverzeichnis}
\else
  \newcommand\listfigurename{Abbildungsverzeichnis}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{Tabellenverzeichnis}
\else
  \newcommand\listtablename{Tabellenverzeichnis}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Abbildung}
\else
  \newcommand\figurename{Abbildung}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Tabelle}
\else
  \newcommand\tablename{Tabelle}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{Listingverzeichnis}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{ngerman}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Kausalanalyse und Machinelles Lernen mit R},
  pdfauthor={Martin C. Arnold, Christoph Hanck},
  pdflang={de},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Kausalanalyse und Machinelles Lernen mit R}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Ein Leitfaden für reproduzierbare Forschung}
\author{Martin C. Arnold, Christoph Hanck}
\date{2023-11-01}

\begin{document}
\maketitle
\RecustomVerbatimEnvironment{verbatim}{Verbatim}{
showspaces = false,
showtabs = false,
breaksymbolleft={},
breaklines
}

\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[sharp corners, enhanced, boxrule=0pt, borderline west={3pt}{0pt}{shadecolor}, interior hidden, breakable, frame hidden]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Inhaltsverzeichnis}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\setstretch{1.2}
\bookmarksetup{startatroot}

\hypertarget{start}{%
\chapter{Start}\label{start}}

\bookmarksetup{startatroot}

\hypertarget{statistische-programmierung-mit-r}{%
\chapter{Statistische Programmierung mit
R}\label{statistische-programmierung-mit-r}}

Dieses Kapitel ist \emph{nicht} als umfassende Einführung in R gedacht,
sondern behandelt Kernfunktionen aus der Paketsammlung
\texttt{tidyverse}. Wenngleich die Inhalte deutlich über ein
Hallo-Welt-Beispiel\footnote{https://de.wikipedia.org/wiki/Hallo-Welt-Programm}
hinausgehen, betrachten wir hier grundlegene Funktionen für
Datenmanipulation und Visualisierung. Diese sind Vorraussetzung für das
Verständnis fortgeschrittener Code-Bausteine in späteren Kapiteln. Falls
Sie bereits über Grundkenntnisse im Umgang mit \texttt{tidyverse}
verfügen, können Sie dieses Kapitel überspringen. Sollten Sie nicht oder
nur teilweise mit den hier gezeigten Befehlen vertraut sein oder
keinerlei Erfahrung mit R haben, empfiehlt sich vorab eine Erarbeitung
bzw. Wiederholung der Inhalte. Nachstehede Ressourcen finden wir
hilfreich:

\begin{itemize}
\item
  Feedbackgestütze interaktive Übungsaufgaben bei DataCamp\footnote{Ein
    Teil des hier angebotenen Katalogs (exlusive \emph{Einführung in R})
    ist kostenpflichtig.}, bspw.

  \begin{itemize}
  \tightlist
  \item
    \href{https://campus.datacamp.com/courses/einfuhrung-in-r/}{Einführung
    in R}
  \item
    \href{https://www.datacamp.com/courses/introduction-to-data-visualization-with-ggplot2}{Introduction
    to Data Visualization with ggplot2}
  \item
    \href{https://www.datacamp.com/courses/data-manipulation-with-dplyr}{Data
    Manipulation with dplyr}
  \end{itemize}
\item
  Open-source-Literatur wie

  \begin{itemize}
  \tightlist
  \item
    der umfangreiche Leitfaden von
    \href{https://methodenlehre.github.io/einfuehrung-in-R/}{Ellis und
    Mayer (2023)}
  \item
    \href{https://r4ds.hadley.nz/}{R for Data Science}
  \item
    \href{https://rstudio-education.github.io/hopr/}{Hands-On
    Programming with R}
  \end{itemize}
\end{itemize}

Wir laden zunächst die Paketsammlung \texttt{tidyverse}. Für die
Reproduktion mit dem \href{https://www.r-project.org/}{R GUI} oder mit
\href{https://posit.co/download/rstudio-desktop/}{RStudio} muss das
Paket vorab mit \texttt{install.packages()} installiert werden. In den
interaktiven R-Konsolen in diesem Kapitel (und im Rest des Buchs) sind
die benötigten R-Pakete bereits installiert \emph{und} geladen, sofern
nicht anders beschrieben.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Paket tidyverse installieren}
\CommentTok{\# install.packages("tidyverse")}

\CommentTok{\# Paket \textquotesingle{}tidyverse\textquotesingle{} laden}
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

Für das Verständnis von Code-Chunks ist es hilfreich, Zwischenergebnisse
explizit zu evaluieren und in der Konsole auszugeben. Hierfür
umschließen wir häufig Code-Zeilen mit runden Klammern. Der nächste
Chunk illustriert dies für die Variable \texttt{x}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Variable definieren...}
\NormalTok{x \textless{}{-} pi}
\NormalTok{\# ... und evaluieren}
\NormalTok{x}

\NormalTok{\# Äquivalent:}
\NormalTok{(}
\NormalTok{  x \textless{}{-} pi}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{lange-weite-und-tidy-datenformate}{%
\section{Lange, weite und ``tidy''
Datenformate}\label{lange-weite-und-tidy-datenformate}}

Wir betrachten den in Tabelle~\ref{tbl-Klausurergebnisse} dargestellten
Datensatz \emph{Klausurergebnisse}.

\hypertarget{tbl-Klausurergebnisse}{}
\begin{longtable}{lrrr}
\caption{\label{tbl-Klausurergebnisse}Datensatz \emph{Klausurergebnisse} }\tabularnewline

\toprule
Name & Mikro & Makro & Mathe \\ 
\midrule\addlinespace[2.5pt]
Tim & NA & $1.3$ & $3$ \\ 
Lena & $1$ & $3$ & NA \\ 
Ricarda & $2$ & $1.7$ & $1.3$ \\ 
Simon & $2.3$ & $3.3$ & NA \\ 
\bottomrule
\end{longtable}

Der Datensatz ist noch nicht in der R-Arbeitsumgebung verfügbar. Mit der
Funktion \texttt{tribble()} können wir
Tabelle~\ref{tbl-Klausurergebnisse} händisch als R-Objekt der Klasse
\texttt{tibble} definieren

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# \textquotesingle{}klasurergebnisse\textquotesingle{} als tibble definieren}
\NormalTok{(}
\NormalTok{  klausurergebnisse \textless{}{-} tribble(}
\NormalTok{    \textasciitilde{}Name,    \textasciitilde{}Mikro, \textasciitilde{}Makro, \textasciitilde{}Mathe,}
\NormalTok{    "Tim",        NA,    1.3,    3.0,}
\NormalTok{    "Lena",      1.0,    3.0,     NA,}
\NormalTok{    "Ricarda",   2.0,    1.7,    1.3,}
\NormalTok{    "Simon",     2.3,    3.3,     NA}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\texttt{klausurergebnisse} enhält die Klausurnoten der vier Studierenden
(Boebachtungen) spaltenweise \emph{pro Modul}, d.h. die Spaltennamen
\texttt{Mikro}, \texttt{Makro} und \texttt{Mathe} sind Ausprägungen der
Variable \emph{Modul}. Der Datensatz liegt also \emph{nicht} im s.g.
\emph{Tidy-Format} vor.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, bottomtitle=1mm, arc=.35mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tidy-Format}, opacitybacktitle=0.6, colback=white, left=2mm, breakable, leftrule=.75mm, opacityback=0, coltitle=black, titlerule=0mm, colframe=quarto-callout-tip-color-frame, toprule=.15mm, toptitle=1mm]

Tidy-Format: Jede Spalte ist \textbf{\emph{eine}} Variable, jede Reihe
ist \textbf{\emph{eine}} Beobachtung und jede Zelle enthält einen
\textbf{\emph{einen}} Wert. Datensätze im Tidy-Format sind häufig lang:
Die Zeilendimension ist größer als die Spaltendimension.

\end{tcolorbox}

Das Tidy-Format ist hilfreich für statistische Analysen mit
\texttt{tidyverse}-Funktionen wie bspw. \texttt{ggplot()}. Wir nutzen
die Funktion \texttt{tidyr::pivot\_longer()}, um
\texttt{klausurergebnisse} ein (langes) Tidy-Format zu transformieren.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# \textquotesingle{}klausurergebnisse\textquotesingle{} in Tidy{-}Format überführen}
\NormalTok{(}
\NormalTok{  long \textless{}{-} pivot\_longer(}
\NormalTok{    data = klausurergebnisse, }
\NormalTok{    cols = Mikro:Mathe, }
\NormalTok{    names\_to = "Modul", }
\NormalTok{    values\_to = "Note"}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Beachte, dass die Spalte \texttt{Name} die Zugehörigkeit der
Ausprägungen (\texttt{Note}) jeder Variable (\texttt{Modul}) zu einer
Beobachtung identifiziert. Mit dieser Information können wir den langen
Datensatz wieder in das ursprüngliche (weite) Format zurückführen. Wir
nutzen hierzu \texttt{tidyr::pivot\_wider()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# langes Format in das Ausgangsformat transformieren}
\NormalTok{(}
\NormalTok{  wide \textless{}{-} pivot\_wider(}
\NormalTok{    data = long,}
\NormalTok{    id\_cols = "Name",}
\NormalTok{    names\_from = "Modul", }
\NormalTok{    values\_from = "Note"}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Wenn die Zuweisung von Zwischenergebnissen in Variablen nicht benötigt
wird, kann eine Verkettung von Funktionsaufrufen die Verständlichkeit
des Codes verbessern. Hierzu wird der
\href{https://magrittr.tidyverse.org/reference/pipe.html}{Pipe-Operator}
\texttt{\%\textgreater{}\%} genutzt. Wir wiederholen die
Transformationen mit den \texttt{tidyr::pivot\_*}-Funktion bei
Verwendung von \texttt{\%\textgreater{}\%}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# langes Format mit \%\textgreater{}\%}
\NormalTok{(}
\NormalTok{  long \textless{}{-} klausurergebnisse \%\textgreater{}\% }
\NormalTok{    pivot\_longer(}
\NormalTok{      cols = Mikro:Mathe, }
\NormalTok{      names\_to = "Modul", }
\NormalTok{      values\_to = "Note"}
\NormalTok{    )}
\NormalTok{)}

\NormalTok{\# weites Format mit \%\textgreater{}\%}
\NormalTok{(}
\NormalTok{  wide \textless{}{-} long \%\textgreater{}\% }
\NormalTok{    pivot\_wider(}
\NormalTok{      id\_cols = "Name",}
\NormalTok{      names\_from = "Modul", }
\NormalTok{      values\_from = "Note"}
\NormalTok{    )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Ein Beispiel für den Nachteil des weiten Formats im Umgang mit
\texttt{tidyverse}-Paketen ist die Funktion \texttt{tidyr::drop\_na()}.
Diese entfernt sämtliche \emph{Zeilen} eines Datensatzes, die
\texttt{NA}-Einträge (d.h. fehlende Werte) aufweisen. Beachte, dass
diese Operation im ursprünglichen weiten Format zum Entfernen ganzer
Beobachtungen aus \texttt{wide} führt.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# NA{-}Einträge aus dem "weiten" Format entfernen}
\NormalTok{wide \%\textgreater{}\% }
\NormalTok{  drop\_na()}
\end{Highlighting}
\end{Shaded}

Im Tidy-Format \texttt{long} hingegen bleiben die übrigen Informationen
betroffener Beobachtungen erhalten.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# NA{-}Einträge aus dem "langen" Format entfernen}
\NormalTok{long \%\textgreater{}\% }
\NormalTok{  drop\_na()}
\end{Highlighting}
\end{Shaded}

\hypertarget{pinguine-und-pipes}{%
\section{Pinguine und Pipes}\label{pinguine-und-pipes}}

In diesem Abschnitt zeigen wir die Verwendung häufig verwendeter
\texttt{dplyr}-Funktionen (s.g. \emph{Verben}) für die Transformation
von Datensätzen: \texttt{mutate()}, \texttt{select()},
\texttt{filter()},\texttt{summarise()} und \texttt{arrange()}.

Für die Illustration verwenden wir den Datensatz \texttt{penguins} aus
dem R-Paket \texttt{palmerpenguins}. Dieser Datensatz wurde im Zeitraum
2007 bis 2009 von Dr.~Kristen Gorman im Rahmen des \emph{Palmer Station
Long Term Ecological Research Program} zusammengetragen und enthält
Größenmessungen für drei Pinguinarten, die auf den Inseln des
\href{https://en.wikipedia.org/wiki/Palmer_Archipelago}{Palmer-Archipels}
in der Antarktis beobachtet wurden.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Paket \textquotesingle{}palmerpenguins\textquotesingle{} installieren}
\CommentTok{\# install.packages("palmerpenguins")}

\CommentTok{\# Paket \textquotesingle{}palmerpenguins\textquotesingle{} laden}
\FunctionTok{library}\NormalTok{(palmerpenguins)}
\end{Highlighting}
\end{Shaded}

Mit \texttt{data()} wird der Datensatz in der Arbeitsumgebung verfügbar
gemacht. Wir nutzen \texttt{glimpse()}, um einen Überblick zu erhalten.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Datensatz in der Arbeitsumgebung verfügbar machen}
\NormalTok{data(penguins)}

\NormalTok{\# Übersicht anzeigen lassen}
\NormalTok{glimpse(penguins)}
\end{Highlighting}
\end{Shaded}

\hypertarget{dplyrmutate}{%
\subsection{\texorpdfstring{\texttt{dplyr::mutate()}}{dplyr::mutate()}}\label{dplyrmutate}}

Mit \texttt{mutate()} können bestehende Variablen überschrieben oder
neue Variablen als Funktion bestehender Variablen definiert werden.
\texttt{mutate()} operiert in der Spaltendimension des Datensatz.

Wir definieren eine neue Variable \texttt{body\_mass\_kg} als
Transformation \texttt{body\_mass\_g/1000}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Neue Variable mit Gewicht in Kg definieren}
\NormalTok{penguins \%\textgreater{}\% }
\NormalTok{  mutate(}
\NormalTok{    body\_mass\_kg = body\_mass\_g/1000}
\NormalTok{  ) \%\textgreater{}\%}
\NormalTok{  glimpse()}
\end{Highlighting}
\end{Shaded}

Mit \texttt{across()} kann die dieselbe Operation auf mehrere Variablen
angewendet werden.

Im nachstehenden Beispiel ändern wir den typ (\texttt{type}) der
Variablen \texttt{species}, \texttt{island}, \texttt{sex} und
\texttt{year} zu \texttt{character}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# species, island, sex und year in Typ \textquotesingle{}character\textquotesingle{} umwandeln}
\NormalTok{penguins \%\textgreater{}\% }
\NormalTok{  mutate(}
\NormalTok{    across(}
\NormalTok{      c(species, island, sex, year), }
\NormalTok{      .fns = as.character}
\NormalTok{    )}
\NormalTok{  ) \%\textgreater{}\%}
\NormalTok{  glimpse()}
\end{Highlighting}
\end{Shaded}

\texttt{transmute()} ist eine Variante von \texttt{mutate()}, die
lediglich die transformierten Variablen beibehält.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Nur transformierte Variablen behalten}
\NormalTok{penguins \%\textgreater{}\% }
\NormalTok{  transmute(}
\NormalTok{    body\_mass\_kg = body\_mass\_g/1000}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{dplyrselect}{%
\subsection{\texorpdfstring{\texttt{dplyr::select()}}{dplyr::select()}}\label{dplyrselect}}

Mit \texttt{select()} werden Variablen aus dem Datensatz ausgewählt.
Dies geschieht entweder über den Variablennamen\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# \textquotesingle{}species\textquotesingle{} auswählen}
\NormalTok{penguins \%\textgreater{}\% }
\NormalTok{  select(species)}
\end{Highlighting}
\end{Shaded}

\ldots{} oder über eine Indexmenge.\footnote{Hilfreich:
  \texttt{dplyr::pull()} selektiert eine Variable und wandelt diese in
  einen Vektor um.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Teilmenge von Variablen per Index auswählen}
\NormalTok{penguins \%\textgreater{}\% }
\NormalTok{  select(}
\NormalTok{    c(1, 2, 3)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Variablen können anhand eines Muster im Namen selektiert werden. Die
Selektion von \texttt{ends\_with("mm")} bezieht nur Variablen mit Endung
\texttt{mm} im Namen ein:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Nur in mm gemessene Variablen auslesen}
\NormalTok{penguins \%\textgreater{}\% }
\NormalTok{  select(}
\NormalTok{    ends\_with("mm")}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Mit \texttt{where()} können wir Variablen aufgrund bestimmter
Eigenschaften ihrer Ausprägungen selektieren.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Nur numerische Variablen auswählen}
\NormalTok{penguins \%\textgreater{}\% }
\NormalTok{  select(}
\NormalTok{    where(is.numeric)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{dplyrfilter}{%
\subsection{\texorpdfstring{\texttt{dplyr::filter()}}{dplyr::filter()}}\label{dplyrfilter}}

Das Verb \texttt{filter()} filtert den Datensatz in der Zeilendimension.
So können Beobachtungen ausgewält werden, deren Merkmalsausprägungen
bestimmte Kriterien erfüllen. Hierzu muss \texttt{filter()} ein
logischer (\texttt{logical}) Ausdruck übergeben werden. Häufig erfolgt
dies über Vergleichsoperatoren.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Nur Pinguine mit bill\_length\_mm \textgreater{}= 39}
\NormalTok{penguins \%\textgreater{}\% }
\NormalTok{  filter(}
\NormalTok{    bill\_length\_mm \textgreater{}= 39}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Nur Pinguine mit bill\_length\_mm \textless{}= 40}
\NormalTok{penguins \%\textgreater{}\% }
\NormalTok{  filter(}
\NormalTok{    bill\_length\_mm \textless{}= 40}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Oft ist es praktisch, mehrere Kriterien zu kombinieren.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Kombinierter Filter {-}{-} Variante 1}
\NormalTok{penguins \%\textgreater{}\% }
\NormalTok{  filter(}
\NormalTok{    bill\_length\_mm \textgreater{}= 39 \& bill\_length\_mm \textless{}= 40}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Analog: komma-getrennte Kriterien werden intern über den Und-Operator
(\texttt{\&}) verknüpft.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Kombinierter Filter {-}{-} Variante 2}
\NormalTok{penguins \%\textgreater{}\% }
\NormalTok{  filter(}
\NormalTok{    bill\_length\_mm \textgreater{}= 39, }
\NormalTok{    bill\_length\_mm \textless{}= 40}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Ähnlich wie bei \texttt{select()} verwenden wir häufig nützliche
Funktionen, welche die Interpretation des Codes erleichtern.
\texttt{dplyr::between()} erlaubt filtern innerhalb eines Intervals.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Filtern mit Hilfsfunktion}
\NormalTok{penguins \%\textgreater{}\% }
\NormalTok{  filter(}
\NormalTok{    between(}
\NormalTok{      bill\_length\_mm, left = 39, right = 40}
\NormalTok{    )}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Mit diesen Verben sind wir bereits in der Lage, den Datensatz gemäß
folgender Vorschrift zu bereinigen:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Entfernen der Maßeinheiten aus den Variablennamen
\item
  Entfernen von Pinguinen mit fehlenden Werten (\texttt{NA})
\item
  Entfernen von Pinguinen mit einem Gewicht \emph{oberhalb} des
  95\%-Stichprobenquantils
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Schritt 1}
\NormalTok{(}
\NormalTok{  penguins\_cleaned \textless{}{-} penguins \%\textgreater{}\% }
\NormalTok{    rename(}
\NormalTok{      bill\_length = bill\_length\_mm,}
\NormalTok{      bill\_depth  = bill\_depth\_mm,}
\NormalTok{      flipper\_length = flipper\_length\_mm,}
\NormalTok{      body\_mass = body\_mass\_g}
\NormalTok{    )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Schritt 2}
\NormalTok{(}
\NormalTok{  penguins\_cleaned \textless{}{-} penguins\_cleaned \%\textgreater{}\%}
\NormalTok{    drop\_na()}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Schritt 3}
\NormalTok{penguins\_cleaned \%\textgreater{}\% }
\NormalTok{  filter(}
\NormalTok{    body\_mass \textless{} quantile(body\_mass, probs = .95)}
\NormalTok{  ) \%\textgreater{}\%}
\NormalTok{  glimpse()}
\end{Highlighting}
\end{Shaded}

Durch die Verkettung mit \texttt{\%\textgreater{}\%} können wir
sämtliche Schritte für die Bereinigung ohne das Abspeichern von
Zwischenergebnissen durchführen.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Verketter Funktionsaufruf für Datensatzbereinigung}
\NormalTok{penguins\_cleaned \textless{}{-} penguins \%\textgreater{}\% }
\NormalTok{  rename(}
\NormalTok{    bill\_length = bill\_length\_mm,}
\NormalTok{    bill\_depth  = bill\_depth\_mm,}
\NormalTok{    flipper\_length = flipper\_length\_mm,}
\NormalTok{    body\_mass = body\_mass\_g}
\NormalTok{  ) \%\textgreater{}\% }
\NormalTok{  drop\_na() \%\textgreater{}\% }
\NormalTok{  filter(}
\NormalTok{    body\_mass \textless{} quantile(body\_mass, .95)}
\NormalTok{  )}

\NormalTok{penguins\_cleaned \%\textgreater{}\% }
\NormalTok{  glimpse()}
\end{Highlighting}
\end{Shaded}

\hypertarget{dplyrsummarise}{%
\subsection{\texorpdfstring{\texttt{dplyr::summarise()}}{dplyr::summarise()}}\label{dplyrsummarise}}

Das Verb \texttt{summarise()} fasst Variablen über Beobachtungen hinweg
zusammen. Der nachstehende Code-Chunk erzeugt eine Tabelle mit
Stichprobenmittelwert und -standardabweichung von
\texttt{flipper\_length\_mm}.\footnote{\texttt{dplyr::summarise()} darf
  nicht mit \texttt{base::summary()} verwechselt werden!} Um zu
vermeiden, dass die Auswertung aufgrund fehlender Werte (\texttt{NA}) in
\texttt{flipper\_length\_mm} scheitert, lassen wir \texttt{NA}s mit
\texttt{na.rm\ =\ TRUE} bei der Berechnung unberücksichtigt (wir
verwenden weiterhin den unbereinigten Datensatz \texttt{penguins}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# statistische Zusammenfassung mit \textquotesingle{}summarise()\textquotesingle{}}
\NormalTok{penguins \%\textgreater{}\% }
\NormalTok{  select(flipper\_length\_mm) \%\textgreater{}\% }
\NormalTok{  summarise(}
\NormalTok{    mean = mean(flipper\_length\_mm, na.rm = TRUE), }
\NormalTok{    sd = sd(flipper\_length\_mm, na.rm = TRUE)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Varianten von \texttt{summarise()} können über mehrere Variablen
angewendet werden. Wir verwenden \texttt{across()} und \texttt{where()},
um lediglich numerische Variablen mit den in der liste definierten
Funktionen zusammenzufassen. Beachte, dass
\texttt{\textbackslash{}(x)\ mean(x)} eine anonyme Funktion definiert.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins \%\textgreater{}\% }
\NormalTok{  summarise(}
\NormalTok{    across(}
\NormalTok{      where(is.numeric), }
\NormalTok{      .fns = list(}
\NormalTok{        mean = \textbackslash{}(x) mean(x, na.rm = TRUE), }
\NormalTok{        sd = \textbackslash{}(x) sd(x, na.rm = TRUE)}
\NormalTok{      )}
\NormalTok{    )}
\NormalTok{  ) \%\textgreater{}\%}
\NormalTok{  glimpse()}
\end{Highlighting}
\end{Shaded}

\hypertarget{dplyrarrange}{%
\subsection{\texorpdfstring{\texttt{dplyr::arrange()}}{dplyr::arrange()}}\label{dplyrarrange}}

Mit \texttt{arrange()} können Datensätze in Abhängigkeit der
beobachteten Ausprägungen von Variablen sortiert werden.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Datensatz aufsteigend nach \textquotesingle{}body\_mass\_g\textquotesingle{} sortieren}
\NormalTok{penguins \%\textgreater{}\% }
\NormalTok{  arrange(body\_mass\_g)}
\end{Highlighting}
\end{Shaded}

Die Funktion \texttt{dplyr::desc()} kehrt die Reihenfolge zu einer
absteigenden Sortierung um.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Absteigende Sortierung nach \textquotesingle{}body\_mass\_g\textquotesingle{}}
\NormalTok{penguins \%\textgreater{}\% }
\NormalTok{  arrange(}
\NormalTok{    desc(body\_mass\_g)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Komplexe Sortier-Muster werden durch Übergabe von Variablennamen in der
gewünschten Reihenfolge erreicht.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Erst Sortierung nach \textquotesingle{}sex\textquotesingle{}, dann gruppenweise absteigend }
\NormalTok{\# nach \textquotesingle{}body\_mass\_g\textquotesingle{} sortieren}
\NormalTok{penguins \%\textgreater{}\% }
\NormalTok{  arrange(}
\NormalTok{    sex, desc(body\_mass\_g)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{operationen-mit-gruppierten-datensuxe4tzen}{%
\subsection{Operationen mit gruppierten
Datensätzen}\label{operationen-mit-gruppierten-datensuxe4tzen}}

Für manche Transformationen ist eine Gruppierung der Daten hilfreich.
Wir illustrieren nachfolgend die unterschiedlichen Verhaltensweisen
ausgewählter Verben durch Vergleiche von gruppierten und
nicht-gruppierten Anwendungen.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Datensatz gruppieren}
\NormalTok{penguins\_grouped \textless{}{-} penguins \%\textgreater{}\% }
\NormalTok{  group\_by(species)}

\NormalTok{\# Datensatz hat nun die Eigenschaft \textquotesingle{}Groups\textquotesingle{}}
\NormalTok{glimpse(penguins\_grouped)}
\end{Highlighting}
\end{Shaded}

\texttt{species} hat drei Ausprägungen. Entsprechend ist
\texttt{penguins\_grouped} nun in drei Gruppen eingeteilt.

Bei gruppierten Datensätzen fasst \texttt{summarise()} die Variablen pro
Guppe zusammen.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# summarise {-}{-} ungruppiert:}
\NormalTok{penguins \%\textgreater{}\%}
\NormalTok{  summarise(}
\NormalTok{    across(}
\NormalTok{      where(is.numeric), \textbackslash{}(x) mean(x, na.rm = T)}
\NormalTok{      )}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# summarise {-}{-} gruppiert:    }
\NormalTok{penguins\_grouped \%\textgreater{}\%}
\NormalTok{  summarise(}
\NormalTok{    across(}
\NormalTok{      where(is.numeric), }
\NormalTok{      \textasciitilde{} mean(., na.rm = T)}
\NormalTok{    )}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\texttt{mutate()} definiert bzw. transformiert für jede Gruppe separat.
Im dies zu veranschaulichen, ziehen wir eine Zufallsstichprobe von 10
Pinguinen aus der Datensatz.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Zufallsstichprobe generieren}
\NormalTok{set.seed(123)}
\NormalTok{(}
\NormalTok{  penguins\_sample \textless{}{-} penguins \%\textgreater{}\%}
\NormalTok{    slice\_sample(n = 10)  }
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# mutate() {-}{-} ungruppiert:}
\NormalTok{penguins\_sample \%\textgreater{}\%}
\NormalTok{  transmute(}
\NormalTok{    mean = mean(bill\_length\_mm)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Für den ungruppierten Datensatz berechnet \texttt{mutate()} das
Stichprobenmittel von \texttt{bill\_length\_mm} über \emph{alle} zehn
Datenpunkte und weißt diesen Wert jeweils in der Variable \texttt{mean}
zu.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# mutate() {-}{-} gruppiert}
\NormalTok{penguins\_sample \%\textgreater{}\%}
\NormalTok{  group\_by(species) \%\textgreater{}\%}
\NormalTok{  transmute(}
\NormalTok{    mean = mean(bill\_length\_mm)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Bei gruppierten Daten berechnet \texttt{mutate()} das Stichprobenmittel
\emph{pro Gruppe} und weist die Mittelwerte entsprechend zu.

\hypertarget{eine-explorative-analyse-mit-ggplot2}{%
\section{\texorpdfstring{Eine explorative Analyse mit
\texttt{ggplot2}}{Eine explorative Analyse mit ggplot2}}\label{eine-explorative-analyse-mit-ggplot2}}

Der bereinigte Datensatz \texttt{penguins\_cleaned} eignet sich gut für
eine graphische Auswertung mit dem R-Paket \texttt{ggplot2}, welches
Bestandteil des \texttt{tidyverse} ist. Nachfolgend untersuchen wir
Zusammenhänge zwischen den Körpermaßen der Pinguine.

Wir erstellen zunächst einen einfachen Punkteplot des Gewichts
(\texttt{body\_mass}) und der Schnabeltiefe (\texttt{bill\_depth}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Punkteplot: body\_mass vs. bill\_depth}
\NormalTok{penguins\_cleaned \%\textgreater{}\%}
\NormalTok{  ggplot(}
\NormalTok{    mapping = aes(}
\NormalTok{      x = body\_mass, }
\NormalTok{      y = bill\_depth}
\NormalTok{    )}
\NormalTok{  ) +}
\NormalTok{  geom\_point()}
\end{Highlighting}
\end{Shaded}

Die Grafik zeigt einen positiven Zusammenhang zwischen dem Gewicht und
der Schnabeltiefe. Als nächstes passen wir den Code so an, dass die
Datenpunkte entsprechend der Art (\texttt{species}) eingefärbt sind.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Punkteplot: Farbliche Darstellung verschiedener Arten}
\NormalTok{penguins\_cleaned \%\textgreater{}\%}
\NormalTok{  ggplot(}
\NormalTok{    mapping = aes(}
\NormalTok{      x = body\_mass, }
\NormalTok{      y = bill\_depth, }
\NormalTok{      color = species}
\NormalTok{    )}
\NormalTok{  ) +}
\NormalTok{  geom\_point()}
\end{Highlighting}
\end{Shaded}

Offenbar gibt es deutliche Unterschiede in der (gemeinsamen) Verteilung
von Gewicht und Schnabeltiefe zwischen den verschiedenen Arten.

Um den Zusammenhang zwischen Gewicht und Schnabeltiefe zu untersuchen,
schätzen wir lineare Regressionen
\[body\_mass = \beta_0 + \beta_1 bill\_depth + u\] separat für jede der
drei Pinguinarten mit der KQ-Methode. Anschließend zeichnen wir die
geschätzten Regressionsgeraden ein.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Lineare Regression per Art}
\NormalTok{penguins\_cleaned \%\textgreater{}\%}
\NormalTok{  ggplot(}
\NormalTok{    aes(}
\NormalTok{      x = body\_mass, }
\NormalTok{      y = bill\_depth, }
\NormalTok{      color = species}
\NormalTok{    )}
\NormalTok{  ) +}
\NormalTok{  geom\_point() +}
\NormalTok{  geom\_smooth(method = "lm", se = F)}
\end{Highlighting}
\end{Shaded}

Die Schätzungen bekräftigen die Vermutung, dass der lineare Zusammenhang
zwischen Gewicht und Schnabeltiefe sich nicht zwischen den verschiedenen
Pinguinarten unterscheidet: Pinguine der Art \emph{Gentoo} sind im
Mittel schwerer als Pinguine der übrigen Arten, haben jedoch eine
geringere Schnabeltiefe.

Der nachfolgende Code fügt der Grafik eine Regressionsline \emph{über
alle} Arten hinzu. Wir setzen hierbei das Argment
\texttt{inherit\_aes\ =\ FALSE} und legen damit fest, dass die
Regression für \texttt{body\_mass} und \texttt{bill\_depth} ohne
Differenzierung per \texttt{species} durchgeführt wird.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Zusatz: Globale Regression}
\NormalTok{penguins\_cleaned \%\textgreater{}\%}
\NormalTok{  ggplot(}
\NormalTok{    mapping = aes(}
\NormalTok{      x = body\_mass, }
\NormalTok{      y = bill\_depth, }
\NormalTok{      color = species}
\NormalTok{    )}
\NormalTok{  ) +}
\NormalTok{  geom\_point() +}
\NormalTok{  geom\_smooth(method = "lm", se = F) +}
\NormalTok{  \# Regression für alle Datenpunkte}
\NormalTok{  geom\_smooth(}
\NormalTok{    mapping = aes(}
\NormalTok{      x = body\_mass, }
\NormalTok{      y = bill\_depth}
\NormalTok{    ),}
\NormalTok{    method = "lm", }
\NormalTok{    se = F, }
\NormalTok{    inherit.aes = F}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Offenbar ist die vorherige Analyse per Spezies sinnvoller: Die
Regression über alle Arten suggeriert einen negativen Zusammenhang
zwischen Gewicht und Schnabeltiefe.

\emph{Facetting} mit \texttt{facet\_wrap()} erlaubt eine Untersuchung
des Zusammenhangs je Insel (\texttt{island}), auf der die Messung
erfolgt ist.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Facettierung des per In}
\NormalTok{penguins\_cleaned \%\textgreater{}\%}
\NormalTok{  ggplot(}
\NormalTok{    mapping = aes(}
\NormalTok{      x = body\_mass, }
\NormalTok{      y = bill\_depth, }
\NormalTok{      color = species)}
\NormalTok{  ) +}
\NormalTok{  geom\_point() +}
\NormalTok{  geom\_smooth(method = "lm", se = F) +}
\NormalTok{  facet\_wrap(\textasciitilde{} island)}
\end{Highlighting}
\end{Shaded}

Wir sehen, dass es hinsichtlich des Zusammenhangs von Gewicht und
Schnabeltiefe keine wesentlichen Diskrepanzen zwischen den drei Inseln
gibt. Darüber hinaus lässt sich anhand der Facetten leicht erkennen, wie
die drei Arten über die Inseln verteilt sind.

\bookmarksetup{startatroot}

\hypertarget{matching}{%
\chapter{Matching}\label{matching}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\#| context: setup}
\NormalTok{webr::install("dplyr")}
\NormalTok{webr::install("tidyr")}

\NormalTok{\# create dataset directory}
\NormalTok{dir.create("datasets")}
\NormalTok{\# Download the dataset}
\NormalTok{download.file(}
\NormalTok{    "https://raw.githubusercontent.com/mca91/kausal\_data/main/darkmode.csv",}
\NormalTok{    \textquotesingle{}datasets/darkmode.csv\textquotesingle{},}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\#| context: setup}
\NormalTok{library(dplyr)}
\NormalTok{\# make darkmode available using read.csv for now}
\NormalTok{\# because there\textquotesingle{}s some issue with readr::read\_csv}
\NormalTok{\# I can\textquotesingle{}t fix right now}
\NormalTok{darkmode \textless{}{-} read.csv(}
\NormalTok{    file = "datasets/darkmode.csv", }
\NormalTok{    colClasses = c("numeric", "logical", "numeric", "numeric", "numeric") }
\NormalTok{)}

\NormalTok{options(pillar.bold = TRUE, pillar.subtle = FALSE)}
\end{Highlighting}
\end{Shaded}

In randomisierten kontrollierten Studien stellt eine randomisierte
Behandlung sicher, dass die Individuen beider Gruppen im Mittel
vergleichbar sind, dass heißt es gibt keine systematischen Unterschiede
der Studiensubjekte hinsichtlich der Verteilung von Charakteristika
zwischen den Gruppen. Dann ist es plausibel eine beobachtete mittlere
Differenz in der Outcome-Variable alleine auf die Behandlung
zurückzuführen.

In der Praxis, insbesondere in ökonomischen Studien, sind randomisierte
kontrollierte Studien aus ethischen und/oder finanziellen Gründen nicht
durchführbar. Stattdessen werden nicht-experimentelle Daten genutzt, die
jedoch nur sehr selten eine Vergleichbarkeit von Behandlungs- und
Kontrollgruppe gewährleisten.

In diesem Kapitel betrachten wir Methoden, die in solchen
Forschungsdesigns -- bei hinreichenden Informationen über die
Studiensubjekte -- eine Schätzung kausaler Effekte ermöglichen, indem
eine statistische Vergleichbarkeit von Behandlungs- und Kontrollgruppe
hergestellt wird. Dies kann durch eine gezielte Gewichtung von
Beobachtungen anhand invididueller Merkmale bei der Schätzung des
Behandlungseffekts erfolgen. Andere etablierte Methoden schätzen den
kausalen Effekt nach Selektion von vergleichbaren Teilmengen von
Subjekten beider Gruppen aus der ursprünglichen Stichprobe, sogenanntes
\emph{Matching}.

Da \emph{Matching} ähnliche Beobachtungen basierend auf beobachtbaren
Merkmalen vergleicht, kann die Wahrscheinlichkeit einer verzerrten
Schätzung des kausalen Effekt durch falsche Modellspezifikationen
geringer sein als für eine Schätzung des Effekts anhand multipler
Regression. Weiterhin basieren Matching-Methoden nicht auf der Annahme
eines linearen Zusammenhangs zwischen Kovariablen und der erklärenden
Variable und können für die Schätzung unterschiedlicher Spezifikationen
von Behandlungseffekten herangezogen werden.

Für die Gültigkeit eines Schätzers basierend auf \emph{Matching} sind
zwei Annahmen erforderlich.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{\emph{Bedingte Unabhängigkeit.}} Seien \(Y^{(0)}_i\) und
  \(Y^{(1)}_i\) potentielle Ergebnisse der Outcome-Variable \(Y\) für
  ein Subjekt \(i\) mit \(B_i=0\) (keine Zuweisung zur Behandlung)
  beziehungsweise \(B_i=1\) (Behandlung) und \(X_i\) die beobachteten
  Kovariablen. Wir nehmen an, dass \begin{align}
    \left\{Y^{(0)}_i, Y^{(1)}_i\right\} \perp B_i\vert X_i, \label{eq:cia}
  \end{align} d.h. die Behandlungszuweisung/-selektion ist unanabhängig
  von den potentielle Ergebnissen \(Y^{(0)}_i\) und \(Y^{(1)}_i\), wenn
  wir für die Kovariablen \(X\) kontrollieren.
\item
  \textbf{\emph{Überlappung.}} Für jede mögliche Kombination von
  Kovariablen \(X_i\) gibt es eine positive Wahrscheinlichkeit \(<1\),
  sowohl zur Behandlungsgruppe (\(B_i = 1\)) als auch zur Kontrollgruppe
  (\(B_i = 0\)) zugewiesen zu werden, \begin{align}
    0 < \text{P}(B_i=1\lvert X_i) < 1, \label{eq:overlap}
  \end{align} d.h. keine Beobachtung hat eine
  Behandlungswahrscheinlichkeit von exakt \(0\) oder \(1\).
\end{enumerate}

Annahme 1 stellt sicher, dass die Zuweisung zur Behandlungsgruppe nach
Kontrolle für die Kovariablen \(X\) als zufällig betracht werden kann.
Somit ist es möglich den kausalen Effekt der Behandlung zu
identifizieren, indem wir hinsichtlich der Kovariablen \(X\) ähnliche
Subjekte (vgl. Annahme 2) aus Kontroll- und Behandlungsgruppe
vergleichen.

Annahme 2 setzt vorraus, dass es eine ausreichende Überlappung in den
Verteilungen der Kovariablen zwischen Behandlungs- und Kontrollgruppe
gibt. Dann ist sichergestellt, dass für jedes Subjekt in einer Gruppe
ein hinsichtlich seiner Charakteristika vergleichbares Subjekt in der
anderen Gruppe geben kann, sodass ein Vergleich möglich ist.

\hypertarget{sec-balance}{%
\section{\texorpdfstring{\emph{Balance}: Vergleichbarkeit von
Behandlungs- und
Kontrollgruppe}{Balance: Vergleichbarkeit von Behandlungs- und Kontrollgruppe}}\label{sec-balance}}

Der Lehrstuhl für Ökonometrie an der Universität Duisburg-Essen betreibt
einen Ökonometrie-Blog und interessiert sich für den kausalen Effekt der
Einführung eines
\href{https://en.wikipedia.org/wiki/Wikipedia:Dark_mode}{darkmode} auf
die Verweildauer der User auf der Webseite. Die Webseite ist zwar
nicht-kommerziell, hat sich allerdings insb. für die Aquise
internationaler Studierender für den Studiengang MSc. Econometrics als
wichtiges Marketing-Instrument erwiesen. Ein anprechendes Design wird
daher als hoch-relevant erachtet.

Idealerweise sollte der Effekt des Design-Relaunches auf die
Nutzungsintensität in einem kontrollierten randomisierten Experiment
untersucht werden. Hierbei würden wir Nutzern zufällig das neue oder das
alte Design zuweisen und den Effekt als Differnz des durchschnittlichen
Verweildauer für die Gruppen bestimmen. Eine solche Studie ist jedoch
aus technischen und finanziellen Gründen nicht realisierbar, sodass die
Auswirkungen des darkmode mit vorliegenden nicht-experimenellen
Nutzungsstatistiken für die Webseite geschätzt werden sollen.

Die Nutzungsstatistiken sind im Datensatz
\href{https://raw.githubusercontent.com/mca91/kasa_data/main/darkmode.csv}{\emph{darkmode.csv}}
enthalten und sollen der Analyse des Effekts des darkmode
(\texttt{dark\_mode}) auf die Verweildauer der Leser auf der Webseite
(\texttt{read\_time}) dienen.

Tabelle~\ref{tbl-darkmode} zeigt die Definitionen der Variablen in
\emph{darkmode.csv}.

\hypertarget{tbl-darkmode}{}
\begin{longtable}{ll}
\caption{\label{tbl-darkmode}Variablen im Datensatz \emph{darkmode} }\tabularnewline

\toprule
Variable & Beschreibung \\ 
\midrule\addlinespace[2.5pt]
read\_time & Lesezeit (Minuten/Woche) \\ 
dark\_mode & Indikator: Beobachtung nach Einführung darkmode \\ 
male & Indikator: Individuum männlich \\ 
age & Alter (in Jahren) \\ 
hours & Bisherige Verweildauer auf der Seite \\ 
\bottomrule
\end{longtable}

Für die Analyse lesen wir zunächst den Datensatz \emph{darkmode.csv} mit
\texttt{readr::read\_csv()} ein und verschaffen uns einen Überblick über
die verfügbaren Variablen.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Paket \textasciigrave{}tidyverse\textasciigrave{} laden}
\FunctionTok{library}\NormalTok{(tidyverse)}

\CommentTok{\# Datensatz \textquotesingle{}darkmode\textquotesingle{} einlesen}
\NormalTok{darkmode }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}
  \AttributeTok{file =} \StringTok{"datasets/darkmode.csv"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\texttt{dark\_mode} hat den Typ \texttt{logical}. Mit
\texttt{dplyr::mutate\_all()} können wir komfortabel alle Spalten in den
Typ \texttt{numeric} transformieren.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Alle Variablen zu typ \textquotesingle{}numeric\textquotesingle{} formatieren...}
\NormalTok{darkmode }\OtherTok{\textless{}{-}}\NormalTok{ darkmode }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate\_all}\NormalTok{(}\AttributeTok{.funs =}\NormalTok{ as.numeric)}

\CommentTok{\# ... und überprüfen}
\FunctionTok{glimpse}\NormalTok{(darkmode)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 300
Columns: 5
$ read_time <dbl> 14.4, 15.4, 20.9, 20.0, 21.5, 19.5, 22.0, 17.4, 23.6, 15.7, ~
$ dark_mode <dbl> 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, ~
$ male      <dbl> 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ~
$ age       <dbl> 43, 55, 23, 41, 29, 64, 18, 53, 59, 53, 43, 38, 42, 23, 39, ~
$ hours     <dbl> 65.6, 125.4, 642.6, 129.1, 190.2, 185.3, 333.5, 279.3, 1302.~
\end{verbatim}

Eine naive Schätzung des durchschnittlichen Behandlungseffekts (ATE)
\(\widehat{\tau}^{\text{naiv}}\) erhalten wir als Mittelwertdifferenz
von \texttt{read\_time} für die Behandlungsgruppe
(\texttt{dark\_mode\ ==\ 1}) und die Kontrollgruppe
(\texttt{dark\_mode\ ==\ 0}) \begin{align}
  \widehat{\tau}^{\text{naiv}} = \overline{\text{read\_time}}_{\text{Behandlung}} - \overline{\text{read\_time}}_{\text{Kontrolle}}.\label{eq:naivATEdarkmode}
\end{align}

Diese Berechnung ist schnell mit R durchgeführt.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Naiver Schätzer für ATE: }
\CommentTok{\# Differenz der Gruppen{-}Durchschnitte}

\CommentTok{\# Outcome in Behandlungsgruppe}
\NormalTok{read\_time\_mTG }\OtherTok{\textless{}{-}}\NormalTok{ darkmode }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(dark\_mode }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pull}\NormalTok{(}\StringTok{"read\_time"}\NormalTok{)}

\CommentTok{\# Outcome in Kontrollgruppe}
\NormalTok{read\_time\_mKG }\OtherTok{\textless{}{-}}\NormalTok{ darkmode }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(dark\_mode }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pull}\NormalTok{(}\StringTok{"read\_time"}\NormalTok{)}

\CommentTok{\# Mittelwert{-}Differenz}
\FunctionTok{mean}\NormalTok{(read\_time\_mTG) }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(read\_time\_mKG)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -0.4446331
\end{verbatim}

Die Schätzung ergibt einen negativen Behandlungseffekt, mit der
Interpreation, dass das neue Design zu einer Reduktion der Lesezeit um
etwa 0.44 Minuten pro Woche führt. Dieses Ergebnis ist allerdings
zweifelhaft, weil eine Isolierung des Behandlungseffekts aufgrund von
Backdoor-Pfaden im DGP vermutlich nicht gewähleistet ist. Ein Indikator
hierfür sind systematische Unterschiede hinsichtlich von (möglicherweise
unbeobachtbaren) Charakteristika von Kontrollgruppe und
Behandlungsgruppe.

Da die User sich beim Aufrufen der Seite aktiv für oder gegen den das
neue Design entscheiden müssen (und somit selektieren, ob Sie in der
Behandlungs- oder Kontrollgruppe landen), liegt wahrscheinlich
\emph{Confounding} vor: Unsere Hypothese ist zunächst, dass männliche
User eine durchschnittlich längere Lesezeit aufweisen \emph{und} mit
größerer Wahrscheinlichkeit auf das neue Design wechseln als
nicht-männliche Leser. Dann ist \texttt{male} eine Backdoor-Variable.
Diese Situation ist unter der Annahme, dass nur diese Faktoren den DGP
bestimmen, in Abbildung~\ref{fig-maleCDdarkmode} dargestellt.

\begin{figure}

{\centering 

\begin{figure}[H]

{\centering \includegraphics[width=4in,height=3in]{Matching_files/figure-latex/dot-figure-1.png}

}

\end{figure}

}

\caption{\label{fig-maleCDdarkmode}Backdoor durch `male' im
Website-Design-Bespiel}

\end{figure}

Der DGP in Abbildung~\ref{fig-maleCDdarkmode} führt zu einer verzerrten
Schätzung des kausalen Effekts von \texttt{dark\_mode} auf
\texttt{read\_time} mit \eqref{eq:naivATEdarkmode}, wenn das Verhältnis
von männlichen und nicht-männlichen Usern in Bahandlungs- und
Kontrollgruppe nicht ausgeglichen ist. Wir überprüfen dies mit R.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Anteile männlicher und nicht{-}männlicher User}
\NormalTok{(}
\NormalTok{  anteile }\OtherTok{\textless{}{-}}\NormalTok{ darkmode }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(dark\_mode) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{gesamt =} \FunctionTok{n}\NormalTok{(),}
    \AttributeTok{ant\_m =} \FunctionTok{mean}\NormalTok{(male),}
    \AttributeTok{ant\_nm =} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ ant\_m,}
    \AttributeTok{anz\_m =} \FunctionTok{sum}\NormalTok{(male),}
    \AttributeTok{anz\_nm =}\NormalTok{ gesamt }\SpecialCharTok{{-}}\NormalTok{ anz\_m}
\NormalTok{    )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 6
  dark_mode gesamt ant_m ant_nm anz_m anz_nm
      <dbl>  <int> <dbl>  <dbl> <dbl>  <dbl>
1         0    151 0.338  0.662    51    100
2         1    149 0.658  0.342    98     51
\end{verbatim}

Die Zusammenfassung \texttt{anteile\_m} zeigt, dass der Anteil
männlicher User in der Behandlungsgruppe deutlich höher ist als in der
Kontrollgruppe.

\hypertarget{matching-durch-gewichtung}{%
\subsection{Matching durch Gewichtung}\label{matching-durch-gewichtung}}

Matching eliminiert die Variation von \texttt{male} zwischen den
Gruppen. Eine Möglichkeit hierfür ist die Gewichtung der Beobachtungen
in der Kontrollgruppe entsprechend der Anteile von Männern und
Nicht-Männern in der Behandlungsgruppe, sodass die Vergleichbarkeit mit
der Behandlungsgruppe hinsichtlich des Geschlechts gewährleistet ist.
Dies wird in der Literatur als \emph{Balance} bezeichnet. Der
Behandlungseffekt wird dann analog zu \eqref{eq:naivATEdarkmode}
geschätzt.

Die Gewichte für Beobachtungen in der Kontrollgruppe \(w_i\) werden
berechnet als \begin{align}
  w_i = 
  \begin{cases}
    \text{ant\_m}_B/\text{anz\_m}_{K}, & \text{falls } \text{male}_i = 1\\
        \text{ant\_nm}_B/\text{anz\_nm}_{K}, & \text{sonst.}\\
  \end{cases}\label{eq:darkmodeweights}
\end{align} Anhand der Formel für einen gewichteten Durchschnitt,
\begin{align}
  \overline{X}_w = \frac{\sum_i w_i \cdot X_i}{\sum_i w_i},
\end{align} berechnen wir die gewichteten Mittelwerte für \texttt{male}
und \texttt{read\_time} in der Kontrollgruppe.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Anteile und Anzahlen aus \textasciigrave{}anteile\textasciigrave{} auslesen}
\NormalTok{anz\_m\_K }\OtherTok{\textless{}{-}}\NormalTok{ anteile }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(dark\_mode }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pull}\NormalTok{(anz\_m)}

\NormalTok{anz\_nm\_K }\OtherTok{\textless{}{-}}\NormalTok{ anteile }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(dark\_mode }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pull}\NormalTok{(anz\_nm)}

\NormalTok{ant\_m\_B }\OtherTok{\textless{}{-}}\NormalTok{ anteile }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(dark\_mode }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pull}\NormalTok{(ant\_m)}

\NormalTok{ant\_nm\_B }\OtherTok{\textless{}{-}}\NormalTok{ anteile }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(dark\_mode }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pull}\NormalTok{(ant\_nm)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Gewichtete Mittel für Kontrollgruppe berechnen}
\NormalTok{(}
\NormalTok{gew\_K }\OtherTok{\textless{}{-}}\NormalTok{ darkmode }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(dark\_mode }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(read\_time, male) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{w =} \FunctionTok{ifelse}\NormalTok{(}
\NormalTok{    male }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }
\NormalTok{    ant\_m\_B}\SpecialCharTok{/}\NormalTok{anz\_m\_K, }
\NormalTok{    ant\_nm\_B}\SpecialCharTok{/}\NormalTok{anz\_nm\_K)}
\NormalTok{    ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{male\_k =} \FunctionTok{sum}\NormalTok{(male }\SpecialCharTok{*}\NormalTok{ w) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(w),}
    \AttributeTok{mean\_read\_time\_wK =} \FunctionTok{sum}\NormalTok{(read\_time }\SpecialCharTok{*}\NormalTok{ w) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(w)}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
  male_k mean_read_time_wK
   <dbl>             <dbl>
1  0.658              18.1
\end{verbatim}

Ein Vergleich des gewichteten Mittelwertes von \texttt{male} in der
Kontrollgruppe mit dem Mittelwert in der Behandlungsgruppe
(\texttt{male\_k}) zeigt, dass die Gewichte die Variation in
\texttt{male} zwischen beiden Gruppen eliminieren, sodass die Backdoor
durch \texttt{male} geschlossen ist. Mit \texttt{wmean\_read\_time\_K}
haben wir einen entsprechend gewichteten Mittelwert der Verweildauer für
die Kontrollgruppe berechnet. Wir schätzen den Behandlungseffekt nun als
\begin{align}
  \widehat{\tau}^{\text{w}} = \overline{\text{read\_time}}_{B} - \overline{\text{read\_time}}_{w,K}.\label{eq:weightedATEdarkmode}
\end{align}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(read\_time\_mTG)  }\SpecialCharTok{{-}}\NormalTok{ gew\_K}\SpecialCharTok{$}\NormalTok{mean\_read\_time\_wK}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.6383579
\end{verbatim}

Entgegen der naiven Schätzung andhand von \eqref{eq:naivATEdarkmode}
erhalten wir nach Matching für \texttt{male} eine positive Schätzung des
Behandlungseffekts von etwa \(0.64\).

Die Schätzung des Behandlungseffekts anhand von
\eqref{eq:weightedATEdarkmode} entspricht dem geschätzten Koeffizienten
\(\widehat{\beta}_1\) aus einer gewichteten KQ-Regression im Modell
\begin{align*}
  \text{read\_time} = \beta_0 + \beta_1 \text{dark\_mode} + u,
\end{align*} wobei die Beobachtungen der Kontrollgruppe wie in
\eqref{eq:darkmodeweights} gewichtet werden und \(w_i=1\) für
Beobachtungen der Behandlungsgruppe ist. Wir überprüfen dies mit R.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{darkmode\_w }\OtherTok{\textless{}{-}}\NormalTok{ darkmode }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{w =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      male }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ dark\_mode }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\textasciitilde{}}\NormalTok{ ant\_m\_B}\SpecialCharTok{/}\NormalTok{anz\_m\_K,}
\NormalTok{      male }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&}\NormalTok{ dark\_mode }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\textasciitilde{}}\NormalTok{ ant\_nm\_B}\SpecialCharTok{/}\NormalTok{anz\_nm\_K,}
\NormalTok{      T }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}
\NormalTok{    )}
\NormalTok{  ) }

\FunctionTok{lm}\NormalTok{(read\_time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ dark\_mode, }\AttributeTok{weights =}\NormalTok{ w, }\AttributeTok{data =}\NormalTok{ darkmode\_w) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = read_time ~ dark_mode, data = darkmode_w, weights = w)

Weighted Residuals:
     Min       1Q   Median       3Q      Max 
-11.4302  -0.6929   0.0814   0.7230  12.8698 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  18.0918     3.4796   5.199 3.72e-07 ***
dark_mode     0.6384     3.4912   0.183    0.855    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.48 on 298 degrees of freedom
Multiple R-squared:  0.0001122, Adjusted R-squared:  -0.003243 
F-statistic: 0.03343 on 1 and 298 DF,  p-value: 0.855
\end{verbatim}

Der geschätzte Koeffizient von \texttt{dark\_mode} entspricht
\(\widehat{\tau}^w\).

Da \texttt{male} eine binäre Variable ist, reduziert sich eine
Beurteilung der Vergleichbarkeit der Verteilungen von \texttt{male} in
Behandlungs- und Kontrollgruppe auf einen simplen Vergleich des
Männeranteils beider Gruppen. In der Praxis gibt es meist eine Vielzahl
potentieller Backdoor-Variablen, die zudem kontinuierlich verteilt sind.
Es scheint plausibel, dass das Alter der Nutzer sowohl die Akzeptanz des
Design-Updates als auch die Lesezeit beeinflusst. Die bisherige
Verweildauer ist mindestens eine plausible Determinante der Lesezeit.

Der erweiterte DGP ist in Abbildung Abbildung~\ref{fig-CDdarkmode}
dargestellt, wobei der zusätzliche Backdoor-Pfad durch \texttt{age}
ebenfalls mit roten Pfeilen gekennzeichnet sind.

\begin{figure}

{\centering 

\begin{figure}[H]

{\centering \includegraphics[width=4in,height=3in]{Matching_files/figure-latex/dot-figure-3.png}

}

\end{figure}

}

\caption{\label{fig-CDdarkmode}Erweiterter DGP im
Website-Design-Beispiel}

\end{figure}

Die Beurteilung der \emph{Balance} von Kontrollgruppe und
Behandlungsgruppe kann durch eine grafische Gegenüberstellung der
empirischen Verteilungen der Kovariablen beider Gruppen erfolgen. Wir
visualisieren die empirischen Verteilungen mit \texttt{ggplot2}. Hierzu
standardisieren wir \texttt{age} und \texttt{hours} zunächst mit
\texttt{scale()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Datensatz für graphische Darstellung formatieren}
\NormalTok{darkmode\_p }\OtherTok{\textless{}{-}}\NormalTok{ darkmode }\SpecialCharTok{\%\textgreater{}\%} 
  \CommentTok{\# Standardisierung mit \textquotesingle{}scale()\textquotesingle{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{dark\_mode =} \FunctionTok{as\_factor}\NormalTok{(dark\_mode),}
    \AttributeTok{age =} \FunctionTok{scale}\NormalTok{(age), }
    \AttributeTok{hours =} \FunctionTok{scale}\NormalTok{(hours)}
\NormalTok{  )}

\FunctionTok{head}\NormalTok{(darkmode\_p)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 5
  read_time dark_mode  male age[,1] hours[,1]
      <dbl> <fct>     <dbl>   <dbl>     <dbl>
1      14.4 0             0  0.0377    -0.591
2      15.4 0             1  1.11      -0.459
3      20.9 1             0 -1.74       0.684
4      20   0             0 -0.141     -0.451
5      21.5 1             0 -1.21      -0.316
6      19.5 0             0  1.91      -0.327
\end{verbatim}

Für \texttt{age} und \texttt{hours} eignen sich die geschätzten
Dichtefunktionen für einen Vergleich der Verteilungen in Behandlungs-
und Kontrollgruppe.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Vergleich mit Dichteschätzungen}
\NormalTok{darkmode\_p }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(dark\_mode, hours, age) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# in langes Format überführen}
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{dark\_mode)) }\SpecialCharTok{\%\textgreater{}\%}
  
  \FunctionTok{ggplot}\NormalTok{(}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ value, }\AttributeTok{fill =}\NormalTok{ dark\_mode)}
\NormalTok{    ) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{alpha =}\NormalTok{ .}\DecValTok{5}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}
    \AttributeTok{facets =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ name, }
    \AttributeTok{scales =} \StringTok{"free"}\NormalTok{, }
    \AttributeTok{nrow =} \DecValTok{2}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{Matching_files/figure-pdf/unnamed-chunk-14-1.pdf}

}

\end{figure}

Die graphische Analyse zeigt deutliche Unterschiede in den Verteilungen
von \texttt{age} zwischen Kontroll- und Behandlungsgruppe. Für einen
Beurteilung mit deskriptiven Statistiken wird häufig eine sogenannte
\emph{Balance Table} herangezogen. Wir berechnen diese für \texttt{age},
\texttt{hours} und \texttt{male} mit \texttt{cobalt::bal.tab()}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(cobalt)}

\CommentTok{\# Balance table mit \textquotesingle{}cobalt::bal.tab()\textquotesingle{}}
\FunctionTok{bal.tab}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ darkmode }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{select}\NormalTok{(age, hours, male), }
  \AttributeTok{treat =}\NormalTok{ darkmode}\SpecialCharTok{$}\NormalTok{dark\_mode, }
   \CommentTok{\# berechne SMD für KG und TG:}
  \AttributeTok{disp =} \StringTok{"m"}\NormalTok{, }
  \AttributeTok{s.d.denom =} \StringTok{"pooled"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Balance Measures
         Type   M.0.Un   M.1.Un Diff.Un
age   Contin.  46.0132  39.0940 -0.6469
hours Contin. 337.7775 328.5738 -0.0203
male   Binary   0.3377   0.6577  0.3200

Sample sizes
    Control Treated
All     151     149
\end{verbatim}

Die Einträge \texttt{M.0.Un} und \texttt{M.1.Un} zeigen die jeweiligen
Stichprobenmittelwerte der Variablen für Kontroll- und
Behandlungsgruppe. \texttt{Diff.Un} gibt eine standardisierte
Mittelwertdifferenz \(SMD\) an, wobei \begin{align*}
  SMD_j := \left(\overline{X}_{j,B} - \overline{X}_{j,K}\right) \bigg/ \sqrt{\frac{1}{2}\left(\widehat{\text{Var}}(X_{j,B}) + \widehat{\text{Var}}(X_{j,K})\right)},
\end{align*} mit Stichprobenmitteln \(\overline{X}_{j,B}\) und
\(\overline{X}_{j,K}\) und Stichprobenvarianzen
\(\widehat{\text{Var}}(X_{j,B})\) und \(\widehat{\text{Var}}(X_{j,K})\)
für eine kontinuierliche Kovariable \(j\).\footnote{Siehe P. Austin
  (2011) für einen Überblick zu Balance-Statistiken.} Obwohl es keinen
einheitlichen Schwellenwert für die standardisierte Differenz gibt, der
ein erhebliches Ungleichgewicht anzeigt, gilt für kontinuierliche
Variablen eine standardisierte (absolute) Differenz von weniger als
\(0.1\) als Hinweis auf einen vernachlässigbaren Unterschied zwischen
den Gruppen.

Die Balance Table weist also auf einen vernachlässigbaren Unterschied
für \texttt{hours} hin und bestätigt den aus den Grafiken abgeleiteten
Eindruck einer relevanten Differenzen für \texttt{age}.

\hypertarget{entropy-balancing}{%
\subsection{Entropy Balancing}\label{entropy-balancing}}

\emph{Entropy Balancing} (Hainmueller 2012) ist eine weitere Methode zur
Gewährleistung der Vergleichbarkeit von Behandlungs- und Kontrollgruppe
anhand von Gewichten. Das Verfahren nutzt Konzepte aus der
\href{https://de.wikipedia.org/wiki/Informationstheorie}{Informationstheorie}
um die Gewichte für Subjekte in der Kontrollgruppe so anzupassen, dass
die Verteilung der Matchingvariablen in der Kontrollgruppe die
Verteilung in der Behandlungsgruppe möglichst gut approximiert. Dies
geschieht unter der Restriktion, dass bestimmte empirische Momente
(meist Mittelwerte und Varianzen) der Matchingvariablen exakt
übereinstimmen. Mathematisch werden die Gewichte für
Kontrollgruppenbeobachtungen durch Minimierung der
\href{https://de.wikipedia.org/wiki/Kullback-Leibler-Divergenz}{Kullback-Leibler-Divergenz}
zwischen den Verteilungen gefunden, wobei die Divergenz ein Maß für den
Unterschied von Wahrscheinlichkeitsverteilungen ist.

Entropy Balancing ist im R-Paket \texttt{WeightIt} implementiert. Wir
zeigen, wie die benötigten Gewichte für eine Schätzungen des ATT im
Website-Beispiel mit \texttt{WeightIt::wightit()} bestimmt werden
können. Über das Argument \texttt{moments} legen wir fest, dass die
Gewichte unter der Restriktion übereinstimmender Mittelwerte aller
Matching-Variablen zwischen Behandlungs- und Kontrollgruppe erfolgen
soll.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(WeightIt)}

\CommentTok{\# Gewichte für Entropy Balancing}
\NormalTok{(}
\NormalTok{  W1 }\OtherTok{\textless{}{-}} \FunctionTok{weightit}\NormalTok{(}
\NormalTok{  dark\_mode }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ male }\SpecialCharTok{+}\NormalTok{ hours,}
  \AttributeTok{data =}\NormalTok{ darkmode,}
  \AttributeTok{method =} \StringTok{"ebal"}\NormalTok{, }
  \AttributeTok{estimand =} \StringTok{"ATT"}\NormalTok{,}
  \AttributeTok{moments =} \DecValTok{1}
\NormalTok{  )}
\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
A weightit object
 - method: "ebal" (entropy balancing)
 - number of obs.: 300
 - sampling weights: none
 - treatment: 2-category
 - estimand: ATT (focal: 1)
 - covariates: age, male, hours
\end{verbatim}

Wir schätzen den Behandlungseffekt nach Entropy Balancing mit
gewichteter Regression.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Mittelwert{-}Vergleich mit lm()}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ read\_time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ dark\_mode, }
  \AttributeTok{data =}\NormalTok{ darkmode, }
  \AttributeTok{weights =}\NormalTok{ W1}\SpecialCharTok{$}\NormalTok{weights}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = read_time ~ dark_mode, data = darkmode, weights = W1$weights)

Weighted Residuals:
     Min       1Q   Median       3Q      Max 
-16.6662  -2.3552   0.8786   2.9583  27.1840 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  17.7598     0.4093  43.394   <2e-16 ***
dark_mode     0.9704     0.5807   1.671   0.0958 .  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.029 on 298 degrees of freedom
Multiple R-squared:  0.009283,  Adjusted R-squared:  0.005958 
F-statistic: 2.792 on 1 and 298 DF,  p-value: 0.09578
\end{verbatim}

Beachte, dass die von \texttt{summary()} berechneten Standardfehler bei
Entropy Balancing ungültig sind. In Abschnitt
Kapitel~\ref{sec-bootmatching} erläutern wir die Berechnung von
Standardfehlern für Matching-Schätzer mit dem Bootstrap.

\hypertarget{sec-PSM}{%
\subsection{Mehrere Matching-Variablen und der Propensity
Score}\label{sec-PSM}}

Bei mehreren Backdoor-Variablen kann eine Gewichtung anhand der
Behandlungswahrscheinlichkeit (\emph{Treatment Propensity}) erfolgen.
Die Idee hierbei ist, dass der DGP wie in
Abbildung~\ref{fig-propCDdarkmode} dargestellt werden kann.

\begin{figure}

{\centering 

\begin{figure}[H]

{\centering \includegraphics[width=4in,height=3in]{Matching_files/figure-latex/dot-figure-2.png}

}

\end{figure}

}

\caption{\label{fig-propCDdarkmode}Propensity im
Website-Design-Beispiel}

\end{figure}

Hierbei beeinflussen die Backdoor-Variablen \emph{age} und \emph{male}
die Behandlungsvariable \emph{dark\_mode} lediglich durch die
Behandlungswahrscheinlichkeit \emph{Treatment Propensity}. Diese
Darstellung zeigt, das die mehrdimensionale Information bzgl. der
Ähnlichkeit von Subjekten hinsichtlich der beobachteten Kovariablen in
einer einzigen Variable zusammengefasst werden kann. Die Backdoor-Pfade
können daher geschlossen werden, indem wir Subjekte anhand von
\emph{Treatment Propensity} derart gewichten, dass beide Gruppen
hinsichtlich der Verteilung der Behandlungswahrscheinlichkeit
vergleichbar sind. Betrachte erneut \eqref{eq:cia} und beachte, dass
\begin{align}
  Y_i = Y_i^{(1)} D_i + Y_i^{(0)} (1-D_i).
\end{align} Rosenbaum und Rubin (1983) zeigen, dass es hinsichtlich
\eqref{eq:cia} äquivalent ist für die \emph{Treatment Propensity}
\(P_i(X_i):=P(B_i=1\vert X_i = x)\) zu kontrollieren, d.h. \begin{align}
  \left\{Y_i^{(1)},Y_i^{(0)}\right\} \perp B_i\vert X_i \quad\Leftrightarrow\quad \left\{Y_i^{(1)},Y_i^{(0)}\right\} \perp B_i\vert P_i(X_i).
\end{align}

Der Behandlungseffekt kann so als Differenz von gewichteten
Gruppenmittelwerten berechnet werden, mit inversem
Wahrscheinlichkeitsgewicht (IPW) \(w_{i,B} = 1/P_i(X_i)\) für
Beobachtungen in der Behandlungsgruppe und \(w_{i,K} = 1/(1-P_i(X_i))\)
für Beobachtungen in der Kontrollgruppe, \begin{align}
  \tau^{\text{IPW}} = \frac{1}{n}\sum_{i=1}^n \left[\frac{B_i Y_i}{P_i(X_i)} - \frac{(1-B_i)Y_i}{1-P_i(X_i)} \right].\label{eq:tauipw}
\end{align}

Grundsätzlich ist \emph{TreatmentPropensity} eine nicht beobachtbare
Variable und muss daher aus den Daten geschätzt werden. Eine geschätzte
Behandlungswahrscheinlichkeiten \(\widehat{P}_i(X_i)\) wird als
\emph{Propensity Score} bezeichnet. In der Praxis erfolgt die Schätzung
von \emph{Propensity Scores} meist mit logistischer Regression. Ein
erwartungstreuer Schätzer des ATE ist \begin{align}
  \widehat{\tau}^{\text{IPW}} = \frac{1}{n}\sum_{i=1}^n \left[\frac{B_i Y_i}{\widehat{P}_i(X_i)} - \frac{(1-B_i)Y_i}{1-\widehat{P}_i(X_i)} \right].\label{eq:hattauipw}
\end{align} Hirano, Imbens, und Ridder (2003) diskutieren Alternativen
zu \eqref{eq:hattauipw} für die Schätzung anderer Typen von
Behandlungseffekten.

Wir schätzen nachfolgend die \emph{Propensity Scores} für unser
Anwendungsbeispiel, erläutern die Berechnung der Gewichte sowie die
Schätzung von Behandlungseffekten mit gewichteter Regression. Hierbei
betrachten wir eine Variante von \eqref{eq:hattauipw} mit normalisierten
Gewichten \(\tilde{w}_{i,B} = w_{i,B}/\sum_i w_{i,B}\) und
\(\tilde{w}_{i,K} = w_{i,K}/\sum_i w_{i,K}\) die sich jeweils zu 1
summieren.\footnote{Eine Normalisierung der Gewichte reduziert die
  Varianz des Schätzers, vgl. Hirano, Imbens, und Ridder (2003)} Dies
ergibt den Hájek-Schätzer\footnote{Siehe Hájek (1971).} \begin{align}
    \widehat{\tau}_N^{\text{IPW}} = \frac{\sum_i\tilde{w}_{i,B}Y_i}{\sum_i\tilde{w}_{i,B}} -  \frac{\sum_i\tilde{w}_{i,K}Y_i}{\sum_i\tilde{w}_{i,K}}.\label{eq:hattauhajek}
\end{align}

Zunächst Schätzen wir ein logistisches Regressionsmodell mit
\texttt{age}, \texttt{male} und \texttt{hours} als erklärende Variablen
für \texttt{dark\_mode}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Logit{-}Modell mit \textquotesingle{}glm()\textquotesingle{} schätzen}
\NormalTok{(}
\NormalTok{  darkmode\_ps\_logit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(}
    \AttributeTok{formula =}\NormalTok{ dark\_mode }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ male }\SpecialCharTok{+}\NormalTok{ hours,}
    \AttributeTok{data =}\NormalTok{ darkmode,}
    \AttributeTok{family =}\NormalTok{ binomial}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:  glm(formula = dark_mode ~ age + male + hours, family = binomial, 
    data = darkmode)

Coefficients:
(Intercept)          age         male        hours  
  2.330e+00   -7.330e-02    1.623e+00   -9.293e-05  

Degrees of Freedom: 299 Total (i.e. Null);  296 Residual
Null Deviance:      415.9 
Residual Deviance: 346.4    AIC: 354.4
\end{verbatim}

Die \emph{Propensity Scores} erhalten wir als angepasste Werte aus der
Regression \texttt{darkmode\_ps\_logit} mit \texttt{fitted()}. Wir
erweitern den Datensatz mit den Ergebnissen.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Datensatz um Propensity Scores erweitern}
\NormalTok{(}
\NormalTok{  darkmode\_probs }\OtherTok{\textless{}{-}} 
\NormalTok{    darkmode }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}
      \AttributeTok{PS =} \FunctionTok{fitted}\NormalTok{(darkmode\_ps\_logit)}
\NormalTok{    )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 300 x 6
   read_time dark_mode  male   age  hours     PS
       <dbl>     <dbl> <dbl> <dbl>  <dbl>  <dbl>
 1      14.4         0     0    43   65.6 0.304 
 2      15.4         0     1    55  125.  0.478 
 3      20.9         1     0    23  643.  0.642 
 4      20           0     0    41  129.  0.335 
 5      21.5         1     0    29  190.  0.547 
 6      19.5         0     0    64  185.  0.0849
 7      22           1     0    18  334.  0.727 
 8      17.4         0     0    53  279.  0.171 
 9      23.6         0     0    59 1303.  0.108 
10      15.7         0     0    53   16.1 0.174 
# i 290 more rows
\end{verbatim}

Zur Beurteilung der Überlappung (vgl. Annahme \eqref{eq:overlap} können
wir die Verteilung der \emph{Propensity Scores} nach
Behandlungs-Indikator mit Histogrammen visualisieren.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Überlappung prüfen:}
\CommentTok{\# Histogramme der PS nach Treatment{-}Indikator}
\NormalTok{darkmode\_probs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}
    \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}
      \AttributeTok{x =}\NormalTok{ PS, }
      \AttributeTok{fill =} \FunctionTok{factor}\NormalTok{(dark\_mode)}
\NormalTok{    )}
\NormalTok{  ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}
    \AttributeTok{alpha =}\NormalTok{ .}\DecValTok{5}\NormalTok{, }
    \AttributeTok{bins =} \DecValTok{25}\NormalTok{, }
    \AttributeTok{position =} \StringTok{"identity"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{Matching_files/figure-pdf/unnamed-chunk-21-1.pdf}

}

\end{figure}

Ein Vergleich der Histogramme zeigt, dass die Überlappung der
\emph{Propensity Scores} in der linken Flanken der Verteilungen der
Kontrollgruppe und in der rechten Flanke der Behandlungsgruppe
schlechter wird. Wir entfernen zunächst Beobachtungen aus der Stichprobe
deren \emph{Propensity Scores} wenig bzw. keine Überlappung aufweisen.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Datensatz nach PS trimmen}
\NormalTok{darkmode\_probs }\OtherTok{\textless{}{-}}\NormalTok{ darkmode\_probs }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}
    \FunctionTok{between}\NormalTok{(}
      \AttributeTok{x =}\NormalTok{ PS,}
      \AttributeTok{left =}\NormalTok{ .}\DecValTok{25}\NormalTok{,}
      \AttributeTok{right =}\NormalTok{ .}\DecValTok{75}
\NormalTok{    )}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Überlappung nach trimming prüfen:}
\CommentTok{\# Dichteschätzung der PS nach Treatment{-}Indikator}
\NormalTok{darkmode\_probs }\SpecialCharTok{\%\textgreater{}\%}
\FunctionTok{ggplot}\NormalTok{(}
  \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ PS, }
    \AttributeTok{fill =} \FunctionTok{factor}\NormalTok{(dark\_mode))}
\NormalTok{  ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}
    \AttributeTok{alpha =}\NormalTok{ .}\DecValTok{5}\NormalTok{, }
    \AttributeTok{bins =} \DecValTok{25}\NormalTok{, }
    \AttributeTok{position =} \StringTok{"identity"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{Matching_files/figure-pdf/unnamed-chunk-23-1.pdf}

}

\end{figure}

IPWs anhand der \emph{Propensity Scores} können schnell mit der
Vorschrift \begin{align}
  \text{IPW} = \frac{\text{dark\_mode}}{\text{PS}} + \frac{1 - \text{dark\_mode}}{1 - \text{PS}},
\end{align} berechnet werden.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Datensatz um IPWs erweitern}
\NormalTok{darkmode\_IPW }\OtherTok{\textless{}{-}}\NormalTok{ darkmode\_probs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{IPW =}\NormalTok{ dark\_mode }\SpecialCharTok{/}\NormalTok{ PS }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ dark\_mode) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ PS)}
\NormalTok{  )}

\NormalTok{darkmode\_IPW }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(IPW)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 194 x 1
     IPW
   <dbl>
 1  1.44
 2  1.91
 3  1.56
 4  1.50
 5  1.83
 6  1.38
 7  1.43
 8  1.47
 9  2.71
10  1.42
# i 184 more rows
\end{verbatim}

Eine Schätzung des durchschnittlichen Behandlungseffekts gemäß
\eqref{eq:hattauhajek} implementieren wir mit \texttt{dplyr}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{darkmode\_IPW }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(dark\_mode) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{w =}\NormalTok{ IPW }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(IPW)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{weighted\_mean =} \FunctionTok{sum}\NormalTok{(read\_time }\SpecialCharTok{*}\NormalTok{ w)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{diff =} \FunctionTok{diff}\NormalTok{(weighted\_mean))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
   diff
  <dbl>
1  1.90
\end{verbatim}

Diese Schätzung des Behandlungseffekts ist äquivalent zur gewichteten
KQ-Schätzung anhand eines einfachen linearen Regressionsmodells.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Mit IPWs gewichteter KQ{-}Schaetzer berechnet den ATE}
\NormalTok{model\_ipw }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ read\_time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ dark\_mode, }
  \AttributeTok{data =}\NormalTok{ darkmode\_IPW,}
  \AttributeTok{weights =}\NormalTok{ IPW}
\NormalTok{)}

\FunctionTok{summary}\NormalTok{(model\_ipw)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = read_time ~ dark_mode, data = darkmode_IPW, weights = IPW)

Weighted Residuals:
     Min       1Q   Median       3Q      Max 
-18.5086  -4.4579   0.6096   4.1345  20.4566 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  17.9709     0.4942  36.362  < 2e-16 ***
dark_mode     1.9011     0.6952   2.735  0.00683 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.913 on 192 degrees of freedom
Multiple R-squared:  0.03749,   Adjusted R-squared:  0.03248 
F-statistic: 7.478 on 1 and 192 DF,  p-value: 0.006829
\end{verbatim}

Unsere Schätzung des ATE ist der geschätzte Koeffizient von
\texttt{dark\_mode}. Die ausgegebenen Standardfehler und
Inferenzstatistiken sind jedoch \emph{ungültig} aufgrund der Gewichtung
mit IPWs, den inversen \emph{geschätzten} Wahrscheinlichkeiten für eine
Behandlung. Der Grund hierfür ist, dass die Berechnung der
Standardfehler in \texttt{summary()} die zusätzliche Unsicherheit durch
die geschätzen \emph{Propensity Scores} nicht berücksichtigt! Später im
Kapitel erläutern wir die Berechnung gültiger Standardfehler für
IPW-Schätzer basierend auf \emph{Propensity Scores} mit dem Bootstrap.

\hypertarget{selektierende-matching-verfahren}{%
\section{Selektierende
Matching-Verfahren}\label{selektierende-matching-verfahren}}

Das grundsätzliche Konzept von selektierendem Matching wird in der
nachstehenden interaktiven Grafik veranschaulicht. Hier betrachten wir
beobachtete Ausprägungen von zwei (unabhängig und identisch verteilten)
Matching-Variablen für Subjekte in der Behandlungsgruppe (blau) sowie
Kontrollgruppe (rot). Per Klick auf eine Beobachtung werden Matches aus
der anderen Gruppe in cyan farblich kenntlich gemacht. Als Matches
zählen sämtliche Beobachtungen der anderen Gruppe, deren
\href{https://de.wikipedia.org/wiki/Euklidischer_Abstand}{Euklidische
Distanz} zu dem ausgewählten Punkt das über den Slider eingestellte
Maximum \emph{Caliper} nicht überschreitet.\footnote{Es handelt sich
  hierbei um einen Spezialfall von Matching anhand der
  Mahalanobis-Distanz.} Diese Region wird durch den gestrichelten Kreis
gekennzeichnet. Die Grafik illustriert inbesondere, dass Beobachtungen
mehrfach (s.g. Matching mit zurücklegen) oder gar nicht gematcht werden
können.

Für die nachfolgenden Code-Beispiele verwenden wir das R-Paket
\texttt{MatchIt}. \texttt{MatchIt::matchit()} nutzt standardmäßig
Eins-zu-Eins-Matching (ohne Zurücklegen) von Beobachtungen der
Treatment-Gruppe mit Beobachtungen der Kontrollgruppe.\footnote{Dieses
  Schema zielt auf eine Schätzung des ATT ab.} Die für das Matching zu
verwendenden Variablen werden über das Argument \texttt{formula} als
Funktion des Behandlungsindikators definiert. \texttt{matchit()}
bereitet das Objekt für eine Schätzung des ATT mit einer geeigneten
Funktionen, s. \texttt{?matchit} und hier insb. die Erläuterungen der
Argumente \texttt{replace\ =\ F}, \texttt{ratio\ =\ 1} und
\texttt{estimand\ =\ "ATT"} für Details. Mit \texttt{cobalt::balt.tab()}
erhalten wir eine \emph{balance table} für den gematchten Datensatz.

Wir zeigen als nächstes, wie \texttt{MatchIt::matchit()} für Matching
anhand der Regressoren \texttt{age}, \texttt{hours}, und \texttt{male}
in unserem Website-Beispiel für unterschiedliche Varianten durchgeführt
werden kann.

\hypertarget{exaktes-matching}{%
\subsection{Exaktes Matching}\label{exaktes-matching}}

Exaktes Matching ordnet einem Subjekt aus der Behandlungsgruppe ein oder
mehrere Subjekte aus der Kontrollgruppe zu, wenn die boebachteten
Ausprägung der Matching-Variablen \emph{exakt} übereinstimmen. Hierbei
muss die `Distanz' zwischen den Ausprägung der Matching-Variablen
folglich \(0\) sein. Dieses Verfahren findet meist bei ausschließlich
diskret verteilten Merkmalen Anwendung. Bei kontinuierlich verteilten
Merkmalen (vgl. die obige interaktive Grafik) sind exakte Matches zwar
theoretisch unmöglich, ergeben sich jedoch in der Praxis aus der
Datenerfassung, bspw. durch Rundungsfehler. In \texttt{matchit()}
erhalten wir exaktes Ein-zu-eins-Matching mit
\texttt{method\ =\ "exact"}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MatchIt)}

\CommentTok{\# Exaktes Eins{-}zu{-}Eins{-}Matching durchführen}
\NormalTok{res\_em }\OtherTok{\textless{}{-}} \FunctionTok{matchit}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ dark\_mode }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ male }\SpecialCharTok{+}\NormalTok{ hours, }
  \AttributeTok{data =}\NormalTok{ darkmode,}
  \AttributeTok{estimand =} \StringTok{"ATT"}\NormalTok{,}
  \AttributeTok{method =} \StringTok{"exact"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Error in `matchit()`:
! No exact matches were found.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res\_em}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Error in eval(expr, envir, enclos): object 'res_em' not found
\end{verbatim}

Aufgrund der kontinulierliche Verteilten Variable \texttt{hours} gibt es
in unserem Website-Beispiel keine exakten Matches. Dieses Verfahren ist
hier folglich ungeeignet.

\hypertarget{coarsened-exact-matching}{%
\subsection{Coarsened Exact Matching}\label{coarsened-exact-matching}}

Bei dieser Methode werden kontinuierliche Matching-Variablen grob (Engl.
\emph{coarse}) klassiert, ähnlich wie bei einem Histogram. Diese
Diskretisierung ermöglicht es exakte Übereinstimmungen zwischen
Behandlungs- und Kontrollgruppenbeobachtungen hinsichtlich ihrer
klassierten Ausprägungen zu finden. Sowohl Behandlungs- als auch
Kontrollbeobachtungen die mindestents einen exakten Match haben, werden
Teil des gematchten Datensatzes. In \texttt{matchit()} wird Coarsened
Exact Matching mit \texttt{method\ =\ "cem"} durchgeführt. Über das
Argument \texttt{cutpoints} geben wir an, dass \texttt{hours} in 6
Klassen und \texttt{age} in 4 Klassen eingeteilt werden soll.\footnote{Diese
  Werte wurden ad-hoc gewählt da sie zu einem guten Ergebnis führen.}
Mit \texttt{k1k\ =\ TRUE} erfolgt Eins-zu-eins-Matching: Bei mehreren
exakten Matches wird die Beobachtung mit der geringsten
Mahalanobis-Distanz (für die unklassierten Matching-Variablen) gewählt.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Coarsened Exact Matching}
\NormalTok{res\_CEM }\OtherTok{\textless{}{-}} \FunctionTok{matchit}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ dark\_mode }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ male }\SpecialCharTok{+}\NormalTok{ hours, }
  \AttributeTok{data =}\NormalTok{ darkmode, }
  \AttributeTok{estimand =} \StringTok{"ATT"}\NormalTok{,}
  \AttributeTok{method =} \StringTok{"cem"}\NormalTok{, }
  \AttributeTok{k2k =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{cutpoints =} \FunctionTok{list}\NormalTok{(}
    \StringTok{"hours"} \OtherTok{=} \DecValTok{6}\NormalTok{, }
    \StringTok{"age"} \OtherTok{=} \DecValTok{4}
\NormalTok{  ) }
\NormalTok{)}
\NormalTok{res\_CEM}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
A matchit object
 - method: Coarsened exact matching
 - number of obs.: 300 (original), 164 (matched)
 - target estimand: ATT
 - covariates: age, male, hours
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Balance{-}Table Coarsened Exact Matching}
\FunctionTok{bal.tab}\NormalTok{(res\_CEM)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Balance Measures
         Type Diff.Adj
age   Contin.   0.0106
male   Binary   0.0000
hours Contin.  -0.0135

Sample sizes
          Control Treated
All           151     149
Matched        82      82
Unmatched      69      67
\end{verbatim}

Mit Coarsened Exact Matching erhalten wir einen Datensatz mit 82
Beobachtungen und guter Balance.

\hypertarget{matching-mit-der-mahalanobis-distanz}{%
\subsection{Matching mit der
Mahalanobis-Distanz}\label{matching-mit-der-mahalanobis-distanz}}

Die Euklidische Distanz misst den direkten Abstand zwischen zwei Punkten
und ist nicht invariant gegenüber Transformationen, insbesondere bei
unterschiedlichen Skalierungen und bei Korrelation der
Matching-Variablen. Die Mahalanobis-Distanz hingegen ist ein
standardisiertes Distanzmaß, das unter Berücksichtigung der
Varianz-Kovarianz-Struktur der Daten angibt, wie viele
Standardabweichungen zwei Datenpunkte voneinander entfernt sind. Die
Mahalanobis-Distanz ist invariant gegenüber linearen Transformationen
(Skalierung, Translation und Rotation) der Daten und bietet ein
genaueres Maß für die Unähnlichkeit zweier Beobachtungen hinsichtlich
ihrer Ausprägungen der Matching-Variablen.

Betrachte die Datenpunkte \(P_1=(X_1,Y_1)'\) und \(P_2=(X_2,Y_2)'\) für
die Matching-Variablen \(X\) und \(Y\). Die Mahalanobis-Distanz zwischen
\(P_1\) und \(P_2\) ist definiert als \begin{align*}
  d_M(P_1,\,P_2) = \sqrt{(P_1 - P_2)'\boldsymbol{S}^{-1} (P_1 - P_2)},
\end{align*} wobei \(\boldsymbol{S}\) die Varianz-Kovarianz-Matrix von
\(X\) und \(Y\) ist. Die Mahalanobis-Distanz \(d_M(\cdot,\cdot)\) ist
also die Euklidische Distanz zwischen den standardisierten Datenpunkten.

Für beobachtete Daten ersetzen wir die Komponenten der
Varianz-Kovarianz-Matrix durch Stichprobenmaße. Dies ergibt die Formel

\begin{align*}
  \widehat{d}_M(P_1,\,P_2) = \sqrt{
  \begin{pmatrix}
    X_1 - X_2\\
    Y_1 - Y_2
  \end{pmatrix}'
  \begin{pmatrix}
    \widehat{\text{Var}}(X^2) & \widehat{\text{Cov}}(X, Y) \\
     \widehat{\text{Cov}}(X, Y) & \widehat{\text{Var}}(X^2) 
  \end{pmatrix}^{-1}
    \begin{pmatrix}
    X_1 - X_2\\
    Y_1 - Y_2
  \end{pmatrix}
}.
\end{align*}

Die nachstehende interaktive Grafik zeigt Beobachtungen zweier
Matching-Variablen, die aus einer bivariaten Normalverteilung mit
positiver Korrelation generiert wurden. Diese bivariate Verteilung ist
identisch für Beobachtungen aus der Kontrollgruppe (rot) und
Beobachtungen aus der Behandlungsgruppe (blau). Für die ausgewählte
Beobachtung aus der Behandlungsgruppe (schwarzer Rand) werden
potentielle Matches in der Kontrollgruppe innerhalb der vorgegebenen
Mahalanobis-Distanz in Cyan kenntlich gemacht. Beachte, dass die
Mahalanobis-Distanz Varianzen und Kovarianzen der Daten berücksichtigt,
sodass die gematchten Beobachtungen in einem elliptischen Bereich um die
betrachtete behandelte Beobachtung liegen. Eine Euklidische Distanz
hingegen (gestrichelte Linie) ignoriert die Skalierung der Daten.

Für Eins-zu-Eins-Matching im Website-Beispiel anhand der
Mahalanobis-Distanz mit \texttt{matchit()} setzen wir
\texttt{distance\ =\ "mahalanobis"} und wählen
\texttt{method\ =\ "nearest"}. Mit diesen Parametern wird jeder
Behandlung aus der Behandlungsgruppe die gemäß \(d_M\) am ehesten
vergleichbarste Beobachtung aus der Kontrollgruppe zugewiesen, wobei
keine mehrfachen Matches zulässig sind.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 1:1 Mahalanobis{-}Distanz{-}Matching}
\NormalTok{res\_maha }\OtherTok{\textless{}{-}} \FunctionTok{matchit}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ dark\_mode }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ male }\SpecialCharTok{+}\NormalTok{ hours, }
  \AttributeTok{data =}\NormalTok{ darkmode, }
  \AttributeTok{estimand =} \StringTok{"ATT"}\NormalTok{,}
  \AttributeTok{distance =} \StringTok{"mahalanobis"}\NormalTok{, }
  \AttributeTok{method =} \StringTok{"nearest"}
\NormalTok{)}
\NormalTok{res\_maha}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
A matchit object
 - method: 1:1 nearest neighbor matching without replacement
 - distance: Mahalanobis
 - number of obs.: 300 (original), 298 (matched)
 - target estimand: ATT
 - covariates: age, male, hours
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Balance{-}Table für 1:1 Mahalanobis{-}Matching}
\FunctionTok{bal.tab}\NormalTok{(res\_maha)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Balance Measures
         Type Diff.Adj
age   Contin.  -0.5826
male   Binary   0.3154
hours Contin.   0.0106

Sample sizes
          Control Treated
All           151     149
Matched       149     149
Unmatched       2       0
\end{verbatim}

Die Ergebnisse zeigen, dass für sämtliche \(149\) Beobachtungen aus der
Behandlungsgruppe ein individueller Match in der Kontrollgruppe gefunden
werden konnte. Es werden lediglich \(2\) Beobachtungen der \(151\)
Beobachtungen in der Kontrollgruppe nicht gematcht.

Entsprechend zeigt die Balance-Table eine ähnliche Diskrepanz beider
Gruppen hinsichtlich der Matching-Variablen an.

\textbf{Mahalanobis-Distanz mit Caliper .25 für Propensity Scores
basierend auf logistischer Regression}

Für eine strengeres Matching-Kriterium kann ein \emph{Caliper}, d.h.
eine maximal zulässige Distanz, herangezogen werden. Die
Mahalanobis-Distanz hat jedoch keine einheitliche Skala: Ob eine Distanz
als groß oder klein betrachten werden kann, hängt von der Anzahl der
Matching-Variablen und dem Überlappungsgrad zwischen den Gruppen ab.
Daher wird die Beschränkung durch einen Caliper nicht auf
\(\widehat{d}_M\) sondern auf Propensity Scores angewendet.

Im nächsten Code-Beispiel spezifizieren wir mit
\texttt{distance\ =\ "glm"}, dass Propensity Scores gemäß der Vorschrift
in \texttt{formula} geschätzt werden. Mit
\texttt{mahvars\ =\ \textasciitilde{}\ age\ +\ male\ +\ hours} legen wir
die Matching-Variablen für die Berechnung von \(\widehat{d}_M\) fest.
\texttt{caliper\ =\ .25} legt fest, dass lediglich Beobachtungen der
Kontrollgruppe bei einer absoluten Differenz der Propensity Scores von
höchstens \(0.25\) Standardabweichungen als Match für eine Beobachtung
in der Behandlungsgruppe qualifiziert sind.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Mahalanobis{-}Matchig mit PS{-}Caliper}
\NormalTok{res\_mahaC }\OtherTok{\textless{}{-}} \FunctionTok{matchit}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ dark\_mode }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ male }\SpecialCharTok{+}\NormalTok{ hours, }
  \AttributeTok{data =}\NormalTok{ darkmode, }
  \AttributeTok{distance =} \StringTok{"glm"}\NormalTok{,}
  \AttributeTok{estimand =} \StringTok{"ATT"}\NormalTok{,}
  \AttributeTok{method =} \StringTok{"nearest"}\NormalTok{,}
  \AttributeTok{mahvars =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ male }\SpecialCharTok{+}\NormalTok{ hours,}
  \AttributeTok{caliper =}\NormalTok{ .}\DecValTok{25}
\NormalTok{)}
\NormalTok{res\_mahaC}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
A matchit object
 - method: 1:1 nearest neighbor matching without replacement
 - distance: Mahalanobis [matching]
             Propensity score [caliper]
             - estimated with logistic regression
 - caliper: <distance> (0.058)
 - number of obs.: 300 (original), 208 (matched)
 - target estimand: ATT
 - covariates: age, male, hours
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Balance Table}
\FunctionTok{bal.tab}\NormalTok{(res\_mahaC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Balance Measures
             Type Diff.Adj
distance Distance   0.1812
age       Contin.  -0.1176
male       Binary   0.0481
hours     Contin.  -0.0001

Sample sizes
          Control Treated
All           151     149
Matched       104     104
Unmatched      47      45
\end{verbatim}

Die Balance-Table zeigt einen deutlichen Effekt der Beschränkung
qualifizierter Beobachtungen durch \texttt{caliper\ =\ .25}: Aufgrund
der oberen Grenze für die Propensity-Score-Differenz von \(0.058\) wird
für lediglich \(104\) Beobachtungen aus der Behandlungsgruppe ein
individueller Match in der Kontrollgruppe gefunden.\footnote{Die durch
  \texttt{caliper} implizierte Obergrenze ergibt sich als
  \texttt{.25\ *\ sd(fitted(darkmode\_ps\_logit)))}.} Weiterhin finden
wir eine verbesserte Balance für den gematchten Datensatz.

\hypertarget{propensity-score-matching}{%
\subsection{Propensity Score Matching}\label{propensity-score-matching}}

Eine gängige Variante ist Matching ausschließlich anhand von Propensity
Scores innerhalb eines Calipers.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 1:1 Matching mit PS und Caliper}
\NormalTok{res\_PSC }\OtherTok{\textless{}{-}} \FunctionTok{matchit}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ dark\_mode }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ male }\SpecialCharTok{+}\NormalTok{ hours, }
  \AttributeTok{data =}\NormalTok{ darkmode, }
  \AttributeTok{estimand =} \StringTok{"ATT"}\NormalTok{,}
  \AttributeTok{distance =} \StringTok{"glm"}\NormalTok{, }
  \AttributeTok{method =} \StringTok{"nearest"}\NormalTok{, }
  \AttributeTok{caliper =}\NormalTok{ .}\DecValTok{25}
\NormalTok{)}
\NormalTok{res\_PSC}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
A matchit object
 - method: 1:1 nearest neighbor matching without replacement
 - distance: Propensity score [caliper]
             - estimated with logistic regression
 - caliper: <distance> (0.058)
 - number of obs.: 300 (original), 208 (matched)
 - target estimand: ATT
 - covariates: age, male, hours
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Balance Table}
\FunctionTok{bal.tab}\NormalTok{(res\_PSC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Balance Measures
             Type Diff.Adj
distance Distance   0.1640
age       Contin.  -0.0976
male       Binary   0.0481
hours     Contin.   0.0134

Sample sizes
          Control Treated
All           151     149
Matched       104     104
Unmatched      47      45
\end{verbatim}

Laut Balance-Table führt Eins-zu-Eins-Matching basierend auf Propensity
Scores zu einem Datensatz mit \(104\) gematchten Beobachtungen in der
Behandlungsgruppe. Hinsichtlich der standardisierten Mittelwertdifferenz
(\texttt{Diff.Adj}) erzielt diese Methode die beste Balance unter den
betrachteten Ansätzen.

\textbf{Vergleich der Balance verschiedener Verfahren mit Love-Plot}

Standardisierte Mittelwertdifferenzen für verschiedene
Matching-Verfahren können grafisch mit einem Love-Plot (Love 2004)
veranschaulicht werden. Hierzu nutzen wir \texttt{cobalt::love.plot()}
und übergeben die mit \texttt{matchit()} generierten Objekte im Argument
\texttt{weights}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Love{-}Plot für}
\FunctionTok{love.plot}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ dark\_mode }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ male }\SpecialCharTok{+}\NormalTok{ hours, }
  \AttributeTok{weights =} \FunctionTok{list}\NormalTok{(}
    \AttributeTok{CEM =}\NormalTok{ res\_CEM,}
    \AttributeTok{Mahalanobis =}\NormalTok{ res\_maha,}
    \AttributeTok{Mahalanobis\_Cal =}\NormalTok{ res\_mahaC,}
    \AttributeTok{PSC =}\NormalTok{ res\_PSC}
\NormalTok{  ),}
  \AttributeTok{data =}\NormalTok{ darkmode, }
  \AttributeTok{line =}\NormalTok{ T,}
  \CommentTok{\# absolute Mittelwertdifferenz plotten}
  \AttributeTok{abs =}\NormalTok{ T}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{Matching_files/figure-pdf/unnamed-chunk-36-1.pdf}

}

\end{figure}

Die Grafik zeigt, dass Coarsened Exact Matching (CEM) unter allen
betrachteten Verfahren die Stichprobe mit der besten Balance ergibt.
Diesen gematchten Datensatz erhalten wir mit
\texttt{MatchIt::match.data()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# gematchten Datensatz zuweisen}
\NormalTok{darkmode\_matched\_CEM }\OtherTok{\textless{}{-}} \FunctionTok{match.data}\NormalTok{(res\_CEM)}
\FunctionTok{head}\NormalTok{(darkmode\_matched\_CEM)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 7
  read_time dark_mode  male   age hours weights subclass
      <dbl>     <dbl> <dbl> <dbl> <dbl>   <dbl> <fct>   
1      15.4         0     1    55  125.       1 26      
2      20.9         1     0    23  643.       1 70      
3      21.5         1     0    29  190.       1 79      
4      22           1     0    18  334.       1 80      
5      17.4         0     0    53  279.       1 11      
6      20.4         0     0    43  138.       1 9       
\end{verbatim}

\texttt{darkmode\_matched} enthält Gewichte (\texttt{weights}) für die
jeweilige Gruppe zu denen gemachte Beobachtungen gehören
(\texttt{subclass}). Dies ist relevant, falls Beobachtungen mehrfach
gematcht werden. Wegen Eins-zu-eins-Matching \emph{ohne} Zurücklegen
gibt es in unserem Beispiel 82 Beobachtungspaare und sämtliche Gewichte
sind 1. Die Berücksichtigung der Gewicht in den nachfolgenden Aufrufen
von Schätzfunktionen (bspw.\texttt{lm()}) ist daher nicht nötig und
erfolgt lediglich zur Illustration der grundsätzlichen Vorgehensweise.

Eine Wiederholung der grafischen Analyse in Kapitel~\ref{sec-balance}
zeigt eine deutlich verbesserte Vergleichbarkeit hinsichtlich der
Verteilung der Matching-Variablen in \texttt{darkmode\_matched}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{darkmode\_matched\_CEM }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(dark\_mode) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(age, hours) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate\_all}\NormalTok{(scale) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{dark\_mode)) }\SpecialCharTok{\%\textgreater{}\%}
  
  \FunctionTok{ggplot}\NormalTok{(}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ value, }\AttributeTok{fill =} \FunctionTok{as.factor}\NormalTok{(dark\_mode))}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{( }\AttributeTok{alpha =}\NormalTok{ .}\DecValTok{5}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}
    \AttributeTok{facets =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ name, }
    \AttributeTok{scales =} \StringTok{"free"}\NormalTok{, }
    \AttributeTok{nrow =} \DecValTok{3}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{Matching_files/figure-pdf/unnamed-chunk-38-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{darkmode\_matched\_CEM }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(dark\_mode) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{male =} \FunctionTok{as.factor}\NormalTok{(male), }
    \AttributeTok{dark\_mode =} \FunctionTok{as.factor}\NormalTok{(dark\_mode)}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  
  \FunctionTok{ggplot}\NormalTok{(}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ dark\_mode, }\AttributeTok{fill =}\NormalTok{ male)}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position =} \StringTok{"fill"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Anteil"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{Matching_files/figure-pdf/unnamed-chunk-38-2.pdf}

}

\end{figure}

Wir beobachten eine bessere Balance bei \texttt{age} und \texttt{hours}.
Inbesondere ist \texttt{male} für Kontroll- und Behandlungsgruppe
ausgeglichen.

\hypertarget{schuxe4tzung-und-inferenz-fuxfcr-den-behandlungseffekt-nach-matching}{%
\section{Schätzung und Inferenz für den Behandlungseffekt nach
Matching}\label{schuxe4tzung-und-inferenz-fuxfcr-den-behandlungseffekt-nach-matching}}

Wir schätzen nun den Behandlungseffekt von \texttt{dark\_mode} auf
\texttt{read\_time} für die mit CEM und Propensity Score Matching
ermittelten Datensätzen und vergleichen die Ergebniss anschließend mit
einer Regressionsschätzung ohne Matching.

Wir kombinieren die Matching-Verfahren mit linearer Regression, d.h. wir
Schätzen den Behandlungseffekt anhand es gematchten Datensatzes als
Mittelwertdifferenz nach zusätzlicher Kontrolle für die
Matching-Variablen. Diese Kombination von Matching mit Regression wird
in der Literatur als \emph{Regression Adjustment} bezeichnet und ist
insbesondere hilfreich, wenn Backdoors mit Matching geschlossen werden
sollen, der kausale Effekt jedoch nur unter Verwendung einer
nicht-trivialen Regressionsfunktion ermittelt werden kann. Zum Beispiel
kann bei einer kontinuierlichen Behandlungsvariable und einem
nicht-linearen Zusammenhang mit \(Y\) der kausale Effekt nicht durch
einen bloßen Mittelwertvergleich erfasst werden, sondern erfordert eine
adäquate Modellierung dieses Zusammenhangs in der Regressionsfunktion.
Die zusätzliche Kontrolle für Matching-Variablen kann die Varianz der
Schätzung verringern und das Risiko einer verzerrten Schätzung
abmildern, falls nach Matching noch Unterschiede in der Balance von
Behandlungs- und Kontrollgruppe vorliegen.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# ATT mit linearem Modell schätzen: CEM Datensatz}
\NormalTok{ATT\_mod\_CEM }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ read\_time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ male }\SpecialCharTok{+}\NormalTok{ hours }\SpecialCharTok{+}\NormalTok{ dark\_mode,}
  \AttributeTok{data =}\NormalTok{ darkmode\_matched\_CEM, }
  \AttributeTok{weights =}\NormalTok{ weights }
\NormalTok{)}

\FunctionTok{summary}\NormalTok{(ATT\_mod\_CEM)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = read_time ~ age + male + hours + dark_mode, data = darkmode_matched_CEM, 
    weights = weights)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.9310 -2.3856 -0.0194  2.5407 14.0020 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) 16.6088953  1.5255529  10.887  < 2e-16 ***
age          0.0380982  0.0344699   1.105  0.27072    
male        -4.0915725  0.6858131  -5.966 1.53e-08 ***
hours        0.0050129  0.0006977   7.185 2.45e-11 ***
dark_mode    1.6532234  0.6149439   2.688  0.00794 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.937 on 159 degrees of freedom
Multiple R-squared:  0.414, Adjusted R-squared:  0.3992 
F-statistic: 28.08 on 4 and 159 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Datensatz für Propensity Score Matching zuweisen}
\NormalTok{darkmode\_matched\_PSC }\OtherTok{\textless{}{-}} \FunctionTok{match.data}\NormalTok{(res\_PSC)}

\CommentTok{\# ATT mit linearem Modell schätzen: PSM Datensatz}
\NormalTok{ATT\_mod\_PSC }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ read\_time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ male }\SpecialCharTok{+}\NormalTok{ hours }\SpecialCharTok{+}\NormalTok{ dark\_mode,}
  \AttributeTok{data =}\NormalTok{ darkmode\_matched\_PSC, }
  \AttributeTok{weights =}\NormalTok{ weights }
\NormalTok{)}

\FunctionTok{summary}\NormalTok{(ATT\_mod\_PSC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = read_time ~ age + male + hours + dark_mode, data = darkmode_matched_PSC, 
    weights = weights)

Residuals:
   Min     1Q Median     3Q    Max 
-9.972 -2.605 -0.044  2.587 14.951 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) 17.3654519  1.2762648  13.606  < 2e-16 ***
age          0.0283941  0.0301484   0.942  0.34741    
male        -3.9858702  0.6480072  -6.151 4.03e-09 ***
hours        0.0046119  0.0006685   6.899 6.53e-11 ***
dark_mode    1.6346636  0.5769812   2.833  0.00507 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.141 on 203 degrees of freedom
Multiple R-squared:  0.3171,    Adjusted R-squared:  0.3037 
F-statistic: 23.57 on 4 and 203 DF,  p-value: 5.098e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# ATT mit linearem Modell ohne Matching}
\NormalTok{ATT\_mod\_org }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ read\_time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ male }\SpecialCharTok{+}\NormalTok{ hours }\SpecialCharTok{+}\NormalTok{ dark\_mode,}
  \AttributeTok{data =}\NormalTok{ darkmode}
\NormalTok{)}

\FunctionTok{summary}\NormalTok{(ATT\_mod\_org)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = read_time ~ age + male + hours + dark_mode, data = darkmode)

Residuals:
     Min       1Q   Median       3Q      Max 
-10.2697  -2.6710   0.0164   2.5909  14.5739 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 16.859075   1.082303  15.577  < 2e-16 ***
age          0.051332   0.022215   2.311  0.02154 *  
male        -4.485545   0.498957  -8.990  < 2e-16 ***
hours        0.004348   0.000516   8.427 1.58e-15 ***
dark_mode    1.385810   0.523793   2.646  0.00859 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.03 on 295 degrees of freedom
Multiple R-squared:  0.3434,    Adjusted R-squared:  0.3345 
F-statistic: 38.57 on 4 and 295 DF,  p-value: < 2.2e-16
\end{verbatim}

Beachte, dass für die gematchten Datensätze jeweils ein
durchschnittlicher Behandlungseffekt für die Beobachtungen \emph{mit}
erfolgter Behandlung ermittelt wird: In sämtlichen oben gezeigten
Matchibg-Verfahren werden mit \texttt{estimand\ =\ "ATT"}
vergleichbarere Kontrollbeobachtungen für die behandelten Beobachtungen
ermittelt. Wir schätzen den Effekt der Behandlung, indem wir die
Ergebnisse von behandelten Personen mit denen von gematchten Personen
vergleichen, die keine Behandlung erhalten haben. Diese Vergleichsgruppe
dient als Ersatz für den hypothetischen Zustand der Behandlungsgruppe,
wenn keine Behandlung erfolgt wäre. Dies entspricht der Definition eines
ATT --- ein average treatment effect \emph{on the treated}.

\hypertarget{cluster-robuste-standardfehler}{%
\subsection{Cluster-robuste
Standardfehler}\label{cluster-robuste-standardfehler}}

Für Matching-Verfahren sind die mit \texttt{summary()} berechneten
Standardfehler (und damit auch Konfidenzintervalle, t-Statistiken und
p-Werte) für den Behandlungseffekt \emph{grundsätzlich ungültig}. Je
nach Matching-Verfahren liegen unterschiedliche Quellen von
Schätzunsicherheit vor, die bei der Berechnung von Standardfehlern
zusätzlich zu der ``üblichen'' Stichproben-Variabilität berücksichtig
werden müssen. Gründe hierfür sind der Matching-Prozess ansich oder
weitere Unsicherheit durch die Schätzung zusätzlicher Parameter, etwa
bei der Berechnung von Propensity Scores mit logistischer Regression.
Eine weiterere Ursache zusätzlicher Variation durch den
Matching-Prozess, die wir bisher nicht näher betrachtet haben ensteht
durch Zurücklegen, d.h. wenn Beobachtungen mehrfach gematcht werden
können. Auch dieser Faktor wird in der von \texttt{summary()}
verwendeten Formel für den Standardfehler des Effekt-Schätzers nicht
berücksichtigt.

Standardfehlerberechnung für Matching-Schätzer von Behandlungseffekten
ist Gegenstand aktueller Forschung. P. C. Austin und Small (2014) und
Abadie und Spiess (2022) belegen die Gültigkeit von cluster-robusten
Standardfehlern mit Clustering auf Ebene der Beobachtungsgruppen
(\texttt{subclass} im output von \texttt{match.data()}) bei Matching
ohne Zurücklegen. Für Matching anhand von Propensity Scores (auch mit
Zurücklegen) zeigen Imbens (2016), dass ignorieren der zusätzlichen
Unsicherheit durch die Schätzung der Propensity Scores zu konservativer
Inferenz für den ATE anhand eines cluster-robusten
Standardfehlerschätzers führt, jedoch ungültige Inferenz für die
Schätzung des ATT bedeuten kann. Ähnlich zu P. C. Austin und Small
(2014) deuten die Ergebnisse der Simulationsstudie von Bodory u.~a.
(2020) auf grundsätzlich bessere Eigenschaften der Schätzung hin, wenn
die Standardfehler nicht für die Schätzung der Propensity Scores
adjustiert werden.

Weiterhin ist die Kontrolle für Kovariablen mit Erklärungskraft für die
Outcome-Variable und für die Matching-Variablen (wie oben erfolgt) mit
\emph{Regression Adjustment} für die Schätzung des Behandlungseffekts
nach Matching eine etablierte Strategie, vgl. Hill und Reiter (2006) und
Abadie und Spiess (2022). So können die Varianz der Schätzung und das
Risiko einer Verzerrung der Standardfehler aufgrund verbleibender
Imbalance von Behandlungs- und Kontrollgruppe nach Matching verringert
werden.

Zur Demonstration von (cluster)-robuster Inferenz und für eine
tabellarische Zusammenfassung der Ergebnisse nutzen wir die Pakete
\texttt{marginaleffects} und \texttt{modelsummary}. Mit
\texttt{marginaleffects::avg\_comparisons()} können p-Werte und
Kofindenzintervalle unter Berücksichtigung von robuster Standardfehlern
und der Gewichte aus dem Matching-Verfahren berechnet werden.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(marginaleffects)}

\CommentTok{\# Inferenz: Multiple Regression, ungematchter Datensatz}
\NormalTok{(}
\NormalTok{  sum\_orig }\OtherTok{\textless{}{-}} \FunctionTok{avg\_comparisons}\NormalTok{(}
    \AttributeTok{model =}\NormalTok{ ATT\_mod\_org,}
    \AttributeTok{variables =} \StringTok{"dark\_mode"}\NormalTok{,}
    \CommentTok{\# Heteroskedastie{-}robuste SE:}
    \AttributeTok{vcov =} \StringTok{"HC3"}\NormalTok{, }
    \CommentTok{\# Identifizierung der Kontrollgruppe:}
    \AttributeTok{newdata =} \FunctionTok{subset}\NormalTok{(darkmode, dark\_mode }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }
\NormalTok{  )}
\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

      Term Contrast Estimate Std. Error    z Pr(>|z|)   S 2.5 % 97.5 %
 dark_mode    1 - 0     1.39      0.537 2.58  0.00988 6.7 0.333   2.44

Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high 
Type:  response 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Inferenz: Multiple Regression bei CEM}
\NormalTok{(}
\NormalTok{  sum\_CEM }\OtherTok{\textless{}{-}} \FunctionTok{avg\_comparisons}\NormalTok{(}
  \AttributeTok{model =}\NormalTok{ ATT\_mod\_CEM ,}
  \AttributeTok{variables =} \StringTok{"dark\_mode"}\NormalTok{,}
  \CommentTok{\# Cluster{-}robuste SE}
  \AttributeTok{vcov =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ subclass, }
  \AttributeTok{newdata =} \FunctionTok{subset}\NormalTok{(darkmode\_matched\_CEM, dark\_mode }\SpecialCharTok{==} \DecValTok{1}\NormalTok{),}
  \AttributeTok{wts =} \StringTok{"weights"}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

      Term Contrast Estimate Std. Error    z Pr(>|z|)   S 2.5 % 97.5 %
 dark_mode    1 - 0     1.65      0.549 3.01  0.00262 8.6 0.576   2.73

Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high 
Type:  response 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Inferenz: Multiple Regression bei PSM}
\NormalTok{(}
\NormalTok{  sum\_PSC }\OtherTok{\textless{}{-}} \FunctionTok{avg\_comparisons}\NormalTok{(}
    \AttributeTok{model =}\NormalTok{ ATT\_mod\_PSC ,}
    \AttributeTok{variables =} \StringTok{"dark\_mode"}\NormalTok{,}
    \AttributeTok{vcov =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ subclass, }
    \AttributeTok{newdata =} \FunctionTok{subset}\NormalTok{(darkmode\_matched\_PSC, dark\_mode }\SpecialCharTok{==} \DecValTok{1}\NormalTok{),}
    \AttributeTok{wts =} \StringTok{"weights"}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

      Term Contrast Estimate Std. Error    z Pr(>|z|)   S 2.5 % 97.5 %
 dark_mode    1 - 0     1.63      0.565 2.89  0.00381 8.0 0.527   2.74

Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high 
Type:  response 
\end{verbatim}

Wir fassen die Ergebnisse mit \texttt{modelsummary::modelsummary()}
tabellarisch zusammen.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(modelsummary)}

\CommentTok{\# Tabellarische Zusammenfassung erzeugen}
\FunctionTok{modelsummary}\NormalTok{(}
  \AttributeTok{models =} \FunctionTok{list}\NormalTok{(}
   \StringTok{"Kein Matching"} \OtherTok{=}\NormalTok{ sum\_orig, }
   \StringTok{"Coarsened Exact"} \OtherTok{=}\NormalTok{ sum\_CEM, }
   \StringTok{"Propensity Scores"} \OtherTok{=}\NormalTok{ sum\_PSC}
\NormalTok{  ),}
  \AttributeTok{stars =}\NormalTok{ T, }
  \AttributeTok{gof\_map =} \StringTok{"nobs"}\NormalTok{, }
  \AttributeTok{output =} \StringTok{"gt"}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tabopts}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\setlength{\LTpost}{0mm}
\begin{longtable*}{lccc}
\toprule
  & Kein Matching & Coarsened Exact & Propensity Scores \\ 
\midrule\addlinespace[2.5pt]
dark\_mode & 1.386** & 1.653** & 1.635** \\ 
 & (0.537) & (0.549) & (0.565) \\ 
Num.Obs. & 300 & 164 & 208 \\ 
\bottomrule
\end{longtable*}
\begin{minipage}{\linewidth}
+ p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\\
\end{minipage}

\hypertarget{sec-bootmatching}{%
\section{Bootstrap-Schätzung kausaler Effekte bei
Matching}\label{sec-bootmatching}}

Ein Bootstrap-Verfahren generiert mit Resampling (wiederholtes Ziehen
mit Zurücklegen) aus dem Original-Datensatz (viele) künstliche
Datensätze, für die der Schätzer (d.h. das gesamte Verfahren inkl.
Matching!) jeweils berechnet wird. Die Verteilung der so gewonnenen
Bootstrap-Schätzwerte approximiert die wahre, unbekannte
Stichprobenverteilung des Schätzers des Behandlungseffekts. Mit dieser
simulierten Verteilung können wir Inferenz betreiben: Wir können einen
Bootstrap-Punktschätzer des Behandlungseffekts (Stichprobenmittel der
Bootstrap-Schätzungen) sowie Standardfehler (Standardabweichung der der
Bootstrap-Schätzungen) und p-Werte berechnen.

Der Bootstrap kann hilfreich sein, wenn unklar ist, wie Standardfehler
für die Unsicherheit des Matching-Prozesses zu adjustieren sind, um
gültige Inferenzsstatistiken zu erhalten. Abadie und Imbens (2008)
zeigen analytisch, dass der Standard-Bootstrap die Stichprobenverteilung
für Schätzer kausaler Effekte anhand von gematchten Datensätzen (d.h.
bei Zuordnung/Selektion von Beobachtungen mit Matching) nicht korrekt
abbilden kann. Grundsätzlich problematisch hierbei ist, wenn der
Bootstrap eine verzerrte Schätzung produziert und/oder zu kleine
Standardfehler liefert. Abadie und Imbens (2008) belegen die Tendenz des
Bootstraps zu \emph{konservative} (d.h. zu große) Standardfehler zu
produzieren. Simulationsnachweise (Bodory u.~a. 2020; Hill und Reiter
2006; P. C. Austin und Small 2014; P. C. Austin und Stuart 2017) finden,
dass Bootstrap-Standardfehler u.a. bei Propensity Score Matching mit
Zurücklegen leicht konservativ sind somit das gewünschte nominale
Signifikanzniveau eines Bootstrap-Hypothesentests nicht überschritten
wird, weshalb der Standard-Bootstrap trotz seiner Schwächen in der
empirischen Forschung oft angewendet wird.

Wir betrachen als nächstes einen Bootstrap-Algorithmus für Inferenz
bezüglich eines kausalen Effekts nach Matching und demonstrieren die
Schätzung anhand unseres Website-Beispiels für den ATT nach
Propensity-Score-Matching.

\textbf{Algorithmus: Bootstrap-Schätzer für Propensity-Score-Matching}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Generiere eine Bootstrap-Stichprobe durch \(N\) Züge \emph{mit
  Zurücklegen} aus der \(N\)-elementigen originalen Stichprobe.
\item
  Wende das Matching-Verfahren für die Bootstrap-Stichprobe an. Schätze
  den Behandlungseffekt \(\beta\) anhand der gematchten Stichprobe mit
  Regression. Speichere den Punktschätzer des Behandlungseffekts
  \(\widehat{\beta}_b^*\).
\item
  Fürhre die Schritte 1 und 2 für \(b=1,\dots,B\) aus, wobei \(B\) eine
  hinreichend große Anzahl von Bootstrap-Replikationen ist.
\item
  Berechne den Bootstrap-Schätzer des Behandlungseffekts
  \(\overline{\beta}^* = \frac{1}{B}\sum_{b=1} \widehat{\beta}_b^*\) und
  den Standardfehler
  \(\text{SE}(\overline{\beta}^*) = \sqrt{\frac{1}{B-1}\sum_{b=1}^B(\widehat{\beta}_b^*-\overline{\beta}^*)^2}\).
  Berechne Inferenz-Statistiken mit den üblichen Formeln.
\end{enumerate}

Wir Implementieren nun einen Bootstrap-Schätzer des ATT im
Website-Beispiel für Propensity-Score-Matching. Hierzu definieren wir
eine \texttt{R}-Funktion \texttt{boot\_fun()} für die Schritte 1 und 2
im obigen Algorithmus.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Bootstrap{-}Funktion für Schritte 1 und 2}
\NormalTok{boot\_fun }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}
\NormalTok{    data, }\CommentTok{\# originale Stichprobe}
\NormalTok{    i     }\CommentTok{\# Indexmenge f. Bootstrap{-}Stichprobe}
\NormalTok{) \{}
  
  \CommentTok{\# Bootstrap{-}Stichprobe}
\NormalTok{  boot\_data }\OtherTok{\textless{}{-}}\NormalTok{ data[i, ]}
  
  \CommentTok{\# 1:1 PS Matching}
\NormalTok{  match\_res }\OtherTok{\textless{}{-}} \FunctionTok{matchit}\NormalTok{(}
\NormalTok{    dark\_mode }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ hours }\SpecialCharTok{+}\NormalTok{ male,}
    \AttributeTok{estimand =} \StringTok{"ATT"}\NormalTok{,}
    \AttributeTok{distance =} \StringTok{"glm"}\NormalTok{, }
    \AttributeTok{method =} \StringTok{"nearest"}\NormalTok{, }
    \AttributeTok{caliper =}\NormalTok{ .}\DecValTok{25}\NormalTok{,}
    \AttributeTok{data =}\NormalTok{ boot\_data}
\NormalTok{  ) }
  
  \CommentTok{\# Gematchten Datensatz zuweisen}
\NormalTok{  darkmode\_matched }\OtherTok{\textless{}{-}} \FunctionTok{match.data}\NormalTok{(}
    \AttributeTok{object =}\NormalTok{ match\_res, }
    \AttributeTok{data =}\NormalTok{ boot\_data}
\NormalTok{  )}
  
  \CommentTok{\# Outcome{-}Modell schätzen}
\NormalTok{  ATT\_mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}
    \AttributeTok{formula =}\NormalTok{ read\_time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ male }\SpecialCharTok{+}\NormalTok{ hours }\SpecialCharTok{+}\NormalTok{ dark\_mode,}
    \AttributeTok{data =}\NormalTok{ darkmode\_matched, }
    \AttributeTok{weights =}\NormalTok{ weights }
\NormalTok{  )}
  
  \CommentTok{\#  ATT{-}Schätzung zurückgeben}
  \FunctionTok{return}\NormalTok{(}
\NormalTok{    ATT\_mod}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"dark\_mode"}\NormalTok{]  }
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Wir berechnen nun eine Bootstrap-Schätzung des ATT von
\texttt{dark\_mode} auf \texttt{readingtime} nach Propensity Score
Matching mit einem caliper von 0.25 sowie den zugehörigen Standardfehler
und ein 95\%-KI mit der zuvor definierten Funktion \texttt{boot\_fun}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"boot"}\NormalTok{)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{4321}\NormalTok{)}

\CommentTok{\# Anz. Bootstrap{-}Replikationen}
\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{999}

\CommentTok{\# Bootstrap durchführen}
\NormalTok{(}
\NormalTok{  boot\_out }\OtherTok{\textless{}{-}} \FunctionTok{boot}\NormalTok{(darkmode, boot\_fun, }\AttributeTok{R =}\NormalTok{ B)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = darkmode, statistic = boot_fun, R = B)


Bootstrap Statistics :
    original      bias    std. error
t1* 1.634664 -0.08144129   0.5935746
\end{verbatim}

Den Bootstrap-Schätzer des ATT sowie den Bootstrap-Standardfehler
berechnen wir mit \texttt{mean()} und \texttt{sd()} anhand der 999
Bootstrap-Replikationen in \texttt{boot\_out\$t}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Bootstrap{-}Schätzer für den Treatment{-}Effekt}
\FunctionTok{mean}\NormalTok{(boot\_out}\SpecialCharTok{$}\NormalTok{t) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.553222
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# = mean(t0) + bias = mean(Bootstrap\_samples)}
\CommentTok{\# vgl. \textquotesingle{}t0 = boot\_fun(darkmode, i = 1:1e3)\textquotesingle{}}

\CommentTok{\# Bootstrap{-}Standardfehler}
\FunctionTok{sd}\NormalTok{(boot\_out}\SpecialCharTok{$}\NormalTok{t)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5935746
\end{verbatim}

Ein 95\%-Konfidenzintervall für den kausalen Effekt erhalten wir mit
\texttt{boot::boot.ci()}.\footnote{\texttt{type\ =\ "bca"}
  (bias-corrected accelerated) ist eine gängige Implementierung für die
  Berechnung des Konfidenz-Intervalls.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 95\% Bootstrap{-}KI für den Treatment{-}Effekt}
\FunctionTok{boot.ci}\NormalTok{(}
  \AttributeTok{boot.out =}\NormalTok{ boot\_out, }
  \AttributeTok{type =} \StringTok{"bca"}\NormalTok{, }
  \AttributeTok{conf =}\NormalTok{ .}\DecValTok{95}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
Based on 999 bootstrap replicates

CALL : 
boot.ci(boot.out = boot_out, conf = 0.95, type = "bca")

Intervals : 
Level       BCa          
95%   ( 0.527,  2.839 )  
Calculations and Intervals on Original Scale
\end{verbatim}

Beachte, dass der Bootstrap-Standardfehler sowie das
Bootstrap-Konfidenzintervall nahe der mit \texttt{avg\_comarisons}
berechneten Werte für \texttt{sum\_PSC} sind.

\textbf{Bootstrap-Standardfehler für IPW-Schätzer des ATE berechnen}

Die Bootstrap-Funktion \texttt{boot\_fun} kann leicht für eine Schätzung
des Standardfehlers für den IPW-Schätzer des ATE aus
Kapitel~\ref{sec-PSM} angepasst werden. Statt einer Matching-Prozedur
berechnen wir hierzu für \(B\) Bootstrap-Stichproben den Schätzer
\(\widehat{\tau}^\text{IPW}\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# IPW estimation with regression adjustment}
\NormalTok{IPW\_boot }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}
\NormalTok{    data, }
\NormalTok{    i}
\NormalTok{) \{}
  
  \CommentTok{\# Bootstrap{-}Stichprobe erstellen}
\NormalTok{  data\_boot  }\OtherTok{\textless{}{-}}\NormalTok{ data }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{slice}\NormalTok{(i)}
  
  \CommentTok{\# Logistischee Regression}
\NormalTok{  glm\_fit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(}
    \AttributeTok{formula =}\NormalTok{ dark\_mode }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ hours }\SpecialCharTok{+}\NormalTok{ male,}
    \AttributeTok{data =}\NormalTok{ data\_boot, }
    \AttributeTok{family =}\NormalTok{ binomial}
\NormalTok{  )}
  
  \CommentTok{\# Propensity Scores berechnen}
\NormalTok{  data\_boot }\OtherTok{\textless{}{-}}\NormalTok{ data\_boot }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}
      \AttributeTok{ps =} \FunctionTok{predict}\NormalTok{(glm\_fit, }\AttributeTok{type =} \StringTok{\textquotesingle{}response\textquotesingle{}}\NormalTok{)}
\NormalTok{    )}
  
  \CommentTok{\# Beobachtungen anhand Propensity Scores trimmen}
\NormalTok{  data\_boot }\OtherTok{\textless{}{-}}\NormalTok{ data\_boot }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{filter}\NormalTok{(}
      \FunctionTok{between}\NormalTok{(}
        \AttributeTok{x =}\NormalTok{ ps,}
        \AttributeTok{left =}\NormalTok{ .}\DecValTok{2}\NormalTok{,}
        \AttributeTok{right =}\NormalTok{ .}\DecValTok{7}
\NormalTok{      )}
\NormalTok{    )}
  
  \CommentTok{\# IPW berechnen}
\NormalTok{  data\_boot }\OtherTok{\textless{}{-}}\NormalTok{ data\_boot }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}
      \AttributeTok{ipw =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{        dark\_mode }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ ps,}
\NormalTok{        dark\_mode }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ ps))}
\NormalTok{    )}
  
  \CommentTok{\# Gewichtete Mittelwerte der Gruppen   }
\NormalTok{  w\_means }\OtherTok{\textless{}{-}}\NormalTok{ data\_boot }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(dark\_mode) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{summarize}\NormalTok{(}\AttributeTok{m =} \FunctionTok{weighted.mean}\NormalTok{(read\_time, }\AttributeTok{w =}\NormalTok{ ipw)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{arrange}\NormalTok{(dark\_mode)}
  
  \CommentTok{\# ATT{-}Schätzwert}
  \FunctionTok{return}\NormalTok{(w\_means}\SpecialCharTok{$}\NormalTok{m[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ w\_means}\SpecialCharTok{$}\NormalTok{m[}\DecValTok{1}\NormalTok{])}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Bootstrap durchführen}
\NormalTok{b\_IPW }\OtherTok{\textless{}{-}} \FunctionTok{boot}\NormalTok{(}
  \AttributeTok{data =}\NormalTok{ darkmode,}
  \AttributeTok{statistic =}\NormalTok{ IPW\_boot, }
  \AttributeTok{R =} \DecValTok{999}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Bootstrap{-}Schätzer und Standardfehler berechnen}
\FunctionTok{mean}\NormalTok{(b\_IPW}\SpecialCharTok{$}\NormalTok{t)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.026179
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sd}\NormalTok{(b\_IPW}\SpecialCharTok{$}\NormalTok{t)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.6823184
\end{verbatim}

In diesem Fall ist der Bootstrap-Standardfehler von ca. 0.68 gut mit dem
anhand von \texttt{summary(model\_ipw)} berechneten Standardfehler
vergleichbar. Ein 95\%-Konfidenzintervall für den ATE erhalten wir mit
\texttt{boot.ci()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 95\% Bootstrap{-}KI für den ATE}
\FunctionTok{boot.ci}\NormalTok{(b\_IPW, }\AttributeTok{type =} \StringTok{"bca"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
Based on 999 bootstrap replicates

CALL : 
boot.ci(boot.out = b_IPW, type = "bca")

Intervals : 
Level       BCa          
95%   ( 0.658,  3.225 )  
Calculations and Intervals on Original Scale
\end{verbatim}

\hypertarget{regression-matching-und-doubly-robust-schuxe4tzung}{%
\section{Regression, Matching und
Doubly-Robust-Schätzung}\label{regression-matching-und-doubly-robust-schuxe4tzung}}

Als \emph{Doubly-Robust-Schätzer} bezeichnet man Methoden, die bei
Fehlspezifikationen im Matching-Verfahrens \emph{oder} der funktionalen
Form der Regressionsfunktion für die Outcome-Variable eine zuverlässige
Schätzungen des kausalen Effekts ermöglichen.\footnote{Bspw. kann eine
  falsche funktionale Form bei logistischer Regression eine verzerrte
  Schätzung von Propensity Scores und damit eine unzureichende Balance
  bedeuten.} Unter der Vorraussetzung, dass die verwendeten
Matching-Variablen sämliche Backdoors schließen, ist so mit
Doubly-Robust-Schätzung eine konsistente Schätzung des
Behandlungseffekts unter abgeschwächten Annahmen gewähtleistet. Dies
macht Doubly-Robust-Schätzer besonders nützlich in Forschungskontexten,
in denen präzise Modellspezifikationen herausfordernd sind.

Der von Wooldridge (2010) vorgeschlagene Doubly-Robust-Schätzer (IPWRA)
für den ATE erreicht seine vorteilhaften Eigenschaften durch eine
geschickte Kombination von IPW und Regression Adjustment.

\textbf{Algorithmus: IPWRA-Schätzer des ATE (Wooldridge 2010)}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Berechne IPW anhand von Propensity Score mit logistischer Regression
  unter Verwendung der Matching-Variablen.
\item
  Regression Adjustment:
\end{enumerate}

A. Schätze für die \emph{Behandlungsgruppe} eine mit den IPW gewichtete
Regressionspezifikation der Outcome-Variable mit Kontrolle für die
Matching-Variablen.

B. Wiederhole Schritt A für die Kontrollgruppe.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  Berechne Vorhersagen der Outcome-Variable für die angepassten Modelle
  aus 2.A und 2.B.
\item
  Schätze den ATE als Mittelwert-Differenz der Vorhersagen aus Schritt
  3.
\end{enumerate}

Wir implementieren den Doubly-Robust-Schätzer des ATE von Wooldridge
(2010) für das Website-Beispiel in der Funktion \texttt{IPWRA()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# IPW estimation with regression adjustment}
\NormalTok{IPWRA }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}
\NormalTok{    br, }
    \AttributeTok{i =}\NormalTok{ i) \{}
    \CommentTok{\# slice bootstrapped observations}
\NormalTok{    br }\OtherTok{\textless{}{-}}\NormalTok{ br }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{slice}\NormalTok{(i)}
    
    \CommentTok{\# estimate and predict propensity score}
\NormalTok{    m }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(}
      \AttributeTok{formula =}\NormalTok{ dark\_mode }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ hours }\SpecialCharTok{+}\NormalTok{ male,}
      \AttributeTok{data =}\NormalTok{ br, }
      \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{\textquotesingle{}logit\textquotesingle{}}\NormalTok{)}
\NormalTok{    )}
    
\NormalTok{    br }\OtherTok{\textless{}{-}}\NormalTok{ br }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{mutate}\NormalTok{(}\AttributeTok{ps =} \FunctionTok{predict}\NormalTok{(m, }\AttributeTok{type =} \StringTok{\textquotesingle{}response\textquotesingle{}}\NormalTok{))}

\NormalTok{    br }\OtherTok{\textless{}{-}}\NormalTok{ br }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{filter}\NormalTok{(}
        \FunctionTok{between}\NormalTok{(}
          \AttributeTok{x =}\NormalTok{ ps,}
          \AttributeTok{left =}\NormalTok{ .}\DecValTok{2}\NormalTok{,}
          \AttributeTok{right =}\NormalTok{ .}\DecValTok{7}
\NormalTok{          )}
\NormalTok{    )}
    
    \CommentTok{\# compute IPWs}
\NormalTok{    br }\OtherTok{\textless{}{-}}\NormalTok{ br }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{mutate}\NormalTok{(}
        \AttributeTok{ipw =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{          dark\_mode }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ ps,}
\NormalTok{          dark\_mode }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ ps))}
\NormalTok{      )}
    
    \CommentTok{\# Do regression adjustment for \_ATE\_ estimate}
    \CommentTok{\# TE prediction for whole sample based on TG model}
\NormalTok{    mtreat }\OtherTok{\textless{}{-}}\NormalTok{ br }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{filter}\NormalTok{(dark\_mode }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{lm}\NormalTok{(read\_time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ hours }\SpecialCharTok{+}\NormalTok{ male, }\AttributeTok{data =}\NormalTok{ ., }\AttributeTok{weights =}\NormalTok{ .}\SpecialCharTok{$}\NormalTok{ipw) }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{predict}\NormalTok{(}\AttributeTok{newdata =}\NormalTok{ br) }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{mean}\NormalTok{()}

    \CommentTok{\# TE prediction for whole sample based on CG model}
\NormalTok{    mcont }\OtherTok{\textless{}{-}}\NormalTok{ br }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{filter}\NormalTok{(dark\_mode }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{lm}\NormalTok{(read\_time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ hours }\SpecialCharTok{+}\NormalTok{ male, }\AttributeTok{data =}\NormalTok{ ., }\AttributeTok{weights =}\NormalTok{ .}\SpecialCharTok{$}\NormalTok{ipw) }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{predict}\NormalTok{(}\AttributeTok{newdata =}\NormalTok{ br) }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{mean}\NormalTok{()}

    \FunctionTok{return}\NormalTok{(mtreat }\SpecialCharTok{{-}}\NormalTok{ mcont) }\CommentTok{\# Regression adjusted \_ATE\_ estimate}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{b }\OtherTok{\textless{}{-}} \FunctionTok{boot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ darkmode, IPWRA, }\AttributeTok{R =} \DecValTok{999}\NormalTok{)}
\CommentTok{\# Bootstrap estimate and standard error}
\FunctionTok{mean}\NormalTok{(b}\SpecialCharTok{$}\NormalTok{t)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.947387
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sd}\NormalTok{(b}\SpecialCharTok{$}\NormalTok{t)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5959568
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 95\% Bootstrap{-}KI für den Treatment{-}Effekt}
\FunctionTok{boot.ci}\NormalTok{(b, }\AttributeTok{type =} \StringTok{"bca"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
Based on 999 bootstrap replicates

CALL : 
boot.ci(boot.out = b, type = "bca")

Intervals : 
Level       BCa          
95%   ( 0.737,  3.057 )  
Calculations and Intervals on Original Scale
\end{verbatim}

\bookmarksetup{startatroot}

\hypertarget{regression-discontiniuty-designs}{%
\chapter{Regression Discontiniuty
Designs}\label{regression-discontiniuty-designs}}

Regression Discontinuity Design (RDD) ist ein Ansatz für die Schätzung
von Behandlungseffekten mit Regression, wenn durch einen experimentell
oder natürlich gegebenen Umstand die Behandlung an einem Schwellenwert
(\(c\)) einer \emph{Laufvariable} (\(X\)) sprunghaft beeinflusst wird.
Ein RDD-Schätzer wird so implementiert, dass lediglich Beobachtungen mit
Ausprägungen von \(X\), die knapp ober- oder knapp unterhalb von \(c\)
liegen, berücksichtigt werden. Die zentrale Idee hierbei ist, dass
Individuen nahe bei \(c\) im Durchschnitt ähnliche Merkmale aufweisen.
Beobachtungen nahe \(c\) sind dann insbesondere hinsichtlich
potentieller Backdoor-Variablen vergleichbar, sodass deren
problematische Pfade geschlossen sind. Das kausale Diagram in
Abbildung~\ref{fig-CDRDD} zeigt den grundsätzlichen Zusammenhang.

\begin{figure}

{\centering 

\begin{figure}[H]

{\centering \includegraphics[width=5in,height=3in]{RDD_files/figure-latex/dot-figure-1.png}

}

\end{figure}

}

\caption{\label{fig-CDRDD}Kausales Diagramm für Sharp RDD}

\end{figure}

RDD isoliert Variation auf dem Pfad \emph{Oberhalb C → Behandlung B →
Y}. Somit können Backdoor-Pfade über \(X\) oder weitere (möglichweise
unbeobachtbare) Confounder (\(Z\)) vermieden werden, siehe
Abbildung~\ref{fig-CDRDD}. Der kausale Effekt wird dabei als (lokaler)
durchschnittlicher Behandlungseffekt der Diskontinuität auf die
Outcome-Variable (\(Y\)) anhand von Beobachtungen \emph{nahe bei c}
ermittelt.

Hinsichtlich der Beeinflussung der Behandlung unterscheiden wir zwischen
\emph{Sharp} und \emph{Fuzzy} Regression Discontinuity Designs
(SRDD/FRDD). Bei einem SRDD ist die Zuweisung der Behandlung
\emph{deterministisch}, d.h. der Schwellenwert in der Laufvariable ist
eine harte Grenze für die Gruppenzugehörigkeit: Die
\emph{Wahrscheinlichkeit} der Behandlung \(p\) springt bei \(X=c\) von
\(p=0\) um \(\Delta p = 1\) auf \(p=1\).

Bei einem FRDD ist die Zuordnung in Behandlungs- und Kontrollgruppe
nicht perfekt durch den Schwellenwert \(c\) bestimmt: Die
Behandlungswahrscheinlichkeit \(p\) springt bei \(X=c\) um
\(\Delta p<1\). Im FRDD können grundsätzlich also sowohl behandelte
Subjekte als auch Kontroll-Beobachtungen auf beiden Seiten der
Diskontinuität vorliegen -- die Trennung der Gruppen ist
``unscharf''\footnote{Engl. \emph{fuzzy}.}. Dieser Umstand ist oft in
empirischen Studien mit nicht-experimentellen Daten gegeben, wenn es
neben der Überschreitung von \(c\) weitere Determinanten der Behandlung
gibt (für die wir nicht kontrollieren können). Die Wahl zwischen SRDD
und FRDD hängt grundsätzlich vom datenerzeugenden Prozess und der
Forschungsfrage ab.

\hypertarget{sharp-regression-discontinuity-design}{%
\section{Sharp Regression Discontinuity
Design}\label{sharp-regression-discontinuity-design}}

\textbf{Modell und funktionale Form}

Die korrekte Spezifikation der funktionalen Form für ein RDD ist
wichtig, um eine verzerrte Schätzung des Effekts zu vermeiden. Die
einfachste Form eines SRDD kann anhand der linearen Regression
\begin{align}
Y_i = \beta_0 + \beta_1 B_i + \beta_2 X_i + u_i\label{eq-simpleSRDD}
\end{align} geschätzt werden, wobei \(B_i\) eine Dummy-Variable für das
Überschreiten des Schwellenwertes \(c\) ist, d.h. \begin{align*}
  B_i=\begin{cases}
    0 & X_i < c\\
    1 & X_i \geq c.
  \end{cases}
\end{align*} Damit ist \(B_i\) eine \emph{deterministische} Funktion der
Laufvariable \(X_i\) und zeigt die Zugehörigkeit zur Behandlungs- oder
Treatmentgruppe an. Der Koeffizient \(\beta_1\) misst den
Behandlungseffekt.

Das Modell \eqref{eq-simpleSRDD} unterstellt, dass \(X\) links- und
rechtsseitig von \(c\) denselben Effekt auf \(Y\) hat. Diese Annahme ist
restriktiv. Eine Alternative ist ein lineares Interaktionsmodell
\begin{align}
Y_i = \beta_0 + \beta_1 B_i + \beta_2 (X_i - c) + \beta_3(X_i - c)\times B_i + u_i.\label{eq:linearSRDD}
\end{align} Das Modell \eqref{eq:linearSRDD} kann unterschiedliche
lineare Effekte von \(X\) auf \(Y\) unterhalb (\(\beta_2\)) und oberhalb
(\(\beta_2 + \beta_3\)) von \(c\) abbilden. Beachte, dass \((X_i - c)\)
die um den Schwellenwert zentrierte Laufvariable ist, sodass \(\beta_1\)
wie in \eqref{eq-simpleSRDD} den Unterschied des Effekts von \(X\) auf
\(Y\) für Beoabachtungen am Schwellenwert erfasst.

Um unterschiedliche nicht-lineare Zusammenhänge von \(X\) und \(Y\)
unterhalb und oberhalb von \(c\) abzubilden, können (interargierte)
Polynom-Terme in \(X\) verwendet werden. Häufig wird eine quadratische
Regressionsfunktion genutzt, \begin{align}
  Y_i =&\, \beta_0 + \beta_1 B_i + \beta_2 (X_i - c) + \beta_3 (X_i - c)^2\\ 
       &+\, \beta_4(X_i - c)\times B_i + \beta_5(X_i - c)\times B_i + u_i.\label{eq:quadSRDD}
\end{align} Gelman und Imbens (2019) zeigen, dass Polynome höherer
Ordnung zu verzerrten Schätzern und hoher Varianz führen
können.\footnote{Ursachen sind Überanpassung an die Daten sowie
  instabiles Verhalten der Schätzung nahe des Schwellenwertes.} Die
Authoren empfehlen stattdessen die Schätzung mit lokaler Regression.

\textbf{Nicht-parametrische Schätzung und Bandweite}

Aktuelle Studien nutzen nicht-parametrische Schätzer, die den
Behandlungseffekt als Differenz der geschätzten Regressionsfunktionen am
Schwellenwert \(c\) berechnen. Um auch nicht-lineare
Regressionsfunktionen abzubilden zu können, wird häufig lokale
Regression verwendet. Dieses Verfahren liefert eine ``lokale'' Schätzung
der Regressionsfunktionen am Schwellenwert, bei der nur Beobachtungen
nahe \(X = c\) für die Schätzung berücksichtigt werden. Hinreichende
Nähe wird hierbei durch eine sogenannte Bandweite \(h\) festgelegt,
wobei \begin{align}
  \lvert(X_i-c)\rvert\leq h \label{eq:bwc}
\end{align} das Kriterium für eine Berücksichtigung von Beobachtung
\(i\) bei der Schätzung ist.

Unter Verwendung einer Bandweite \(h\) wird der Regressionsansatz
\eqref{eq:linearSRDD} als \emph{lokale lineare Regression} mit
Uniform-Kernelfunktion bezeichnet. Der Uniform-Kernel gibt allen
Beobachtungen, innerhalb der Bandweite \(h\) dasselbe Gewicht. Ist \(h\)
so groß, dass der gesamte Datensatz in die Schätzung einbezogen wird,
entspricht der lokale lineare Regressions-Schätzer mit Uniform-Kernel
dem (globalen) KQ-Schätzer in einem linearen Interaktionsmodell anhand
aller Beobachtungen. Neben dem Uniform-Kernel ist der Triangular-Kernel
eine in der Praxis häufig genutzte lineare Kernelfunktion. Der
nachstehende Code plottet die Uniform- (grün) sowie die
Triangular-Kernelfunktion (blau), siehe Abbildung~\ref{fig-linearkern}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(cowplot)}

\CommentTok{\# Kernelfunktionen zeichnen}
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
    \FunctionTok{geom\_function}\NormalTok{(}
      \AttributeTok{fun =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{ifelse}\NormalTok{(}
        \AttributeTok{test =} \FunctionTok{abs}\NormalTok{(.) }\SpecialCharTok{\textless{}=} \DecValTok{1}\NormalTok{,}
        \AttributeTok{yes =}  \DecValTok{1}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }
        \AttributeTok{no =} \DecValTok{0}
\NormalTok{      ), }
      \AttributeTok{col =} \StringTok{"green"}\NormalTok{, }
      \AttributeTok{n =} \DecValTok{1000}
\NormalTok{      ) }\SpecialCharTok{+} 
    \FunctionTok{geom\_function}\NormalTok{(}
      \AttributeTok{fun =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{ifelse}\NormalTok{(}
        \AttributeTok{test =} \FunctionTok{abs}\NormalTok{(.) }\SpecialCharTok{\textless{}=} \DecValTok{1}\NormalTok{, }
        \AttributeTok{yes =} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{abs}\NormalTok{(.), }
        \AttributeTok{no =} \DecValTok{0}
\NormalTok{      ), }
      \AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }
      \AttributeTok{n =} \DecValTok{100}
\NormalTok{      ) }\SpecialCharTok{+} 
    \FunctionTok{scale\_x\_continuous}\NormalTok{(}
      \AttributeTok{name =} \StringTok{"x"}\NormalTok{, }
      \AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{1.5}\NormalTok{, }\FloatTok{1.5}\NormalTok{), }
      \AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{    ) }\SpecialCharTok{+}
    \FunctionTok{scale\_y\_continuous}\NormalTok{(}
      \AttributeTok{name =} \StringTok{"K(x)"}\NormalTok{, }
      \AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }
      \AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{1.25}\NormalTok{)}
\NormalTok{    ) }\SpecialCharTok{+}
    \FunctionTok{theme\_cowplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{RDD_files/figure-pdf/fig-linearkern-1.pdf}

}

\caption{\label{fig-linearkern}Kernelfunktionen auf {[}-1, 1{]}}

\end{figure}

In empirischen Studien wird als Basis-Spezifikation oft eine lokale
lineare Regression anhand von \eqref{eq:linearSRDD} mit einer linearen
Kernelfunktionen und geringer bandweite \(h\) genutzt. Anschließend wird
die Robustheit der Ergebnisse anhand flexiblerer Spezifikationen, die
Nicht-Linearitäten in der Regressionsfunktion besser abbilden können,
geprüft.

Die nachstehende Visualisierung zeigt die Schätzung des kausalen
Effektes der Behandlung \(B_i\) anhand lokaler linearer Regression mit
einem Uniform-Kernel für wiefolgt simulierte Daten: \begin{align*}
  Y_i =&\, \beta_1 X_i + \beta_2 B + \beta_3 X_i^2 \times B_i + u_i,\\
  \\
  u_i \sim&\, N(0, 0.5), \quad X_i \sim U(0, 10), \quad B = \mathbb{I}(X_i \geq c = 5)\\
  \beta_1 =&\, .5, \quad \beta_2 = 1.5, \quad \beta_3 = -0.15
\end{align*}

Diese Vorschrift ist schnell mit R umgesetzt:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\CommentTok{\# Anz.Beobachtungen}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{750}

\CommentTok{\# Parameter definieren}
\NormalTok{c }\OtherTok{\textless{}{-}} \DecValTok{5}
\NormalTok{beta\_1 }\OtherTok{\textless{}{-}}\NormalTok{ .}\DecValTok{5}
\NormalTok{beta\_2 }\OtherTok{\textless{}{-}} \FloatTok{1.5}
\NormalTok{beta\_3 }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\NormalTok{.}\DecValTok{15}

\CommentTok{\# Regressionsfunktion definieren}
\NormalTok{f }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(X) \{}
\NormalTok{  beta\_1 }\SpecialCharTok{*}\NormalTok{ (X }\SpecialCharTok{{-}}\NormalTok{ c) }\SpecialCharTok{+}\NormalTok{ beta\_2 }\SpecialCharTok{*}\NormalTok{ B }\SpecialCharTok{+}\NormalTok{ beta\_3 }\SpecialCharTok{*}\NormalTok{ B }\SpecialCharTok{*}\NormalTok{ (X }\SpecialCharTok{{-}}\NormalTok{ c)}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{\}}

\CommentTok{\# Daten erzeugen}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(n, }\DecValTok{0}\NormalTok{, }\DecValTok{11}\NormalTok{)}
\NormalTok{B }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(X }\SpecialCharTok{{-}}\NormalTok{ c }\SpecialCharTok{\textgreater{}=} \DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{Y }\OtherTok{\textless{}{-}} \FunctionTok{f}\NormalTok{(X) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{sd =}\NormalTok{ .}\DecValTok{5}\NormalTok{)}

\CommentTok{\# Beoabchtungen sammeln}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Y =}\NormalTok{ Y, }\AttributeTok{X =}\NormalTok{ X }\SpecialCharTok{{-}}\NormalTok{ c, }\AttributeTok{B =}\NormalTok{ B}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{\emph{Diese interaktive Komponente des Buchs ist nur in der
Online-Version verfügbar}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Der interssierende Effekt am Schwellenwert \(c=5\) beträgt
\(\beta_2 = 1.5\). Beachte, dass aufgrund des Terms
\(\beta_3 X_i^2 \times B_i\) ein quadratischer Zusammenhang von \(Y\)
und \(X\) oberhalb von \(X_i = c\) vorliegt. Es können folgende
Eigenschaften der Schätzung in Abhängigkeit von der Bandweite \(h\)
beobachtet werden:

\begin{itemize}
\item
  Für die voreingestellte Bandweite \(h = 1.3\) liefert die lokale
  lineare Regression eine gute Approximation des
  Regressionszusammenhangs auf beiden Seiten des Schwellenwertes und die
  Schätzung des Behandlungseffekts liegt nahe beim wahren Wert
  \(\beta_2 = 1.5\).
\item
  Für kleinere Bandweiten verringert sich die Datenbasis der Schätzung.
  Die Varianz der Schätzung nimmt zu und die Approximation der
  Regressionsfunktion verschlechtert sich. Wir beobachten eine mit
  \(h\to0\) zunehmende Verzerrung bei der Schätzung des
  Behandlungseffekts.
\item
  Größere Bandweiten \(h\) erhöhen die Datenbasis der Schätzung, führen
  aber zu einer Annäherung der lokalen Schätzung an die globale
  KQ-Schätzung. Linksseitig des Schwellenwertes erzielen wir damit eine
  Schätzung mit hoher Güte. Rechsseitig von \(X_i = c\) verschlechtert
  sich die lokale Anpassung am Schwellenwert deutlich, weil die lineare
  Schätzung den tatsächlichen (nicht-linearen) Zusammenhang nicht
  adäquat abbilden kann. Die Schätzung des Behandlungseffekts ist hier
  deutlich verzerrt.
\end{itemize}

Die Wahl der Bandweite ist also eine wichtige Komponenten der
RDD-Schätzung: Kleine Bandweiten erlauben eine Schätzung der
Regressionsfunktion nahe des Schwellenwertes mit wenig Verzerrung.
Allerdings kann diese Schätzung unpräzise sein, wenn nur wenige
Beobachtungen \eqref{eq:bwc} erfüllen. In der Praxis wird \(h\) daher
mit einem analytischen Schätzer (vgl. G. Imbens und Kalyanaraman 2012)
oder anhand von \emph{Cross Validation} (bspw. G. W. Imbens und Lemieux
2008) bestimmt. Die später in diesem Kapitel betrachteten R-Pakete
halten diese Methoden bereit.

\hypertarget{manipulation-am-schwellenwert}{%
\section{Manipulation am
Schwellenwert}\label{manipulation-am-schwellenwert}}

Eine wichtige Annahmen für die Gültigkeit einer RDD-Schätzung ist, dass
keine Manipulation der Gruppenzugehörigkeit am Schwellenwert vorliegt.
Wenn sich Subjekte nahe des Schwellenwertes \(c\) --- d.h. in
Abhängigkeit der Laufvariable \(X\) --- systematisch in den Confoundern
\(Z\) unterscheiden, können wir den Backdoor-Pfad \emph{Oberhalb C →
Behandlung B → Y} nicht isolieren. Wir erhalten dann eine verzerrte
Schätzung des Behandlungseffekts.

In empirischen Studien mit Individuen kann Selbstselektion auftreten:
Menschen mit \(X<c\) aber nahe \(c\) (hier Kontrollgruppe) könnten
aufgrund unbeobachtbarer Eigenschaften \(Z\) die Ausprägung ihrer
Laufvariable zu \(X>c\) (hier Behandlungsgruppe) manipulieren. Wenn
\(Z\) die Outcome-Variable beeinflusst, bleibt der Backdoor-Pfad
\emph{Oberhalb C → Behandlung B → Y} so bestehen.

Manipulation resultiert in Häufung von Beobachtungen am Schwellenwert.
Dei Verteilung der Laufvariable kann auf diese Unregelmäßigkeit hin
untersucht werden. McCrary (2008) schlägt hierfür einen Verfahren vor,
das die Kontinuität der Dichtefunktion von \(X\) am Schwellenwert
testet.

Der Test von McCrary (2008) ist in \texttt{rdd::DCdensity()}
implementiert. Wir zeigen die Anwendung des Tests anhand der oben
simulierten Daten. Beachte, dass \(X_i\sim U(0, 10)\), d.h. die
Laufvariable ist bei \(X_i = c\) kontinuierlich verteilt. Die
Nullhypothese (keine Manipulation) gilt für die simulierten Daten

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# McCrary{-}Test durchführen}
\NormalTok{p\_mccrary }\OtherTok{\textless{}{-}}\NormalTok{ rdd}\SpecialCharTok{::}\FunctionTok{DCdensity}\NormalTok{(}
  \AttributeTok{runvar =}\NormalTok{ X, }
  \AttributeTok{cutpoint =}\NormalTok{ c, }
  \AttributeTok{plot =}\NormalTok{ F}
\NormalTok{)}

\CommentTok{\# p{-}Wert}
\NormalTok{p\_mccrary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5013939
\end{verbatim}

Der p-Wert 0.5 ist größer als jedes übliche Signifikanzniveau. Damit
liegt starke Evidenz für die Nullhypothese (keine Diskontinuität) und
gegen Manipulation am Schwellenwert vor.

Cattaneo, Jansson, und Ma (2020) (CMJ) schlagen eine Weiterentwicklung
des McCrary-Tests vor, die höhere statistische Macht gegenüber
Diskontinuitäten hat am Schwellenwert hat. Der CJM-Test ist im Paket
\texttt{rddensity} implementiert.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rddensity)}

\CommentTok{\# CJM Schätzer berechnen}
\NormalTok{CJM }\OtherTok{\textless{}{-}} \FunctionTok{rddensity}\NormalTok{(X, }\AttributeTok{c =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Mit der Funktion \texttt{rddensity::rdplotdensity()} erzeugen wir
Abbildung~\ref{fig-cjmtsim}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Plot für Dichtefunktion erstellen}
\NormalTok{plot }\OtherTok{\textless{}{-}} \FunctionTok{rdplotdensity}\NormalTok{(}
  \AttributeTok{rdd =}\NormalTok{ CJM, }
  \AttributeTok{X =}\NormalTok{ X, }
  \CommentTok{\# für Punkte{-} und Linienplots:}
  \AttributeTok{type =} \StringTok{"both"} 
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{RDD_files/figure-pdf/fig-cjmtsim-1.pdf}

}

\caption{\label{fig-cjmtsim}CJM-Test -- geschätzte Dichtefunktionen der
Laufvariable auf beiden Seiten des Schwellenwerts c = 5}

\end{figure}

Abbildung~\ref{fig-cjmtsim} zeigt die geschätzten Dichtefunktionen.
Erwartungsgemäß finden wir eine große Überlappung der zugehörigen
Konfidenzbänder (schattierte Flächen) am Schwellenwert \(c=5\).

Mit \texttt{summary()} erhalten wir eine detaillierte Zusammenfassung
des Tests.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Statistische Zusammenfassung des CJM{-}Tests}
\FunctionTok{summary}\NormalTok{(CJM)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Manipulation testing using local polynomial density estimation.

Number of obs =       750
Model =               unrestricted
Kernel =              triangular
BW method =           estimated
VCE method =          jackknife

c = 5                 Left of c           Right of c          
Number of obs         329                 421                 
Eff. Number of obs    133                 154                 
Order est. (p)        2                   2                   
Order bias (q)        3                   3                   
BW est. (h)           1.918               2.124               

Method                T                   P > |T|             
Robust                -0.3338             0.7385              


P-values of binomial tests (H0: p=0.5).

Window Length              <c     >=c    P>|T|
0.346     + 0.346          20      21    1.0000
0.521     + 0.544          34      37    0.8126
0.696     + 0.742          44      57    0.2323
0.870     + 0.939          54      64    0.4075
1.045     + 1.137          62      77    0.2349
1.220     + 1.334          73      98    0.0661
1.394     + 1.532          86     106    0.1701
1.569     + 1.729          96     124    0.0685
1.743     + 1.927         119     140    0.2139
1.918     + 2.124         133     154    0.2377
\end{verbatim}

Gemäß des p-Werts (\texttt{P\ \textgreater{}\ \textbar{}T\textbar{}})
von 0.74 spricht der CJM-Test noch deutlicher gegen eine Diskontinuität
als der McCrary-Test.

\hypertarget{case-study-amtsinhaber-vorteil-lee2008}{%
\subsection{Case Study: Amtsinhaber-Vorteil (Lee
2008)}\label{case-study-amtsinhaber-vorteil-lee2008}}

Lee (2008) untersucht den Einfluss des Amtsinhaber-Vorteils auf die Wahl
von Mitgliedern des US-Repräsentantenhaus. In den meisten Wahlkreisen
entfallen große Anteile der Stimmen (oder gar ausschließlich) auf
demokratische und republikanische Kanditat*innen, sodass sich die Studie
auf diese Parteien beschränkt. Entfällt die Mehrheit der Stimmen auf
eine*n Kandiat*in, gewinnt diese*r den Sitz für den Wahlkreis. Durch die
Analyse der 6558 Wahlen im Zeitraum 1946-1998 mit einem SRDD kommt die
Studie zu dem Ergebnis, dass Amtsinhabende im Durchschnitt einen Vorteil
von etwa 8\% bis 10\% bei der Wahl haben. Dieses Ergebnis kann
verschiedene Ursachen haben, bspw. dass die amtierende Partei höhere
finanzielle Ressourcen besitzt und von einer besseren Organisation und
durch Instrumenalisierung staatlicher Strukturen für die eigenen Zwecke
profitiert.

Anhand der Datensätze \texttt{house} und \texttt{house\_binned}
illustrieren wir nachfolgend die Schätzung von SRDD-Modellen für den
Wahlerfolg der demokratischen Partei, wenn diese Amtsinhaber ist. Wir
lesen hierfür zunächst die Datensätze \texttt{house} und
\texttt{house\_binned} ein und verschaffen uns einen Überblick.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(modelsummary)}

\CommentTok{\# Daten einlesen}
\NormalTok{house }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"datasets/house.csv"}\NormalTok{)}
\CommentTok{\# Gruppierter Datensatz}
\NormalTok{house\_binned }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"datasets/house\_binned.csv"}\NormalTok{)}

\CommentTok{\# Überblick verschaffen}
\FunctionTok{glimpse}\NormalTok{(house)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 6,558
Columns: 2
$ StimmenTm1 <dbl> 0.1049, 0.1393, -0.0736, 0.0868, 0.3994, 0.1681, 0.2516, 0.~
$ StimmenT   <dbl> 0.5810, 0.4611, 0.5434, 0.5846, 0.5803, 0.6244, 0.4873, 0.5~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(house\_binned)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 100
Columns: 2
$ StimmenT   <dbl> 0.5995600, 0.5657000, 0.4272554, 0.5637456, 0.6868627, 0.60~
$ StimmenTm1 <dbl> 0.104764444, 0.135005263, -0.075690769, 0.084570886, 0.3951~
\end{verbatim}

Der Datensatz \texttt{house} enthält die Stimmenanteile demokratischer
Kandidat*innen bei der Wahl zum Zeitpunkt \(T\) (\(StimmenT\)) sowie die
Differenz zwischen demokratischen und republikanischen Stimmenanteilen
bei der vorherigen Wahl, d.h. zum Zeitpunkt \(T-1\) (\(StimmenTm1\)).
Der Schwellenwert für einen Wahlsieg liegt bei Stimmengleichheit, d.h.
\(StimmenTm1 = 0\).

\texttt{house\_binned} ist eine aggregierte Version von \texttt{house}
mit Mittelwerten von jeweils 50 gleichgroßen Intervallen oberhalb und
unterhalb der Schwelle von \(StimmenTm1 = 0\). Dieser Datensatz eignet
sich, um einen ersten Eindruck des funktionalen Zusammenhangs auf beiden
Seiten zu erhalten. Wir stellen zunächst diese klassierten Daten mit
\texttt{ggplot2} graphisch dar.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Klassierte Daten plotten}
\NormalTok{house\_binned }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ StimmenTm1, }\AttributeTok{y =}\NormalTok{ StimmenT)}
\NormalTok{    ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{RDD_files/figure-pdf/fig-LeeDataClass-1.pdf}

}

\caption{\label{fig-LeeDataClass}Klassierte Daten aus Lee (2008)}

\end{figure}

Die Grafik zeigt eindeutig einen Sprung von \(StimmenT\) bei
\(StimmenTm1 = 0\). Weiterhin erkennen wir, dass der Zusammenhang nahe
\(0\) vermutlich jeweils gut durch eine lineare Funktion approximiert
werden kann. Eine Modell-Spezifikation mit gleicher Steigung auf beiden
Seiten des Schwellenwertes scheint hingegen weniger gut geeignet. Wir
vergleichen diese Spezifikationen nachfolgend.

Zunächst fügen wir dem Datensatz eine Dummyvariable \texttt{B} hinzu.
Diese dient als Indikator für den Wahlgewinn in der letzten Wahl und
zeigt die Amtsinhaberschaft (Behandlung) an.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Behandlungsindikator B hinzufügen}
\NormalTok{house }\OtherTok{\textless{}{-}}\NormalTok{ house }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{B =}\NormalTok{ StimmenTm1 }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{)}

\FunctionTok{glimpse}\NormalTok{(house)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 6,558
Columns: 3
$ StimmenTm1 <dbl> 0.1049, 0.1393, -0.0736, 0.0868, 0.3994, 0.1681, 0.2516, 0.~
$ StimmenT   <dbl> 0.5810, 0.4611, 0.5434, 0.5846, 0.5803, 0.6244, 0.4873, 0.5~
$ B          <lgl> TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE~
\end{verbatim}

Wir überprüfen die Laufvariable mit dem CJM-Test auf Manipulation am
Schwellenwert \(c=0\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# CJM{-}Test durchführen}
\NormalTok{CJM\_Lee }\OtherTok{\textless{}{-}} \FunctionTok{rddensity}\NormalTok{(}\AttributeTok{X =}\NormalTok{ house}\SpecialCharTok{$}\NormalTok{StimmenTm1)}

\CommentTok{\# Zusammenfassung anzeigen}
\FunctionTok{summary}\NormalTok{(CJM\_Lee)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Manipulation testing using local polynomial density estimation.

Number of obs =       6558
Model =               unrestricted
Kernel =              triangular
BW method =           estimated
VCE method =          jackknife

c = 0                 Left of c           Right of c          
Number of obs         2740                3818                
Eff. Number of obs    1297                1360                
Order est. (p)        2                   2                   
Order bias (q)        3                   3                   
BW est. (h)           0.236               0.243               

Method                T                   P > |T|             
Robust                1.4346              0.1514              


P-values of binomial tests (H0: p=0.5).

Window Length / 2          <c     >=c    P>|T|
0.004                      21      24    0.7660
0.007                      38      46    0.4452
0.011                      50      60    0.3909
0.014                      73      77    0.8066
0.018                      91     104    0.3902
0.022                     124     132    0.6618
0.025                     149     149    1.0000
0.029                     163     174    0.5860
0.032                     176     202    0.1984
0.036                     197     223    0.2225
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# CJM{-}Plot}
\NormalTok{plot }\OtherTok{\textless{}{-}} \FunctionTok{rdplotdensity}\NormalTok{(}
  \AttributeTok{rdd =}\NormalTok{ CJM\_Lee,}
  \AttributeTok{X =}\NormalTok{ house}\SpecialCharTok{$}\NormalTok{StimmenTm1, }
  \AttributeTok{type =} \StringTok{"both"}\NormalTok{, }
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{RDD_files/figure-pdf/fig-cjm-lee-1.pdf}

}

\caption{\label{fig-cjm-lee}CJM-Test -- geschätzte Dichtefunktionen der
Laufvariable}

\end{figure}

Abbildung~\ref{fig-cjm-lee} und der p-Wert von \(0.15\) sind Evidenz
gegen eine Manipulation am Schwellenwert.

Um den Behandlungseffekt anhand eines SRDDs zu ermitteln, schätzen wir
das Interaktionsmodell \begin{align*}
  \text{StimmenT}_i =&\, \beta_0 + \beta_1 B_i + \beta_2 (\text{StimmenTm1}_i - 50)\\ 
  +&\, \beta_3(\text{StimmenTm1}_i - 50)\times B_i + u_i
\end{align*} zunächst für eine Bandweite von \(h = 0.5\). Aufgrund der
Skalierung der Daten (Wahlergebnisse in \%) bedeutet dies die Verwendung
des \emph{gesamten} Datensatzes für die Schätzung.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Interaktionsmodell schätzen}
\NormalTok{house\_llr1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ StimmenT }\SpecialCharTok{\textasciitilde{}}\NormalTok{ B }\SpecialCharTok{*}\NormalTok{ StimmenTm1, }
  \AttributeTok{data =}\NormalTok{ house}
\NormalTok{)}

\CommentTok{\# Zusammenfassung anzeigen  }
\FunctionTok{modelsummary}\NormalTok{(}
  \AttributeTok{models =}\NormalTok{ house\_llr1, }
  \AttributeTok{vcov =} \StringTok{"HC1"}\NormalTok{, }\CommentTok{\# robuste Standardfehler}
  \AttributeTok{stars =}\NormalTok{ T, }
  \AttributeTok{gof\_map =} \StringTok{"nobs"}\NormalTok{, }
  \AttributeTok{output =} \StringTok{"gt"}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  tabopts}
\end{Highlighting}
\end{Shaded}

\setlength{\LTpost}{0mm}
\begin{longtable*}{lc}
\toprule
  & (1) \\ 
\midrule\addlinespace[2.5pt]
(Intercept) & 0.433*** \\ 
 & (0.004) \\ 
BTRUE & 0.118*** \\ 
 & (0.006) \\ 
StimmenTm1 & 0.297*** \\ 
 & (0.016) \\ 
BTRUE × StimmenTm1 & 0.046* \\ 
 & (0.018) \\ 
Num.Obs. & 6558 \\ 
\bottomrule
\end{longtable*}
\begin{minipage}{\linewidth}
+ p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\\
\end{minipage}

Der geschätzte Koeffizient von \(B\) (\texttt{BTRUE}) beträgt etwa
\(0.12\) und ist hochsignifikant. Übereinstimmend mit
Abbildung~\ref{fig-LeeDataClass} erhalten wir also eine positive
Schätzung des Behandlungseffekts. Die Interpretation ist, dass die
amtierenden Demokraten bei der Wahl von einem Amtsinhabervorteil
profitieren. Dieser Effekt schlägt sich als Stimmenbonus von geschätzten
12\% nieder. Diese Schätzung des Behandlungseffekts könnte jedoch
verzerrt sein:

\begin{itemize}
\item
  Die (implizite) Wahl von \(h=0.5\) in unserer Schätzung macht die
  Isolation des relevanten Frontdoor-Paths (\(c=0\) → Treatment →
  StimmenT) wenig plausibel. \(h\) sollte mit einer datengetriebenen
  Methode gewählt werden.
\item
  Weiterhin könnte die lineare funktionale Form der Regression inadäquat
  sein: Die lineare Approximation der wahren Regressionsfunktion nahe
  des Schwellenwerts \(0\) könnte unzureichend sein und in einer
  verzerrten Schätzung des Effekts resultieren. Zur Überprüfung der
  Robustheit der Ergebnisse sollte mit Schätzungen anhand nicht-linearer
  Spezifikationen verglichen werden.
\end{itemize}

Um diesen Gefahren für die Validität der Studie zu begegnen, schätzen
wir nun weitere Spezifikationen. Im Folgenden verwenden wir eine
Bandweitenschätzung gemäß G. Imbens und Kalyanaraman (2012).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Bandweite mit Schätzer von IK (2012) berechnen}
\NormalTok{(}
\NormalTok{IK\_BW }\OtherTok{\textless{}{-}} 
\NormalTok{  rdd}\SpecialCharTok{::}\FunctionTok{IKbandwidth}\NormalTok{(}
    \AttributeTok{X =}\NormalTok{ house}\SpecialCharTok{$}\NormalTok{StimmenTm1, }
    \AttributeTok{Y =}\NormalTok{ house}\SpecialCharTok{$}\NormalTok{StimmenT}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.2685123
\end{verbatim}

Wir schätzen zunächst erneut das lineare Interaktionsmodell, diesmal
jedoch mit der Bandweite \texttt{IK\_BW}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Lineares Interaktionsmodelle mit IK{-}Bandweite}
\NormalTok{house\_llin\_IK }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ StimmenT }\SpecialCharTok{\textasciitilde{}}\NormalTok{ B }\SpecialCharTok{*}\NormalTok{ StimmenTm1, }
  \AttributeTok{data =}\NormalTok{ house }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{filter}\NormalTok{(}
      \FunctionTok{abs}\NormalTok{(StimmenTm1) }\SpecialCharTok{\textless{}=}\NormalTok{ IK\_BW}
\NormalTok{    )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Für den Vergleich mit einer nicht-linearen Spezifikation schätzen wir
auch ein quadratisches Interaktionsmodell.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Quadratisches Interaktionsmodell mit IK{-}Bandweite}
\NormalTok{house\_poly\_IK }\OtherTok{\textless{}{-}} \FunctionTok{update}\NormalTok{(}
  \AttributeTok{object =}\NormalTok{ house\_llin\_IK,}
  \AttributeTok{formula =}\NormalTok{ StimmenT }\SpecialCharTok{\textasciitilde{}}\NormalTok{ B }\SpecialCharTok{*} \FunctionTok{poly}\NormalTok{(StimmenTm1, }\AttributeTok{degree =} \DecValTok{2}\NormalTok{, }\AttributeTok{raw =}\NormalTok{ T)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Für eine Gegenüberstellung der Ergebnisse verwenden wir
\texttt{modelsummary()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Tabellarischer Modellvergleich}
\FunctionTok{modelsummary}\NormalTok{(}
  \AttributeTok{models =} \FunctionTok{list}\NormalTok{(}
    \StringTok{"Linear int."} \OtherTok{=}\NormalTok{ house\_llin\_IK, }
    \StringTok{"Quadratisch int."} \OtherTok{=}\NormalTok{ house\_poly\_IK}
\NormalTok{  ),  }
  \AttributeTok{vcov =} \StringTok{"HC1"}\NormalTok{, }
  \AttributeTok{stars =}\NormalTok{ T,}
  \AttributeTok{gof\_map =} \StringTok{"nobs"}\NormalTok{, }
  \AttributeTok{output =} \StringTok{"gt"}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  tabopts}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-intmodsLee}{}
\setlength{\LTpost}{0mm}
\begin{longtable}{lcc}
\caption{\label{tbl-intmodsLee}Vergleich von SRDD-Interaktionsmodellen für Lee (2008) }\tabularnewline

\toprule
  & Linear int. & Quadratisch int. \\ 
\midrule\addlinespace[2.5pt]
(Intercept) & 0.450*** & 0.460*** \\ 
 & (0.005) & (0.008) \\ 
BTRUE & 0.085*** & 0.068*** \\ 
 & (0.008) & (0.012) \\ 
StimmenTm1 & 0.360*** &  \\ 
 & (0.036) &  \\ 
BTRUE × StimmenTm1 & 0.055 &  \\ 
 & (0.059) &  \\ 
poly(StimmenTm1, degree = 2, raw = T)1 &  & 0.573*** \\ 
 &  & (0.138) \\ 
poly(StimmenTm1, degree = 2, raw = T)2 &  & 0.798 \\ 
 &  & (0.493) \\ 
BTRUE × poly(StimmenTm1, degree = 2, raw = T)1 &  & 0.036 \\ 
 &  & (0.219) \\ 
BTRUE × poly(StimmenTm1, degree = 2, raw = T)2 &  & -1.529+ \\ 
 &  & (0.834) \\ 
Num.Obs. & 2956 & 2956 \\ 
\bottomrule
\end{longtable}
\begin{minipage}{\linewidth}
+ p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\\
\end{minipage}

Die Spalte (1) in Tabelle~\ref{tbl-intmodsLee} zeigt die lokale
Schätzung mit einem linearen Interaktionsmodell. Wir erhalten damit
einen Behandlungseffekt von etwa \(8.5\%\). Der Schätzwert fällt also
etwas geringer aus als für die globale KQ-Schätzung des linearen
Interaktionsmodells. Für das Modell (2) mit quadratischer Spezifikation
liegt der Schätzwert mit \(6.8\%\) in der selben Größenordnung. Beide
Schätzungen ergeben einen signifikant von \(0\) verschieden Effekt.
Weiterhin fällt auf, dass in beiden Modellen keine Evidenz für
unterschiedliche Formen der Regressionsfunktionen auf beiden Seiten des
Schwellenwerts vorliegen: sämtliche Koeffizientenschätzwerte der
Interaktionsterme haben hohe Standardfehler und sind nicht signifikant.
Im quadratischen Modell hat auch der Term \(StimmenTm1^2\) keinen
signifikanten Effekt. Diese Ergebnisse deuten darauf hin, dass eine
lineare Spezifikation ausreichend ist.

\textbf{SRDD-Schätzung mit LOESS}

Wir illustrieren nachfolgend die Schätzung des Behandlungseffekts mit
einer flexiblen und in der Praxis häufig verwendeten Methode für lokale
Regression. Die nachfolgende interaktive Grafik zeigt die klassierten
Daten aus Lee (2008) auf dem Intervall \([-0.5,0.5]\) gemeinsam mit
einer nicht-parametrischen Schätzung des Zusammenhangs von
\texttt{StimmenT} und \texttt{StimmenTm1} mittels LOESS.\footnote{\href{https://en.wikipedia.org/wiki/Local_regression}{LOESS}
  ist eine Variante von lokaler Polynom-Regression.} Diese
Implementierung von lokaler Regression nutzt einen
\href{https://en.wikipedia.org/wiki/Kernel_(statistics)}{tricube
kernel}. Über den Input kann eine Bandweite \(l\in(0,1]\) für den
LOESS-Schätzer auf beiden Seiten des Schwellenwerts \(0\) gewählt
werden. Die Bandweite ist hier der \emph{Anteil der Beobachtungen an der
gesamten Anzahl an Beobachtungen}, die in die Schätzung einbezogen
werden sollen.

Für die Schätzung am Schwellenwert berücksichtigte Daten sind in orange
kenntlich gemacht. Die rote linie zeigt die geschätzte
Regressionsfunktion über gleichmäßig verteilte Werte von
\texttt{StimmenTm1} auf \([-0.5,0.5]\). Die Grafik verdeutlicht, dass
die LOESS-Methode flexibel genug ist, um lineare und nicht-lineare
Zusammenhänge abbilden zu können. Wie zuvor ist eine adäquate Wahl der
Bandweite wichtig:

\begin{itemize}
\item
  Der mit LOESS geschätzte Zusammenhang auf beiden Seiten des
  Schwellenwerts ist etwa linear für den voreingestellten Parameter
  (\(l = 0.28\)).
\item
  Für größere Werte von \(l\) nähert sich die Schätzung weiter einem
  linearen Verlauf an. Die Schätzung des Effekts bleibt vergleichbar mit
  den Ergebnissen des linearen Interaktionsmodell (s. oben).
\item
  Für kleinere \(l\) erhalten wir eine stärkere Anpassung der Schätzung
  an die Daten. Zu kleine Werte führen zu einer Überanpassung
  (\emph{overfitting}). Insbesondere tendiert die geschätzte Funktion zu
  extremer Steigung nahe des Schwellenwerts → stark verzerrte Schätzung
  des Effekts!
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{\emph{Diese interaktive Komponente des Buchs ist nur in der
Online-Version verfügbar.}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{fuzzy-regression-discontinuity-design}{%
\section{Fuzzy Regression Discontinuity
Design}\label{fuzzy-regression-discontinuity-design}}

\begin{figure}

{\centering 

\begin{figure}[H]

{\centering \includegraphics[width=5in,height=3in]{RDD_files/figure-latex/dot-figure-2.png}

}

\end{figure}

}

\caption{\label{fig-CDFRDD}Kausales Diagram für FRDD}

\end{figure}

Ein FRDD liegt vor, wenn die Zuweisung der Behandlung \(B\) durch die
Laufvariable \(X\) (und möglicherweise weitere Variablen \(Z\))
beeinflusst wird. Im Vergleich zum SRDD ist die Behandlung dann also
\emph{nicht} ausschließlich durch Überschreiten des Schwellenwerts
\(X = c\) bestimmt.

Abbildung~\ref{fig-CDFRDD} zeigt den grundsätzlichen Zusammenhang. Hier
genügt es weiterhin für \(X\) (und ggf. \(Z\)) zu kontrollieren, um den
Pfad \emph{oberhalb \(C\) → Behandlung \(B\) → \(Y\)} zu isolieren. Der
so für \emph{Behandlung \(B\)} ermittelte Effekt auf \(Y\) entspricht
jedoch \emph{nicht} dem ``vollständigen'' Behandlungseffekt, da bei
\(c\) die Zuweisung der Behandlung nicht von \(0\) auf \(100\%\)
springt. Die Schätzung des FRDD berücksichtigt dies und skaliert den
geschätzten Effekt entsprechend.

Wir betrachten zunächst den Zusammenhang \begin{align}
  Y_i = \beta_0 + \beta_1 B_i + \beta_2 (X_i - c) + u_i.\label{eq-simpleFRDD}
\end{align} In einem FRDD springt die Behandlungswahrscheinlichkeit am
Schwellenwert \(c\) um \(\Delta p<1\). Wir können \(B\) also nicht als
deterministische Funktion von \(X\), welche die Zuweisung zu
Behandlungs- bzw. Kontrollgruppe am Schwellenwert \(c\) anzeigt (wie im
SRDD), definieren. Stattdessen betrachten wir \begin{align}
  P(B_i=1\vert X_i) = 
  \begin{cases}
    g_{X_i<c}(X_i), & X_i < c \\ 
    g_{X_i\geq c}(X_i) & X_i \geq c
  \end{cases}\,. \label{eq-BFRDD}
\end{align} Die Funktionen \(g_{X_i<c}\) und \(g_{X_i\geq c}\) können
verschieden sein. Es muss jedoch
\[g_{X_i<c}(X_i = c) \neq g_{X_i\geq c}(X_i = c)\] gelten. Die
Behandlungsvariable \(B_i\) ist im FRDD also eine (binäre)
Zufallsvariable, deren bedingte Wahrscheinlichkeitsfunktion
\(P(B_i=1\vert X_i)\) am Schwellenwert \(c\) eine Diskontinuität
aufweist. Abbildung~\ref{fig-FRDDprobD} zeigt heispielhafte Verläufe
nicht-linearer bedingter Wahrscheinlichkeitsfunktion für die Behandlung
mit einer Diskontinuität bei \(X_i = c\).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(cowplot)}

\CommentTok{\# Bedingte Behandlungswahrscheinlichkeit im FRDD illustrieren}
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_function}\NormalTok{(}
    \AttributeTok{fun =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{ifelse}\NormalTok{(}
\NormalTok{      . }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{, }
      \SpecialCharTok{{-}}\NormalTok{.}\DecValTok{1} \SpecialCharTok{*}\NormalTok{ .}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{ .}\DecValTok{25}\NormalTok{, }
      \SpecialCharTok{{-}}\NormalTok{.}\DecValTok{1} \SpecialCharTok{*}\NormalTok{ (.}\SpecialCharTok{{-}}\FloatTok{1.5}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{1}
\NormalTok{    ), }
    \AttributeTok{n =} \DecValTok{1000}
\NormalTok{  ) }\SpecialCharTok{+} 
    \FunctionTok{geom\_function}\NormalTok{(}
    \AttributeTok{fun =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{ifelse}\NormalTok{(}
\NormalTok{      . }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{, }
\NormalTok{     .}\DecValTok{35}\NormalTok{, }
\NormalTok{     .}\DecValTok{65}
\NormalTok{    ),}
    \AttributeTok{n =} \DecValTok{1000}\NormalTok{, }
    \AttributeTok{lty =} \DecValTok{2}\NormalTok{, }
    \AttributeTok{col =} \StringTok{"red"}
\NormalTok{  ) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}
    \AttributeTok{name =} \StringTok{"Laufvariable X"}\NormalTok{, }
    \AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{1.5}\NormalTok{, }\FloatTok{1.5}\NormalTok{),}
    \AttributeTok{labels =} \ConstantTok{NULL}\NormalTok{,}
    \AttributeTok{breaks =} \ConstantTok{NULL}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}
    \AttributeTok{name =} \StringTok{"P(D=1|X)"}\NormalTok{, }
    \AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }
    \AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{theme\_cowplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{RDD_files/figure-pdf/fig-FRDDprobD-1.pdf}

}

\caption{\label{fig-FRDDprobD}Bedingte Behandlungswahrscheinlichkeiten
im FRDD}

\end{figure}

Definition \eqref{eq-BFRDD} bedeutet, dass eine KQ-Schätzung von
\(\beta_1\) anhand \eqref{eq-simpleFRDD} eine \emph{verzerrte} Schätzung
des Behandlungseffekts ist: Der in \(\widehat{\beta}_1\) erfasste Effekt
auf \(Y\) ist auf einen Sprung der Behandlungswahrscheinlichkeit bei
\(X_i = c\) um \emph{weniger} als \(100\%\) zurückzuführen. Der wahre
Behandlungseffekt wird also \emph{unterschätzt}. Daher muss
\(\widehat{\beta}_1\) skaliert werden, sodass die Schätzung als Effekt
einer Änderung der Behandlungswahrscheinlichkei um \(100\%\)
interpretiert werden kann --- der erwartete Effekt, wenn ausschließlich
Subjekte mit \(X_i\geq c\) behandelt würden. Diese skalierte Schätzung
erhalten wir mit IV-Regression (vgl. Kapitel XYZ). Hierfür nutzen wir
für \(B_i\) die Instrumentvariable \begin{align*}
  D_i = \begin{cases}
    0, & X_i < c \\ 
    1, & X_i \geq c.
  \end{cases}
\end{align*}

Angenommen \(g_{X_i\geq c}(X_i) = \alpha_0\) und
\(g_{X_i<c}(X_i) = \alpha_0 + \alpha_1\) mit \(\alpha_0 + \alpha_1 < 1\)
(vgl. rote Funktion in Abbildung~\ref{fig-CDFRDD}). Der FRDD-Schätzer
des Behandlungseffekts ist dann \(\widehat{\gamma}_\textup{FRDD}\) im
2SLS-Verfahren mit den Regressionen \begin{align}
  \begin{split}
  (\mathrm{I})\qquad B_i =&\, \alpha_0 + \alpha_1 D_i + \alpha_2 (X_i - c) + e_i,\\
  (\mathrm{II})\qquad Y_i =&\, \gamma_0 + \gamma_1 \widehat{B}_i + \gamma_2 (X_i - c) + \epsilon_i,
  \end{split}\label{eq:FRDD_simpleIV}
\end{align} wobei \(\widehat{B}_i\) die angepassten Werte aus Stufe
\((\mathrm I)\) und \(e_i\) sowie \(\epsilon_i\) Fehlterterme sind.

Analog zum SRDD müssen in empirischen Anwendungen geeignete
Spezifikationen für die Regressionsfunktionen \eqref{eq-simpleFRDD} und
\eqref{eq-BFRDD} gewählt und der 2SLS-Schätzer \eqref{eq:FRDD_simpleIV}
entsprechend angepasst werden. Ein einfaches Interaktionsmodell wäre
\begin{align}
  \begin{split}
  (\mathrm{I})\qquad B_i =&\, \alpha_0 + \alpha_1 D_i + \alpha_2 (X_i - c)\\ 
  +&\, \alpha_3 (X_i - c) \times D_i + e_i,\\
  \\
  (\mathrm{II})\qquad Y_i =&\, \gamma_0 + \gamma_1 \widehat{B}_i\\
  +&\, \gamma_2 (X_i - c) + \gamma_3 (X_i-c)\times\widehat{B}_i, \epsilon_i
  \end{split},\label{eq:FRDD_lintIV}
\end{align} d.h. wir instrumentieren \(B_i\) mit \(D_i\) und dem
Interaktionsterm \((X_i-c)\times D_i\).

Wie im SRDD werden die IV-Ansätze für das FRDD \eqref{eq:FRDD_simpleIV}
und \eqref{eq:FRDD_lintIV} in empirischen Studien unter Berücksichtigung
einer Bandweite (i.d.R. dieselbe Bandweite für beide Stufen) angewendet.

\hypertarget{case-study-protestantische-arbeitsethik}{%
\section{Case Study: Protestantische
Arbeitsethik}\label{case-study-protestantische-arbeitsethik}}

Die Studie \emph{Beyond Work Ethic: Religion, Individual, and Political
Preferences} (Basten und Betz 2013) untersucht den Zusammenhang zwischen
Religion, individuellen Merkmalen und politischen Präferenzen. Das
Hauptaugenmerk ist die Rolle von Religiosität als Einflussfaktor auf
politische Einstellungen. Die Hypothese der Autoren ist, dass
Religiosität eines Individuums über den traditionellen Rahmen von
Moralvorstellungen und sozialen Normen hinaus auch die politischen
Präferenzen beeinflusst. Eine entsprechende Theorie wurde zu Beginn des
20. Jahrhunderts entwickelt und prominent von Max Weber (vgl. Weber
2004) vertreten. Weber argumentiert, dass die protestantische
Arbeitsethik einen entscheidenden Einfluss auf die Entwicklung des
Kapitalismus hatte. Laut Weber führte der protestantische Glaube an
harte Arbeit, ein sparsames Leben und ethisches Verhalten zur einer in
den damaligen Gesellschaften weit verbreiteten Geisteshaltung, die
wirtschaftliches Wachstum förderte und den Aufstieg des Kapitalismus
begünstigte.

Basten und Betz (2013) nutzen Wahlergebnisse sowie geo- und
soziodemographische Datensätze für schweizer Gemeinden, um den
Zusammenhang zwischen Religiosität und politischen Präferenzen wie
links-rechts-Ausrichtung, Einstellungen zur Umverteilung und
Einwanderung zu untersuchen. Hierfür verwenden die Autoren ein FRDD,
dass eine historisch bedingte Diskontinuität der geographischen
Verteilung von evanglischer bzw. katholischer Religionszugehörigkeit
zwischen den Kantonen Freiburg (überwiegend dunkelrote Region, frz.
\emph{Fribourg}) und Waadt (kleinere hellrote Region, frz. \emph{Vaud})
ausnutzt. Die historische Verteilung der Konfessionen in der
betrachteten Region im 16. Jahrhundert durch Abspaltung des Kantons
Freiburg ist in Abbildung~\ref{fig-vaudfb} dargestellt.

Aufgrund von Bevölkerungsbewegungen ist die Verteilung der Konfessionen
zwar nicht mehr eindeutig durch die Kantonsgrenze bestimmt, jedoch sind
die Gemeinden der betrachteten Kantone auch heute noch mehrheitlich
protestantisch bzw. katholisch. Es ist plausibel, dass eine Prägung
gemäß Webers Theorie vorliegt, sich die Gemeinden nahe der Grenz aber
hinsichtlich anderer Charakteristika (insb. der Bevölkerungsstruktur)
nicht systematisch unterscheiden. Somit liegt ein quai

\begin{figure}[t]

\sidecaption{\label{fig-vaudfb}Historische Verteilung von
Religionszugehörigkeit in Schweizer Gemeinden im 16. Jahrhundert.
Quelle: Basten und Betz (2013).}

{\centering \includegraphics[width=4.16667in,height=\textheight]{img/WaadtFribourg.png}

}

\end{figure}

Die Ergebnisse der Studie zeigen einen signifikanten Einfluss von
Protestantismus auf politische Präferenzen, die über traditionelle
Moralvorstellungen hinausgehen: Die Autoren finden Hinweise, dass
Einwohner evangelisch geprägter Gemeinden eher konservative soziale und
politische Ansichten vertreten. Eine mögliche Erklärung für diesen
Effekt ist, dass religiöse Institutionen auch eine soziale und
politische Agenda verfolgen, die von den Gläubigen internalisiert wird.

\hypertarget{aufbereitung-der-daten}{%
\subsection{Aufbereitung der Daten}\label{aufbereitung-der-daten}}

In diesem Kapitel zeigen wir, wie die Kernergebnisse der Studie mit R
reproduziert werden können. Hierfür werden folgende Pakete benötigt.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(haven)}
\FunctionTok{library}\NormalTok{(vtable)}
\FunctionTok{library}\NormalTok{(rdrobust)}
\end{Highlighting}
\end{Shaded}

Das Papier sowie der Datensatz \texttt{BastenBetz.dta} sind auf der
\href{https://www.aeaweb.org/articles?id=10.1257/pol.5.3.67}{Übersichtsseite
der AEA} verfügbar und liegt im STATA-Format \texttt{.dta}
vor.\footnote{Siehe alternativ das
  \href{https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2133848}{working
  paper}, falls kein Abbonement für AEA-Journals vorliegt.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Datensatz einlesen}
\NormalTok{BastenBetz }\OtherTok{\textless{}{-}} \FunctionTok{read\_dta}\NormalTok{(}\StringTok{\textquotesingle{}BastenBetz.dta\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Der Datensatz \texttt{BastenBetz} enthält Beobachtungen zu 509 schweizer
Gemeinden. Eine Vielzahl an Variablen ist lediglich für
Robustheits-Checks relevant. Für die Reproduktion der Kernergebnisse
erstellen wir zunächst einen reduzierten Datensatz und transformieren
einige Variablen.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Reduzierten Datensatz erstellen}
\NormalTok{BastenBetz }\OtherTok{\textless{}{-}}\NormalTok{ BastenBetz }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{transmute}\NormalTok{(}
    \AttributeTok{gini =}\NormalTok{ Ecoplan\_gini,}
    \AttributeTok{prot =}\NormalTok{ prot1980s,}
    \AttributeTok{bord =}\NormalTok{ borderdis, }
\NormalTok{    vaud,}
\NormalTok{    pfl, }
\NormalTok{    pfr, }
\NormalTok{    pfi}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Die Definitionen der Variablen sind in Tabelle~\ref{tbl-BastenBetzRed}
gegeben. Die Präferenzen \texttt{pfl}, \texttt{pfr} und \texttt{pfi}
basieren auf Wahlergebnissen auf Gemeindeebene zu Volksentscheiden.

\hypertarget{tbl-BastenBetzRed}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1806}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.8194}}@{}}
\caption{\label{tbl-BastenBetzRed}\texttt{BastenBetz} -- Variablen und
Definitionen}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{prot} & Anteil Prothestanten im Jahr 1980 (\%) \\
\texttt{gini} & Gini-Koeffizient \\
\texttt{bord} & Laufdistanz zur Kantonsgrenze (Km) \\
\texttt{vaud} & Dummyvariable: Gemeine im Kanton Waadt \\
\texttt{pfl} & Präferenz für Freizeit (\%) \\
\texttt{pfr} & Präferenz für Umverteilung (\%) \\
\texttt{pfi} & Präferenz für wirtschaftliche Intervention des Staats
(\%) \\
\end{longtable}

Für die Berechnung der optimalen Bandweite des FRDD verwenden wir einen
MSE-optimalen Schätzer, der in der Funktion
\texttt{rdrobust::rdbwselect()} implementiert ist.\footnote{Basten und
  Betz (2013) setzen BW = 5.01, den Durchschnitt von IK-Schätzungen über
  Modelle sämtlicher betrachteter Outcome-Variablen. Diese Bandweite
  liegt nahe des Ergebnisses von \texttt{rdbwselect}. Wir verwenden
  nachfolgend die Schätzung \texttt{OB}.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Bandweite schätzen (Bsp. für Freizeitpräferenz)}
\NormalTok{bw\_selection }\OtherTok{\textless{}{-}} \FunctionTok{rdbwselect}\NormalTok{(}
  \AttributeTok{y =}\NormalTok{ BastenBetz}\SpecialCharTok{$}\NormalTok{pfl,}
  \AttributeTok{x =}\NormalTok{ BastenBetz}\SpecialCharTok{$}\NormalTok{bord,}
  \AttributeTok{fuzzy =}\NormalTok{ BastenBetz}\SpecialCharTok{$}\NormalTok{prot, }
  \AttributeTok{bwselect =} \StringTok{"mserd"}\NormalTok{, }
  \AttributeTok{kernel =} \StringTok{"uniform"}
\NormalTok{) }

\CommentTok{\# Bandweite auslesen und zuweisen}
\NormalTok{(OB }\OtherTok{\textless{}{-}}\NormalTok{ bw\_selection}\SpecialCharTok{$}\NormalTok{bws[}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 5.078001
\end{verbatim}

\hypertarget{deskriptive-statistiken}{%
\subsection{Deskriptive Statistiken}\label{deskriptive-statistiken}}

Zur Reproduktion von Tabelle 1 aus Basten und Betz (2013) erzeugen wir
eine nach Kantonen gruppierte Zusammenfassung der Daten und berechnen
deskriptive Statistiken. Wie im Paper berücksichtigen wir hierbei nur
Gemeinden innerhalb der geschätzten optimalen Bandweite \texttt{OB}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Datensatz für Reproduktion von Table 1 formatieren}
\NormalTok{T1 }\OtherTok{\textless{}{-}}\NormalTok{ BastenBetz }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{abs}\NormalTok{(bord) }\SpecialCharTok{\textless{}}\NormalTok{ OB) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{vaud =} \FunctionTok{ifelse}\NormalTok{(}
      \AttributeTok{test =}\NormalTok{ vaud }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }
      \AttributeTok{yes =} \StringTok{"Waadt"}\NormalTok{, }
      \AttributeTok{no =} \StringTok{"Freiburg"}
\NormalTok{    ),}
    \AttributeTok{prot =}\NormalTok{ prot }\SpecialCharTok{*} \DecValTok{100}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(vaud) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}
    \FunctionTok{across}\NormalTok{(}
      \FunctionTok{everything}\NormalTok{(), }
      \FunctionTok{list}\NormalTok{(}
        \AttributeTok{Mean =}\NormalTok{ mean, }
        \AttributeTok{SD =}\NormalTok{ sd, }
        \AttributeTok{N =}\NormalTok{ length}
\NormalTok{      )}
\NormalTok{    )}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}
    \AttributeTok{cols =} \SpecialCharTok{{-}}\NormalTok{vaud,}
    \AttributeTok{names\_to =} \FunctionTok{c}\NormalTok{(}\StringTok{"variable"}\NormalTok{, }\StringTok{"statistic"}\NormalTok{), }
    \AttributeTok{names\_sep =} \StringTok{"\_"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Für die tabellarische Darstellung transformieren wir in ein weites
Format, sodass die Tabelle die deskriptive Statistiken spaltenweise für
die Kantone zeigt.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Daten in weites Format überführen}
\NormalTok{T1\_wider }\OtherTok{\textless{}{-}}\NormalTok{ T1 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}
    \AttributeTok{names\_from =} \FunctionTok{c}\NormalTok{(}\StringTok{"vaud"}\NormalTok{, }\StringTok{"statistic"}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Die Tabelle erzeugen wir mit \texttt{gt::gt()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Tabelle mit gt() erzeugen}
\NormalTok{T1\_wider }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{gt}\NormalTok{(}\AttributeTok{rowname\_col =} \StringTok{"Variable"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{tab\_spanner\_delim}\NormalTok{(}
    \AttributeTok{delim =} \StringTok{"\_"}\NormalTok{,}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{ tabopts}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-sumstat}{}
\begin{longtable}{lrrrrrr}
\caption{\label{tbl-sumstat}Datensatz \texttt{BastenBetz} -- Zusammenfassende Statistiken }\tabularnewline

\toprule
 & \multicolumn{3}{c}{Freiburg} & \multicolumn{3}{c}{Waadt} \\ 
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
variable & Mean & SD & N & Mean & SD & N \\ 
\midrule\addlinespace[2.5pt]
gini & $0.302$ & $0.029$ & $49$ & $0.367$ & $0.052$ & $84$ \\ 
prot & $9.428$ & $5.695$ & $49$ & $83.245$ & $11.411$ & $84$ \\ 
bord & $-2.327$ & $1.274$ & $49$ & $2.493$ & $1.201$ & $84$ \\ 
pfl & $48.239$ & $4.774$ & $49$ & $39.508$ & $5.723$ & $84$ \\ 
pfr & $43.049$ & $2.634$ & $49$ & $39.19$ & $5.025$ & $84$ \\ 
pfi & $52.642$ & $2.94$ & $49$ & $47.086$ & $3.368$ & $84$ \\ 
\bottomrule
\end{longtable}

Die Statistiken in Tabelle~\ref{tbl-sumstat} scheinen konsistent mit der
(historischen) Verteilung der Religionszugehörigkeit und politischen
Einstellung gemäß der Hypothese: Im überwiegend katholischen Freiburg
finden wir eine größere Einkommensungleichkeit und höhere aus
Wahlergebnissen abgeleitete Präferenzen für Freizeit, Umverteilung sowie
staatliche Interventionen.

\hypertarget{modellspezifikation-und-first-stage-ergebnisse}{%
\subsection{Modellspezifikation und
First-Stage-Ergebnisse}\label{modellspezifikation-und-first-stage-ergebnisse}}

Die Kantone Waadt und Freiburg haben bis heute mehrheitlich
protestantische bzw. katholische Gemeinden. Die Verteilung von
Protestantismus ist also, u.a. aufgrund von Bevölkerungsbewegungen,
nicht mehr deterministisch. An der Kantonsgrenze besteht jedoch eine
deutliche Diskontinuität im Anteil protestantischer Einwohner, die auf
die historische Verteilung der Religionszugehörigkeit zurückzuführen
ist. Damit kann ein FRDD implementiert werden, bei dem die Distanz zur
Grenze (\texttt{bord}) die zentrierte Laufvariable ist und die
Zugehörigkeit zum Kanton Waadt (\texttt{vaud}) ein Instrument für die
Behandlungsvariable (\texttt{prot}) ist.

Wir nutzen die Funktion \texttt{rdrobust::rdplot} um diesen Zusammenhang
für verschiedene Bandweiten anhand des linearen Interaktionsmodells
\begin{align}
  \begin{split}
  prot_i =&\, \alpha_0 + \alpha_1 vaud_i + \alpha_2 bord_i \\
  +&\, \alpha_3 bord_i \times vaud_i + u_i
  \end{split}\label{eq:BBFSR}
\end{align} grafisch darzustellen. Dies ist die First-Stage-Regression
für die 2SLS-Schätzung der Behandlungseffekte.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Reproduktion von Abbildung 3 in Basten und Betz (2013)}
\NormalTok{plots\_BB }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \CommentTok{\# gesch. optimale Bandweite}
  \AttributeTok{p\_OB =} \FunctionTok{rdplot}\NormalTok{(}
    \AttributeTok{y =}\NormalTok{ BastenBetz}\SpecialCharTok{$}\NormalTok{prot, }
    \AttributeTok{x =}\NormalTok{ BastenBetz}\SpecialCharTok{$}\NormalTok{bord, }
    \AttributeTok{h =} \FunctionTok{c}\NormalTok{(OB, OB), }
    \AttributeTok{x.label =} \StringTok{"Distanz zur Grenze (bord)"}\NormalTok{,}
    \AttributeTok{y.label =} \StringTok{"Anteil Protestanten (prot)"}\NormalTok{, }
    \AttributeTok{title =} \StringTok{"Gesch. Bandweite"}\NormalTok{,}
    \AttributeTok{p =} \DecValTok{1}\NormalTok{, }
    \AttributeTok{nbins =} \FunctionTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{14}\NormalTok{), }
    \AttributeTok{masspoints =} \StringTok{"off"}
\NormalTok{  ),}
  
  \CommentTok{\# Bandweite 10}
  \AttributeTok{p\_BW10 =} \FunctionTok{rdplot}\NormalTok{(}
    \AttributeTok{y =}\NormalTok{ BastenBetz}\SpecialCharTok{$}\NormalTok{prot, }
    \AttributeTok{x =}\NormalTok{ BastenBetz}\SpecialCharTok{$}\NormalTok{bord, }
    \AttributeTok{h =} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{), }
    \AttributeTok{x.label =} \StringTok{"Distanz zur Grenze (bord)"}\NormalTok{,}
    \AttributeTok{y.label =} \StringTok{"Anteil Protestanten  (prot)"}\NormalTok{, }
    \AttributeTok{title =} \StringTok{"Bandweite = 10"}\NormalTok{,}
    \AttributeTok{p =} \DecValTok{1}\NormalTok{, }
    \AttributeTok{nbins =} \FunctionTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{14}\NormalTok{),}
    \AttributeTok{masspoints =} \StringTok{"off"}
\NormalTok{  ),}
  
  \CommentTok{\# Bandweite 20}
  \AttributeTok{p\_BW20 =} \FunctionTok{rdplot}\NormalTok{(}
    \AttributeTok{y =}\NormalTok{ BastenBetz}\SpecialCharTok{$}\NormalTok{prot, }
    \AttributeTok{x =}\NormalTok{ BastenBetz}\SpecialCharTok{$}\NormalTok{bord, }
    \AttributeTok{h =} \FunctionTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{20}\NormalTok{), }
    \AttributeTok{x.label =} \StringTok{"Distanz zur Grenze (bord)"}\NormalTok{,}
    \AttributeTok{y.label =} \StringTok{"Anteil Protestanten  (prot)"}\NormalTok{, }
    \AttributeTok{title =} \StringTok{"Bandweite = 20"}\NormalTok{,}
    \AttributeTok{p =} \DecValTok{1}\NormalTok{, }
    \AttributeTok{nbins =} \FunctionTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{14}\NormalTok{),}
    \AttributeTok{masspoints =} \StringTok{"off"}
\NormalTok{  ),}
  
  \CommentTok{\# Gesamter Datensatz}
  \AttributeTok{p\_G =} \FunctionTok{rdplot}\NormalTok{(}
    \AttributeTok{y =}\NormalTok{ BastenBetz}\SpecialCharTok{$}\NormalTok{prot, }
    \AttributeTok{x =}\NormalTok{ BastenBetz}\SpecialCharTok{$}\NormalTok{bord,}
    \AttributeTok{x.label =} \StringTok{"Distanz zur Grenze (bord)"}\NormalTok{,}
    \AttributeTok{y.label =} \StringTok{"Anteil Protestanten"}\NormalTok{, }
    \AttributeTok{title =} \StringTok{"Ges. Datensatz"}\NormalTok{,}
    \AttributeTok{p =} \DecValTok{1}\NormalTok{, }
    \AttributeTok{nbins =} \FunctionTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{14}\NormalTok{),}
    \AttributeTok{masspoints =} \StringTok{"off"}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Wir sammeln die Ergebnisse in einem Plot-Gitter mit
\texttt{cowplot::plot\_grid()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Reproduktion von Abbildung 3 in Basten und Betz (2013)}
\FunctionTok{plot\_grid}\NormalTok{(}
  \AttributeTok{plotlist =} \FunctionTok{map}\NormalTok{(plots\_BB, }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .}\SpecialCharTok{$}\NormalTok{rdplot), }\AttributeTok{ncol =} \DecValTok{2}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{RDD_files/figure-pdf/fig-BastenBetzFS-1.pdf}

}

\caption{\label{fig-BastenBetzFS}First-Stage-Regressionen}

\end{figure}

Die Grafiken in Abbildung~\ref{fig-BastenBetzFS} zeigen deutliche
Hinweise auf die Diskontinuität in \texttt{prot} nahe der Kantonsgrenze.
Die Größe des geschätzten Sprungs scheint nur wenig sensitiv gegenüber
der gewählten Bandweite zu sein. Die Signifikanz des Effekts können wir
anhand der jeweiligen KQ-Regressionen beurteilen.\footnote{Wir nutzen
  \texttt{update()} um die Regression mit weniger Code für verschiedene
  Bandweiten zu schätzen.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Reproduktion der First{-}Stage{-}Regressionen}
\CommentTok{\# s. Tabelle 2 in Basten und Betz (2013)}

\CommentTok{\# (1) BW = OB}
\NormalTok{FS1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ prot }\SpecialCharTok{\textasciitilde{}}\NormalTok{ vaud }\SpecialCharTok{+}\NormalTok{ bord }\SpecialCharTok{+}\NormalTok{ vaud }\SpecialCharTok{*}\NormalTok{ bord, }
  \AttributeTok{data =}\NormalTok{ BastenBetz }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{filter}\NormalTok{(}
      \FunctionTok{abs}\NormalTok{(bord) }\SpecialCharTok{\textless{}=}\NormalTok{ OB}
\NormalTok{    )}
\NormalTok{)}

\CommentTok{\# (2) BW = 10}
\NormalTok{FS2 }\OtherTok{\textless{}{-}} \FunctionTok{update}\NormalTok{(}
\NormalTok{  FS1,}
  \AttributeTok{data =}\NormalTok{ BastenBetz }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{filter}\NormalTok{(}
      \FunctionTok{abs}\NormalTok{(bord) }\SpecialCharTok{\textless{}=} \DecValTok{10}
\NormalTok{    )}
\NormalTok{)}

\CommentTok{\# (3) BW = 20}
\NormalTok{FS3 }\OtherTok{\textless{}{-}} \FunctionTok{update}\NormalTok{(}
\NormalTok{  FS1,}
  \AttributeTok{data =}\NormalTok{ BastenBetz }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{filter}\NormalTok{(}
      \FunctionTok{abs}\NormalTok{(bord) }\SpecialCharTok{\textless{}=} \DecValTok{20}
\NormalTok{    )}
\NormalTok{)}

\CommentTok{\# (4) Ges. Datensatz}
\NormalTok{FS4 }\OtherTok{\textless{}{-}} \FunctionTok{update}\NormalTok{(}
  \AttributeTok{object =}\NormalTok{ FS1,}
  \AttributeTok{data =}\NormalTok{ BastenBetz}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Tabellarische Darstellung}
\FunctionTok{modelsummary}\NormalTok{(}
  \FunctionTok{list}\NormalTok{(}
    \StringTok{"BW = OB"}\OtherTok{=}\NormalTok{ FS1, }
    \StringTok{"BW = 10"} \OtherTok{=}\NormalTok{ FS2, }
    \StringTok{"BW=20"} \OtherTok{=}\NormalTok{ FS3, }
    \StringTok{"Ges. Datensatz"} \OtherTok{=}\NormalTok{ FS4}
\NormalTok{  ), }
  \AttributeTok{vcov =} \StringTok{"HC1"}\NormalTok{, }
  \AttributeTok{stars =}\NormalTok{ T, }
  \AttributeTok{gof\_map =} \StringTok{"nobs"}\NormalTok{, }
  \AttributeTok{output =} \StringTok{"gt"}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tabopts}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-BastenBetzFS}{}
{
\makeatletter
\def\LT@makecaption#1#2#3{%
  \noalign{\smash{\hbox{\kern\textwidth\rlap{\kern\marginparsep
  \parbox[t]{\marginparwidth}{%
    \footnotesize{%
      \vspace{(1.1\baselineskip)}
    #1{#2: }\ignorespaces #3}}}}}}%
    }
\makeatother

\setlength{\LTpost}{0mm}
\begin{longtable}{lcccc}
\caption{\label{tbl-BastenBetzFS}First-Stage Regressionen }\tabularnewline

\toprule
  & BW = OB & BW = 10 & BW=20 & Ges. Datensatz \\ 
\midrule\addlinespace[2.5pt]
(Intercept) & 0.134*** & 0.100*** & 0.103*** & 0.109*** \\ 
 & (0.017) & (0.013) & (0.010) & (0.009) \\ 
vaud & 0.671*** & 0.726*** & 0.756*** & 0.710*** \\ 
 & (0.034) & (0.022) & (0.018) & (0.014) \\ 
bord & 0.017** & 0.001 & 0.001 & 0.002* \\ 
 & (0.006) & (0.003) & (0.001) & (0.001) \\ 
vaud × bord & -0.006 & -0.001 & -0.009*** & -0.004*** \\ 
 & (0.012) & (0.005) & (0.003) & (0.001) \\ 
Num.Obs. & 133 & 207 & 312 & 509 \\ 
\bottomrule
\end{longtable}
\begin{minipage}{\linewidth}
+ p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\\
\end{minipage}

}

Für die geschätze Bandweite schätzen wir einen hochsignifikanten Sprung
in \texttt{prot} von etwa 67\% an der Kantonsgrenze. Auch für größere
Bandweiten von 10km und 20km sowie für den gesamten Datensatz finden wir
vergleichbare signifikante Effekte, was eine bei zunehmender Distanz zur
Grenze persistente Diskrepanz der Religionszugehörigkeit bestätigt.

\hypertarget{second-stage-ergebnisse}{%
\subsection{Second-Stage-Ergebnisse}\label{second-stage-ergebnisse}}

Wir schätzen nun den LATE von Protestantismus für die Outcome-Variablen
\texttt{gini}, \texttt{pfl}, \texttt{pfi} und \texttt{pfr}, vgl.
Tabelle~\ref{tbl-BastenBetzRed}. Die Spezifikation für die
Second-Stage-Regression der FRDD-Schätzung ist \begin{align}
  \begin{split}
    Y_i = \gamma_0 + \gamma_1 \widehat{prot}_i +  \gamma_2 bord_i + \gamma_3 bord_i  \times vaud_i + e_i
  \end{split},
\end{align} wobei \(\widehat{prot}_i\) angepasste Werte aus der
KQ-Schätzung von \eqref{eq:BBFSR} mit Bandweite \texttt{OB} sind. Dazu
erzeugen wir zunächst eine angepasste Version des Objekts
\texttt{BastenBetz}, welche nur Gemeinden innerhalb der Bandweite
enthält.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Gemeinden innerhalb der Bandweite filtern}
\NormalTok{BastenBetz\_OB }\OtherTok{\textless{}{-}}\NormalTok{ BastenBetz }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}
    \FunctionTok{abs}\NormalTok{(bord) }\SpecialCharTok{\textless{}=}\NormalTok{ OB}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Zur Illustration schätzen wir nun die Second-Stage-Regression für
\(Y = pfl\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Second{-}Stage{-}Regression für \textasciigrave{}pfl\textasciigrave{}}
\NormalTok{BastenBetz\_OB }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{prot\_fitted =} \FunctionTok{fitted}\NormalTok{(FS1)}
\NormalTok{    ) }\SpecialCharTok{\%\textgreater{}\%}

\FunctionTok{lm}\NormalTok{(}
\NormalTok{  pfl }\SpecialCharTok{\textasciitilde{}}\NormalTok{ prot\_fitted }\SpecialCharTok{+}\NormalTok{ bord }\SpecialCharTok{+}\NormalTok{ vaud}\SpecialCharTok{:}\NormalTok{bord, }
  \AttributeTok{data =}\NormalTok{ .}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = pfl ~ prot_fitted + bord + vaud:bord, data = .)

Residuals:
     Min       1Q   Median       3Q      Max 
-12.8870  -3.8621  -0.0423   3.4993  12.1636 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  50.5275     1.9721  25.621  < 2e-16 ***
prot_fitted -13.4600     3.1749  -4.240 4.24e-05 ***
bord          0.4380     0.6528   0.671    0.503    
bord:vaud    -0.3636     0.7939  -0.458    0.648    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.433 on 129 degrees of freedom
Multiple R-squared:  0.383, Adjusted R-squared:  0.3686 
F-statistic: 26.69 on 3 and 129 DF,  p-value: 1.704e-13
\end{verbatim}

Der Koeffizient \texttt{prot\_fitted} ist der gesuchte
Behandlungseffekt. Beachte, dass die von \texttt{summary()} berechneten
Standardfehler ungültig sind, weil diese die zusätzliche Unsicherheit
durch die Berechnung von \(\widehat{prot}\) über die
First-Stage-Regression nicht berücksichtigen. Nachfolgend nutzen wir
\texttt{AER::ivreg()}, um komfortabel gültige (heteroskedastie-robuste)
Inferenz betreiben zu können.\footnote{Die Autoren geben an, robuste SEs
  zu nutzen. Das scheint nicht der Fall zu sein, denn
  \texttt{vcov\ =\ "HC0"} liefert die Ergebinsse im Paper. Die von Stata
  berechneten HC1-SEs weichen ab. Dies ändert allerdings nichts an der
  Signifikanz der Koeffizienten. Wir nutzen \texttt{vcov\ =\ "HC1"}.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Schätzung mit 2SlS}
\CommentTok{\# s. Tabelle 4 in Basten und Betz (2013)}
\CommentTok{\#}
\CommentTok{\# Wir instrumentieren Treatment (\textasciigrave{}prot1980s\textasciigrave{}) mit dem Schwellenindikator (\textasciigrave{}vaud\textasciigrave{})}
\CommentTok{\# ivreg: exogene Variablen instrumentieren sich selbst, daher}
\CommentTok{\# \textquotesingle{} | vaud * borderdis \textquotesingle{}}
\FunctionTok{library}\NormalTok{(AER)}
\CommentTok{\# (1) Präferenz für Freizeit}
\NormalTok{SS\_pfl }\OtherTok{\textless{}{-}} \FunctionTok{ivreg}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ pfl }\SpecialCharTok{\textasciitilde{}}\NormalTok{ prot }\SpecialCharTok{+}\NormalTok{ bord}\SpecialCharTok{:}\NormalTok{vaud }\SpecialCharTok{+}\NormalTok{ bord }\SpecialCharTok{|}\NormalTok{ vaud }\SpecialCharTok{*}\NormalTok{ bord,}
  \AttributeTok{data =}\NormalTok{ BastenBetz\_OB}
\NormalTok{)}

\CommentTok{\# (2) Präferenz für Umverteilung}
\NormalTok{SS\_pfr }\OtherTok{\textless{}{-}} \FunctionTok{update}\NormalTok{(}
  \AttributeTok{object =}\NormalTok{ SS\_pfl,}
  \AttributeTok{formula =}\NormalTok{ pfr }\SpecialCharTok{\textasciitilde{}}\NormalTok{ prot }\SpecialCharTok{+}\NormalTok{ bord}\SpecialCharTok{:}\NormalTok{vaud }\SpecialCharTok{+}\NormalTok{ bord }\SpecialCharTok{|}\NormalTok{ vaud }\SpecialCharTok{*}\NormalTok{ bord,}
\NormalTok{)}

\CommentTok{\# (3) Präferenz für Intervention}
\NormalTok{SS\_pfi }\OtherTok{\textless{}{-}} \FunctionTok{update}\NormalTok{(}
  \AttributeTok{object =}\NormalTok{ SS\_pfl,}
  \AttributeTok{formula =}\NormalTok{ pfi }\SpecialCharTok{\textasciitilde{}}\NormalTok{ prot }\SpecialCharTok{+}\NormalTok{ bord}\SpecialCharTok{:}\NormalTok{vaud }\SpecialCharTok{+}\NormalTok{ bord }\SpecialCharTok{|}\NormalTok{ vaud }\SpecialCharTok{*}\NormalTok{ bord,}
\NormalTok{)}

\CommentTok{\# (4) Einkommensungleichheit}
\NormalTok{SS\_gini }\OtherTok{\textless{}{-}} \FunctionTok{update}\NormalTok{(}
  \AttributeTok{object =}\NormalTok{ SS\_pfl,}
  \AttributeTok{formula =}\NormalTok{ pfi }\SpecialCharTok{\textasciitilde{}}\NormalTok{ prot }\SpecialCharTok{+}\NormalTok{ bord}\SpecialCharTok{:}\NormalTok{vaud }\SpecialCharTok{+}\NormalTok{ bord }\SpecialCharTok{|}\NormalTok{ vaud }\SpecialCharTok{*}\NormalTok{ bord,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Tabellarische Darstellung}
\FunctionTok{modelsummary}\NormalTok{(}
  \FunctionTok{list}\NormalTok{(}
    \StringTok{"(1) Freizeit"}\OtherTok{=}\NormalTok{ SS\_pfl, }
    \StringTok{"(2) Umverteilung"} \OtherTok{=}\NormalTok{ SS\_pfr, }
    \StringTok{"(3) Intervention"} \OtherTok{=}\NormalTok{ SS\_pfi, }
    \StringTok{"(4) Ungleichheit"} \OtherTok{=}\NormalTok{ SS\_gini}
\NormalTok{  ), }
  \AttributeTok{vcov =} \StringTok{"HC1"}\NormalTok{, }
  \AttributeTok{stars =}\NormalTok{ T, }
  \AttributeTok{gof\_map =} \StringTok{"nobs"}\NormalTok{, }
  \AttributeTok{output =} \StringTok{"gt"}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tabopts}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-BastenBetzSS}{}
{
\makeatletter
\def\LT@makecaption#1#2#3{%
  \noalign{\smash{\hbox{\kern\textwidth\rlap{\kern\marginparsep
  \parbox[t]{\marginparwidth}{%
    \footnotesize{%
      \vspace{(1.1\baselineskip)}
    #1{#2: }\ignorespaces #3}}}}}}%
    }
\makeatother

\setlength{\LTpost}{0mm}
\begin{longtable}{lcccc}
\caption{\label{tbl-BastenBetzSS}Ergebnisse der Second-Stage-Regressionen }\tabularnewline

\toprule
  & (1) Freizeit & (2) Umverteilung & (3) Intervention & (4) Ungleichheit \\ 
\midrule\addlinespace[2.5pt]
(Intercept) & 50.528*** & 44.560*** & 52.871*** & 52.871*** \\ 
 & (1.918) & (0.950) & (1.063) & (1.063) \\ 
prot & -13.460*** & -5.061* & -6.487*** & -6.487*** \\ 
 & (3.161) & (2.161) & (1.738) & (1.738) \\ 
bord & 0.438 & 0.444 & -0.165 & -0.165 \\ 
 & (0.639) & (0.357) & (0.332) & (0.332) \\ 
bord × vaud & -0.364 & -0.909 & 0.011 & 0.011 \\ 
 & (0.811) & (0.561) & (0.432) & (0.432) \\ 
Num.Obs. & 133 & 133 & 133 & 133 \\ 
\bottomrule
\end{longtable}
\begin{minipage}{\linewidth}
+ p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\\
\end{minipage}

}

Die Koeffizienten von \texttt{prot} in Tabelle~\ref{tbl-BastenBetzSS}
sind die mit 2SLS ermittelten erwarteten Behandlungseffekte einer
100\%-Reformation (d.h. von 100\% katholisch zu 100\% protestantisch)
für eine durchschnittliche Gemeine nahe der Kantonsgrnze. Es handelt
sich jeweils um einen lokalen durchschnittlichen Behandlungseffekt
(LATE). Gem. der Definition der abhängigen Variablen, interpretieren wir
die Koeffizienten von \texttt{prot} in de Regressionen (1), (2) und (3)
als erwartete Prozentänderung durch Reformation. Der Koeffizient in
Regression (4) gibt die erwartete Änderung des Gini-Index an. Sämtliche
geschätzte Effekte sind signifikant und haben ein mit der Hypothese der
Autoren konsistentes negatives Vorzeichen.

Die Ergebnisse sind Evidenz, dass Protestantismus zu verringerter
Präferenz für Freizeit, Umverteilung sowie wirtschaftspolitische
Intervention seitens des Staats führt. Auch die ökonomische Ungleichheit
ist signifikant geringer, als in einer durchschnittlichen vollständig
katholischen Gemeinde.

\hypertarget{addendum-frdd-schuxe4tzung-mit-rdrobust}{%
\subsection{\texorpdfstring{Addendum: FRDD-Schätzung mit
\texttt{rdrobust()}}{Addendum: FRDD-Schätzung mit rdrobust()}}\label{addendum-frdd-schuxe4tzung-mit-rdrobust}}

Die Funktion \texttt{rdrobust::rdrobust()} erlaubt die Schätzung von
SRDD und FRDD mit einer Vielzahl von Optionen, s. \texttt{?rdrobust}.
Dies erleichtert die Schätzung mehrerer Modellspezifikationenen und
Bandweiten. Mit dem nachstehenden Befehl schätzen wir den LATE von
Reformation auf die Präferenz für Umverteilung anhand lokaler
quadratischer Regression. Der Output gibt einen Überblick der
Bandweitenschätzung sowie der 2 Stufen des 2SLS-Schätzers, inkl.
robuster Inferenzstatistiken.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pfr\_rdr }\OtherTok{\textless{}{-}} \FunctionTok{rdrobust}\NormalTok{(}
  \AttributeTok{y =}\NormalTok{ BastenBetz}\SpecialCharTok{$}\NormalTok{pfr,}
  \AttributeTok{x =}\NormalTok{ BastenBetz}\SpecialCharTok{$}\NormalTok{bord,}
  \AttributeTok{fuzzy =}\NormalTok{ BastenBetz}\SpecialCharTok{$}\NormalTok{prot, }
  \AttributeTok{p =} \DecValTok{2}\NormalTok{,}
  \AttributeTok{kernel =} \StringTok{"uniform"}\NormalTok{,}
  \AttributeTok{vce =} \StringTok{"HC1"}
\NormalTok{) }

\NormalTok{pfr\_rdr }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Fuzzy RD estimates using local polynomial regression.

Number of Obs.                  509
BW type                       mserd
Kernel                      Uniform
VCE method                      HC1

Number of Obs.                  127          382
Eff. Number of Obs.              85          131
Order est. (p)                    2            2
Order bias  (q)                   3            3
BW est. (h)                  10.796       10.796
BW bias (b)                  22.271       22.271
rho (h/b)                     0.485        0.485
Unique Obs.                      97          261

First-stage estimates.

=============================================================================
        Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       
=============================================================================
  Conventional     0.701     0.039    17.782     0.000     [0.624 , 0.778]     
        Robust         -         -    15.837     0.000     [0.599 , 0.768]     
=============================================================================

Treatment effect estimates.

=============================================================================
        Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       
=============================================================================
  Conventional    -5.047     2.254    -2.239     0.025    [-9.464 , -0.629]    
        Robust         -         -    -2.210     0.027   [-10.114 , -0.607]    
=============================================================================
\end{verbatim}

Auch für die quadratische Spezifikation erhalten wir mit -5.047 ein
vergleichbares signifikantes Ergebnis für den LATE von Protestantismus
auf Umverteilung, vgl. Spalte (2) in Tabelle~\ref{tbl-BastenBetzSS}.

Mit der Option \texttt{bwselect\ =\ "msetwo"} kann die Bandweite jeweils
für die lokale Regression links- und rechtssetig des Schwellenwerts
geschätzt werden.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pfr\_rdr }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{update}\NormalTok{(}\AttributeTok{bwselect =} \StringTok{"msetwo"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Fuzzy RD estimates using local polynomial regression.

Number of Obs.                  509
BW type                      msetwo
Kernel                      Uniform
VCE method                      HC1

Number of Obs.                  127          382
Eff. Number of Obs.              51          134
Order est. (p)                    2            2
Order bias  (q)                   3            3
BW est. (h)                   5.340       11.387
BW bias (b)                  13.917       22.330
rho (h/b)                     0.384        0.510
Unique Obs.                      97          261

First-stage estimates.

=============================================================================
        Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       
=============================================================================
  Conventional     0.649     0.046    14.216     0.000     [0.560 , 0.739]     
        Robust         -         -    11.970     0.000     [0.534 , 0.743]     
=============================================================================

Treatment effect estimates.

=============================================================================
        Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       
=============================================================================
  Conventional    -7.487     3.378    -2.216     0.027   [-14.109 , -0.866]    
        Robust         -         -    -2.156     0.031   [-14.750 , -0.704]    
=============================================================================
\end{verbatim}

Trotz Diskrepanz der geschätzten Bandweiten erhalten wir eine größere
aber vergleichbare Schätzung für einen negativen Effekt.

\bookmarksetup{startatroot}

\hypertarget{regularisierte-regression}{%
\chapter{Regularisierte Regression}\label{regularisierte-regression}}

In diesem Kapitel betrachten wir Varianten von Koeffizientenschätzern im
linearen Modell \begin{align}
 Y_i = \beta_1 X_{1,i} + \dots + \beta_k X_{k,i} + u_i, \quad i = 1,\dots,n,\label{eq:slm}
\end{align} deren Motivation die Schätzung von
\(\boldsymbol{\beta} := (\beta_1, \dots,\beta_k)'\) in Anwendungen ist,
in denen der KQ-Schätzer \begin{align}
  \begin{split}
  \widehat{\boldsymbol{\beta}} =&\, \arg\min_{\boldsymbol{\beta}}\mathrm{RSS}(\boldsymbol{\beta})\\ 
  =&\,  \arg\min_{\boldsymbol{\beta}}  \sum_{i=1}^n\left(Y_i-\beta_1 X_{1,i} + \dots + \beta_k X_{k,i}\right)^2
  \end{split}\label{eq:KQLoss}
\end{align} keine stabile Schätzung zulässt oder nicht eindeutig
definiert ist, und damit gar nicht erst berechnet werden kann. Solche
Szenarien ergeben sich in der empirischen Forschung, wenn die
Regressoren stark korreliert sind und/oder das Modell viele Regressoren
enthält (\(k\lesssim n\)), oder das Regressionsproblem hoch-dimensional
ist (\(k>n\)).

Regularisierte Regressionsschätzer begegnen dieser Problematik mit einer
Modifikation der Verlustfunktion \(\mathrm{RSS}\) in \eqref{eq:KQLoss},
\begin{align}
  \mathrm{RSS}(\boldsymbol{\beta}, p, \lambda) := \mathrm{RSS}(\boldsymbol{\beta}) + \lambda\lVert\boldsymbol{\beta}\rVert_p.
\end{align} Hierbei ist \(\lambda>0\) ein Tuning-Parameter und
\(p\geq1\) definiert die \(p\)-Norm des Koeffizientenvektors,
\begin{align}
  \lVert\boldsymbol{\beta}\rVert_p := \left(\sum_{j=1}^k \lvert\beta_j\rvert^{p}\right)^{1/p}>0.\label{eq:pnorm}
\end{align}

Wegen \(\lambda\lVert\boldsymbol{\beta}\rVert_p>0\) kann die \(p\)-Norm
des Koeffizientenvektors \(\boldsymbol{\beta}\) das Optimierungsproblem
\[\min_{\boldsymbol{\beta}} \mathrm{RSS}(\boldsymbol{\beta}, p, \lambda) \vert\, p,\, \lambda\]
derart restringieren, dass die geschätzten Koeffizienten \begin{align*}
  \widehat{\boldsymbol{\beta}}_{p,\,\lambda} := \arg\min_{\boldsymbol{\beta}} \mathrm{RSS}(\boldsymbol{\beta}, p, \lambda)
\end{align*} im Erwartungswert absolut kleiner ausfallen als bei der
KQ-Schätzung: Der Schätzer ist in Richtung 0 verzerrt.\footnote{Beachte,
  dass für \(\lambda=0\) die Verlustfunktion des KQ-Schätzers folgt.}
Dieser Effekt der Regularisierung wird in der Literatur als
\emph{Shrinkage} bezeichnet.

Die grundlegenden Eigenschaften des Schätzers
\(\widehat{\boldsymbol{\beta}}_{p,\,\lambda}\) werden maßgeblich durch
den Parameter \(p\) bestimmt, der hinsichtlich des zu lösenden
Regressionsproblems \emph{a priori} gewählt wird.\footnote{D.h. wir
  wählen \(p\), um einen Schätzer mit für die konkrete Anwendung
  hilfreichen Eigenschaften zu erhalten.}

Shrinkage ist eine Motivation für die Anwendung regularisierter Schätzer
in Modellen, die auch mit KQ geschätzt werden könnten. Um dies zu
verstehen, nehmen wir an, dass die Gauss-Markov-Annahmen in
\eqref{eq:slm} gelten. Dann hat der KQ-Schätzer die kleinste Varianz
unter allen \emph{unverzerrten} Schätzern. Aufgrund der Shrinkage fallen
regularisierte Schätzer zwar nicht unter das Gauss-Markov-Theorem,
können dafür aber eine geringere Varianz haben als KQ. Schätzer mit
solchen Eigenschaften sind nützlich, wenn eine unverzerrte Schätzung von
\(\boldsymbol{\beta}\) nicht unser primäres Ziel ist: Für Vorhersagen
kann es hilfreich sein, etwas Verzerrung bei der Koeffizientenschätzung
in Kauf zu nehmen, um eine hinreichend große Varianzreduktion zu
erreichen, sodass ein geringerer erwarteter Vorhersagefehler als für KQ
resultiert. Hierbei liegt, eine Abwägung zwischen Verzerrung und Varianz
(\emph{Bias Variance Tradeoff}) vor, der durch den
Regularisierungsparameter \(\lambda\) beeinflusst wird.

Für die Berechnung des Schätzers in empirischen Anwendungen wird
\(\lambda\) meist datengetrieben (mit
\href{https://de.wikipedia.org/wiki/Kreuzvalidierungsverfahren}{Cross
Validation} oder einem Informationskriterium) geschätzt oder mit einer
analytisch fundierten Faustregel gewählt.

Nachfolgend betrachten wir zwei häufig verwendete regularisierte
Schätzer, die sich durch die Wahl \(p=1\) (Lasso Regression) bzw.
\(p=2\) (Ridge Regression) ergeben und illustrieren ihre Anwendung mit
R.

\hypertarget{ridge-regression}{%
\section{Ridge Regression}\label{ridge-regression}}

Ridge Regression wurde von Hoerl und Kennard (1970) als Alternative zur
KQ-Schätzung bei hoch-korrelierten Regressoren eingeführt. Die
Verlustfunktion lautet \begin{align}
  \mathrm{RSS}(\boldsymbol{\beta},p=2,\lambda) = \mathrm{RSS}(\boldsymbol{\beta}) + \lambda \lVert\boldsymbol{\beta}\rVert_2,\label{eq:ridgeloss}
\end{align} d.h. der Parameter \(\lambda\) reguliert den Einfluss eines
\(\ell_2\)-Strafterms \begin{align*}
  \lVert\boldsymbol{\beta}\rVert_2 = \sqrt{\sum_{j=1}^k\beta_j^2}
\end{align*} auf die Verlustfunktion
\(\mathrm{RSS}(\boldsymbol{\beta},p=2,\lambda)\). Der Ridge-Schätzer
ergibt sich als \begin{align}
  \widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda := \arg\min_{\boldsymbol{\beta}}\mathrm{RSS}(\boldsymbol{\beta}) + \lambda \lVert\boldsymbol{\beta}\rVert_2.\label{eq:ridgereg}
\end{align}

Für Das Optimierungsproblem \eqref{eq:ridgereg} kann wir aus den
Bedingungen 1. Ordnung \begin{align}
  -2\boldsymbol{X}'(\boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\beta}) + 2\lambda\boldsymbol{\beta} = \boldsymbol{0}
\end{align} die analytische Lösung \begin{align}
  \widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda = (\boldsymbol{X}'\boldsymbol{X} + \lambda\boldsymbol{I}_p)^{-1}\boldsymbol{X}'\boldsymbol{Y},\label{eq:ridgecf}
\end{align} bestimmt werden, wobei \(\boldsymbol{I}_k\) die
\(k\times k\) Einheitsmatrix ist. Aus Gleichung \eqref{eq:ridgecf} kann
die Wirkungsweise des Strafterms
\(\lambda \lVert\boldsymbol{\beta}\rVert_2\) abgeleitet werden: Ridge
Regression modifiziert die Diagonale der zu invertierenden Matrix
\(\boldsymbol{X}'\boldsymbol{X}\) durch Addition von \(\lambda>0\). Dies
ist hilfreich, wenn

\begin{itemize}
\item
  \(k\geq n\) und damit \(\boldsymbol{X}'\boldsymbol{X}\) nicht
  invertiertbar (singulär) ist. Dann kann der KQ-Schätzer nicht
  berechnet werden.\footnote{Beispiel:
    \texttt{X\ \textless{}-\ matrix(rnorm(100),\ ncol\ =\ 10)}.
    Vergleiche \texttt{solve(t(X)\ \%*\%\ X)} und
    \texttt{solve(t(X)\ \%*\%\ X\ +\ diag(.01,\ nrow\ =\ 10))}} Die
  Inverse
  \((\boldsymbol{X}'\boldsymbol{X} + \lambda\boldsymbol{I}_p)^{-1}\)
  hingegen existiert unter milden Bedingungen.
\item
  hohe Kollinearität vorliegt, sodass
  \((\boldsymbol{X}'\boldsymbol{X})^{-1}\) zwar existiert, aber zu einer
  instablilen KQ-Schätzung mit hoher Varianz führt.
\end{itemize}

Für eine grafische Betrachtung des Optimierungskalküls
\eqref{eq:ridgereg} betrachten wir die äquivalente Darstellung als
Lagrange-Problem \begin{align}
  \widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda := \arg\min_{\lVert\boldsymbol{\beta}\rVert<t}\mathrm{RSS}(\boldsymbol{\beta}).\label{eq:ridgeLg}
\end{align} In der folgenden interaktiven Grafik illustrieren wir das
Optimierungsproblem \eqref{eq:ridgeLg} sowie den resultierenden Schätzer
der Koeffizienten \((\beta_1, \beta_2)\) in einem multiplen
Regressionsmodell mit den Regressoren \(X_1\) und \(X_2\).

\begin{itemize}
\item
  Die blaue Ellipse ist die Menge aller Schätzwerte
  \(\left(\widehat\beta_{1},\, \widehat\beta_{2}\right)\) für den
  angegebenen Wert von \(\mathrm{RSS}\). Im Zentrum der Ellipse liegt
  der KQ-Schätzer, welcher \(\mathrm{RSS}\) minimiert.
\item
  Der blaue Kreis ist die Menge aller Koeffizienten-Paare
  \((\beta_1, \beta_2)\), welche die Restriktion
  \(\beta_1^2 + \beta_2^2\leq t\) erfüllen. Beachte, dass die Größe des
  Kreises nur durch den Parameter \(t\) bestimmt wird, welcher für einen
  vorgegebenen Wertebereich variiert werden kann.
\item
  Der blaue Punkt ist der Ridge-Schätzer
  \((\widehat\beta^R_{1,t},\, \widehat\beta^R_{2,t})\). Dieser ergibt
  sich als Schnittpunkt zwischen der blauen \(\mathrm{RSS}\)-Ellipse und
  der Restriktionsregion und variiert mit \(t\). Die gestrichelte rote
  Kurve zeigt den Ridge-Lösungspfad.
\item
  Für kleine Werte \(t\) drückt die Shrinkage die geschätzten
  Koeffizienten Richtung 0, wobei der Lösungspfad i.d.R. nicht-linear
  verläuft, d.h. die Shrinkage auf den Koeffizienten ist grundsätzlich
  unterschiedlich. Die Lösung
  \((\widehat\beta^R_{1,t},\, \widehat\beta^R_{2,t}) = (0,0)\) existiert
  nur als Grenzwert für \(t\to0\).
\item
  Beachte, dass der Effekt von \(t\) auf die Schätzung umgekehrt für
  \(\lambda\) verläuft: Größere \(\lambda\) führen zu stärkerer
  Regularisierung.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{\emph{Diese Interaktive Komponente des Buchs ist nur in der
Online-Version verfügbar.}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{eigenschaften-des-schuxe4tzers}{%
\subsection{Eigenschaften des
Schätzers}\label{eigenschaften-des-schuxe4tzers}}

Der Ridge-Schätzer \(\widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda\)
ist nicht invariant gegenüber der Skalierung der Regressoren. Für
empirische Daten sollte daher vorab eine Standardisierung der
erklärenden Variablen durchgeführt werden.\footnote{Bspw. mit der
  Funktion \texttt{scale()}.} Um die Eigenschaften des Ridge-Schätzers
besser zu verstehen, betrachten wir hier den Fall orthonormaler
Regressoren \(\boldsymbol{X}_j\).\footnote{Orthonormalität heißt
  \(\boldsymbol{X}_i'\boldsymbol{X}_j = 1\) für \(i=j\) und \(0\) sonst.
  Dann ist \(\boldsymbol{X}\)'\(\boldsymbol{X} = \boldsymbol{I}_k\).}
Dann ist \begin{align}
  \widehat{\beta}^{\mathrm{R}}_{\lambda,\,j} = (1+\lambda)^{-1} \cdot\widehat{\beta}_j,\quad j = 1,\dots,k,\label{eq:ridgeortho}
\end{align} d.h. der Ridge-Schätzer skaliert die KQ-Lösung mit einem von
\(\lambda\) abhängigen Faktor.\footnote{\((1+\lambda)^{-1}\) wird auch
  als \emph{Shrinkage-Faktor} bezeichnet.}

Wir illustrieren dies, indem wir den Zusammenhang zwischen KQ- und
Ridge-Schätzer im orthonormalen Fall als R-Funktion
\texttt{ridge\_ortho()} implementieren und für die Parameterwerte
\(\lambda\in\{0,0.5,2\}\) plotten.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\CommentTok{\# Funktion für Rige Regression bei orthonormalen Regressoren}
\NormalTok{ridge\_ortho }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(KQ, lambda) \{}
  \DecValTok{1}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1} \SpecialCharTok{+}\NormalTok{ lambda) }\SpecialCharTok{*}\NormalTok{ KQ}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# KQ{-}Schätzer gegen Ridge{-}Schätzer plotten}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{KQ =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, .}\DecValTok{01}\NormalTok{))}

\FunctionTok{ggplot}\NormalTok{(dat) }\SpecialCharTok{+}
  \FunctionTok{geom\_function}\NormalTok{(}\AttributeTok{fun =}\NormalTok{ ridge\_ortho, }
                \AttributeTok{args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{lambda =}  \DecValTok{0}\NormalTok{), }
                \AttributeTok{lty =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_function}\NormalTok{(}\AttributeTok{fun =}\NormalTok{ ridge\_ortho, }
                \AttributeTok{args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{lambda =}\NormalTok{ .}\DecValTok{5}\NormalTok{), }
                \AttributeTok{col =} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_function}\NormalTok{(}\AttributeTok{fun =}\NormalTok{ ridge\_ortho, }
                \AttributeTok{args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{lambda =} \DecValTok{2}\NormalTok{), }
                \AttributeTok{col =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{xlim}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{.}\DecValTok{4}\NormalTok{, .}\DecValTok{4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"KQ{-}Schätzer von beta\_1"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Ridge{-}Schätzer von beta\_1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{RegReg_files/figure-pdf/fig-ridgeortho-1.pdf}

}

\caption{\label{fig-ridgeortho}Shrinkage des OLS-Schätzers bei Ridge
Regression}

\end{figure}

Abbildung~\ref{fig-ridgeortho} zeigt, dass der Ridge-Schätzer eine
lineare Transformation des KQ-Schätzers (gestrichelte Linie) ist.
Größere Werte des Regularisierungsparameters \(\lambda\) führen zu
stärkerer Shrinkage des Koeffizientenschätzers in Richtung 0. Die
\(\ell_2\)-Norm führt zu proportional zum Absolutwert des KQ-Schätzers
verlaufender Shrinkage: Größere Koeffizienten werden stärker bestraft
als kleine Koeffizienten.

Die Eigenschaft
\[\mathrm{E}\left(\widehat{\boldsymbol{\beta}}^{\mathrm{R}}_{\lambda,\,j}\right) = (1+\lambda)^{-1} \cdot \beta_j\]
zeigt, dass \(\widehat{\boldsymbol{\beta}}^{\mathrm{R}}_{\lambda,\,j}\)
(für fixes \(\lambda>0\)) nicht erwartungstreu für \(\beta_j\) ist.
Weiterhin ist \begin{align*}
  \mathrm{Var}\left(\widehat{\beta}^{\mathrm{R}}_{\lambda,\,j}\right) =&\, 
  \mathrm{Var}\left(\widehat{\beta}_j\right) \cdot \left(\frac{\lambda}{1+\lambda^2}\right)\\
    =&\, \sigma^2\cdot \left(\frac{\lambda}{1+\lambda^2}\right),
\end{align*} wobei \(\sigma^2\) die Varianz des Regressionsfehlers \(u\)
ist. Wegen \(\lambda<(1+\lambda)^2\) für \(\lambda>0\) gilt
\[\mathrm{Var}\left(\widehat{\beta}^{\mathrm{R}}_{\lambda,\,j}\right)<\mathrm{Var}\left(\widehat{\beta}_j\right).\]
Der Ridge-Schätzer hat also eine kleinere Varianz als der KQ-Schätzer.
Diese Eigenschaften können auch für korrelierte Regressoren gezeigt
werden.

\hypertarget{ridge-regression-mit-glmnet}{%
\subsection{\texorpdfstring{Ridge Regression mit
\texttt{glmnet}}{Ridge Regression mit glmnet}}\label{ridge-regression-mit-glmnet}}

Wir zeigen nun anhand simulierter Daten, wie der Ridge-Lösungspfad mit
dem R-Paket \texttt{glmnet} berechnet werden kann. Wir erzeugen zunächst
Daten gemäß der Vorschrift \begin{align}
  \begin{split}
  Y_i =&\, \boldsymbol{X}_i' \boldsymbol{\beta} + u_i,\\
  \\
  \beta_j =&\,  \frac{5}{j^2}, \qquad\qquad\ j=1,\dots,5,\\ 
  \beta_j =&\, -\frac{5}{(j-5)^2}, \quad j=6,\dots,10,\\
  \\
  \boldsymbol{X}_i \sim&\, N(\boldsymbol{0}, \boldsymbol{\Sigma}), \quad u_i \overset{u.i.v.}{\sim} N(0, 1), \quad i = 1,\dots,25.
  \end{split} \label{eq:ridgedgp1}
\end{align} Hierbei wird \(\boldsymbol{\Sigma}\) so definiert, dass
jeder Regressor \(N(0,1)\)-verteilt ist und eine Korrelation von \(0.8\)
mit allen anderen Regressoren aufweist. Mit der Vorschrift für die
\(\beta_j\) stellen wir sicher, dass es wenige Variablen gibt, die \(Y\)
stark beeinflussen, da der Absolutbetrag der Koeffizienten in \(j\)
abnimmt.\footnote{Für bessere Interpretierbarkeit der Grafischen
  Auswertung, wählen wir positive und negative Koeffizienten mit
  gleichem Bertag.}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gendata)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}

\CommentTok{\# Parameter definieren}
\NormalTok{N }\OtherTok{\textless{}{-}} \DecValTok{80}
\NormalTok{k }\OtherTok{\textless{}{-}} \DecValTok{10}

\NormalTok{coefs }\OtherTok{\textless{}{-}} \DecValTok{5}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(k}\SpecialCharTok{/}\DecValTok{2}\NormalTok{))}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{beta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(coefs, }\SpecialCharTok{{-}}\NormalTok{coefs)}

\CommentTok{\# Beobachtungen simulieren}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(}
  \FunctionTok{genmvnorm}\NormalTok{(}
    \AttributeTok{k =}\NormalTok{ k, }
    \AttributeTok{cor =} \FunctionTok{rep}\NormalTok{(.}\DecValTok{8}\NormalTok{, (k}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{{-}}\NormalTok{k)}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), }
    \AttributeTok{n =}\NormalTok{ N)}
\NormalTok{  )}
\NormalTok{Y }\OtherTok{\textless{}{-}}\NormalTok{ X }\SpecialCharTok{\%*\%}\NormalTok{ beta }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(N)}
\end{Highlighting}
\end{Shaded}

Wir schätzen nun ein Modell mit allen 10 Regressoren mit
\texttt{glmnet}. Beachte, dass für den Ridge-Strafterm
\texttt{alpha\ =\ 0} gesetzt werden muss.\footnote{\texttt{alpha} ist
  ein Mischparameter im Algorithmus für
  \href{https://en.wikipedia.org/wiki/Elastic_net_regularization}{elastic
  net}, siehe \texttt{?glmnet}.}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(glmnet)}

\CommentTok{\# Ridge{-}Regression anpassen}
\NormalTok{ridge\_fit }\OtherTok{\textless{}{-}} \FunctionTok{glmnet}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ X, }
  \AttributeTok{y =}\NormalTok{ Y, }
  \AttributeTok{alpha =} \DecValTok{0} \CommentTok{\# für Ridge{-}Strafterm}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Der Lösungspfad der Ridge-Schätzung kann nach Transformation der
geschätzen Koeffizienten und der zugehörigen \(\lambda\)-Werte in ein
langes Format überführt und komfortabel mit \texttt{ggplot2} dargestellt
werden.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Lambda{-}Sequenz auslesen}
\NormalTok{lambdas }\OtherTok{\textless{}{-}}\NormalTok{ ridge\_fit}\SpecialCharTok{$}\NormalTok{lambda}

\CommentTok{\# Ridge{-}Schätzung für Lambdas im langen Format }
\FunctionTok{as.matrix}\NormalTok{(ridge\_fit}\SpecialCharTok{$}\NormalTok{beta) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rownames\_to\_column}\NormalTok{(}\StringTok{"Variable"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Variable) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Variable) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lambda =}\NormalTok{ lambdas) }\SpecialCharTok{\%\textgreater{}\%}
  
  \CommentTok{\# Grafik mit ggplot erzeugen}
  \FunctionTok{ggplot}\NormalTok{(}
    \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}
      \AttributeTok{x =}\NormalTok{ lambda, }
      \AttributeTok{y =}\NormalTok{ value, }
      \AttributeTok{col =}\NormalTok{ Variable}
\NormalTok{    )}
\NormalTok{  ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"gesch. Koeffizienten"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{(}\StringTok{"log\_10(lambda)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{RegReg_files/figure-pdf/fig-ridgesolpath-1.pdf}

}

\caption{\label{fig-ridgesolpath}Lösungspfad für Ridge-Schätzung}

\end{figure}

Abbildung~\ref{fig-ridgesolpath} zeigt den nicht-linearen Verlauf der
Shrinkage auf den geschätzten Modellkoeffizienten. Die Koeffizienten
werden mit zunehmendem \(\lambda\) von der KQ-Lösung ausgehend (linkes
Ende der Skala) in Richtung 0 gezwungen.

Über die Funktion \texttt{cv.glmnet()} kann ein optimales \(\lambda\)
mit Cross Validation (CV) ermittelt werden. Ähnlich wie bei
\texttt{glmnet()} wird für die Validierung automatisch eine
\(\lambda\)-Sequenz erzeugt. Wir nutzen \texttt{autoplot()} aus dem
R-Paket \texttt{ggfortify} für die Visualisierung der Ergebnisse mit
\texttt{ggplot2}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggfortify)}

\CommentTok{\# Cross{-}validierte Bestimmung von lambda}
\NormalTok{ridge\_cvfit }\OtherTok{\textless{}{-}} \FunctionTok{cv.glmnet}\NormalTok{(}
  \AttributeTok{y =}\NormalTok{ Y, }
  \AttributeTok{x =}\NormalTok{ X, }
  \AttributeTok{intercept =}\NormalTok{ F,}
  \AttributeTok{alpha =} \DecValTok{0}
\NormalTok{) }

\CommentTok{\# Ergebnisse plotten}
\NormalTok{ridge\_cvfit }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{autoplot}\NormalTok{(}\AttributeTok{label.n =} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{RegReg_files/figure-pdf/fig-ridgecvplot-1.pdf}

}

\caption{\label{fig-ridgecvplot}Lösungspfad für Ridge-Schätzung}

\end{figure}

Abbildung~\ref{fig-ridgecvplot} zeigt \texttt{ridge\_cvfit\$lambda.min},
das optimale \(\lambda\) mit dem geringsten CV Mean-Squarred-Error
(linke gestrichelte Linie) und \texttt{ridge\_cvfit\$lambda.1se}, das
größte \(\lambda\), welches innerhalb einer Standardabweichung entfernt
ist (rechte gestrichelte Linie).\footnote{Die Wahl von
  \texttt{lambda.1se} ist eine Heuristik, welche die Schätzunsicherheit
  berücksichtigt und zu einem ``sparsameren'' Modell tendiert.} Wir
berechnen die Schätzung für \texttt{lambda.min}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}
\NormalTok{  ridge\_coefs }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(}
    \AttributeTok{object =}\NormalTok{ ridge\_cvfit, }
    \AttributeTok{s =}\NormalTok{ ridge\_cvfit}\SpecialCharTok{$}\NormalTok{lambda.min}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
11 x 1 sparse Matrix of class "dgCMatrix"
                    s1
(Intercept)  .        
X1           4.1302194
X2           1.0245661
X3           0.3139297
X4           0.5697498
X5           0.2928664
X6          -4.1693524
X7          -0.7509305
X8          -0.3844761
X9          -0.3841997
X10         -0.4078514
\end{verbatim}

Wir schätzen das Modell nun mit KQ und vergleichen die Koeffizienten mit
der Ridge-Schätzung.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# KQ{-}Schätzung durchführen}
\NormalTok{KQ\_fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}

\CommentTok{\# Koeffizienten auslesen und transformieren:}
\FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{Ridge =} \FunctionTok{as.matrix}\NormalTok{(ridge\_coefs)[}\DecValTok{2}\SpecialCharTok{:}\DecValTok{11}\NormalTok{, ],}
  \AttributeTok{KQ =}\NormalTok{ KQ\_fit}\SpecialCharTok{$}\NormalTok{coefficients}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{j =} \FunctionTok{factor}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}
    \AttributeTok{cols =}\NormalTok{ Ridge}\SpecialCharTok{:}\NormalTok{KQ, }
    \AttributeTok{names\_to =} \StringTok{"Methode"}\NormalTok{, }
    \AttributeTok{values\_to =} \StringTok{"Koeffizient"}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}

\CommentTok{\# Bar{-}Plot für Koeffizientenvergleich erzeugen  }
  \FunctionTok{ggplot}\NormalTok{(}
    \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}
      \AttributeTok{x =}\NormalTok{ j, }
      \AttributeTok{y =}\NormalTok{ Koeffizient, }
      \AttributeTok{fill =}\NormalTok{ Methode}
\NormalTok{    )}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}
    \AttributeTok{position =} \StringTok{"dodge"}\NormalTok{, }
    \AttributeTok{stat =} \StringTok{"identity"}\NormalTok{, }
    \AttributeTok{width =}\NormalTok{ .}\DecValTok{5}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{RegReg_files/figure-pdf/fig-KoefRidgeVsKQ-1.pdf}

}

\caption{\label{fig-KoefRidgeVsKQ}Koeffizientenvergleich: Ridge vs.~KQ}

\end{figure}

Der Vergleich anhand von Abbildung~\ref{fig-KoefRidgeVsKQ} zeigt
deutlich, dass Ridge Regression im Vergleich mit KQ zu absolut kleineren
Koeffizientenschätzungen tendiert. Inwiefern dies Konsequenzen für die
Prognosegüte der Schätzung hat, können wir Anhand eines Testdatensatzes
bestimmen. Hierzu vergleichen wir die mittleren Fehler (MSE) bei der
Prognose von \(Y\) für die Beobachtungen im Testdatensatz. Für die
Simulation des Testdatensatzes nutzen wir erneut die Vorschrift
\eqref{eq:ridgedgp1} um 80 neue Beobachtungen zu erzeugen.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Test{-}Datensatz erstellen}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{4321}\NormalTok{)}
\CommentTok{\# Regressoren}
\NormalTok{new\_X }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(}
  \FunctionTok{genmvnorm}\NormalTok{(}
    \AttributeTok{k =}\NormalTok{ k, }
    \AttributeTok{cor =} \FunctionTok{rep}\NormalTok{(.}\DecValTok{85}\NormalTok{, (k}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{{-}}\NormalTok{k)}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), }
    \AttributeTok{n =}\NormalTok{ N}
\NormalTok{  )}
\NormalTok{)}
\CommentTok{\# Abh. Variable}
\NormalTok{new\_Y }\OtherTok{\textless{}{-}}\NormalTok{ new\_X }\SpecialCharTok{\%*\%}\NormalTok{ beta }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(N)}
\end{Highlighting}
\end{Shaded}

Für beide Methoden können wir \texttt{predict()} für die Prognosen von
\(Y\) für den Testdatensatz (\texttt{new\_Y}) nutzen.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Ridge: Vorhersage von new\_Y für Test{-}Datensatz}
\NormalTok{Y\_predict\_ridge }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(}
  \AttributeTok{object =}\NormalTok{ ridge\_cvfit, }
  \AttributeTok{newx =}\NormalTok{ new\_X, }
  \AttributeTok{s =}\NormalTok{ ridge\_cvfit}\SpecialCharTok{$}\NormalTok{lambda.min}
\NormalTok{)}

\CommentTok{\# Ridge: MSE für Test{-}Datensatz berechnen}
\FunctionTok{mean}\NormalTok{((Y\_predict\_ridge }\SpecialCharTok{{-}}\NormalTok{ new\_Y)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.288457
\end{verbatim}

Die Vorhersage für \texttt{lm()} benötigt dieselben Variablennamen wie
im angepassten Modell, s. \texttt{KQ\_fit\$coefficients}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Test{-}Datensatz für predict.lm() formatieren}
\NormalTok{new\_X }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(new\_X)}
\FunctionTok{colnames}\NormalTok{(new\_X) }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"X"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{k)}

\CommentTok{\# KQ: Vorhersage von new\_Y für Test{-}Datensatz}
\NormalTok{Y\_predict\_KQ }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(}
  \AttributeTok{object =}\NormalTok{ KQ\_fit, }
  \AttributeTok{newdata =}\NormalTok{ new\_X}
\NormalTok{)}

\CommentTok{\# KQ: MSE für Test{-}Datensatz berechnen}
\FunctionTok{mean}\NormalTok{((Y\_predict\_KQ }\SpecialCharTok{{-}}\NormalTok{ new\_Y)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 29.33797
\end{verbatim}

Die Ergebnisse zeigen, dass der Ridge-Schätzer trotz seiner Verzerrung
einen deutlich geringeren mittleren Vorhersagefehler für die Testdaten
erzielt als der KQ-Schätzer. Diese Eigenschaft der
Koeffizientenschätzung kann die Prognosegüte von Ridge Regression
gegenüber der KQ-Regression verbessern.

\hypertarget{beispiel-vorhersage-von-abschlussnoten-in-mathe}{%
\subsection{Beispiel: Vorhersage von Abschlussnoten in
Mathe}\label{beispiel-vorhersage-von-abschlussnoten-in-mathe}}

Zur Illustration von Ridge Regression nutzen wir den Datensatz
\texttt{SP} aus Cortez und Silva (2008).\footnote{Wir verwenden eine
  Auszug aus dem Orignaldatensatz, der nebst ausführlicher
  Variablenbeschreibung
  \href{https://archive.ics.uci.edu/dataset/320/student+performance}{hier}
  verfügbar ist.} \texttt{SP} enhält Beobachtungen zu Leistungen von
insgesamt 100 Schülerinnen und Schülern im Fach Mathematik in der
Sekundarstufe an zwei portugiesischen Schulen. Neben der Abschlussnote
in Mathe (\texttt{G3}, Skala von 0 bis 20) beinhaltet \texttt{SP}
diverse demografische, soziale und schulbezogene Merkmale, die mithilfe
von Schulberichten und Fragebögen erhoben wurden. Ziel ist es, ein
Modell für die Prognose von \texttt{G3} anzupassen.

Wir lesen zunächst die Daten (im .csv-Format) ein.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Daten einlesen}
\NormalTok{SP }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"datasets/SP.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Ein Überblick zeigt, dass der Großteil der Regressoren aus kategorialen
Variablen mit sozio-ökonomischen Informationen besteht.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Überblick}
\FunctionTok{glimpse}\NormalTok{(SP)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 100
Columns: 31
$ school     <chr> "GP", "GP", "GP", "MS", "GP", "GP", "GP", "GP", "GP", "GP",~
$ sex        <chr> "M", "M", "F", "F", "M", "F", "F", "F", "F", "F", "M", "M",~
$ age        <dbl> 17, 18, 19, 17, 16, 16, 19, 16, 16, 16, 18, 16, 15, 17, 17,~
$ address    <chr> "R", "R", "U", "U", "U", "U", "U", "U", "U", "R", "U", "U",~
$ famsize    <chr> "GT3", "GT3", "LE3", "GT3", "LE3", "GT3", "GT3", "GT3", "GT~
$ Pstatus    <chr> "T", "T", "T", "T", "A", "T", "T", "T", "A", "T", "T", "T",~
$ Medu       <dbl> 1, 4, 3, 2, 3, 2, 0, 2, 3, 4, 4, 2, 1, 2, 2, 3, 3, 4, 4, 2,~
$ Fedu       <dbl> 2, 3, 2, 2, 4, 3, 1, 1, 1, 4, 4, 2, 2, 3, 2, 3, 1, 3, 4, 2,~
$ Mjob       <chr> "at_home", "teacher", "services", "other", "services", "oth~
$ Fjob       <chr> "other", "services", "other", "at_home", "other", "other", ~
$ reason     <chr> "home", "course", "reputation", "home", "home", "reputation~
$ guardian   <chr> "mother", "mother", "other", "mother", "mother", "mother", ~
$ traveltime <dbl> 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1,~
$ studytime  <dbl> 2, 3, 2, 3, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 3, 1, 2,~
$ failures   <dbl> 0, 0, 1, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~
$ schoolsup  <chr> "no", "no", "no", "no", "yes", "yes", "no", "no", "no", "no~
$ famsup     <chr> "no", "no", "yes", "no", "yes", "yes", "yes", "no", "yes", ~
$ paid       <chr> "no", "no", "yes", "no", "no", "yes", "no", "no", "yes", "n~
$ activities <chr> "no", "no", "no", "yes", "yes", "yes", "no", "no", "no", "y~
$ nursery    <chr> "yes", "yes", "no", "yes", "yes", "yes", "no", "yes", "yes"~
$ higher     <chr> "yes", "yes", "yes", "yes", "yes", "yes", "no", "yes", "yes~
$ internet   <chr> "no", "yes", "yes", "no", "yes", "no", "no", "yes", "yes", ~
$ romantic   <chr> "no", "yes", "yes", "yes", "no", "no", "no", "yes", "no", "~
$ famrel     <dbl> 3, 5, 4, 3, 5, 4, 3, 4, 2, 2, 1, 5, 4, 5, 3, 5, 4, 4, 5, 5,~
$ freetime   <dbl> 1, 3, 2, 4, 3, 4, 4, 5, 3, 4, 4, 4, 3, 3, 4, 4, 5, 2, 3, 4,~
$ goout      <dbl> 3, 2, 2, 3, 3, 3, 2, 2, 3, 4, 2, 4, 2, 3, 4, 2, 4, 2, 3, 4,~
$ Dalc       <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1,~
$ Walc       <dbl> 5, 2, 2, 1, 1, 3, 1, 1, 2, 3, 2, 4, 1, 3, 3, 1, 3, 2, 1, 1,~
$ health     <dbl> 3, 4, 1, 3, 5, 4, 5, 5, 4, 4, 1, 5, 5, 3, 5, 5, 1, 3, 5, 5,~
$ absences   <dbl> 4, 9, 22, 8, 4, 6, 2, 20, 5, 6, 5, 0, 2, 2, 12, 0, 17, 0, 4~
$ G3         <dbl> 10, 16, 11, 11, 11, 10, 9, 12, 7, 11, 16, 12, 9, 12, 12, 13~
\end{verbatim}

Um die Prognosegüte des Modells beurteilen zu können, partitionieren wir
\texttt{SP} zufällig in einen Test- sowie einen Trainingsdatensatz (mit
30 und 70 Beobachtungen), jeweils für die Regressoren und die abhängige
Variable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# ID für Beobachtungen im Testdatensatz zufällig erzeugen}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{ID }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(SP), }\AttributeTok{size =} \DecValTok{30}\NormalTok{)}

\CommentTok{\# Regressoren aufteilen}
\NormalTok{SP\_test }\OtherTok{\textless{}{-}}\NormalTok{ SP[ID,]}
\NormalTok{SP\_train }\OtherTok{\textless{}{-}}\NormalTok{ SP[}\SpecialCharTok{{-}}\NormalTok{ID,]}

\CommentTok{\# Abh. Variable aufteilen}
\NormalTok{Y\_test }\OtherTok{\textless{}{-}}\NormalTok{ SP\_test}\SpecialCharTok{$}\NormalTok{G3}
\NormalTok{Y\_train }\OtherTok{\textless{}{-}}\NormalTok{ SP\_train}\SpecialCharTok{$}\NormalTok{G3}
\end{Highlighting}
\end{Shaded}

Als nächstes passen wir ein Ridge-Regressionsmodell für alle Regressoren
in \texttt{SP\_train} an und ermitteln ein optimales \(\lambda\) mit
Cross Validation. Beachte, dass \texttt{cv.glmnet} nicht für Regressoren
im \texttt{data.frame}/\texttt{tibble}-Format ausgelegt ist, sondern ein
\texttt{matrix}-Format erwartet. Wir transformieren \texttt{SP\_train}
daher mit \texttt{data.matrix()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Ridge{-}Regression und CV für Trainingsdaten}
\NormalTok{SP\_fit\_cv }\OtherTok{\textless{}{-}} \FunctionTok{cv.glmnet}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{data.matrix}\NormalTok{(SP\_train }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{G3)), }
  \AttributeTok{y =}\NormalTok{ Y\_train, }
  \AttributeTok{alpha =} \DecValTok{0}
\NormalTok{)}

\CommentTok{\# CV{-}Ergebnisse für lambda visualisieren}
\NormalTok{SP\_fit\_cv }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{autoplot}\NormalTok{(}\AttributeTok{label.n =} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{RegReg_files/figure-pdf/unnamed-chunk-16-1.pdf}

}

\end{figure}

Wie für das Beispiel mit simulierten Daten erhalten wir mit
\texttt{predict()} Vorhersagen für die erzielte Punktzahl. Beachte, dass
wir den MSE nicht für die Trainingsdaten \texttt{SP\_train}, sondern für
die Testdaten \texttt{SP\_test} berechnen.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Prognose von G3 anhand des Ridge{-}Modells}
\NormalTok{Y\_predict\_ridge }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(}
  \AttributeTok{object =}\NormalTok{ SP\_fit\_cv, }
  \AttributeTok{newx =} \FunctionTok{data.matrix}\NormalTok{(}
\NormalTok{    SP\_test }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{G3)}
\NormalTok{    ), }
  \AttributeTok{s =}\NormalTok{ SP\_fit\_cv}\SpecialCharTok{$}\NormalTok{lambda.min}
\NormalTok{)}

\CommentTok{\# MSE für Testdaten berechnen}
\FunctionTok{mean}\NormalTok{((Y\_predict\_ridge }\SpecialCharTok{{-}}\NormalTok{ Y\_test)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 21.13249
\end{verbatim}

Auch in diesem empirischen Beispiel zeigt ein Vergleich der MSEs, dass
Ridge Regression dem KQ-Schätzer hinsichtlich der Vorhersagegüte
überlegen ist.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Modell mit KQ schätzen}
\NormalTok{SP\_fit\_KQ }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(G3 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., SP\_train)}

\CommentTok{\# Prognose}
\NormalTok{Y\_predict\_KQ }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(}
  \AttributeTok{object =}\NormalTok{ SP\_fit\_KQ, }
  \AttributeTok{newdata =}\NormalTok{ SP\_test }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{G3)}
\NormalTok{)}

\CommentTok{\# Testset{-}MSE berechnen}
\FunctionTok{mean}\NormalTok{((Y\_predict\_KQ }\SpecialCharTok{{-}}\NormalTok{ Y\_test)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 29.76893
\end{verbatim}

Der MSE für Ridge ist mit \(21.13\) deutlich kleiner als \(29.77\), der
MSE für KQ.

Für die Interpretation der Ridge-Schätzung erweitern den Code für die
\texttt{ggplot2}-Grafik der Koeffizienten-Pfade um eine vertikale Linie
des mit CV ermittelten \(\lambda\) und fügen mit dem Paket
\texttt{ggrepel} Labels für die Pfade der größten Koeffizienten hinzu.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggrepel)}

\CommentTok{\# Lambda{-}Sequenz auslesen}
\NormalTok{lambdas }\OtherTok{\textless{}{-}}\NormalTok{ SP\_fit\_cv}\SpecialCharTok{$}\NormalTok{lambda}

\CommentTok{\# Ridge{-}Schätzung für Lambdas im langen Format }
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(SP\_fit\_cv}\SpecialCharTok{$}\NormalTok{glmnet.fit}\SpecialCharTok{$}\NormalTok{beta) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{Variable =} \FunctionTok{rownames}\NormalTok{(SP\_fit\_cv}\SpecialCharTok{$}\NormalTok{glmnet.fit}\SpecialCharTok{$}\NormalTok{beta)}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Variable) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Variable) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lambda =}\NormalTok{ lambdas) }

\CommentTok{\# Grafik mit ggplot erzeugen}
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}
    \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}
      \AttributeTok{x =}\NormalTok{ lambda, }
      \AttributeTok{y =}\NormalTok{ value, }
      \AttributeTok{col =}\NormalTok{ Variable}
\NormalTok{    )}
\NormalTok{  ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_label\_repel}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ df }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{filter}\NormalTok{(lambda }\SpecialCharTok{==} \FunctionTok{min}\NormalTok{(lambdas)),}
    \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ Variable), }
    \AttributeTok{seed =} \DecValTok{1234}\NormalTok{,}
    \AttributeTok{size =} \DecValTok{5}\NormalTok{, }
    \AttributeTok{max.overlaps =} \DecValTok{8}\NormalTok{, }
    \AttributeTok{nudge\_x =} \SpecialCharTok{{-}}\NormalTok{.}\DecValTok{5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"gesch. Koeffizienten"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{(}\StringTok{"log\_10(lambda)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}
    \AttributeTok{xintercept =}\NormalTok{ SP\_fit\_cv}\SpecialCharTok{$}\NormalTok{lambda.min, }
    \AttributeTok{col =} \StringTok{"red"}\NormalTok{, }
    \AttributeTok{lty =} \DecValTok{2}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{RegReg_files/figure-pdf/fig-ridgAppPlot-1.pdf}

}

\caption{\label{fig-ridgAppPlot}Lösungspfad für Ridge-Schätzung}

\end{figure}

Abbildung~\ref{fig-ridgAppPlot} gibt Hinweise darauf, dass neben der
Schulzugehörigkeit und Indikatoren für schulische Leistung (bspw.
\texttt{failures}) sozio-ökonomische Prädiktoren wie \texttt{internet}
(Internetzugang zuhause), \texttt{Pstatus} (Zusammenleben der Eltern)
und \texttt{address}/\texttt{traveltime} (sozialer Status) relevante
Variablen zu sein scheinen.

Das optimale \(\lambda_\mathrm{cv} \approx 0.21\) (gestrichelte rote
Linie in Abbildung~\ref{fig-ridgAppPlot}) führt zu deutlicher Shrinkage,
was eine mögliche Erklärung für den besseren Testset-MSE von Ridge
Regression ist: Die Koeffizienten von Variablen mit wenig
Erklärungskraft werden durch die Regularisierung in Richtung 0 gezwungen
und reduzieren so die Varianz der Vorhersage gegenüber der
(idealerweise) unverzerrten KQ-Schätzung.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, bottomtitle=1mm, arc=.35mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Key Facts zu Ridge Regression}, opacitybacktitle=0.6, colback=white, left=2mm, breakable, leftrule=.75mm, opacityback=0, coltitle=black, titlerule=0mm, colframe=quarto-callout-note-color-frame, toprule=.15mm, toptitle=1mm]

\begin{itemize}
\item
  Ridge-Regression regularisiert den KQ-Schätzer mit der \(\ell_2\)-Norm
  der Koeffizienten. Diese Form von Regularisierung ist eine Alternative
  für KQ in Anwendungen mit mehr Regressoren als Beobachtugen
  (\(k\geq n\)) und/oder wenn KQ aufgrund starker Kollinearität eine
  hohe Varianz aufweist.
\item
  Der Ridge-Schätzer
  \(\widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda\) ist \emph{nicht}
  erwartungstreu. Die geschätzten Koeffizienten sind auch für
  \(n\to\infty\) verzerrt.
\item
  Aufgrund der verzerrten Schätzung ist statistische Inferenz für
  Koeffizienten mit
  \(\widehat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda\) problematisch.
  Anstatt für strukturelle Modelle oder die Schätzung kausaler Effekte
  wird Ridge Regression in der Praxis daher überwiegend für Prognosen
  verwendet.
\item
  Die Wahl von \(\lambda\) impliziert einen Tradeoff zwischen Verzerrung
  und Varianz: Große \(\lambda\) schrumpfen die Koeffizientenschätzer
  Richtung 0 (mehr Verzerrung), führen aber zu einer kleineren Varianz
  der Schätzung. Entsprechend können Vorhersagen mit mehr Verzerrung
  aber weniger Varianz als mit KQ getroffen werden.
\item
  Ridge Regression kann in R mit dem Paket \texttt{glmnet} berechnet
  werden.
\end{itemize}

\end{tcolorbox}

\hypertarget{lasso-regression}{%
\section{Lasso Regression}\label{lasso-regression}}

Least Absolute Shrinkage and Selection Operator (Lasso) ist ein von
Tibshirani (1996) vorgeschlagener Schätzer, der die Verlustfunktion des
KQ-Schätzers um einen Strafterm für die Summe der (absoluten) Größe der
Koeffizienten \(\boldsymbol\beta = (\beta_1, \dots,\beta_k)'\)
erweitert. Die Verlustfunktion des Lasso-Schätzers von
\(\boldsymbol{\beta}\) lautet \begin{align}
\mathrm{RSS}(\boldsymbol{\beta},p=1,\lambda) = \mathrm{RSS}(\boldsymbol{\beta}) + \lambda \lVert\boldsymbol{\beta}\rVert_1.\label{eq:lassoloss}
\end{align} Für den Strafterm wird also die \(\ell_1\)-norm \[
\lVert\boldsymbol{\beta}\rVert_1 = \sum_{j=1}^k \lvert\beta_j \rvert
\] verwendet. Der Lasso-Schätzer
\(\widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda\) für
\(\boldsymbol{\beta}\) minimiert \eqref{eq:lassoloss}, \begin{align}
\boldsymbol{\beta}^{\mathrm{L}}_\lambda = \arg\min_{\boldsymbol{\beta}} \ \mathrm{RSS}(\boldsymbol{\beta},p=1,\lambda).
\end{align} Entsprechend erhalten wir in Abhängigkeit von \(\lambda\)
ein Kontinuum an Lösungen \begin{align}
  \left\{\widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda\right\}_{\lambda=0}^{\lambda=\infty},\label{eq:LassoPath}
\end{align} der sogenannte \emph{Lasso-Pfad}.

Das Optimierungsproblem \eqref{eq:lassoloss} hat die äquivalente
Darstellung \begin{align}
  \begin{split}
    \widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda =&\, \arg\min_{\boldsymbol{\beta}} \mathrm{RSS}(\boldsymbol{\beta}) + \lambda\left(\lVert\boldsymbol{\beta}\rVert_1 - t\right)\\
    =&\, \arg\min_{\lVert\boldsymbol{\beta}\rVert_1\leq t} \mathrm{RSS}(\boldsymbol{\beta}), 
  \end{split}\label{eq:lassolagrange}
\end{align} welche über den
\href{https://de.wikipedia.org/wiki/Lagrange-Multiplikator\#Beispiel_mit_Anwendungsbezug}{Lagrange-Ansatz}
unter der Nebenbedingung \(\lVert\boldsymbol{\beta}\rVert_1 \leq t\)
gelöst werden kann.

Ähnlich wie der KQ-Schätzer ist der Lasso-Schätzer
\(\widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda\) durch Bedingungen
1. Ordnung bestimmt. Diese Bedingungen lassen sich komfortabel in
Matrix-Schreibweise darstellen als \begin{align}
  -2\boldsymbol{X}_j'(\boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\beta}) + \lambda\cdot\mathrm{sgn}(\beta_j) = 0, \quad j = 1,\dots,k.\label{eq:LassoFOC}
\end{align} Aus Gleichung \eqref{eq:LassoFOC} folgt, dass der
Lasso-Schätzer aufgrund des Strafterms im Allgemeinen nicht algebraisch
bestimmt werden kann.\footnote{Zur Bestimmung des Schätzers werden
  Algorithmen der nicht-linearen Optimierung genutzt.}

In Abhängigkeit von \(\lambda\) zwingt der Lasso-Schätzer die
KQ-Schätzung von \(\beta_j\) zu einem (absolut) kleineren Wert: Ähnlich
wie bei Ridge Regression bewirkt der \(\ell_1\)-Strafterm eine mit
\(\lambda\) zunehmende Schrumpfung der geschätzen Koeffizienten in
Richtung 0. Charakteristisch für die Lösung des Lasso-Schätzers ist,
dass \(\widehat{\boldsymbol{\beta}}^{\mathrm{L}}_j = 0\), wenn die
Bedingung \begin{align}
  \left\lvert\boldsymbol{X}_j'(\boldsymbol{Y} - \boldsymbol{X}\widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda)\right\rvert - \lambda/2 \leq 0 \label{eq:lassoselection}
\end{align} erfüllt ist. In Abhängigkeit von \(\lambda\) kann der
Lasso-Schätzer folglich geschätzte Regressionskoeffizienten nicht nur in
Richtung \(0\), sondern diese auch \emph{exakt} mit \(0\) schätzen und
damit \emph{Variablenselektion} betreiben. Aufgrund der mit \(\lambda\)
zunehmenden Shrinkage bis die Bedingung \eqref{eq:lassoselection}
erfüllt und der Koeffizient gleich \(0\) gesetzt wird, bezeichnet man
Lasso auch als einen \emph{Soft Thresholding Operator}. Im nächsten
Abschnitt betrachten wir die Eigenschaften von Lasso-Regularisierung
unter vereinfachten Annahmen bzgl. der Regressoren.

\hypertarget{lasso-ist-soft-thresholding}{%
\subsection{Lasso ist Soft
Thresholding}\label{lasso-ist-soft-thresholding}}

Wir betrachten nun eine mathematische Darstellung von Selektions- und
Shrinkage-Eigenschaft des Lasso-Schätzers in einem vereinfachten Modell.
Wenn die Regressoren \(\boldsymbol{X}\) orthonormal zueinander sind,
existiert eine analytische Lösung des Lasso-Schätzers, \begin{align}
  \widehat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda =
  \begin{cases}
    \widehat{\boldsymbol{\beta}}_j - \lambda/2 &, \ \ \widehat{\boldsymbol{\beta}}_j > \lambda/2\\
    0 &, \ \ \lvert\widehat{\boldsymbol{\beta}}_j\rvert\leq\lambda/2\\
    \widehat{\boldsymbol{\beta}}_j + \lambda/2 &, \ \ \widehat{\boldsymbol{\beta}}_j < \lambda/2
  \end{cases},\label{eq:lassoST}
\end{align} wobei \(\widehat{\boldsymbol{\beta}}_j\) der KQ-Schätzer von
\(\beta_j\) ist. Anhand von \eqref{eq:lassoST} können wir die
Selektionseigenschaft sowie die Schrumpfung der
KQ-Koeffizientenschätzung in Abhängigkeit der durch \(\lambda\)
regulierten \(\ell_1\)-Strafe erkennen. Für eine Visualisierung
implementieren wir \eqref{eq:lassoST} als R-Funktion
\texttt{lasso\_st()} und zeichnen die resultierenden
Koeffizientenschätzungen für die Parameterwerte
\(\lambda\in\{0, 0.2, 0.4\}\).

Wir definieren zunächst die Funktion \texttt{lasso\_st()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\CommentTok{\# Funktion für Lasso soft{-}thresholding definieren}
\NormalTok{lasso\_st }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(KQ, lambda) \{}
  \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    KQ }\SpecialCharTok{\textgreater{}}\NormalTok{ lambda}\SpecialCharTok{/}\DecValTok{2}         \SpecialCharTok{\textasciitilde{}}\NormalTok{ KQ }\SpecialCharTok{{-}}\NormalTok{ lambda}\SpecialCharTok{/}\DecValTok{2}\NormalTok{,}
    \FunctionTok{abs}\NormalTok{(KQ) }\SpecialCharTok{\textless{}=}\NormalTok{ lambda}\SpecialCharTok{/}\DecValTok{2}   \SpecialCharTok{\textasciitilde{}} \DecValTok{0}\NormalTok{,}
\NormalTok{    KQ }\SpecialCharTok{\textless{}} \SpecialCharTok{{-}}\NormalTok{lambda}\SpecialCharTok{/}\DecValTok{2}        \SpecialCharTok{\textasciitilde{}}\NormalTok{ KQ }\SpecialCharTok{+}\NormalTok{ lambda}\SpecialCharTok{/}\DecValTok{2}\NormalTok{,}
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Im nächsten Schritt zeichnen wir \texttt{lasso\_st()} für eine Sequenz
von KQ-Schätzwerten gegeben \(\lambda\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sequenz von KQ{-}Schätzwerten für Illustration definieren}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{KQ =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, .}\DecValTok{01}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Lasso{-}Schätzer als Funktion des KQ{-}Schätzers plotten}
\FunctionTok{ggplot}\NormalTok{(dat) }\SpecialCharTok{+}
  \FunctionTok{geom\_function}\NormalTok{(}
    \AttributeTok{fun =}\NormalTok{ lasso\_st, }
    \AttributeTok{args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{lambda =} \DecValTok{0}\NormalTok{), }
    \AttributeTok{lty =} \DecValTok{2}
\NormalTok{  ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_function}\NormalTok{(}
    \AttributeTok{fun =}\NormalTok{ lasso\_st, }
    \AttributeTok{args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{lambda =}\NormalTok{ .}\DecValTok{2}\NormalTok{),}
    \AttributeTok{col =} \StringTok{"red"}
\NormalTok{  ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_function}\NormalTok{(}
    \AttributeTok{fun =}\NormalTok{ lasso\_st, }
    \AttributeTok{args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{lambda =}\NormalTok{ .}\DecValTok{4}\NormalTok{), }
    \AttributeTok{col =} \StringTok{"blue"}
\NormalTok{  ) }\SpecialCharTok{+} 
  \FunctionTok{xlim}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{.}\DecValTok{4}\NormalTok{, .}\DecValTok{4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"KQ{-}Schätzer von beta\_1"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Lasso{-}Schätzer von beta\_1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{RegReg_files/figure-pdf/fig-lassoST-1.pdf}

}

\caption{\label{fig-lassoST}Shrinkage und Selektion von
OLS-Koeffizienten mit Lasso}

\end{figure}

Abbildung~\ref{fig-lassoST} zeigt, dass der \(\ell_1\)-Strafterm des
Lasso-Schätzers zu einem linearen Verlauf der auf den KQ-Schätzer
(gezeichnet für \(\lambda = 0\), gestrichelte Linie) applizierten
Shrinkage führt: Der Lasso-Schätzer ist eine abschnittsweise-lineare
Funktion des KQ-Schätzers in \(\lambda\): Je größer der Parameter
\(\lambda\), desto größer ist das Intervall von KQ-Schätzwerten
\([-\lambda/2,\lambda/2]\), wo der Lasso-Schätzer zu Variablenselektion
führt, d.h. hier den Koeffizienten \(\beta_j\) als \(0\) schätzt (rote
bzw. blaue Linie).

Anhand von Abbildung~\ref{fig-lassoST} kann abgeleitet werden, dass der
Lasso-Schätzer nicht invariant gegenüber der Skalierung der Regressoren
ist: Die Stärke der Regularisierung durch \(\lambda\) ist hängt von der
Magnitude des KQ-Schätzers ab. Daher müssen die Regressoren vor
Berechnung der Schätzung standardsiert werden. Üblich ist hierbei eine
Normierung auf einen Mittelwert von \(0\) und eine Varianz von \(1\).

Die nachstehende interaktive Grafik illustriert das
Lasso-Optimierungsproblem \eqref{eq:lassolagrange} sowie den
resultierenden Schätzer der Koeffizienten \((\beta_1, \beta_2)\) in
einem multiplen Regressionsmodell mit korrelierten Regressoren \(X_1\)
und \(X_2\).

\begin{itemize}
\item
  Die blaue Ellipse ist die Menge aller Schätzwerte
  \(\left(\widehat\beta_{1},\, \widehat\beta_{2}\right)\) für den
  angegebenen Wert von \(\mathrm{RSS}\). Im Zentrum der Ellipse liegt
  der KQ-Schätzer, welcher \(\mathrm{RSS}\) minimiert.
\item
  Das graue Quadrat ist die Menge aller Koeffizienten-Paare
  \((\beta_1, \beta_2)\), welche die Restriktion
  \(\lvert\beta_1\rvert+\lvert\beta_2\rvert\leq t\) erfüllen. Beachte,
  dass die Größe dieser Region nur durch den Parameter \(t\) bestimmt
  wird.
\item
  Der blaue Punkt ist der Lasso-Schätzer
  \((\widehat{\boldsymbol{\beta}}^L_{1,t},\, \widehat{\boldsymbol{\beta}}^L_{2,t})\).
  Dieser ergibt sich als Schnittpunkt zwischen der blauen
  \(\mathrm{RSS}\)-Ellipse und der Restriktionsregion und variiert mit
  \(t\). Die gestrichelte rote Linie zeigt den Lasso-Lösungspfad.
\item
  Für kleine Werte, erhalten wir starke Shrinkage auf
  \(\widehat\beta_{1,t}\) bis zum Wertebereich \(t\leq50\), wo
  \(\widehat{\boldsymbol{\beta}}^L_{1,t}=0\). Hier erfolgt
  Variablenselektion: Die Regularisierung führt zu einem geschätzten
  Modell, das lediglich \(X_2\) als erklärende Variable enthält. In
  diesem Bereich von \(t\) bewirkt die Shrinkage, dass
  \(\widehat{\boldsymbol{\beta}}^L_{2,t}\to0\) für \(t\to0\).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{\emph{Diese Interaktive Komponente des Buchs ist nur in der
Online-Version verfügbar.}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Beachte, dass der rote Lasso-Pfad (die Menge aller Lasso-Lösungen)
äquivalent als Funktion von \(\lambda\) im Optimierungsproblem
\eqref{eq:lassoloss} dargestellt werden kann. Implementierungen mit
statistischer Software berechnen die Lasso-Lösung häufig in Abhängigkeit
von \(\lambda\). Ein Algorithmus hierfür ist LARS.

\hypertarget{berechnung-der-lasso-luxf6sung-mit-dem-lars-algorithmus}{%
\subsection{Berechnung der Lasso-Lösung mit dem
LARS-Algorithmus}\label{berechnung-der-lasso-luxf6sung-mit-dem-lars-algorithmus}}

Für die Berechnung des Lasso-Lösungspfads kann der
\href{https://en.wikipedia.org/wiki/Least-angle_regression}{LARS-Algorithmus}
von Efron u.~a. (2004) im Lasso-Modus genutzt werden.\footnote{LARS
  steht für \emph{Least Angle Regression}.} Der Lasso-Lösungspfad
beinhaltet geschätzte Koeffizienten über ein Intervall für \(\lambda\),
welches sämtliche Modellkomplexitäten zwischen der (trivialen) Lösung
mit maximaler Shrinkage auf allen Koeffizienten (\(\lambda\) groß, alle
gesch. Koeffizienten sind \(0\)) und der unregularisierten Lösung
(\(\lambda = 0\), KQ-Schätzung) abbildet. Der LARS-Algorithmus erzeugt
den Lösungspfad sequentiell, sodass die Schätzung als Funktion von
\(\lambda\) veranschaulicht werden kann, ähnlich wie bei Ridge
Regression.

Wir zeigen nun anhand simulierter Daten, wie Lasso-Lösungen mit dem
R-Paket \texttt{lars} berechnet werden können. Hierfür erzeugen wir
Daten gemäß der Vorschrift \begin{align}
  \begin{split}
  Y_i =&\, \boldsymbol{X}_i' \boldsymbol{\beta}_v + u_i\\
  \\
  \boldsymbol{\beta}_v =&\, (-1.25, -.75, 0, 0, 0, 0, 0, .75, 1.25)'\\
  \\
  \boldsymbol{X}_i \sim&\, N(\boldsymbol{0}, \boldsymbol{I}_{9\times9}), \quad u_i \overset{u.i.v.}{\sim} N(0, 1), \quad i = 1,\dots,25.
  \end{split}\label{eq:larsdgp}
\end{align}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lars)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}

\CommentTok{\# Parameter definieren}
\NormalTok{N }\OtherTok{\textless{}{-}} \DecValTok{25}
\NormalTok{beta\_v }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{1.25}\NormalTok{, }\SpecialCharTok{{-}}\NormalTok{.}\DecValTok{75}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, .}\DecValTok{75}\NormalTok{, }\FloatTok{1.25}\NormalTok{)}

\CommentTok{\# Beobachtungen simulieren}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(N }\SpecialCharTok{*} \DecValTok{9}\NormalTok{), }\AttributeTok{ncol =} \DecValTok{9}\NormalTok{)}
\NormalTok{Y }\OtherTok{\textless{}{-}}\NormalTok{ X }\SpecialCharTok{\%*\%}\NormalTok{ beta\_v }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(N)}
\end{Highlighting}
\end{Shaded}

Entsprechend des DGP passen wir ein Modell ohne Konstante an. Damit
\texttt{lars::lars()} den Lösungspfad des Lasso-Schätzers berechnet,
muss \texttt{type\ =\ "lasso"} gewählt werden.\footnote{\texttt{lars()}
  standardisiert die Regressoren standardmäßig (aufgrund des DGPs hier
  nicht nötig).}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Lösungen des Lasso{-}Schätzers mit LARS berechnen}
\NormalTok{(}
\NormalTok{  fit\_lars }\OtherTok{\textless{}{-}} \FunctionTok{lars}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ X, }
    \AttributeTok{y =}\NormalTok{ Y, }
    \AttributeTok{intercept =}\NormalTok{ F,}
    \AttributeTok{type =} \StringTok{"lasso"} \CommentTok{\# Wichtig: Lasso{-}Modus}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lars(x = X, y = Y, type = "lasso", intercept = F)
R-squared: 0.858 
Sequence of LASSO moves:
                      
Var  9 2 8 1 3 5 4 7 6
Step 1 2 3 4 5 6 7 8 9
\end{verbatim}

Die Zusammenfassung zeigt, dass der LARS-Algorithmus als erstes die
(relevante) Variable \(X_9\) aktiviert.\footnote{Aktivierung meint die
  Aufnahme einer Variable in der Modell gegeben eines hinreichend
  kleinen \(\lambda\).} Mit abnehmender Regularisierung (kleinere
\(\lambda\)) werden in den nächsten 3 Schritten die übrigen relevanten
Variablen \(X_2\), \(X_8\) und \(X_1\) aktiviert. Über die weiteren
Schritte nähert der Algorithmus die Lösung an die \emph{saturierte}
Schätzung (das Modell mit allen neun Regressoren) an und aktiviert
schrittweise die übrigen, irrelevanten Variablen.

Wir visualisieren die geschätzen Koeffizienten an jedem Schritt des
Lösungspfads als Funktion von \(\lambda\). In der Praxis wird der
Regularisierungsparameter häufig auf der natürlichen log-Skala
dargestellt.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Transformation in ein weites Format}
\NormalTok{fit\_lars}\SpecialCharTok{$}\NormalTok{beta }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{lambda =} \FunctionTok{c}\NormalTok{(fit\_lars}\SpecialCharTok{$}\NormalTok{lambda, }\FloatTok{1e{-}2}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}
    \AttributeTok{cols =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{9}\NormalTok{, }
    \AttributeTok{names\_to =} \StringTok{"Variable"}\NormalTok{, }
    \AttributeTok{values\_to =} \StringTok{"gesch. Koeffizient"}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%} 
  
\CommentTok{\# Visualisierung mit ggplot  }
  \FunctionTok{ggplot}\NormalTok{(}
    \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}
      \AttributeTok{x =} \FunctionTok{log}\NormalTok{(lambda), }
      \AttributeTok{y =} \StringTok{\textasciigrave{}}\AttributeTok{gesch. Koeffizient}\StringTok{\textasciigrave{}}\NormalTok{, }
      \AttributeTok{color =}\NormalTok{ Variable}
\NormalTok{    )}
\NormalTok{  ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{RegReg_files/figure-pdf/fig-larssolpath-1.pdf}

}

\caption{\label{fig-larssolpath}LARS-Lösungspfad für Lasso-Schätzung}

\end{figure}

Abbildung~\ref{fig-larssolpath} zeigt, dass die Shrinkage der
geschätzten Koeffizienten nach der Aktivierung rasch abnimmt und sich
für kleine Werte von \(\lambda\) der KQ-Lösung annähert. Wir sehen auch,
dass es einen Bereich von \(\lambda\)-Werten gibt, für die das wahre
Modell mit den Variablen \(X_1\), \(X_2\), \(X_8\) und \(X_9\)
selektiert werden kann. Je nach Ziel der Analyse kann es sinnvoll sein,
ein \(\lambda\) in diesem Intervall zu schätzen.

\hypertarget{wahl-des-regularisierungsparameters-lambda-fuxfcr-den-lasso-schuxe4tzer}{%
\subsection{\texorpdfstring{Wahl des Regularisierungsparameters
\(\lambda\) für den
Lasso-Schätzer}{Wahl des Regularisierungsparameters \textbackslash lambda für den Lasso-Schätzer}}\label{wahl-des-regularisierungsparameters-lambda-fuxfcr-den-lasso-schuxe4tzer}}

Wie zuvor bei Ridge Regression muss in empirischen Anwendungen ein Wert
für den Tuning-Parameter \(\lambda\) gewählt werden. Hierbei besteht die
Herausforderung darin, einen geeigneten Wert zu finden, der zu
wünschenswerten Eigenschaften des resultierenden Modells führt. So ist
für gute Vorhersagen wichtig, dass das Modell nicht zu sehr an die Daten
angepasst ist (\emph{Overfitting}), um eine gute Generalisierung auf
neue Daten zu ermöglichen. Gleichzeitig muss das Modell flexibel genug
sein, um wesentliche Eigenschaften des datenerzeugenden Prozesses
hinreichend gut zu erfassen. In der Regel wird hierbei eine sparsame
Modellierung angestrebt, die nur eine Teilmenge der Prädiktoren nutzt.

In der Praxis werden verschiedene Verfahren verwendet, um den Wert für
den Tuning-Parameter \(\lambda\) zu bestimmen. Gängige Methoden sind
\emph{Cross Validation} (CV) und Informationskriterien. In Abhängigkeit
der Methode und der Daten ergeben sich ober- oder unterparameterisierte
Modelle. Aufgrund der Implementierung im R-Paket \texttt{lars}
betrachten wir CV.\footnote{Chetverikov, Liao, and Chernozhukov (2020)
  zeigen, dass CV zu konsistenter Modellselektion führen kann.} Wir
zeigen nachfolgend anhand der simulierten Daten aus dem letzten
Abschnitt, wie für die LARS-Schätzung ein optimales \(\lambda\) mit
leave-one-out CV (LOO-CV) bestimmt werden kann. Hierzu nutzen wir
\texttt{lars::cv.lars()} unter Verwendung derselben Argumente wie zuvor
im Aufruf von \texttt{lars()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# LARS{-}Lösungen mit CV evaluieren}
\NormalTok{fit\_lars\_cv }\OtherTok{\textless{}{-}} \FunctionTok{cv.lars}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ X, }
  \AttributeTok{y =}\NormalTok{ Y, }
  \AttributeTok{intercept =}\NormalTok{ F,}
  \AttributeTok{normalize =}\NormalTok{ T,}
  \AttributeTok{type =} \StringTok{"lasso"}\NormalTok{, }
  \AttributeTok{plot.it =}\NormalTok{ F, }
  \AttributeTok{K =}\NormalTok{ N }\CommentTok{\# für LOO{-}CV}
\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Das Objekt \texttt{fit\_lars\_cv} ist eine Liste mit den CV-Ergebnissen.
Wir können diese einfach mit \texttt{ggplot} visualisieren.
\texttt{index} ist hierbei das Verhältnis der \(\ell_1\)-Norm des
Lasso-Schätzers für einen spezifischen Wert von \(\lambda\) und der
\(\ell_1\)-Norm des KQ-Schätzers. Das optimale \(\lambda\) wird so
implizit geschätzt. \texttt{cv.error} ist der mit CV geschätzte MSE.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# CV{-}MSE}
\NormalTok{fit\_lars\_cv }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}

  \FunctionTok{ggplot}\NormalTok{(}
    \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}
      \AttributeTok{x =}\NormalTok{ index, }
      \AttributeTok{y =}\NormalTok{ cv.error}
\NormalTok{    )}
\NormalTok{  ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"|beta\_lambda| / |beta|"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"CV{-}MSE"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{RegReg_files/figure-pdf/fig-larscv-1.pdf}

}

\caption{\label{fig-larscv}CV-MSE und relative Position von \(\lambda\)
auf dem Lassopfad}

\end{figure}

In der Grafik erkennen wir ein Minimum des CV-MSEs bei etwa 0.73.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# CV{-}MSE{-}minimierendes Lambda bestimmen}
\NormalTok{ID }\OtherTok{\textless{}{-}} \FunctionTok{which.min}\NormalTok{(fit\_lars\_cv}\SpecialCharTok{$}\NormalTok{cv.error) }\CommentTok{\# Index}

\NormalTok{(}
\NormalTok{  fraction\_opt }\OtherTok{\textless{}{-}}\NormalTok{ fit\_lars\_cv}\SpecialCharTok{$}\NormalTok{index[ID]}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.7272727
\end{verbatim}

Die geschätzten Koeffizienten für die optimale Regularisierung können
mit \texttt{coef()} ausgelesen werden.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# LARS{-}Lasso{-}Fit für optimales lambda bestimmen}
\FunctionTok{coef}\NormalTok{(}
  \AttributeTok{object =}\NormalTok{ fit\_lars, }
  \AttributeTok{s =}\NormalTok{ fraction\_opt, }
  \AttributeTok{mode =} \StringTok{"fraction"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -0.6513191 -0.6060906 -0.1946089  0.0000000  0.0000000  0.0000000  0.0000000
[8]  0.4977908  1.3122407
\end{verbatim}

Das Ergebnis veranschaulicht die Selektionseigenschaft von Lasso: Gemäß
DGP \eqref{eq:larsdgp} sind die Variablen \(X_3\) bis \(X_7\)
\emph{irrelevante} Prädiktoren für \(Y\); ihre wahren Koeffizienten sind
\(0\). In der kreuzvalidierten Lasso-Schätzung erreicht die
Regularisierung, dass die Koeffizienten der Variablen \(X_4\) bis
\(X_7\) tatsächlich mit 0 geschätzt werden. Wir schätzen für das mit CV
bestimmte \(\lambda\) also ein leicht überspezifiziertes Modell mit den
Regressoren \(X_1\), \(X_2\), \(X_3\), \(X_8\) und \(X_9\). Beachte,
dass die Lasso-Schätzung einen Kompromiss impliziert: Die Varianz der
Schätzung ist geringer als die des KQ-Schätzers im Modell mit allen
Variablen.\footnote{Wegen \(N=25\) verbleiben bei der KQ-Schätzung mit 9
  Regressoren nur 16 Freiheitsgrade.} Aufgrund der Regularisierung sind
die mit Lasso geschätzten Koeffizienten der relevanten Variablen jedoch
in Richtung \(0\) verzerrt.

Einen positiven Effekt dieses Kompromisses beobachten wir anhand des
mittleren Vorhersagefehlers für Daten, die \emph{nicht} zur Berechnung
des Schätzers verwendet wurden. Wir vergleichen den Vorhersagefehler
nachfolgend anhand eines solchen simulierten Test-Datensatzes mit 25
neuen Beobachtungen. Den Vorhersagefehler bestimmen wir als MSE zwischen
den vorhergesagten und den tatsächlichen Ausprägungen für \(Y\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Test{-}Datensatz erstellen}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{4321}\NormalTok{)}
\NormalTok{new\_X }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(N }\SpecialCharTok{*} \DecValTok{9}\NormalTok{), }\AttributeTok{ncol =} \DecValTok{9}\NormalTok{)}
\NormalTok{new\_Y }\OtherTok{\textless{}{-}}\NormalTok{ new\_X }\SpecialCharTok{\%*\%}\NormalTok{ beta\_v }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(N)}

\CommentTok{\# Lasso: Vorhersage von new\_Y für Test{-}Datensatz}
\NormalTok{Y\_predict\_lars }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(}
  \AttributeTok{object =}\NormalTok{ fit\_lars, }
  \AttributeTok{s =}\NormalTok{ fraction\_opt, }
  \AttributeTok{type =} \StringTok{"fit"}\NormalTok{, }
  \AttributeTok{mode =} \StringTok{"fraction"}\NormalTok{, }
  \AttributeTok{newx =}\NormalTok{ new\_X}
\NormalTok{)}\SpecialCharTok{$}\NormalTok{fit}

\CommentTok{\# Lasso: MSE für Test{-}Datensatz berechnen}
\FunctionTok{mean}\NormalTok{((Y\_predict\_lars }\SpecialCharTok{{-}}\NormalTok{ new\_Y)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.419817
\end{verbatim}

Wir schätzen nun das große Modell mit allen 9 Variablen mit KQ und
berechnen ebenfalls den MSE der Prognosen für den Test-Datensatz.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# KQ{-}Schätzung des großen Modells durchführen}
\NormalTok{KQ\_fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}

\CommentTok{\# Test{-}Datensatz für predict.lm() formatieren}
\NormalTok{new\_X }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(new\_X)}
\FunctionTok{colnames}\NormalTok{(new\_X) }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"X"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{9}\NormalTok{)}

\CommentTok{\# KQ: Vorhersage von new\_Y für Test{-}Datensatz}
\NormalTok{Y\_predict\_KQ }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(}
  \AttributeTok{object =}\NormalTok{ KQ\_fit, }
  \AttributeTok{newdata =}\NormalTok{ new\_X}
\NormalTok{)}

\CommentTok{\# KQ: MSE für Test{-}Datensatz berechnen}
\FunctionTok{mean}\NormalTok{((Y\_predict\_KQ }\SpecialCharTok{{-}}\NormalTok{ new\_Y)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 9.851932
\end{verbatim}

Offenbar führt die Lasso-Schätzung zu einem deutlich geringeren MSE der
Vorhersage von \texttt{Y} für den Test-Datensatz als die KQ-Schätzung
und damit zu einer höheren Vorhersagegüte. Das ``sparsame'' mit
Lasso-Regression geschätzte Modell ist dem ``großen'' mit KQ geschätztem
Modell in dieser Hinsicht also überlegen.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, bottomtitle=1mm, arc=.35mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Key Facts zu Lasso-Regression}, opacitybacktitle=0.6, colback=white, left=2mm, breakable, leftrule=.75mm, opacityback=0, coltitle=black, titlerule=0mm, colframe=quarto-callout-note-color-frame, toprule=.15mm, toptitle=1mm]

\begin{itemize}
\item
  Lasso-Regression bestraft die Verlustfunktion des KQ-Schätzers mit der
  \(\ell_1\)-Norm der Koeffizienten.
\item
  Neben Koeffizientenschätzung mit Shrinkage in Richtung \(0\) kann der
  Lasso-Schätzer Variablenselektion durchführen:
  Regressionskoeffizienten können exakt mit \(0\) geschätzt und so ein
  ``sparsames'', leichter zu interpretierendes Modell gewählt werden.
\item
  Wie bei Ridge Regression impliziert die Wahl von \(\lambda\) einen
  Bias-Variance-Tradeoff, der für Vorhersagen nützlich ist: Für größere
  \(\lambda\) wird mehr Verzerrung induziert und möglicherweise
  relevante Variablen mit kleinen Koeffizienten aus dem Modell entfernt.
  Ein solches sparsames Modell kann eine höhere Prognosegüte haben als
  ein komplexes, unregularisiertes Modell.
\item
  Der Lasso-Schätzer \(\widehat{\boldsymbol{\beta}}_\lambda^L\) ist
  \emph{nicht} erwartungstreu.
\item
  Lasso Regression kann bspw. mit dem LARS-Algorithmus (Paket
  \texttt{lars}) oder mit \texttt{glmnet} berechnet werden.
\end{itemize}

\end{tcolorbox}

\hypertarget{vergleich-von-lasso--und-ridge-regression-mit-simulation}{%
\section{Vergleich von Lasso- und Ridge-Regression mit
Simulation}\label{vergleich-von-lasso--und-ridge-regression-mit-simulation}}

In diesem Kapitel illustrieren wir Vor- und Nachteile von Lasso- und
Ridge-Regression in Prognose-Anwendungen anhand von
Monte-Carlo-Simulationen. Wir betrachten hierbei datenerzeugende
Prozesse, die sich hinsichtlich der Anzahl relevanter Variablen sowie
der Korrelation dieser Variablen unterscheiden.

Die grundlegende Vorschrift für die Simulationen ist \begin{align*}
  Y_i = \sum_{j=1}^{k=40} \beta_j X_{i,j} + u_i, \quad u_i \overset{u.i.v.}{\sim} N(0,1), \quad i=1,\dots,100,
\end{align*} wobei die Regressoren \(X_j\) eine Varianz von \(1\) haben
und aus einer multivariaten Normalverteilung mit Korrelation
\[\rho\in(0,0.5,0.8)\] gezogen werden.

Für die Koeffizienten \(\boldsymbol{\beta}\) unterscheiden wir zwei
Szenarien. In Szenario A ist \[\boldsymbol{\beta} = (1,\dots,1)',\] d.h.
alle Variablen sind relevant und haben denselben Einfluss auf \(Y\). In
Szenario B erzeugen wir \(\boldsymbol{\beta}\) einmalig vorab so, dass
\[\beta_j = \begin{cases}1,\quad \text{mit Wsk.  }p\\ 0,\quad \text{mit Wsk.  }1-p, \end{cases}\]
d.h. nur eine Teilmenge der Variablen beeinflusst \(Y\) jeweils mit
demselben Effekt \(\beta_j = 1\). Die übrigen Variablen sind irrelevant.

Wir schätzen und validieren die Modelle mit \texttt{glmnet()}.

\hypertarget{sec-pdz}{%
\subsection{Prognosegüte in diversen Szenarien}\label{sec-pdz}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simulationsparameter definieren}
\NormalTok{rho }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.8}\NormalTok{)   }\CommentTok{\# Korrelation}
\NormalTok{k }\OtherTok{\textless{}{-}} \DecValTok{40}                 \CommentTok{\# Anz. Regressoren}
\NormalTok{N }\OtherTok{\textless{}{-}} \DecValTok{100}                \CommentTok{\# Anz. Beobachtungen}
\NormalTok{n\_sim }\OtherTok{\textless{}{-}} \DecValTok{100}            \CommentTok{\# Anz. Simulationen}
\end{Highlighting}
\end{Shaded}

Damit der Code für die Simulation möglichst wenig repetitiv ist,
definieren wir eine Funktion \texttt{cv.glmnet\_MSE()}, die unter Angabe
der Daten \texttt{X} und \texttt{Y}, des Trainingssets \texttt{train}
sowie des Parameters \texttt{alpha} den gewünschten regularisierten
Schätzer under Verwendung von Cross Validation anpasst und den
Testset-MSE zurückgibt.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# allg. Funktion für Testset{-}MSE nach CV}
\NormalTok{cv.glmnet\_MSE }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(X, Y, train, alpha) \{}
  
  \CommentTok{\# Modell mit glmnet schätzen; lambda per CV bestimmen}
\NormalTok{  fit\_cv }\OtherTok{\textless{}{-}} \FunctionTok{cv.glmnet}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ X[train,],}
    \AttributeTok{y =}\NormalTok{Y[train],}
    \AttributeTok{alpha =}\NormalTok{ alpha}
\NormalTok{  )}
  
  \CommentTok{\# Vorhersagen treffen}
\NormalTok{  Y\_pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(}
    \AttributeTok{object =}\NormalTok{ fit\_cv, }
    \AttributeTok{s =}\NormalTok{ fit\_cv}\SpecialCharTok{$}\NormalTok{lambda.min, }
    \AttributeTok{newx =}\NormalTok{ X[}\SpecialCharTok{{-}}\NormalTok{train,])}
  
  \FunctionTok{return}\NormalTok{(}
    \CommentTok{\# Testset{-}MSE berechnen}
    \FunctionTok{mean}\NormalTok{(}
\NormalTok{      (Y[}\SpecialCharTok{{-}}\NormalTok{train] }\SpecialCharTok{{-}}\NormalTok{ Y\_pred)}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{      )}
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Wir initialisieren zunächst Matrizen, in welche die MSEs aus den 100
Simulationsdurchläufen reihenweise geschrieben werden.
\texttt{lasso\_mse} und \texttt{ridge\_mse} haben je eine Spalte für
jede Korrelation in \texttt{rho}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Matrizen für simulierte MSEs initialisieren...}
\NormalTok{lasso\_mse }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}
  \AttributeTok{data =} \ConstantTok{NA}\NormalTok{, }
  \AttributeTok{nrow =}\NormalTok{ n\_sim, }
  \AttributeTok{ncol =} \FunctionTok{length}\NormalTok{(rho)}
\NormalTok{) }
\NormalTok{ridge\_mse }\OtherTok{\textless{}{-}}\NormalTok{ lasso\_mse}

\CommentTok{\# ... und benennen}
\FunctionTok{colnames}\NormalTok{(lasso\_mse) }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"Kor="}\NormalTok{, rho)}
\FunctionTok{colnames}\NormalTok{(ridge\_mse) }\OtherTok{\textless{}{-}} \FunctionTok{colnames}\NormalTok{(lasso\_mse)}
\end{Highlighting}
\end{Shaded}

Für die Simulation iterieren wir mit \texttt{purrr::walk} über den
Vektor \texttt{rho} sowie über die Laufvariable \texttt{1:n\_sim}. Beide
Schleifen nutzen den Syntax für anonyme Funktionen:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Die anonyme Funktion}
\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{return}\NormalTok{(x)}
\CommentTok{\# ist äquivalent definiert als}
\NormalTok{\textbackslash{}(x) }\FunctionTok{return}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

In jeden Simulationsdurchlauf erzeugen wir den Datensatz entsprechend
der obigen Vorschrift, teilen die Daten auf und berechnen MSEs für
Lasso- und Ridge-Regression mit \texttt{cv.glmnet\_MSE()}.

\textbf{Szenario A}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Koeffizienten{-}Vektor definieren}
\NormalTok{beta }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, k) }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mvtnorm)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}

\CommentTok{\# Simulation durchführen}
\FunctionTok{walk}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(rho), \textbackslash{}(j) \{}
  
  \CommentTok{\# Korrelationsmatrix definieren}
\NormalTok{  Sigma }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ rho[j], }
    \AttributeTok{nrow =}\NormalTok{ k, }
    \AttributeTok{ncol =}\NormalTok{ k}
\NormalTok{  )}
  \FunctionTok{diag}\NormalTok{(Sigma) }\OtherTok{\textless{}{-}} \DecValTok{1}
  
  \FunctionTok{walk}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_sim, \textbackslash{}(i) \{}
    
  \CommentTok{\# Daten simulieren}
\NormalTok{  X }\OtherTok{\textless{}{-}} \FunctionTok{rmvnorm}\NormalTok{(}
    \AttributeTok{n =}\NormalTok{ N, }
    \AttributeTok{mean =} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, k), }
    \AttributeTok{sigma =}\NormalTok{ Sigma}
\NormalTok{  )}
\NormalTok{  Y }\OtherTok{\textless{}{-}}\NormalTok{ X }\SpecialCharTok{\%*\%}\NormalTok{ beta }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(N)}
    
  \CommentTok{\# Trainingsdaten definieren}
\NormalTok{  ID\_train }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}
    \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{N), }
    \AttributeTok{size =}\NormalTok{ N}\SpecialCharTok{/}\DecValTok{2}
\NormalTok{  )}
    
  \CommentTok{\# Modelle mit CV schätzen und MSEs berechnen}
  \CommentTok{\# Ridge{-}Regression}
\NormalTok{  ridge\_mse[i, j] }\OtherTok{\textless{}\textless{}{-}} \FunctionTok{cv.glmnet\_MSE}\NormalTok{(}
    \AttributeTok{X =}\NormalTok{ X, }
    \AttributeTok{Y =}\NormalTok{ Y, }
    \AttributeTok{train =}\NormalTok{ ID\_train, }
    \AttributeTok{alpha =} \DecValTok{0}
\NormalTok{  )}
  
  \CommentTok{\# Lasso{-}Regression}
\NormalTok{  lasso\_mse[i, j] }\OtherTok{\textless{}\textless{}{-}} \FunctionTok{cv.glmnet\_MSE}\NormalTok{(}
    \AttributeTok{X =}\NormalTok{ X, }
    \AttributeTok{Y =}\NormalTok{ Y, }
    \AttributeTok{train =}\NormalTok{ ID\_train, }
    \AttributeTok{alpha =} \DecValTok{1}
\NormalTok{  )}
  
\NormalTok{  \})}
  
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

Beachte, dass hier der Super-Assignment-Operator
\texttt{\textless{}\textless{}-} genutzt wird, damit \texttt{walk} die
Matrizen \texttt{ridge\_mse} und \texttt{lasso\_mse} in der globalen
Umgebung überschreibt.\footnote{Dies folgt aus der Definition von
  \texttt{walk}. \texttt{\textless{}-} bewirkt hier lediglich Assignment
  in der Funktionsumgebung.}

Wir berechnen jeweils den mittleren MSEs, sammeln die Ergebnisse in
einer \texttt{tibble()} und nutzen \texttt{gt()} für die tabellarische
Darstellung.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gt)}

\CommentTok{\# Ergebnisse tabellarisch darstellen}
\FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{Methode =} \FunctionTok{c}\NormalTok{(}
    \StringTok{"Lasso{-}Regression"}\NormalTok{, }
    \StringTok{"Ridge{-}Regression"}
\NormalTok{  ),}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_cols}\NormalTok{(}
    \FunctionTok{bind\_rows}\NormalTok{(}
      \FunctionTok{colMeans}\NormalTok{(lasso\_mse),}
      \FunctionTok{colMeans}\NormalTok{(ridge\_mse)  }
\NormalTok{    )    }
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{gt}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  tabopts}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-lrsimA}{}
\begin{longtable}{lrrr}
\caption{\label{tbl-lrsimA}Durchschnittliche Testset-MSEs für Setting A }\tabularnewline

\toprule
Methode & Kor=0 & Kor=0.5 & Kor=0.8 \\ 
\midrule\addlinespace[2.5pt]
Lasso-Regression & $7.17$ & $10.398$ & $7.581$ \\ 
Ridge-Regression & $4.841$ & $1.615$ & $1.517$ \\ 
\bottomrule
\end{longtable}

Tabelle~\ref{tbl-lrsimA} zeigt, dass Ridge-Regression gegenüber
Lasso-Regression für jede der drei betrachteten Korrelationen überlegen
ist. Insbesondere bei stärker korrelierten Regressoren ist Ridge
vorteilhaft.

Für Szenario B überschreiben wir \texttt{beta} nach Multiplikation mit
einem zufälligen binären Vektor, sodass einige der Koeffizienten \(0\)
und die zugehörigen Variablen irrelevant für \(Y\) sind.

\textbf{Szenario B}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Wsk. für Relevanz einer Variable}
\NormalTok{p }\OtherTok{\textless{}{-}}\NormalTok{ .}\DecValTok{3}

\CommentTok{\# Koeffizienten{-}Vektor definieren}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{beta }\OtherTok{\textless{}{-}}\NormalTok{ beta }\SpecialCharTok{*} \FunctionTok{sample}\NormalTok{(}
  \AttributeTok{x =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{, }
  \AttributeTok{size =}\NormalTok{ k, }
  \AttributeTok{replace =}\NormalTok{ T, }
  \AttributeTok{prob =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{p, p)}
\NormalTok{)}

\CommentTok{\# Koeffizienten prüfen}
\FunctionTok{head}\NormalTok{(beta, }\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] 0 1 0 1 1 0 0 1 0 0
\end{verbatim}

Eine wiederholung der Simulation für die modifizierten Koeffizienten
\texttt{beta} und liefert folgende tabellarische Auswertung.

\hypertarget{tbl-lrsimB}{}
\begin{longtable}{lrrr}
\caption{\label{tbl-lrsimB}Durchschnittliche Testset-MSEs für Szenario B }\tabularnewline

\toprule
Methode & Kor=0 & Kor=0.5 & Kor=0.8 \\ 
\midrule\addlinespace[2.5pt]
Lasso & $2.51$ & $2.143$ & $1.923$ \\ 
Ridge & $3.331$ & $2.562$ & $2.014$ \\ 
\bottomrule
\end{longtable}

Die Ergebnisse in Tabelle~\ref{tbl-lrsimB} zeigen, dass Ridge-Regression
in Szenario B bis auf den Fall unkorrelierter Regressoren etwas
schlechter abschneidet als in Szenario A. Die hohe Anzahl irrelevanter
Variablen verbessert die Leistung von Lasso deutlich: Hier ist es
plausibel, dass Lasso aufgrund der Thresholding-Eigenschaft die
Koeffizienten einiger irrelevanten Variablen häufig exakt \(0\) setzt
und damit ein sparsameres Modell schätzt als Ridge. Entsprechend erzielt
Lasso in diesem Szenario insbesondere für \(\rho = 0\) genauere
Vorhersagen als Ridge Regression.

\hypertarget{visualisierung-des-bias-variance-tradeoffs-bei-prognosen}{%
\subsection{Visualisierung des Bias-Variance-Tradeoffs bei
Prognosen}\label{visualisierung-des-bias-variance-tradeoffs-bei-prognosen}}

Für ein besseres Verständnis, wie sich der Regularisierungsparameter
\(\lambda\) auf den Bias-Variance-Tradeoff bei Prognosen mit Ridge- und
Lasso-Regression auswirkt, vergleichen wir für beide Methoden
nachfolgend die Abhängigkeit des MSEs der Prognose \(\widehat{Y}_0\) für
den Wert \(Y_0\) der abhängigen Variable eines Datenpunkts anhand seiner
Regressoren \(\boldsymbol{X}_0'\), wobei \begin{align}
  \text{MSE}(\widehat{Y}_0) = \text{Bias}(\widehat{Y}_0)^2 + \text{Var}(\widehat{Y}_0) + \text{Var}(Y_0) \label{eq:pbvdecomp}
\end{align} Beachte, dass \(\text{Var}(Y_0)\) die durch den
datenerzeugenden Prozess (und damit unvermeidbare) Varianz von \(Y_0\)
ist, wohingegen \(\text{Bias}(\widehat{Y}_0)^2\) und
\(\text{Var}(\widehat{Y}_0)\) von dem verwendeten Schätzer für
\(\widehat{Y}_0\) abhängt.

Für die Simulation betrachten wir erneut Szenario A aus
Kapitel~\ref{sec-pdz} mit \(50\) Beobachtungen für ein Modell mit \(40\)
unkorrelierten Regressoren. Wir legen zunächst die Simulationsparameter
fest und erzeugen den vorherzusagenden Datenpunkt (\texttt{X\_0},
\texttt{Y\_0}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Parameter festlegen}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{200} \CommentTok{\# Anz. Iterationen}
\NormalTok{N }\OtherTok{\textless{}{-}} \DecValTok{50}  \CommentTok{\# Anz. Beobachtungen}
\NormalTok{k }\OtherTok{\textless{}{-}} \DecValTok{40}  \CommentTok{\# Anz. Variablen}

\CommentTok{\# Korrelationsmatrix definieren}
\NormalTok{Sigma }\OtherTok{\textless{}{-}} \FunctionTok{diag}\NormalTok{(k) }\CommentTok{\# Diagonalmatrix}
\NormalTok{beta }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \DecValTok{1}\NormalTok{, k)}

\CommentTok{\# Prognose{-}Ziel vorab zufällig generieren:}

\CommentTok{\# Regressoren}
\NormalTok{X\_0 }\OtherTok{\textless{}{-}} \FunctionTok{rmvnorm}\NormalTok{(}
  \AttributeTok{n =} \DecValTok{1}\NormalTok{, }
  \AttributeTok{mean =} \FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \DecValTok{0}\NormalTok{, k)}
\NormalTok{)}

\CommentTok{\# Abh. Variable}
\NormalTok{Y\_0 }\OtherTok{\textless{}{-}}\NormalTok{ X\_0 }\SpecialCharTok{\%*\%}\NormalTok{ beta }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as.vector}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Anhand der Simulationsergebnisse wollen wir die von der verwendeten
Schätzfunktion abhängigen Komponenten von \eqref{eq:pbvdecomp}
untersuchen. Wir initialisieren hierzu die Listen \texttt{ridge\_fits}
und \texttt{lasso\_fits}, in die unsere Simulationsergebnisse
geschrieben werden.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Listen für Simulationsergebnisse initialisieren}
\NormalTok{ridge\_fits }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\NormalTok{lasso\_fits }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Weiterhin definieren wir separate \(\lambda\)-Sequenzen für Lasso- und
Ridge-Schätzer.\footnote{Die Sequenzen haben wir in Abhängigkeit des DGP
  so gewählt, dass die Abhängigkeit der Prognosegüte von \(\lambda\) gut
  visualisiert werden kann.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Lambda{-}Sequenzen festlegen}
\NormalTok{lambdas\_r }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(.}\DecValTok{25}\NormalTok{, }\FloatTok{2.5}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{lambdas\_l }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(.}\DecValTok{05}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Für die Simulation iterieren wir mit \texttt{walk()} über simulierte
Datensätze und schreiben jeweils den vollständigen Output von
\texttt{glmnet()} in die zuvor definierten Listen \texttt{ridge\_fits}
und \texttt{lasso\_fits}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simulation}
\FunctionTok{walk}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, \textbackslash{}(i) \{}
  
  \CommentTok{\# Daten simulieren}
\NormalTok{  X }\OtherTok{\textless{}{-}} \FunctionTok{rmvnorm}\NormalTok{(}
    \AttributeTok{n =}\NormalTok{ N, }
    \AttributeTok{mean =} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, k), }
    \AttributeTok{sigma =}\NormalTok{ Sigma}
\NormalTok{  )}
\NormalTok{  Y }\OtherTok{\textless{}{-}}\NormalTok{ X }\SpecialCharTok{\%*\%}\NormalTok{ beta }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =}\NormalTok{ N, }\AttributeTok{sd =} \DecValTok{5}\NormalTok{)}
  
  \CommentTok{\# Modelle mit glmnet schätzen}
  \CommentTok{\# Ridge{-}Regression}
\NormalTok{  ridge\_fits[[i]] }\OtherTok{\textless{}\textless{}{-}} \FunctionTok{glmnet}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ X, }
    \AttributeTok{y =}\NormalTok{ Y, }
    \AttributeTok{alpha =} \DecValTok{0}\NormalTok{, }
    \AttributeTok{intercept =}\NormalTok{ F}
\NormalTok{  )}
  \CommentTok{\# Lasso{-}Regression}
\NormalTok{  lasso\_fits[[i]] }\OtherTok{\textless{}\textless{}{-}} \FunctionTok{glmnet}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ X, }
    \AttributeTok{y =}\NormalTok{ Y, }
    \AttributeTok{alpha =} \DecValTok{1}\NormalTok{, }
    \AttributeTok{intercept =}\NormalTok{ F}
\NormalTok{  )}
  
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

Wir nutzen Funktionen aus \texttt{purrr} und \texttt{dplyr}, um über die
in den Simulationsdurchläufen angepassten Modelle zu iterieren. Mit
\texttt{predict()} erhalten wir Punktvorhersagen für \texttt{Y\_0} für
jedes \(\lambda\) der zuvor definierten \(\lambda\)-Sequenzen. Beachte,
dass \texttt{map()} jeweils eine Liste mit 200 Punktvorhersagen für
jedes der 100 zurückgibt. Mit \texttt{list\_rbind()} können wir die
Ergebnisse komfortabel jeweils in einer \texttt{tibble} sammeln.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Prognosen für Ridge{-}Regression}
\NormalTok{pred\_r }\OtherTok{\textless{}{-}} \FunctionTok{map}\NormalTok{(}
  \AttributeTok{.x =}\NormalTok{ ridge\_fits, }
  \AttributeTok{.f =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{as\_tibble}\NormalTok{(}
    \FunctionTok{predict}\NormalTok{(}
      \AttributeTok{object =}\NormalTok{ ., }
      \AttributeTok{s =}\NormalTok{ lambdas\_r, }
      \AttributeTok{newx =}\NormalTok{ X\_0}
\NormalTok{    )}
\NormalTok{  ) }
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{list\_rbind}\NormalTok{() }

\CommentTok{\# Prognosen für Lasso{-}Regression}
\NormalTok{pred\_l }\OtherTok{\textless{}{-}} \FunctionTok{map}\NormalTok{(}
  \AttributeTok{.x =}\NormalTok{ lasso\_fits, }
  \AttributeTok{.f =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{as\_tibble}\NormalTok{(}
    \FunctionTok{predict}\NormalTok{(}
      \AttributeTok{object =}\NormalTok{ ., }
      \AttributeTok{s =}\NormalTok{ lambdas\_l, }
      \AttributeTok{newx =}\NormalTok{ X\_0)}
\NormalTok{    ) }
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{list\_rbind}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

Für die statistische Auswertung berechnen wir jeweils
\(\text{MSE}(\widehat{Y}_0)\), \(\text{Bias}(\widehat{Y}_0)^2\) und
\(\text{Var}(\widehat{Y}_0)\) und führen die Ergebnisse mit
\texttt{pivot\_longer()} in ein langes Format \texttt{sim\_data\_r}
über. Wir berechnen weiterhin mit \texttt{MSE\_min\_r} das \(\lambda\),
für das wir über die Simulationsdurchläufe durchschnittlich den
geringsten \(\text{MSE}\) beobachten.

\textbf{Ridge-Regression}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Ergebnisse für Ridge{-}Regression zusammenfassen}
\NormalTok{sim\_data\_r }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  
  \AttributeTok{lambda =}\NormalTok{ lambdas\_r,}
  
  \StringTok{"MSE"} \OtherTok{=} \FunctionTok{map\_dbl}\NormalTok{(}
    \AttributeTok{.x =}\NormalTok{ pred\_r,  }
    \AttributeTok{.f =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{mean}\NormalTok{((.x }\SpecialCharTok{{-}}\NormalTok{ Y\_0)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{  ),}
  
  \StringTok{"Bias\^{}2"} \OtherTok{=} \FunctionTok{map\_dbl}\NormalTok{(}
    \AttributeTok{.x =}\NormalTok{ pred\_r, }
    \AttributeTok{.f =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ (}\FunctionTok{mean}\NormalTok{(.x) }\SpecialCharTok{{-}}\NormalTok{ Y\_0)}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{  ),}
  
  \StringTok{"Varianz"} \OtherTok{=} \FunctionTok{map\_dbl}\NormalTok{(}
    \AttributeTok{.x =}\NormalTok{ pred\_r, }
    \AttributeTok{.f =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{var}\NormalTok{(.x)}
\NormalTok{  )}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}
    \AttributeTok{cols =} \SpecialCharTok{{-}}\NormalTok{lambda, }
    \AttributeTok{values\_to =} \StringTok{"Wert"}\NormalTok{,}
    \AttributeTok{names\_to =} \StringTok{"Statistik"}
\NormalTok{  )}

\CommentTok{\# Lambda bei MSE{-}Minimum bestimmen}
\NormalTok{MSE\_min\_r }\OtherTok{\textless{}{-}}\NormalTok{ sim\_data\_r }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}
\NormalTok{    Statistik }\SpecialCharTok{==} \StringTok{"MSE"}\NormalTok{,}
\NormalTok{    Wert }\SpecialCharTok{==} \FunctionTok{min}\NormalTok{(Wert)}
\NormalTok{  ) }
\end{Highlighting}
\end{Shaded}

\textbf{Lasso-Regression}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Ergebnisse zusammenfassen}
\NormalTok{sim\_data\_l }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  
  \AttributeTok{lambda =}\NormalTok{ lambdas\_l,}
  
  \StringTok{"MSE"} \OtherTok{=} \FunctionTok{map\_dbl}\NormalTok{(}
    \AttributeTok{.x =}\NormalTok{ pred\_l,  }
    \AttributeTok{.f =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{mean}\NormalTok{((. }\SpecialCharTok{{-}}\NormalTok{ Y\_0)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{  ),}
  
  \StringTok{"Bias\^{}2"} \OtherTok{=} \FunctionTok{map\_dbl}\NormalTok{(}
    \AttributeTok{.x =}\NormalTok{ pred\_l, }
    \AttributeTok{.f =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ (}\FunctionTok{mean}\NormalTok{(.) }\SpecialCharTok{{-}}\NormalTok{ Y\_0)}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{  ),}
  
  \StringTok{"Varianz"} \OtherTok{=} \FunctionTok{map\_dbl}\NormalTok{(}
    \AttributeTok{.x =}\NormalTok{ pred\_l, }
    \AttributeTok{.f =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{var}\NormalTok{(.)}
\NormalTok{  )}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}
    \AttributeTok{cols =} \SpecialCharTok{{-}}\NormalTok{lambda, }
    \AttributeTok{values\_to =} \StringTok{"Wert"}\NormalTok{, }
    \AttributeTok{names\_to =} \StringTok{"Statistik"}
\NormalTok{  )}

\CommentTok{\# Lambda bei MSE{-}Minimum bestimmen}
\NormalTok{MSE\_min\_l }\OtherTok{\textless{}{-}}\NormalTok{ sim\_data\_l }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}
\NormalTok{    Statistik }\SpecialCharTok{==} \StringTok{"MSE"}\NormalTok{,}
\NormalTok{    Wert }\SpecialCharTok{==} \FunctionTok{min}\NormalTok{(Wert)}
\NormalTok{  ) }
\end{Highlighting}
\end{Shaded}

Die Datensätze im langen Format, \texttt{sim\_data\_r} und
\texttt{sim\_data\_l}, werden nun für die Visualisierung der Ergebnisse
mit \texttt{ggplo2} genutzt.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# MSE, Bias\^{}2 und Varianz gegen Lambda plotten}

\CommentTok{\# Ridge{-}Regression}
\NormalTok{sim\_data\_r }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}
    \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}
      \AttributeTok{x =}\NormalTok{ lambda, }
      \AttributeTok{y =}\NormalTok{ Wert, }
      \AttributeTok{color =}\NormalTok{ Statistik}
\NormalTok{    )}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ MSE\_min\_r)}

\CommentTok{\# Lasso{-}Regression}
\NormalTok{sim\_data\_l }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}
    \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}
      \AttributeTok{x =}\NormalTok{ lambda, }
      \AttributeTok{y =}\NormalTok{ Wert, }
      \AttributeTok{color =}\NormalTok{ Statistik}
\NormalTok{    )}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ MSE\_min\_l)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}[t]{\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{RegReg_files/figure-pdf/fig-MSEBVT-1.pdf}

}

}

\subcaption{\label{fig-MSEBVT-1}Ridge Regression}
\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{RegReg_files/figure-pdf/fig-MSEBVT-2.pdf}

}

}

\subcaption{\label{fig-MSEBVT-2}Lasso Regression}
\end{minipage}%

\caption{\label{fig-MSEBVT}Simulierte MSE-Komponenten in Abhängigkeit
von Lambda}

\end{figure}

Anhand von Abbildung~\ref{fig-MSEBVT} lässt sich der
Bias-Variance-Tradeoff bei der Vorhersage von \(Y_0\) gut erkennen:
Bereits für kleine \(\lambda\) erzielen beide Methode eine deutliche
Reduktion des MSE. Dies wir durch etwas zusätzlichen Bias, aber eine
überproportionale Verringerung der Varianz erreicht. Der erkennbare
funktionale Zusammenhang zeigt, dass der MSE eine konvexe Funktion von
\(\lambda\) ist. Damit existieren optimale \(\lambda\) mit minimalem MSE
(grüne Punkte), die wir mit Cross Validation schätzen können.

\hypertarget{inferenz-fuxfcr-treatment-effekt-schuxe4tzung-mit-vielen-variablen}{%
\section{Inferenz für Treatment-Effekt-Schätzung mit vielen
Variablen}\label{inferenz-fuxfcr-treatment-effekt-schuxe4tzung-mit-vielen-variablen}}

In empirischen Studien des Effekts einer Behandlungsvariable \(B\) auf
eine Outcome-Variable \(Y\) steht häufig eine Vielzahl potentieller
Kontrollvariablen zur Verfügung. Häufig ist unklar, welche Variablen in
das Modell aufgenommen werden sollten, um das Risiko einer verzerrten
Schätzung durch ausgelassene Variablen zu vermindern und gleichzeitig
eine Schätzung mit geringer Varianz zu gewährleisten. Ist der
Beobachtungsumfang \(N\) relativ zur Variablenanzahl \(k\) groß, so kann
die KQ-Schätzung einer langen Regression (ein Modell mit allen \(k\)
Kontrollvariablen) gute Ergebnisse liefern. In der Praxis liegt diese
wünschenswerte Situation jedoch oft nicht vor und es ist \(k\lesssim N\)
oder sogar \(k>N\). Dann ist eine KQ-Schätzung des Behandlungseffekts
anhand aller \(k\) Variablen mit hoher Varianz behaftet bzw. gar nicht
möglich.\footnote{Beachte, dass der KQ-Schätzer bei \(k>N\) nicht lösbar
  ist.} Ein weiteres Szenario ist \(k(N)>N\), d.h. die Anzahl der
Regressoren kann mit dem Beobachtungsumfang wachsen.\footnote{Dieses
  Szenario wird unter Bedingungen bzgl. der Wachstumsrate und der Größe
  der Koeffizienten betrachet, s. (Belloni und Chernozhukov 2013).}
Lasso-Verfahren können dann hilfreich sein, um Determinanten von \(Y\)
\emph{und} \(B\) zu identifizieren und damit eine Menge an
Kontrollvariablen zu selektieren, für die eine erwartungstreue und
konsistente Schätzung des interessierenden Effekts wahrscheinlich ist.

Betrachte zunächst das Modell mit allen Kontrollvariablen \(X_j\),
\begin{align}
  Y_i = \beta_0 + \alpha_0 B_i + \sum_{j=1}^k \beta_{j} X_{i,j} + u_i, \label{eq:lassotmt}
\end{align} wobei einige \(\beta_{j}=0\) sind und wir annehmen, dass
\(B\) lediglich mit ein paar der \(X_j\) korrelliert. Die Shrinkage der
geschätzten Koeffizienten aus einer naiven Lasso-Regression von
\eqref{eq:lassotmt} führt grundsätzlich zu einer verzerrten Schätzung
des Behandlungseffekts \(\alpha_0\) und damit zu ungültiger
Inferenz.\footnote{Hahn u.~a. (2018) geben eine ausführliche Erläuterung
  dieser Problematik.}

Die Verzerrung von geschätzten Koeffizienten kann vermieden werden,
indem Lasso lediglich zur Selektion von Kontrollvariablen verwendet
wird. Dabei wird mit einer Lasso-Regression von \(Y\) auf die \(X_j\)
eine Teilmenge von Regressoren \(\mathcal{S}\) selektiert und der
Treatment-Effekt anschließend mit der KQ-Schätzung von \begin{align}
  Y_i = \beta_0 + \alpha_0 B_i + \sum_{j\in\mathcal{S}} \beta_{j} X_{i,j} + e_i,
\end{align} basierend auf der Selektion \(\mathcal{S}\) berechnet
wird.\footnote{Solche Verfahren werden \emph{Post-Selection-Schätzer}
  gennant.} Ein solcher \emph{Post-Lasso-Selection-Schätzer} (Belloni
und Chernozhukov 2013) ist jedoch im Allgemeinen und insbesondere in
hoch-dimensionalen Settings nicht konsistent für \(\alpha_0\) und nicht
asymptotisch normalverteilt, da weiterhin die Gefahr einer verzerrten
Schätzung durch in \(\mathcal{S}\) ausgelassene Variablen besteht, die
mit \(B\) korrelieren: Lasso selektiert Variablen \(X_j\), die ``gut''
\(Y\) erklären. Dabei kann nicht ausgeschlossen werden, das ein Modell
gewählt wird, dass relevante Determinanten von \(B\) auslässt. Selbst
wenn wir ein mit Lasso gewähltes Modell mit KQ (d.h. ohne Shrinkage)
schätzen, würde \(\alpha_0\) verzerrt geschätzt!

Belloni, Chernozhukov, und Hansen (2014) schlagen ein alternatives
Verfahren vor, dass auf Selektion der Determinanten \(X_j\) von \(Y\)
und \(B\) basiert. Dieses Verfahren wird als \emph{Post-Double
Selection} bezeichnet und kann wiefolgt implementiert werden:

\textbf{Post-Double-Selection-Schätzer}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Bestimme die Determinanten \(X_j\) von \(Y\) mit Lasso-Regression und
  bezeichne die Menge der selektierten Variablen als \(\mathcal{S}_Y\).
\item
  Bestimme die Determinanten \(X_j\) von \(B\) mit Lasso-Regression und
  bezeichne die Menge der selektierten Variablen als \(\mathcal{S}_B\).
\item
  Bestimme die Schnittmenge
  \(\mathcal{S}_{YB} = \mathcal{S}_Y \cap \mathcal{S}_B\). Schätze den
  Treatment-Effekt als \(\widehat{\alpha}_0\) in der KQ-Regression
  \begin{align}
    Y_i = \beta_0 + \alpha_0 B_i + \sum_{j\in\mathcal{S}_{YB}} \beta_{j} X_{i,j} + v_i.
  \end{align}
\end{enumerate}

Belloni, Chernozhukov, und Hansen (2014) zeigen, dass
\(\widehat{\alpha}_0\) aus diesem Verfahren ein asymptotisch
normalverteiler Schätzer für \(\alpha_0\) ist und herkömmliche t-Tests
und Konfidenzintervalle gültige Inferenz erlauben.

Wir illustrieren die in diesem Abschnitt betrachteten Schätzer nun
anhand simulierter Daten mit R. Die fiktive Problemstellung ist die
Schätzung eines wahren Treatment-Effekts \(\alpha_0 = 2\), wenn so viele
potenzielle Kontrollvariablen vorliegen, dass der KQ-Schätzer gerade
noch berechnet werden kann, aber aufgrund hoher Varianz unzuverlässig
ist. Hierzu erzeugen wir \(Y\) gemäß der Vorschrift \begin{align*}
  Y_i =&\, \alpha_0 B_i + \sum_{j=1}^{k_Y} \beta_{j}^Y X_{i,j}^Y + \sum_{l=1}^{k_{YB}} \beta_{l}^{YB} X_{i,l}^{YB} + u_i,\\
  \\
  \beta_j^{YB} \overset{u.i.v}{\sim}&\,N(10,1), \quad \beta_j^{Y} \overset{u.i.v}{\sim}U(0,1), \quad u_i \overset{u.i.v}{\sim}N(0,1).\\
  \\
  i=&\,1,\dots,550
\end{align*}

Die Behandlungsvariable \(B_i\) entspricht der Vorschrift \begin{align*}
  B_i =&\, \sum_{l=1}^{k_{YB}} \beta_{l}^{YB} X_{i,l}^{YB} + e_i,\\
  \\
  \beta_j^{YB} \overset{u.i.v}{\sim}&\,N(2,0.2), \quad e_i \overset{u.i.v}{\sim}N(0,1).
\end{align*} Wir wählen \(k_{YB} = k_{Y} = 25\). Zusätzlich zu \(B\),
den Determinanten von \(Y\) \emph{und} \(B\) (\(X^{YB}\)) sowie den
Variablen, die ausschließlich \(Y\) beeinflussen (\(X^{Y}\)) gibt es
\(k_U = 499\) Variablen \(X^U\), die weder \(Y\) noch \(B\) beeinflussen
und damit irrelevant für die Schätzung des Behandlungseffekts sind. Wir
haben also \(N=550\) Beobachtungen und insgesamt
\(k = 1+k_{Y} + k_{YB} + k_{U} = 550\) potenzielle Kontrollvariablen von
denen \(k_{YB} = 25\) für eine unverzerrte Schätzung von \(\alpha_0\)
relevant sind.

Der nachstehende Code generiert die Daten gemäß der Vorschrift.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mvtnorm)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{4321}\NormalTok{)}

\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{550}      \CommentTok{\# Beobachtungen}
\NormalTok{p\_Y }\OtherTok{\textless{}{-}} \DecValTok{25}     \CommentTok{\# Determinanten Y}
\NormalTok{p\_B }\OtherTok{\textless{}{-}} \DecValTok{25}     \CommentTok{\# Determinanten B *und* Y}
\NormalTok{p\_U }\OtherTok{\textless{}{-}} \DecValTok{499}    \CommentTok{\# irrelevante Variablen }

\CommentTok{\# Variablen generieren}
\NormalTok{XB }\OtherTok{\textless{}{-}} \FunctionTok{rmvnorm}\NormalTok{(}\AttributeTok{n =}\NormalTok{ n, }\AttributeTok{sigma =} \FunctionTok{diag}\NormalTok{(p\_B))}
\NormalTok{XU }\OtherTok{\textless{}{-}} \FunctionTok{rmvnorm}\NormalTok{(}\AttributeTok{n =}\NormalTok{ n, }\AttributeTok{sigma =} \FunctionTok{diag}\NormalTok{(p\_U))}
\NormalTok{XY }\OtherTok{\textless{}{-}} \FunctionTok{rmvnorm}\NormalTok{(}\AttributeTok{n =}\NormalTok{ n, }\AttributeTok{sigma =} \FunctionTok{diag}\NormalTok{(p\_Y))}

\CommentTok{\# Stetige Behandlungsvariable erzeugen}
\NormalTok{B }\OtherTok{\textless{}{-}}\NormalTok{ XB }\SpecialCharTok{\%*\%} \FunctionTok{rnorm}\NormalTok{(p\_B, }\DecValTok{2}\NormalTok{, }\AttributeTok{sd =}\NormalTok{ .}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\CommentTok{\# Abh. Variable erzeugen, Behandlungseffekt (ATE) ist 2}
\NormalTok{Y }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ B }\SpecialCharTok{+} 
\NormalTok{  XB }\SpecialCharTok{\%*\%} \FunctionTok{rnorm}\NormalTok{(p\_B, }\AttributeTok{mean =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{+} 
\NormalTok{  XY }\SpecialCharTok{\%*\%} \FunctionTok{runif}\NormalTok{(p\_Y) }\SpecialCharTok{+} 
  \FunctionTok{rnorm}\NormalTok{(n)}

\CommentTok{\# Variablen in tibble sammeln}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(B, XB, XU, XY) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as\_tibble}\NormalTok{()}

\CommentTok{\# Namen zuweisen}
\FunctionTok{colnames}\NormalTok{(X) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}
  \StringTok{"B"}\NormalTok{, }
  \FunctionTok{paste0}\NormalTok{(}\StringTok{"XB"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{p\_B), }
  \FunctionTok{paste0}\NormalTok{(}\StringTok{"XU"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{p\_U),}
  \FunctionTok{paste0}\NormalTok{(}\StringTok{"XY"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{p\_Y) }
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Wünschenswert wäre die KQ-Schätzung des wahren Modells. Diese ergibt
eine Schätzung nahe des wahren Treatment-Effekts \(\alpha_0 = 2\). Unter
realen Bedingungen wäre diese Regression jedoch nicht implementierbar,
weil die relevanten Kovariablen \texttt{XB} unbekannt sind.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# KQ: Wahres Modell schätzen}
\FunctionTok{lm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ B }\SpecialCharTok{+}\NormalTok{ XB }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"B"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       B 
1.937031 
\end{verbatim}

Wir schätzen daher zunächst die ``lange'' Regression mit allen \(k\)
verfügbaren Variablen mit KQ. Beachte, dass der KQ-Schätzer für
\(\alpha_0\) zwar implementierbar und erwartungstreu ist, jedoch eine
hohe Varianz aufweist. Wegen \(k=N=550\) erhalten wir eine perfekte
Anpassung an die Daten und können mangels Freiheitsgraden keine
Hypothesentests durchführen.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# KQ: Lange Regression schätzen}
\FunctionTok{lm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{, }\AttributeTok{data =}\NormalTok{ X)}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"B"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       B 
3.079497 
\end{verbatim}

Die KQ-Schätzung von \(\alpha_0\) anhand der langen Regression weicht
deutlich vom wahren Wert \(\alpha_0 = 2\) ab.

Eine ``kurze'' KQ-Regression nur mit der Behandlungsvariable \(B\) führt
wegen Korrelation mit den ausgelassenen Determinanten in \texttt{XB} zu
einer deutlich verzerrten Schätzung.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# KQ: Kurze Regression}
\FunctionTok{lm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ B }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"B"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       B 
6.716837 
\end{verbatim}

Die Methoden von Belloni und Chernozhukov (2013) und Belloni,
Chernozhukov, und Hansen (2014) sind im R-Paket \texttt{hdm}
implementiert. Mit den Funktionen \texttt{hrm::rlasso()} und
\texttt{hdm::rlassoEffect} kann Lasso-Regression sowie Post- und
Double-Post-Selection durchgeführt werden.\footnote{Diese Funktionen
  ermitteln ein optimales \(\lambda\) mit dem in Belloni u.~a. (2012)
  vorgeschlagenen Algorithmus.}

Wir berechnen zunächst den naiven Lasso-Schätzer in einem Modell mit
allen Variablen.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(hdm)}

\CommentTok{\# Naiver Post{-}Lasso{-}Schätzer}
\NormalTok{lasso }\OtherTok{\textless{}{-}} \FunctionTok{rlasso}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ X, }
  \AttributeTok{y =}\NormalTok{ Y, }
  \AttributeTok{intercept =}\NormalTok{ F, }
  \AttributeTok{post =}\NormalTok{ F}
\NormalTok{)}

\CommentTok{\# Koeffizientenschätzer auslesen}
\NormalTok{lasso}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"B"}\NormalTok{] }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       B 
6.368456 
\end{verbatim}

Auch dieser Schätzer ist deutlich verzerrt. Problematisch ist hier nicht
nur die Shrinkage auf \(\widehat{\alpha}_0\), sondern die Selektion der
Variablen in \texttt{XB}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Welche Variablen in XB selektiert Lasso *nicht*?}
\NormalTok{nselektiert }\OtherTok{\textless{}{-}} \FunctionTok{which}\NormalTok{(lasso}\SpecialCharTok{$}\NormalTok{coef[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{26}\NormalTok{] }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)   }\CommentTok{\# ID}

\CommentTok{\# Namen auslesen}
\FunctionTok{names}\NormalTok{(lasso}\SpecialCharTok{$}\NormalTok{coef[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{26}\NormalTok{])[nselektiert]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "XB8"  "XB10" "XB16" "XB18" "XB20"
\end{verbatim}

Durch das Auslassen dieser Determinanten von \(Y\) und \(B\) leidet der
Lasso-Schätzer unter OVB.

Als nächstes berechnen wir den Post-Lasso-Selection-Schätzer.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Post{-}Lasso{-}Selection{-}Schätzer berechnen}
\NormalTok{p\_lasso }\OtherTok{\textless{}{-}} \FunctionTok{rlasso}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ X,}
  \AttributeTok{y =}\NormalTok{ Y, }
  \AttributeTok{intercept =}\NormalTok{ F, }
  \AttributeTok{post =}\NormalTok{ T}
\NormalTok{)}

\CommentTok{\# Schätzung für alpha\_0}
\NormalTok{p\_lasso}\SpecialCharTok{$}\NormalTok{coef[}\StringTok{"B"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       B 
6.362409 
\end{verbatim}

Die Ähnlichkeit der Post-Lasso-Schätzung von \(\alpha_0\) zur
Lasso-Schätzung zeigt deutlich, dass die Verzerrung des Lasso-Schätzers
überwiegend durch ausgelassene Variablen anstatt durch Shrinkage
verursacht wird.

Mit \texttt{rlassoEffect()} können wir den
Post-Double-Selection-Schätzer berechnen.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Post{-}Double{-}Selection{-}Schätzer}
\NormalTok{pds\_lasso }\OtherTok{\textless{}{-}} \FunctionTok{rlassoEffect}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ X }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{B) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{as.matrix}\NormalTok{(),}
  \AttributeTok{y =}\NormalTok{ Y, }
  \AttributeTok{d =}\NormalTok{ B, }
  \AttributeTok{method =} \StringTok{"double selection"}
\NormalTok{)}

\CommentTok{\# Schnittmenge der selektierten Determinanten }
\CommentTok{\# von Y und B}
\NormalTok{(}
\NormalTok{  S\_BY }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(}
    \FunctionTok{which}\NormalTok{(pds\_lasso}\SpecialCharTok{$}\NormalTok{selection.index)}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "XB1"   "XB2"   "XB3"   "XB4"   "XB5"   "XB6"   "XB7"   "XB8"   "XB9"  
[10] "XB10"  "XB11"  "XB12"  "XB13"  "XB14"  "XB15"  "XB16"  "XB17"  "XB18" 
[19] "XB19"  "XB20"  "XB21"  "XB22"  "XB23"  "XB24"  "XB25"  "XU209" "XU241"
[28] "XU295" "XY3"   "XY7"   "XY8"   "XY12"  "XY13"  "XY15"  "XY16"  "XY19" 
[37] "XY23" 
\end{verbatim}

Double Selection führt ebenfalls zu einem Post-Lasso-KQ-Schätzer mit
allen 25 relevaten Variablen in \texttt{XB}. Wir selektieren allerdings
deutlich weniger irrelevante Variablen aus \texttt{XU} als mit Single
Selection und dennoch einige Determinanten von \(Y\) aus \texttt{XY}.
Double Selection führt also zu einer unverzerrten Schätzen mit
geringerer Varianz. Mit \texttt{summary()} erhalten wir gültige Inferenz
bzgl. des Treatment-Effekts.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(pds\_lasso)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Estimates and significance testing of the effect of target variables"
   Estimate. Std. Error t value Pr(>|t|)    
d1   1.94977    0.07127   27.36   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Der Post-Double-Selection-Schätzer liefert unter den betrachteten
Verfahren die beste Schätzung von \(\alpha_0\) und erlaubt gülstige
statistische Inferenz. Der geschätzte Effekt ist hoch-signifikant.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, bottomtitle=1mm, arc=.35mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Key Facts zum Post-Double-Selection-Schätzer}, opacitybacktitle=0.6, colback=white, left=2mm, breakable, leftrule=.75mm, opacityback=0, coltitle=black, titlerule=0mm, colframe=quarto-callout-note-color-frame, toprule=.15mm, toptitle=1mm]

\begin{itemize}
\item
  Durch die sorgfältige Auswahl von Variablen, die mit Behandlung- und
  Outcome-Variable zusammenhängen, ermöglicht die Double-Selection eine
  bessere Kontrolle über das Risiko ausgelassender Variablen in
  Beobachtungsstudien und ermöglicht gültige (asymptotisch normale)
  Inferenz.
\item
  Der Post-Double-Selection-Schätzer besteht aus drei Regressionen:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Es werden Variablen mit Lasso selektiert, welche die
    \emph{Behandlungs-Variable} erklären.
  \item
    Es werden Variablen mit Lasso selektiert, welche die
    \emph{Outcome-Variable} erklären.
  \item
    Der Post-Double-Selection-Schätzer ist der KQ-Schätzer in einer
    Regression, die für die Schnittmenge der ausgewählten Variablen
    kontrolliert.
  \end{enumerate}
\item
  Dank der Selektion mit Lasso kann der Schätzer auch bei
  hoch-dimensionalen Daten (\(k>n\)) angewendet werden.
\item
  Post-Double-Selection-Schätzer für Behandlungseffekte sind im R-Paket
  \texttt{hdm} implementiert.
\end{itemize}

\end{tcolorbox}

\hypertarget{case-study-makrouxf6konomisches-wachstum}{%
\subsection{Case Study: Makroökonomisches
Wachstum}\label{case-study-makrouxf6konomisches-wachstum}}

Zur Illustration des Post-Double-Selection Schätzers betrachten wir eine
empirische Anwendung bzgl. der Validierung von makroökonomischer
Wachstumtheorie. Aus neo-klassischen Ansätzen wie dem
\href{https://de.wikipedia.org/wiki/Solow-Modell}{Solow-Swan-Modell}
kann die Hypothese, dass Volkswirtschaften zu einem gemeinsamen
Wachstumspfad hin konvergieren, abgeleitet werden. Diese
Konvergenzhypothese impliziert die Existenz von Aufholeffekten: Ärmere
Volkswirtschaften müssen im mittel schneller Wachsen als die Wirschaft
wohlhabender Länder. Die grundlegende Spezifikation eines entsprechenden
Regressionsmodells lautet \begin{align}
  \text{WR}_{i} = \alpha_0 \text{BIP0}_i + u_i, \label{eq:growthmodel1}
\end{align} wobei \(\text{WR}_{i}\) die Wachstumsrate des Pro-Kopf-BIP
in Land \(i\) über einen Zeitraum (typischerweise berechnet als
Log-Differenz zwischen zwei Perioden) und \(\text{BIP0}_i\) das
(logarithmierte) Pro-Kopf-BIP zu beginn der Referenzperiode ist. Gemäß
der Konvergenzhypothese muss \(\alpha_0<0\) sein: Je wohlhabender eine
Volkswirtschaft ist, desto geringer ist das Wirtschaftswachstum.

Um Verzerrung durch ausgelassene Kovariablen zu vermeiden, sollte das
Modell \eqref{eq:growthmodel1} um länder-spezifische Regressoren
\(x_{i,j}\), die sowohl das Ausgagnsniveau \(\text{BIP0}\) sowie die
Wachtumsrate beinflussen, erweitert werden. Zu der großen Menge
potentieller Kovariablen gehören makro- und sozio-ökonomische Maße wie
bspw. die Investitionstätigkeit des Staates, Offenheit der
Volkswirtschaft, das politische Umfeld, das Bildungsniveau, die
Demographie usw. Eine bevorzugte Spezifikation ist daher \begin{align}
  \text{WR}_{i} = \alpha_0 \text{BIP0}_i + \sum_{j=1}^k \beta_j x_{i,j} + u_i,\label{eq:growthmodel2}
\end{align} wobei \(\alpha_0\) als Behandlungseffekt interpretiert
werden kann. Beachte, dass \eqref{eq:growthmodel2} eine Regression in
der Form von \eqref{eq:lassotmt} ist.

Wir illustrieren die Schätzung von und Inferenz bzgl. \(\alpha_0\) in
\eqref{eq:growthmodel2} mit Post-Double-Selektion für einen 90 Länder
umfassenden Auszug aus dem Datensatz von Barro und Lee (2013), der als
Objekt \texttt{GrowthData} im R-Paket \texttt{hdm} verfügbar
ist.\footnote{Eine ausführliche Beschreibung der Variablen ist
  \href{https://www2.nber.org/pub/barro.lee/readme.txt}{hier} einsehbar.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Datensatz in Arbeitsumgebung verfügbar machen}
\FunctionTok{library}\NormalTok{(hdm)}
\FunctionTok{data}\NormalTok{(GrowthData)}

\CommentTok{\# Anzahl Beobachtungen und Variablen}
\FunctionTok{dim}\NormalTok{(GrowthData)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 90 63
\end{verbatim}

Die Spalte \texttt{Outcome} ist die jeweilige Wachstumsrate des BIP
zwischen den Perioden 1965-1975 und 1975-1985 und \texttt{gdpsh465} ist
das reale Pro-Kopf-BIP im Jahr 1965 zu Preisen von 1980.

Wir führen zunächst eine graphische Analyse hinsichtlich des Modells
einfachen Modells \eqref{eq:growthmodel1} durch, indem wir
\texttt{gdpsh465} gegen \texttt{Outcome} plotten und die geschätzte
Regressionsgerade einzeichnen.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Einfache grafische Analyse mit ggplot2}
\NormalTok{GrowthData }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}
    \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}
      \AttributeTok{x =}\NormalTok{ gdpsh465, }
      \AttributeTok{y =}\NormalTok{ Outcome}
\NormalTok{    )}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{RegReg_files/figure-pdf/fig-bipsimple-1.pdf}

}

\caption{\label{fig-bipsimple}BIP-Wachstum: Einfache Regression}

\end{figure}

Abbildung~\ref{fig-bipsimple} zeigt einen geringen positiven geschätzten
Effekt \(\widehat{\alpha}_0\). Eine Auswertung mit \texttt{lm()} ergibt,
dass der Effekt \(\alpha_0\) nicht signifikant von \(0\) verschieden
ist.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Einfache Regression durchführen, }
\CommentTok{\# Inferenz für gdpsh465 erhalten}
\FunctionTok{lm}\NormalTok{(Outcome }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gdpsh465, }\AttributeTok{data =}\NormalTok{ GrowthData) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summary}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{coefficients}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  .[}\DecValTok{2}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   Estimate  Std. Error     t value    Pr(>|t|) 
0.001316713 0.006102200 0.215776701 0.829661165 
\end{verbatim}

Der positive Effekt aus der einfachen Schätzung widerspricht der
Konvergenzhypothese. Dieses Ergebnis könnte allerdings durch Auslassen
relevanter Kovariablen ungültig sein. Beispielsweise ist es plausibel,
dass das Bildungsniveau einer Volkswirtschaft sowohl mit dem BIP
korreliert ist als auch die Wachstumsrate beeinflusst. Dann wäre das
Bildungsniveau eine relevante Kovariable, deren Auslassen zu einer
verzerrten Schätzung von \(\alpha_0\) führt.

Eine ``lange'' Regression mit allen Kovariablen ist zwar möglich, aber
problematisch: Das Verhältnis von Beobachtungen (90) zu Regressoren (62)
bedeutet eine hohe Unsicherheit der Schätzung.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Inferenz für alpha\_0 in langer Regression}
\FunctionTok{summary}\NormalTok{(}
  \FunctionTok{lm}\NormalTok{(Outcome }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{ , }\AttributeTok{data =}\NormalTok{ GrowthData)}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{coefficients}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  .[}\DecValTok{2}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    Estimate   Std. Error      t value     Pr(>|t|) 
-0.009377989  0.029887726 -0.313773911  0.756018518 
\end{verbatim}

Der geschätzte Koeffizient \(\widehat{\alpha}_0\) ist nun zwar negativ,
liefert jedoch weiterhin keine Evidenz, dass \(\alpha_0\) von 0
verschieden ist. Ein Vergleich der Standardfehler zeigt aber, dass die
KQ-Schätzung aufgrund Berücksichtigung aller potentiellen Kovariablen
mit deutlich größerer Varianz behaftet ist als in der einfachen
KQ-Regression \eqref{eq:growthmodel1}

Post-Double-Selection erlaubt gültige Inferenz bzgl. \(\alpha_0\) nach
Schätzung der Menge relevanter Kovariablen. Wir weisen die
entsprechenden Variablen R-Objekten zu und berechnen den Schätzer.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Variablen für Post{-}Double{-}Selection vorbereiten}

\CommentTok{\# abh. Variable}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ GrowthData }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pull}\NormalTok{(Outcome)}

\CommentTok{\# "Treatment"}
\NormalTok{d }\OtherTok{\textless{}{-}}\NormalTok{ GrowthData }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pull}\NormalTok{(gdpsh465)}

\CommentTok{\# potentielle Regressoren}
\NormalTok{X }\OtherTok{\textless{}{-}}\NormalTok{ GrowthData }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}
    \SpecialCharTok{{-}}\NormalTok{Outcome, }\SpecialCharTok{{-}}\NormalTok{intercept, }\SpecialCharTok{{-}}\NormalTok{gdpsh465}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Post{-}Double{-}Selection{-}Schätzer berechnen}
\NormalTok{Growth\_DS }\OtherTok{\textless{}{-}} 
  \FunctionTok{rlassoEffect}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ X }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{as.matrix}\NormalTok{(), }
    \AttributeTok{y =}\NormalTok{ y, }
    \AttributeTok{d =}\NormalTok{ d, }
    \AttributeTok{method =} \StringTok{"double selection"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Post-Double-Selection wählt aus der Menge potentieller Kovariablen
lediglich sieben Regressoren aus.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Selektierte Variablen einsehen}
\CommentTok{\# ID}
\NormalTok{Selektion }\OtherTok{\textless{}{-}}\NormalTok{ Growth\_DS}\SpecialCharTok{$}\NormalTok{selection.index}

\CommentTok{\# Namen auslesen}
\FunctionTok{names}\NormalTok{(}
  \FunctionTok{which}\NormalTok{(Selektion }\SpecialCharTok{==}\NormalTok{ T)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "bmp1l"    "freetar"  "hm65"     "sf65"     "lifee065" "humanf65" "pop6565" 
\end{verbatim}

Tabelle~\ref{tbl-growthpdssek} zeigt die Definitionen der ausgewählten
Variablen.

\hypertarget{tbl-growthpdssek}{}
\begin{longtable}{ll}
\caption{\label{tbl-growthpdssek}Mit PDS selektierte Variablen aus \texttt{GrowthData}. Referenzjahr
1965. }\tabularnewline

\toprule
Variable & Beschreibung \\ 
\midrule\addlinespace[2.5pt]
bmp1l & Schwarzmarktprämie d. Währung \\ 
freetar & Maß für Zollbeschränkungen \\ 
hm65 & Einschreibungsquote Uni (Männer)  \\ 
sf65 & Beschulungsquote Sekundarstufe (Frauen) \\ 
lifee065 & Lebenserwartung bei Geburt \\ 
humanf65 & Durschn. Bildung im Alter 25 (Frauen) \\ 
pop6565 & Anteil Bevölkerung ü. 65 Jahre \\ 
\bottomrule
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Gültige Inferenz mit dem Post{-}Double{-}Selection{-}Schätzer}
\FunctionTok{summary}\NormalTok{(Growth\_DS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Estimates and significance testing of the effect of target variables"
   Estimate. Std. Error t value Pr(>|t|)   
d1  -0.05001    0.01579  -3.167  0.00154 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Das Ergebnis der Post-Double-Selection-Schätzung unterstützt die
(bedingte) Konvergenzhypothese mit einer signifikanten negativen
Schätzung \(\widehat{\alpha}_0\approx-0.05\).

\bookmarksetup{startatroot}

\hypertarget{literatur}{%
\chapter*{Literatur}\label{literatur}}
\addcontentsline{toc}{chapter}{Literatur}

\markboth{Literatur}{Literatur}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-AbadieImbens2008}{}}%
Abadie, Alberto, und Guido W. Imbens. 2008. {„On the Failure of the
Bootstrap for Matching Estimators.``} \emph{Econometrica. Journal of the
Econometric Society} 76 (6): 1537--57.
\url{https://doi.org/10.3982/ECTA6474}.

\leavevmode\vadjust pre{\hypertarget{ref-AbadieSpiess2022}{}}%
Abadie, Alberto, und Jann Spiess. 2022. {„Robust Post-Matching
Inference.``} \emph{Journal of the American Statistical Association} 117
(538): 983--95. \url{https://doi.org/10.1080/01621459.2020.1840383}.

\leavevmode\vadjust pre{\hypertarget{ref-Austin2011}{}}%
Austin, P. 2011. {„An Introduction to Propensity Score Methods for
Reducing the Effects of Confounding in Observational Studies``}.
\emph{Multivariate Behavioral Research} 46 (3): 399--424.
\url{https://doi.org/10.1080/00273171.2011.568786}.

\leavevmode\vadjust pre{\hypertarget{ref-AustinSmall2014}{}}%
Austin, Peter C., und Dylan S. Small. 2014. {„The use of bootstrapping
when using propensity-score matching without replacement: A simulation
study.``} \emph{Statistics in Medicine} 33 (24): 4306--19.
\url{https://doi.org/10.1002/sim.6276}.

\leavevmode\vadjust pre{\hypertarget{ref-AustinStuart2017}{}}%
Austin, Peter C., und Elizabeth A. Stuart. 2017. {„Estimating the Effect
of Treatment on Binary Outcomes Using Full Matching on the Propensity
Score.``} \emph{Statistical Methods in Medical Research} 26 (6):
2505--25. \url{https://doi.org/10.1177/0962280215601134}.

\leavevmode\vadjust pre{\hypertarget{ref-BarroLee2013}{}}%
Barro, Robert J., und Jong Wha Lee. 2013. {„A new data set of
educational attainment in the world, 1950--2010``}. \emph{Journal of
Development Economics} 104: 184--98.
https://doi.org/\url{https://doi.org/10.1016/j.jdeveco.2012.10.001}.

\leavevmode\vadjust pre{\hypertarget{ref-BastenBetz2013}{}}%
Basten, Christoph, und Frank Betz. 2013. {„Beyond work ethic: Religion,
individual, and political preferences``}. \emph{American Economic
Journal: Economic Policy} 5 (3): 67--91.

\leavevmode\vadjust pre{\hypertarget{ref-Bellonietal2012}{}}%
Belloni, Alexandre, Daniel Chen, Victor Chernozhukov, und Christian
Hansen. 2012. {„Sparse models and methods for optimal instruments with
an application to eminent domain``}. \emph{Econometrica} 80 (6):
2369--429.

\leavevmode\vadjust pre{\hypertarget{ref-BelloniChernozhukov2013}{}}%
Belloni, Alexandre, und Victor Chernozhukov. 2013. {„Least squares after
model selection in high-dimensional sparse models``}. \emph{Bernoulli},
521--47.

\leavevmode\vadjust pre{\hypertarget{ref-Bellonietal2014}{}}%
Belloni, Alexandre, Victor Chernozhukov, und Christian Hansen. 2014.
{„High-dimensional methods and inference on structural and treatment
effects``}. \emph{Journal of Economic Perspectives} 28 (2): 29--50.

\leavevmode\vadjust pre{\hypertarget{ref-Bodoryetal2020}{}}%
Bodory, Hugo, Lorenzo Camponovo, Martin Huber, und Michael Lechner.
2020. {„The Finite Sample Performance of Inference Methods for
Propensity Score Matching and Weighting Estimators.``} \emph{Journal of
Business \& Economic Statistics}.
\url{https://doi.org/10.2139/ssrn.2731969}.

\leavevmode\vadjust pre{\hypertarget{ref-CJM2020}{}}%
Cattaneo, Matias D, Michael Jansson, und Xinwei Ma. 2020. {„Simple local
polynomial density estimators``}. \emph{Journal of the American
Statistical Association} 115 (531): 1449--55.

\leavevmode\vadjust pre{\hypertarget{ref-CortezSilva2008}{}}%
Cortez, Paulo, und Alice Maria Gonçalves Silva. 2008. {„Using data
mining to predict secondary school student performance``}.

\leavevmode\vadjust pre{\hypertarget{ref-Efronetal2004}{}}%
Efron, Bradley, Trevor Hastie, Iain Johnstone, und Robert Tibshirani.
2004. {„Least angle regression``}.

\leavevmode\vadjust pre{\hypertarget{ref-GelmanImbens2019}{}}%
Gelman, Andrew, und Guido Imbens. 2019. {„Why high-order polynomials
should not be used in regression discontinuity designs``}. \emph{Journal
of Business \& Economic Statistics} 37 (3): 447--56.

\leavevmode\vadjust pre{\hypertarget{ref-Hahnetal2018}{}}%
Hahn, P Richard, Carlos M Carvalho, David Puelz, und Jingyu He. 2018.
{„Regularization and confounding in linear regression for treatment
effect estimation``}.

\leavevmode\vadjust pre{\hypertarget{ref-Hainmueller2012}{}}%
Hainmueller, Jens. 2012. {„Entropy Balancing for Causal Effects: A
Multivariate Reweighting Method to Produce Balanced Samples in
Observational Studies``}. \emph{Political Analysis} 20 (1): 25--46.
\url{https://doi.org/10.1093/pan/mpr025}.

\leavevmode\vadjust pre{\hypertarget{ref-Hajek1971}{}}%
Hájek, J. 1971. {„Comment on {‚An essay on the logical foundations of
survey sampling`} by Basu, D``}. \emph{Foundations of Statistical
Inference} 236.

\leavevmode\vadjust pre{\hypertarget{ref-HillReiter2006}{}}%
Hill, Jennifer, und Jerome P. Reiter. 2006. {„Interval estimation for
treatment effects using propensity score matching. Statistics in
Medicine``}. \emph{Statistics in Medicine} 25 (13): 2230--56.
\url{https://doi.org/10.1002/sim.2277}.

\leavevmode\vadjust pre{\hypertarget{ref-Hiranoetal2003}{}}%
Hirano, Keisuke, Guido Imbens, und Geert Ridder. 2003. {„Efficient
Estimation of Average Treatment Effects Using the Estimated Propensity
Score.``} \emph{Econometrica} 71 (4): 1161--89.
\url{https://doi.org/10.1111/1468-0262.00442}.

\leavevmode\vadjust pre{\hypertarget{ref-HoerlKennard1970}{}}%
Hoerl, Arthur E, und Robert W Kennard. 1970. {„{Ridge regression: Biased
estimation for nonorthogonal problems}``}. \emph{Technometrics} 12 (1):
55--67.

\leavevmode\vadjust pre{\hypertarget{ref-AbadieImbens2016}{}}%
Imbens. 2016. {„Matching on the Estimated Propensity Score.``}
\emph{Econometrica} 84 (2): 781--807.
\url{https://doi.org/10.3982/ecta11293}.

\leavevmode\vadjust pre{\hypertarget{ref-ImbensLemieux2008}{}}%
Imbens, G. W., und Thomas Lemieux. 2008. {„Regression discontinuity
designs: A guide to practice``}. \emph{Journal of econometrics} 142 (2):
615--35.

\leavevmode\vadjust pre{\hypertarget{ref-ImbensKalyanaraman2012}{}}%
Imbens, Guido, und Karthik Kalyanaraman. 2012. {„Optimal bandwidth
choice for the regression discontinuity estimator``}. \emph{The Review
of economic studies} 79 (3): 933--59.

\leavevmode\vadjust pre{\hypertarget{ref-Lee2008}{}}%
Lee, David S. 2008. {„Randomized experiments from non-random selection
in US House elections``}. \emph{Journal of Econometrics} 142 (2):
675--97.

\leavevmode\vadjust pre{\hypertarget{ref-Love2004}{}}%
Love, Thomas. 2004. {„Graphical display of covariate balance``}.
Presentation.

\leavevmode\vadjust pre{\hypertarget{ref-McCrary2008}{}}%
McCrary, Justin. 2008. {„Manipulation of the running variable in the
regression discontinuity design: A density test``}. \emph{Journal of
Econometrics} 142 (2): 698--714.

\leavevmode\vadjust pre{\hypertarget{ref-RosenbaumRubin1983}{}}%
Rosenbaum, Paul R., und Donald R. Rubin. 1983. {„The central role of the
propensity score in observational studies for causal effects``}.
\emph{Biometrika} 70 (1): 170--84.
\url{https://doi.org/10.1017/cbo9780511810725.016}.

\leavevmode\vadjust pre{\hypertarget{ref-Tibshirani1996}{}}%
Tibshirani, Robert. 1996. {„Regression shrinkage and selection via the
lasso``}. \emph{Journal of the Royal Statistical Society Series B:
Statistical Methodology} 58 (1): 267--88.

\leavevmode\vadjust pre{\hypertarget{ref-Weber2004}{}}%
Weber, Max. 2004. \emph{Die protestantische Ethik und der Geist des
Kapitalismus}. Bd. 1614. CH Beck.

\leavevmode\vadjust pre{\hypertarget{ref-Wooldrige2010}{}}%
Wooldridge, Jeffrey. 2010. \emph{Econometric Analysis of Cross Section
and Panel Data}. Second edition. Cambridge, Massachusetts: MIT.

\end{CSLReferences}



\end{document}
