---
webr: 
  show-startup-message: true
  packages: [
            'broom',
            'cowplot',
            'dplyr',
            'ggplot2',
            'palmerpenguins',
            'purrr',
            'tidyr',
            'readr'
            ]
  cell-options:
    editor-font-scale: .85
    autorun: true
    out-width: "80%"
---

# Regression {#sec-regression}

Viele der in diesem Companion behandelten Methoden basiert auf dem Konzept der Schätzung kausaler Effekte mit *Regression*. Als Regressionsansatz bezeichnet man eine Methode, welche die Beziehungen zwischen Variablen durch einen funktionalen Zusammenhang beschreibt und die Parameter der gewählten funktionalen Form anhand von beobachteten Daten schätzt. *Lineare Regression* nimmt eine lineare funktionale Form der Beziehung zwischen einer abhängigen Variable (Outcome-Varibale) und erklärenden Variablen (Regressoren) an. *Nicht-lineare Regressionsmethoden* modellieren die Beziehung etwa durch Polynome höherer Ordnung, exponentielle Funktionen oder andere komplexere Formen. Die Wahl der funktionalen Form hängt von der Natur des datenerzeugenden Prozesses (DGP) und somit stets von der spezifischen Beziehung ab, die untersucht wird. 

Regressionsansätze gehören zu den am häufigsten verwendeten Methoden für Kausalanalysen, da so in vielen Forschungsdesigns kausale Effekte identifiziert werden können, indem Backdoors geschlossen werden: Regression kann die durch die Behandlungsvariable verursachte Variation in der Outcome-Variable isolieren, indem für gemeinsame Einflussfaktoren von Behandlungs- *und* Outcome-Variable kontrolliert wird. In diesem Kapitel erläutern wir Grundlagen der Spezifikation und Schätzung von Regressionsansätzen, die für spätere Kapitel relevant sind. Neben einer Motivation der Schätzung kausaler Effekte mit multipler Regression betrachten wir Modelle für verschiedene Kategorien von Outcome-Variablen und diskutieren deren Implementierung mit R.

```{r}
library(tidyverse)
library(cowplot)
```

## Regression schließt Backdoors: Frish-Waugh-Lovell-Theorem

Das Frisch-Waugh-Lovell-Theorem (FWL) besagt, dass in einer multiplen linearen Regression die geschätzten Koeffizienten für eine Teilmenge der Regressoren numerisch identisch zu Koeffizientenschätzungen aus folgenden Schritten sind 

1. Rechne die Effekte der übrigen Variablen auf (a) die Outcome-Variable und (b) die Teilmenge der erklärenden Variablen mit Regression heraus und 

2. regressiere anschließend die Residuen von Schritt (a) auf die Residuen aus Schritt (b). 

In einem multiplen Modell mit zwei Regressoren $X_1,\ X_2$,
\begin{align}
  Y = \beta_0 + \beta_1 X + \beta_2 X_2 + \epsilon \label{eq:fwlfullreg}
\end{align}
kann der Effekt von $X_1$ auf $Y$ also mit der Regression
\begin{align*}
  \widehat{u}_{Y,X_2} = \beta_1 \widehat{u}_{X_1,X_2} + e \label{eq:fwl2reg}
\end{align*}
geschätzt werden, wobei $\widehat{u}_{Y,X_2}$ und $\widehat{u}_{X_1,X_2}$ die Residuen der Regression von $Y$ auf $X_2$ und von $X_1$ auf $X_2$ sind.

FWL ermöglicht daher eine Vereinfachung der Schätzung komplexer Modelle durch die Zerlegung der Schätzung in Teilschritte. 

Für das Verständnis der Schätzung kausaler Effekte mit linearer Regression ist FWL hilfreich, denn es zeigt, wie sowohl die Variation in der Outcome-Variable ($\widehat{u}_{Y,X_2}$) als auch die Variation in der Behandlungsvariable ($\widehat{u}_{X_1,X_2}$), die jeweils *nicht* durch Kovariablen ($X_2$) verursacht wird, mit multipler Regression isoliert werden kann, sodass Backdoors geschlossen werden.

Wir illustrieren dieses Konzept anhand einer multiplen Regression für die Schnabeltiefe (`body_mass`) von Pinguinen aus dem Datensatz `palmerpenguins::penguins`,

\begin{align}
  \textup{body\_mass} = \beta_0 + \beta_1\cdot\textup{bill\_length} + \beta_2\cdot \textup{flipper\_length} + \epsilon,\label{eq:billdepthmodel}
\end{align}
unter der Annahme, dass $\beta_1$ der interessierende Effekt ist: Die erwartete Änderung des Gewichts eines Pinguins (in Gramm) für eine Änderung der Schnabel-Länge um 1mm.

Vor der Schätzung von Modell \eqref{eq:billdepthmodel} lesen wir den Datensatz ein und erstellen eine bereinigte Variante `penguins_cleaned`, analog zur Vorgehensweise in @sec-pp.

```{webr-r}
library(palmerpenguins)
data(penguins)

# Datensatz bereinigen
penguins_cleaned <- penguins %>% 
  rename(
    bill_depth = bill_depth_mm,
    bill_length = bill_length_mm,
    flipper_length = flipper_length_mm,
    body_mass = body_mass_g
  ) %>% 
  drop_na() %>% 
  filter(
    body_mass < quantile(body_mass, .95)
  )

# Überblick
slice_head(penguins_cleaned, n = 10)
```

Wir schätzen nun Modell \eqref{eq:billdepthmodel} mit `lm()` und erhalten eine Zusammenfassung der geschätzen Koeffizienten mit `broom::tidy()`.

```{webr-r}
library(broom)

# "Großes" Modell schätzen
lm(
  formula = body_mass ~ bill_length + flipper_length, 
  data = penguins_cleaned
) %>%
  tidy()
```

Das Ergebnis der Schätzung ist $\widehat{\beta}_1\approx3.80$. Der nächste Code-Block berechnet die Residuen aus den Regressionen
\begin{align*}
  \textup{body\_mass} =&\, \alpha_0 + \alpha_1 \textup{flipper\_length} + u_{\textup{body\_mass},\,\textup{flipper\_length}},\\
  \textup{bill\_length} =&\, \delta_0 + \delta_1 \textup{flipper\_length} + u_{\textup{bill\_length},\,\textup{flipper\_length}},
\end{align*}

und speichert diese in `body_mass_res` und `bill_length_res`.

```{webr-r}
# FWL-Schritt 1 (a)
peng_mod_fwl1a <- lm(
  formula = body_mass ~ flipper_length, 
  data = penguins_cleaned
)

body_mass_res <- residuals(peng_mod_fwl1a)

# FWL-Schritt 1 (b)
peng_mod_fwl1b <- lm(
  formula = bill_length ~ flipper_length, 
  data = penguins_cleaned
)

bill_length_res <- residuals(peng_mod_fwl1b)
```

```{webr-r}
# FWL-Schritt 2
lm(formula = body_mass_res ~ bill_length_res - 1) %>% 
  tidy()
```

Der geschätzte Koeffizient aus der Regression der Residuen stimmt mit dem geschätzten Koeffizienten von `bill_length` aus der großen Regression \eqref{eq:billdepthmodel} überein.

Wir können den Effekt der Kontrolle für `flipper_length` visualisieren. Wir plotten hierzu:

- Die originalen Datenpunkte für `bill_length` und `body_mass`^[Für eine bessere Lesbarkeit der Grafik zentrieren wir beide Variablen um den jeweiligen Stichprobenmittelwert.] gemeinsam mit der geschätzten Regressionslinie für das Modell $$ \textup{body\_mass} = \beta_0 + \beta_1\textup{bill\_length} + u $$ (keine Kontrolle für `flipper_length`!)^[Der R-Befehl für diese Regression ist `lm(I(body_mass - mean(body_mass)) ~ I(bill_length - mean(bill_length)) - 1, data = penguins_cleaned)`.].
 
- Die um `flipper_length` bereinigten Datenpunkte und die zugehörige geschätzte Regressionslinie.

```{webr-r}
# Residuen in tibble sammeln
tibble(
  body_mass_res, 
  bill_length_res
) %>%
  
  ggplot() +
  # Bereinigte Datenpunkte plotten
  geom_point(
    mapping = aes(
      x = bill_length_res, 
      y = body_mass_res,
      color = "Um flipper_length bereinigte Daten"
    )
  ) +
  # Regressionslinie für bereinigte Datenpunkte
  geom_smooth(
    mapping = aes(x = bill_length_res, y = body_mass_res),
    method = "lm", 
    formula = "y ~ x - 1",
    se = F,
    col = "purple"
  ) +
  # Ursprüngliche Datenpunkte plotten
  geom_point(
    data = penguins_cleaned, 
    mapping = aes(
      x = bill_length - mean(bill_length), 
      y = body_mass - mean(body_mass),
      color = "Ursprüngliche Datenpunkte"
    ),
    alpha = .5,
  ) +
  # Regressionslinie für ursprüngliche Datenpunkte
  geom_smooth(
    data = penguins_cleaned, 
    mapping = aes(
      x = bill_length - mean(bill_length), 
      y = body_mass - mean(body_mass)
    ),
    method = "lm", 
    formula = "y ~ x - 1",
    se = F,
    col = alpha("black", .5)
  ) +
  scale_color_manual(
    name = "",
    values = c(
      "Um flipper_length bereinigte Daten" = "purple",
      "Ursprüngliche Datenpunkte" = "black"
    )
  ) +
  labs(
    title = "Penguins: Anwendung von FWL",
    x = "bill_length",
    y = "body_mass"
  ) +
  theme_cowplot() +
  theme(legend.position = "top")
```

Der grafische Vergleich beider Vorgehensweisen zeigt den Effekt der Kontrolle für `flipper_length`: Die geschätzte (schwarze) Regressionslinie für die bereinigten Daten hat eine deutlich geringere Steigung als die anhand der ursprünglichen Daten geschätzte (lilane) Linie. Der Effekt von `bill_length` auf `body_mass` wird mit der einfachen Regression `lm(body_mass ~ bill_length)` vermutlich *überschätzt*, weil es andere Faktoren (wie `flipper_length` gibt, die mit `bill_length` und `body_mass` korrelieren. Kontrollieren für `flipper_length` in der multiplen Regression `lm(body_mass ~ bill_length + flipper_length)` schließt die Backdoor durch `flipper_length`. Die Konsequenz ist eine deutlich geringere Steigung der lilanen Regressionslinie.

## Binäre Outcome-Variable

Eine binäre Variable, auch als dichotome Variable oder Indikator-Variable bezeichnet, ist eine Variable, die nur zwei Ausprägungen annehmen kann. Diese beiden Ausprägungen werden typischerweise durch die Werte 0 und 1 repräsentiert und dienen dazu, zwei verschiedene Zustände oder Kategorien zu unterscheiden. Formal kann eine binäre Variable $B$ wie folgt definiert werden:

\begin{align}
  B = \begin{cases} 
  1, & \text{Eigenschaft trifft zu,} \\
  0, & \text{Eigenschaft trifft nicht zu.}
  \end{cases}
\end{align}

Eine in späteren Kapiteln dieses Companions als verwendeter *binärer Regressor* ist der Indikator für die Zuordnung von Beobachtungen in Behandlungs- oder Kontrollgruppe (1 = Behandlungsgruppe, 0 = Kontrollgruppe). 

Für viele ökonomische Forschungsfragen ist es hilfreich, eine *binäre Outcome-Variable* mit Regression zu modellieren. Hierzu gibt es verschiedene Ansätze, die wir nachfolgend zusammenfassen und ihre Anwendung mit R zeigen.

### Das lineare Wahrscheinlichkeitsmodell {#sec-lpm}

Das lineare Regressionsmodell

$$Y = \beta_0 + \beta_1 X_{1} + \beta_2 X_{2} + \dots + \beta_k X_{k} + u$$
mit einer binären abhängigen Variablen $Y_i\in\{0,1\}$ wird als *lineares Wahrscheinlichkeitsmodell* bezeichnet. Wie üblich modellieren wir den Erwartungswert der abhängigen Variable gegeben der Regressoren $X_1,\dots,X_k$ als lineare Funktion,

$$E(Y\vert X_1,X_2,\dots,X_k) = P(Y=1\vert X_1, X_2,\dots, X_3).$$ Da $Y$ eine binäre Variable ist, gilt hier

$$ P(Y = 1 \vert X_1, X_2, \dots, X_k) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_k X_k.$$

Das lineare Wahrscheinlichkeitsmodell beschreibt also die *Wahrscheinlichkeit*, dass $Y=1$ als lineare Funktion der Regressoren: $\beta_j$ misst die Änderung in der Wahrscheinlichkeit für das Ereignis $Y_i=1$, unter der Bedingung, dass die anderen $k-1$ Regressoren konstant gehalten werden. Genau wie bei multipler Regression mit einer kontinuierlichen abhängigen Variablen können die $\beta_j$ mit der KQ-Methode geschätzt werden.

Aufgrund der Beschränktheit der $Y_i$ auf $\{0,1\}$ sind die $u_i$ heteroskedastisch. Folglich sollten Inferenzstatistiken mit robusten Standardfehlern berechnet werden. Weiterhin ist zu beachten, dass $R^2$ in den meisten Anwendungen von linearen Wahrscheinlichkeitsmodellen keine hilfreiche Interpretation hat, da das geschätzte Modell die Daten nicht perfekt erklären kann, wenn die abhängige Variable binär, aber die Regressoren kontinuierlich verteilt sind.

Das lineare Wahrscheinlichkeitsmodell hat einen wesentlichen Nachteil: Das Modell nimmt an, dass die bedingte Wahrscheinlichkeitsfunktion linear ist und $P(Y=1\vert X_1,\dots,X_k)$ nicht auf das für Wahrscheinlichkeiten definierte Intervall $[0,1]$ beschränkt ist. Ein angepasstes Modell hat dann für Regressorwerte, die zu Vorhersagen von $Y$ jenseits von $[0,1]$ führen keine sinnvolle Interpretation.

Dieser Umstand verlangt nach Regressionsansätzen, die $P(Y=1)$ durch eine auf $[0,1]$ beschränkte (nicht-lineare) Funktion der Regressoren modellieren. Häufig verwendete Methoden sind Probit- und Logit-Regression.

### Probit-Regression

Bei der Probit-Regression wird die Standardnormalverteilungsfunktion $\Phi(\cdot)$ verwendet, um die Regressionsfunktion bei einer binären abhängigen Variable zu modellieren. Wir nehmen an, dass
\begin{align}
  E(Y\vert X) = P(Y=1\vert X) = \Phi(\beta_0 + \beta_1 X). \label{eq:probitmodel}
\end{align}

$\beta_0 + \beta_1 X$ in \eqref{eq:probitmodel} ist hier ein *Quantil* $z$ der Standardnormalverteilung,
\begin{align}
\Phi(z) = P(Z \leq z) \ , \ Z \sim \mathcal{N}(0,1),
\end{align}
sodass der Koeffizient $\beta_1$ in \eqref{eq:probitmodel} die Änderung in $z$ misst, die mit einer Änderung von einer Einheit in $X$ verbunden ist. Obwohl der Effekt einer Änderung in $X$ auf $z$ linear ist, ist der Zusammenhang zwischen $z$ und der abhängigen Variable $Y$ *nicht linear*, denn $\Phi(\cdot)$ ist eine nicht-lineare Funktion von $X$ (vgl. @fig-snvf)!

```{webr-r}
# N(0,1)-Verteilungsfunktion
ggplot() +
  geom_function(fun = pnorm) +
  scale_x_continuous(
    name = "z", 
    limits = c(-4, 4)
  ) +
  scale_y_continuous(name = "P(Z<z)") +
  labs(title = "Verteilungsfunktion einer N(0,1)-Zufallsvariable") +
  theme_cowplot()
```

Aufgrund der Nicht-Linearität hat der Koeffizient von $X$ keine einfache Interpretation hinsichtlich des Effekts auf $Y$. Die Änderung in der Wahrscheinlichkeit, dass $Y=1$ ist, durch eine Änderung in $X$ (partieller Effekt) kann berechnet werden als:

\begin{align}
  \frac{\partial\textup{E}(Y\vert X)}{\partial X} = \frac{\partial\textup{P}(Y=1\vert X)}{\partial X} = \frac{\partial\Phi(\beta_0 + \beta_1 X)}{\partial X} = \phi(\beta_0 + \beta_1 X) \beta_1,
\end{align}
wobei $\phi(\cdot)$ die Dichtefunktion der Standardnormalverteilung ist. In empirischen Anwendungen wird der partielle Effekt häufig als Differenz in geschätzten Wahrscheinlichkeiten angegeben:

1. Berechne die geschätzte Wahrscheinlichkeit, dass $Y=1$ für einen Bezugswert $X$.
2. Berechne die geschätzte Wahrscheinlichkeit, dass $Y=1$ für $X + \Delta X$.
3. Berechne die Differenz zwischen der geschätzten Wahrscheinlichkeiten.

Wie im linearen Wahrscheinlichkeitsmodell kann das Modell \eqref{eq:probitmodel} auf eine Probit-Regression mit mehreren Regressoren $X_j$, $j=1,\dots,k$ verallgemeinert werden, um das Risiko einer Verzerrung durch ausgelassene Variablen zu mindern. Die Schritte 1 bis 3 für die Berechnung des partiellen Effekts einer Änderung in $X_j$ erfolgen dann unter der Annahme, dass die übrigen $k-1$ Regressoren konstant gehalten werden, wobei der partielle Effekt von den jeweiligen Regressorwerten abhängt. 

```{webr-r}
# Daten simulieren
set.seed(1234)

# Stichprobengröße
n <- 1000 

simdata <- tibble(
  X = rnorm(n = n, mean = 5, sd = 2), # Regressor
  P = pnorm(0.7 * X - 4 + rnorm(n)), # + Rauschen
)

# Binäre Outcome-Variable hinzufügen
simdata <- simdata %>%
  mutate(
    Y = as.integer(runif(n) < P)
    )
```

```{webr-r}
# lineares Wahrscheinlichkeitsmodell schätzen
mod_lp <- lm(formula = Y ~ X, data = simdata)
```

```{webr-r}
# Probit-Modell schätzen
mod_probit <- glm(
  formula = Y ~ X,
  data = simdata, 
  family = binomial(link = "probit")
)
```

```{webr-r}
# geschätzte Wahrscheinlichkeitsfunktion
# für lineares Modell
X <- seq(0, 11, 0.01)

pred <- tibble(
  X = X, 
  LP = predict(
    object = mod_lp, 
    newdata = tibble(X)
  ),
  probit = predict(
    object = mod_probit,
    newdata = tibble(X),
    type = "response"
  )
)
```

```{webr-r}
# geschätztes lineares Modell plotten
simdata %>%
  ggplot(mapping = aes(x = X, y = Y)) +
  geom_point(
    position = position_jitter(
      height = .025,
      seed = 1234
    ),
    alpha = .25,
    color = "blue"
  ) +
  geom_line(
    data = pred, 
    mapping = aes(y = LP),
    lwd = .75
  ) +
  geom_line(
    data = pred, 
    mapping = aes(y = probit),
    lwd = .75
  ) +
  lims(x = c(-2, 14)) +
  theme_cowplot()
```

### Logistische Regression

Bei der logistischen Regression wird die logistische Funktion $\Lambda(\cdot)$ 
\begin{align}
\Lambda(z) = \frac{1}{1 + \exp(-z)},
\end{align}
genutzt, um die Wahrscheinlichkeitsfunktion von $Y$ zu modellieren. Ähnlich wie im Probit-Modell nehmen wir hier an, dass
\begin{align}
E(Y\vert X) = P(Y=1\vert X) = \Lambda(\beta_0 + \beta_1 X). \label{eq:logitmodel}
\end{align}
In diesem Modell entspricht $\beta_0 + \beta_1 X$ dem sogenannten *Logit*, dem logarithmierten Verhältnis von $p = P(Y=1\vert X)$ und $1-p = P(Y=0\vert X)$,
\begin{align*}
  \textup{Logit(p)} = \log\bigg(\frac{p}{1-p}\bigg) = \beta_0 + \beta_1 X.
\end{align*}
wobei $\beta_1$ die Veränderung des Logits pro Einheit Änderung im Regressor $X$ angibt. Wie bereits im Probit-Fall ist der Einfluss von $X$ auf den Logit linear, jedoch besteht auch hier eine nicht-lineare Beziehung zwischen dem Logit und der Wahrscheinlichkeit $P(Y=1)$, da $\Lambda(\cdot)$, ähnlich wie die Normalverteilungsfunktion im Probit-Modell, eine nicht-lineare Funktion mit dem Wertebereich $[0,1]$ ist.

```{webr-r}
# Plot der logistischen Funktion
ggplot() +
  geom_function(fun = function(x) {1 / (1 + exp(-x))}) +
  scale_x_continuous(
    name = "Logit", 
    limits = c(-6, 6)
  ) +
  scale_y_continuous(name = "P(Y=1)") +
  labs(title = "Logistische Funktion") +
  theme_cowplot()
```

Aufgrund dieser Nicht-Linearität lässt sich der Koeffizient $\beta_1$ ebenso wie im Probit-Modell nicht direkt als einfacher Effekt auf die Wahrscheinlichkeit $P(Y=1)$ interpretieren. Um den partiellen Effekt einer Änderung in $X$ auf $P(Y=1)$ zu ermitteln, berechnet man die Ableitung:

\begin{align}
\frac{\partial\textup{E}(Y\vert X)}{\partial X} = \frac{\partial\textup{P}(Y=1\vert X)}{\partial X} = \frac{\partial\Lambda(\beta_0 + \beta_1 X)}{\partial X} = \lambda(\beta_0 + \beta_1 X) \beta_1,
\end{align}
wobei $\lambda(\cdot)$, ähnlich wie die Dichtefunktion der Normalverteilung im Probit-Modell, die Dichtefunktion der logistischen Verteilung darstellt. Diese ist gegeben durch
\begin{align}
\lambda(z) = \Lambda(z) \cdot (1 - \Lambda(z)).
\end{align}

### Schätzung mit R



```{webr-r}
# Logit-Modell schätzen
mod_logit <- glm(
  formula = Y ~ X,
  data = simdata, 
  family = binomial(link = "logit")
)
```

```{webr-r}
# gesch. WSK-Funktion für Probit- und Logit-Modelle
pred <- pred %>%
  mutate(
    Probit = predict(mod_probit, tibble(X), type = "response"),
    Logit = predict(mod_logit, tibble(X), type = "response")
  ) %>%
  pivot_longer(
  cols = LP:Logit, 
  names_to = "Methode", 
  values_to = "Wsk"
)
```

```{webr-r}
# Vergleich mit linearem Modell
simdata %>%
  ggplot(mapping = aes(x = X, y = Y) ) +
  geom_point(
    position = position_jitter(
      height = .01, 
      seed = 1234
    )
  ) +
  geom_line(
    data = pred, 
    mapping = aes(y = Wsk, color = Methode)
  ) +
  scale_y_continuous(breaks = c(0, 1)) +
  theme_cowplot()
```

## Modellierung von Zählvariablen mit Poisson-Regression {#sec-poissonreg}

Die Poisson-Regression ist ein statistisches Modell, das verwendet wird, um Zählvariablen (d.h. Variablen, die diskrete, nicht-negative Werte annehmen) zu modellieren, insbesondere wenn die Zählwerte eine Poisson-Verteilung aufweisen. Dieses Modell wird häufig in Fällen verwendet, in denen die abhängige Variable die Anzahl der Ereignisse in einem bestimmten Zeitraum oder Raum beschreibt, wie z.B. die Anzahl der Verkehrsunfälle in einer Stadt innerhalb eines Monats.

### Poisson-Verteilung

Eine Zufallsvariable $Y$ folgt einer Poisson-Verteilung mit Parameter $\lambda$, wenn ihre Wahrscheinlichkeitsverteilung gegeben ist durch:

\begin{align}
P(Y = y) = \frac{\lambda^y e^{-\lambda}}{y!} \quad \text{für} \quad y = 0, 1, 2, \ldots
\end{align}

Hierbei ist $\lambda$ sowohl der Mittelwert als auch die Varianz der Verteilung ($\mathbb{E}[Y] = \text{Var}(Y) = \lambda$).

```{r}
n <- 500
dat <- tibble(
  Y = rpois(n = n, lambda = 5)
)
```


```{r}
#| fig-cap: "Stichprobenverteilung und Poisson-Dichtefunktion"
#| label: fig-poissonexample
ggplot(
    data = dat, 
    mapping = aes(x = Y)
) +
    geom_histogram(
        mapping = aes(y = after_stat(density)), 
        binwidth = 1, 
        color = "white"
    ) +
    geom_line(
        data = tibble(
            X = 0:13,
            Y = dpois(x = X, lambda = 5)
        ),
        mapping = aes(x = X, y = Y),
        color = "red"
    ) +
    theme_cowplot()
```


### Beispiel: Klassifikation für Palmer-Piniguinen

```{webr-r}
penguins_cleaned <- penguins_cleaned %>% 
  mutate(
    # sex in 0,1-codierte Variable umwandeln
    sex = if_else(sex == "female", 1, 0)
    )

penguins_cleaned %>% 
  slice_head(n = 10)
```

```{webr-r}
penguins_lp <- lm(
  formula = sex ~ bill_depth, 
  data = penguins_cleaned
) 

penguins_lp %>%
  tidy()
```

```{webr-r}
penguins_probit <- glm(
  formula = sex ~ bill_depth, 
  data = penguins_cleaned, 
  family = binomial("probit")
)

penguins_probit %>% 
  tidy()
```

```{webr-r}
penguins_logit <- glm(
  formula = sex ~ bill_depth, 
  data = penguins_cleaned, 
  family = binomial("logit")
)

penguins_logit %>% 
  tidy()
```

```{webr-r}
bill_depth_new <- seq(10, 25, .01)

preds <- tibble(
  bill_depth = bill_depth_new,
  lp = predict(
    object = penguins_lp, 
    newdata = tibble(bill_depth = bill_depth_new)
  ),
  probit = predict(
    object = penguins_probit,
    newdata = tibble(bill_depth = bill_depth_new),
    type = "response"
  ),
  logit = predict(
    object = penguins_logit,
    newdata = tibble(bill_depth = bill_depth_new),
    type = "response"
  )
)
```


```{webr-r}
penguins_cleaned %>%
  ggplot(
    mapping = aes(x = bill_depth, y = sex)
  ) +
  geom_point(
    position = position_jitter(
      height = .025,
      seed = 1234
    ),
    alpha = .25,
    color = "blue"
  ) +
  geom_line(
    data = preds, 
    mapping = aes(y = lp, color = "lineares WSK-Modell")) +
  geom_line(
    data = preds, 
    mapping = aes(y = probit, color = "Probit-Modell")) +
  geom_line(
    data = preds, 
    mapping = aes(y = logit, color = "Logit-Modell")) +
  labs(title = "Modellierung v. Pinguin-Geschlecht mit Regression") +
  theme_cowplot()
```

```{webr-r}
(
  WSK <- penguins_cleaned %>%
  select(sex) %>%
  mutate(
    list(
      linear = penguins_lp, 
      probit = penguins_probit,
      logit = penguins_logit
    ) %>% 
      map_dfc(.f = ~ .$fitted)     
  )
)
```

```{webr-r}
vorh <- WSK %>%
  mutate(
    across(c(linear:logit), 
           .fns = ~ (. > 0.5) == sex,
           .names = "{.col}_pred")
    )

vorh %>% 
  summarise(
    across(linear_pred:probit_pred, mean)
  )
```


### Der Regressionsansatz

In der Poisson-Regression modellieren wir den Erwartungswert der abhängigen Variable $Y$ als eine Funktion der unabhängigen Variablen $\mathbf{X} = (X_1, X_2, \ldots, X_k)$. Der Erwartungswert von $Y$ wird durch den Parameter $\lambda$ repräsentiert, der wiederum eine Funktion der unabhängigen Variablen ist. Die Beziehung wird typischerweise durch eine logarithmische Verknüpfungsfunktion beschrieben:

\begin{align}
\log(\lambda_i) = \mathbf{X}_i^\top \boldsymbol{\beta}
\end{align}

Dies kann auch als

\begin{align}
\lambda_i = \exp(\mathbf{X}_i^\top \boldsymbol{\beta})
\end{align}

geschrieben werden, wobei:

- $\lambda_i$ der Erwartungswert von $Y$ für Beobachtung $i$,

- $\mathbf{X}_i$ der Vektor der unabhängigen Variablen für Beobachtung $i$ und

- $\boldsymbol{\beta}$ der Vektor der Regressionskoeffizienten ist.

### Modellanpassung

Die Parameter $\boldsymbol{\beta}$ werden durch Maximum-Likelihood-Schätzung (MLE) geschätzt. Die Likelihood-Funktion für $n$ Beobachtungen ist gegeben durch

\begin{align}
L(\boldsymbol{\beta}) = \prod_{i=1}^n \frac{\lambda_i^{y_i} e^{-\lambda_i}}{y_i!}.
\end{align}

Die Log-Likelihood-Funktion ist daher

\begin{align}
\mathcal{L}(\boldsymbol{\beta}) = \sum_{i=1}^n \left( y_i \log(\lambda_i) - \lambda_i - \log(y_i!). \right)
\end{align}

Da $\lambda_i = \exp(\mathbf{X}_i^\top \boldsymbol{\beta})$, wird die Log-Likelihood-Funktion zu

\begin{align}
\mathcal{L}(\boldsymbol{\beta}) = \sum_{i=1}^n \left( y_i (\mathbf{X}_i^\top \boldsymbol{\beta}) - \exp(\mathbf{X}_i^\top \boldsymbol{\beta}) - \log(y_i!) \right)
\end{align}

Den Maximum-Likelihood-Schätzer $\widehat{\boldsymbol{\beta}}$ erhalten wir durch Maximierung der Log-Likelihoodfunktion $\mathcal{L}(\boldsymbol{\beta})$. Eine R-Implementierung finden wir in `stats::glm()`.

### Interpretation der Koeffizienten

Die Koeffizienten $\boldsymbol{\beta}$ in der Poisson-Regression haben eine log-lineare Beziehung zur Zählvariable. Für einen bestimmten Koeffizienten $\beta_j$ ist die Interpretation wiefolgt:

- Ein Anstieg der unabhängigen Variable $X_j$ um eine Einheit führt zu einer Änderung des *Logarithmus* des Erwartungswertes von $Y$ um $\beta_j$.

- Der Erwartungswert $\lambda$ ändert sich *multiplikativ* um $\exp(\beta_j)$.

Angenommen, wir haben eine unabhängige Variable $X$ (z.B. die Anzahl der durchgeführten Werbekampagnen) und eine Zählvariable $Y$ (z.B. die Anzahl der Verkäufe). Das Modell könnte wie folgt aussehen:

\begin{align}
\log(\lambda) = \beta_0 + \beta_1 X
\end{align}

Wenn $\beta_1 = 0.5$, bedeutet dies, dass jede zusätzliche Werbekampagne die erwartete Anzahl der Verkäufe um einen Faktor von $\exp(0.5) \approx 1.65$ erhöht. Das heißt, die *Rate* der Verkäufe steigt um 65\% für jede zusätzliche Werbekampagne.

```{r}
# Setze den Zufallszahlengenerator für Reproduzierbarkeit
#set.seed(1234)

# Anzahl der Beobachtungen
n <- 500

# Simuliere die unabhängige Variable X (Anzahl der Werbekampagnen)
X <- sample(1:8, replace = T, size = n)

# Setze die wahren Parameter für das Modell
beta_1 <- 0.4  # Koeffizient für X

# Berechne den Erwartungswert lambda basierend auf dem Modell
lambda <- exp(2 + beta_1 * X)

# Simuliere die abhängige Variable Y (Anzahl der Verkäufe) als Poisson-verteilte Zufallsvariable
Y <- rpois(n, lambda = lambda)

dat <- tibble(X = X, Y = Y)
```

```{r}
ggplot(
  data = dat, 
  mapping = aes(x = Y)
  ) +
  geom_histogram(binwidth = 2) +
  theme_cowplot()
```


```{r}
# Poisson-Regression schätzen
model <- glm(
  formula = Y ~ X, 
  family = poisson(link = "log"), 
  data = dat
)

# Zusammenfassung des gesch. Modells
summary(model)
```

```{r}
#| fig-cap: "Simulierte Daten und angepasstes Poisson-Modell"
#| label: fig-poissonregexample

# Vorhersagen
dat$predicted <- predict(model, type = "response")

# Simulierte Daten und Schätzungen
ggplot(
  data = dat, 
  mapping = aes(x = X, y = Y)
) +
  geom_point(
    mapping = aes(color = "Simulierte Daten"), 
    alpha = 0.5, 
    position = position_jitter(width = .1)
  ) +
  geom_line(
    aes(y = predicted, color = "Geschätztes Modell")
    ) +
  labs(
    x = "Anzahl der Werbekampagnen",
    y = "Anzahl der Eisverkäufe"
  ) +
  scale_color_manual(
    "",
    values = c(
      "Simulierte Daten" = "blue", 
      "Geschätztes Modell" = "red"
    )
  ) +
  theme_cowplot() +
  theme(legend.position = "top")
```

